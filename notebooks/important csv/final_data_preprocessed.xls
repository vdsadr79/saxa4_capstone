2_use_case_name,3_agency,3_abr,4_bureau,8_topic_area,10_commercial_ai,11_purpose_benefits,12_outputs,16_dev_stage,17_impact_type,18_date_initiated,19_date_acq_dev_began,20_date_implemented,22_dev_method,23_contract_piids,24_hisp_support,27_public_info,29_contains_pii,30_saop_review,31_data_catalog,33_agency_data,34_data_docs,37_custom_code,38_code_access,40_has_ato,41_system_name,42_dev_tools_wait,43_infra_provisioned,45_compute_request,47_timely_resources,49_existing_reuse,50_internal_review,51_extension_request,52_impact_assessment,54_key_risks,55_independent_eval,56_monitor_postdeploy,57_autonomous_impact,62_disparity_mitigation,63_stakeholder_consult,65_appeal_process,17_impact_type_clean,row_completeness,text_combined_raw,text_clean
Non-Intrusive Inspection (NII) 3D Imaging Tool,Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,"Utilizes AI/ML to generate high resolution, rapid imaging of objects behind occlusions; create 3D images for existing processes without significant slowdowns; and provide a novel narcotics detection capability for the inspection of packages.",Detection alerts for Items of Interest.,Implementation and Assessment,Neither,7/1/2023,7/1/2023,Unknown,Developed with contracting resources.,20139567,No,No,No,Yes,No,Images and data of baggage inspection.,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,6-12 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Utilizes AI/ML to generate high resolution, rapid imaging of objects behind occlusions; create 3D images for existing processes without significant slowdowns; and provide a novel narcotics detection capability for the inspection of packages. . Detection alerts for Items of Interest.","utilizes ai/ml to generate high resolution, rapid imaging of objects behind occlusions; create 3d images for existing processes without significant slowdowns; and provide a novel narcotics detection capability for the inspection of packages. . detection alerts for items of interest."
Babel,Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,"CBP uses this tool to conduct targeted queries to aid CBP in open source research to monitor potential threats or dangers or identify travelers who may be subject to further inspection for violation of laws CBP is authorized to enforce or administer.
","Babel utilizes AI modules for Text detection and translation as well as object and image recognition to provide analysts with possible matches to manually review in a single interface versus doing multiple manual queries. The output is not singly used for action or decision making and are used to identify additional Open Source or Social Media of a person or identify additional selectors (such as phone and emails) that are previously unknown to CBP and compared by an analyst against Government systems to identify additional derogatory information. These factors can often eliminate the traveler from additional screening.
",Implementation and Assessment,"Rights-Impacting
",8/29/2023,8/29/2023,Unknown,Developed with contracting resources.,70B03C23F00001059,No,No,Yes,Yes,No,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.",Unknown,Yes,No – agency does not have access to source code.,Unknown,Unknown,Less than 6 months,No,Unknown,Unknown,Unknown,Unknown,Yes – Agency requested an extension for this use case.,Planned or in-progress.,"Forthcoming. Office of Management and Budget (OMB) approved a compliance extension through November 30, 2025.  ",Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Rights-Impacting,0.4444444444444444,"CBP uses this tool to conduct targeted queries to aid CBP in open source research to monitor potential threats or dangers or identify travelers who may be subject to further inspection for violation of laws CBP is authorized to enforce or administer. . Babel utilizes AI modules for Text detection and translation as well as object and image recognition to provide analysts with possible matches to manually review in a single interface versus doing multiple manual queries. The output is not singly used for action or decision making and are used to identify additional Open Source or Social Media of a person or identify additional selectors (such as phone and emails) that are previously unknown to CBP and compared by an analyst against Government systems to identify additional derogatory information. These factors can often eliminate the traveler from additional screening. . Forthcoming. Office of Management and Budget (OMB) approved a compliance extension through November 30, 2025.","cbp uses this tool to conduct targeted queries to aid cbp in open source research to monitor potential threats or dangers or identify travelers who may be subject to further inspection for violation of laws cbp is authorized to enforce or administer. . babel utilizes ai modules for text detection and translation as well as object and image recognition to provide analysts with possible matches to manually review in a single interface versus doing multiple manual queries. the output is not singly used for action or decision making and are used to identify additional open source or social media of a person or identify additional selectors (such as phone and emails) that are previously unknown to cbp and compared by an analyst against government systems to identify additional derogatory information. these factors can often eliminate the traveler from additional screening. . forthcoming. office of management and budget (omb) approved a compliance extension through november 30, 2025."
Fivecast ONYX,Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,"The system enhances CBP's capability to monitor, analyze, and assess threats related to border security by processing vast amounts of open-source data, CBP aims to detect potential risks, monitor emerging trends, and uncover connections between individuals, organizations, or networks involved in illegal activities such as human trafficking, smuggling, or terrorism. Thereby streamlining operations and bolster security measures while maintaining a proactive approach to emerging threats
","The system  aggregates vast amounts of publicly and commercially available data from diverse sources, such as social media, news streams, company records, and even the dark web, to provide insights on potential threats and risks, that can be used as actionable intelligence reports, risk assessments, and alerts that support CBP personnel  in real-time decision-making and long-term strategic planning. 
",Implementation and Assessment,"Rights-Impacting
",8/29/2023,8/29/2023,Unknown,Developed with contracting resources.,Unknown,No,No,Yes,Yes,No,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.",Unknown,Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Yes – Agency requested an extension for this use case.,Planned or in-progress.,"Forthcoming. Office of Management and Budget (OMB) approved a compliance extension through November 30, 2025.  ",Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Rights-Impacting,0.4126984126984127,"The system enhances CBP's capability to monitor, analyze, and assess threats related to border security by processing vast amounts of open-source data, CBP aims to detect potential risks, monitor emerging trends, and uncover connections between individuals, organizations, or networks involved in illegal activities such as human trafficking, smuggling, or terrorism. Thereby streamlining operations and bolster security measures while maintaining a proactive approach to emerging threats . The system  aggregates vast amounts of publicly and commercially available data from diverse sources, such as social media, news streams, company records, and even the dark web, to provide insights on potential threats and risks, that can be used as actionable intelligence reports, risk assessments, and alerts that support CBP personnel  in real-time decision-making and long-term strategic planning. . Forthcoming. Office of Management and Budget (OMB) approved a compliance extension through November 30, 2025.","the system enhances cbp's capability to monitor, analyze, and assess threats related to border security by processing vast amounts of open-source data, cbp aims to detect potential risks, monitor emerging trends, and uncover connections between individuals, organizations, or networks involved in illegal activities such as human trafficking, smuggling, or terrorism. thereby streamlining operations and bolster security measures while maintaining a proactive approach to emerging threats . the system aggregates vast amounts of publicly and commercially available data from diverse sources, such as social media, news streams, company records, and even the dark web, to provide insights on potential threats and risks, that can be used as actionable intelligence reports, risk assessments, and alerts that support cbp personnel in real-time decision-making and long-term strategic planning. . forthcoming. office of management and budget (omb) approved a compliance extension through november 30, 2025."
Advanced Analytics for X-ray Images (AAXI),Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,"AAXI aims to address the problem of anomaly detection in empty commercial vehicles entering the United States at land border ports of entry. The AI models achieve this goal by encoding past X-Ray images of vehicular border crossings in a semantically meaningful way and comparing the current crossing to detect differences amongst the images to identify anomalies. Benefits include enhancement of the capability of humans to consistently detect items of interest/concern present (and possibly concealed) in vehicles crossing into the United States, and increased clearance rate at border crossings so that vehicles operating safely and lawfully may pass through the border faster.",AAXI produces bounding boxes around an anomaly or unidentifiable object(s) within an image or any portion of the image that cannot be identified or explained.,Acquisition and/or Development,"Rights-Impacting
",1/1/2022,6/1/2022,Unknown,Developed with contracting resources.,IAA with S&T,No,Unknown,Unknown,Unknown,No,NII Images,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,Unknown,Unknown,Unknown,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Rights-Impacting,0.36507936507936506,"AAXI aims to address the problem of anomaly detection in empty commercial vehicles entering the United States at land border ports of entry. The AI models achieve this goal by encoding past X-Ray images of vehicular border crossings in a semantically meaningful way and comparing the current crossing to detect differences amongst the images to identify anomalies. Benefits include enhancement of the capability of humans to consistently detect items of interest/concern present (and possibly concealed) in vehicles crossing into the United States, and increased clearance rate at border crossings so that vehicles operating safely and lawfully may pass through the border faster. . AAXI produces bounding boxes around an anomaly or unidentifiable object(s) within an image or any portion of the image that cannot be identified or explained.","aaxi aims to address the problem of anomaly detection in empty commercial vehicles entering the united states at land border ports of entry. the ai models achieve this goal by encoding past x-ray images of vehicular border crossings in a semantically meaningful way and comparing the current crossing to detect differences amongst the images to identify anomalies. benefits include enhancement of the capability of humans to consistently detect items of interest/concern present (and possibly concealed) in vehicles crossing into the united states, and increased clearance rate at border crossings so that vehicles operating safely and lawfully may pass through the border faster. . aaxi produces bounding boxes around an anomaly or unidentifiable object(s) within an image or any portion of the image that cannot be identified or explained."
Advanced Trade Analytics Platform (ATAP),Department of Homeland Security,DHS,CBP,Diplomacy & Trade,None of the above.,"To create efficiencies and unlock key insights in CBP's trade mission execution through the application of data analytics, machine learning, and AI.",Model output is provided in dashboards and other visualization mechansims for operator assessment and action determination.,Operation and Maintenance,Neither,7/1/2019,12/1/2019,3/7/2022,Developed with contracting resources.,BPA: 70B06C20A00000036  Call Order 1: 70B06C20F00000895 Call Order 2: 70B06C21F00000842 Call Order 3: 70B06C22F00000700 Call Order 4: 70B06C23F00000819 Call Order 5: 70B06C24F00000450,No,No,Yes,Yes,Yes,"ATAP relies on CBP source system information from CBP's ACE, ATS, and SEACATS systems, including import/export filing information, compliance reviews, targeting, seizure, and fine/penalty information.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,ATAP,6-12 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"To create efficiencies and unlock key insights in CBP's trade mission execution through the application of data analytics, machine learning, and AI. . Model output is provided in dashboards and other visualization mechansims for operator assessment and action determination.","to create efficiencies and unlock key insights in cbp's trade mission execution through the application of data analytics, machine learning, and ai. . model output is provided in dashboards and other visualization mechansims for operator assessment and action determination."
CBP Employee Experience,Department of Homeland Security,DHS,CBP,Mission-Enabling,None of the above.,"CBP Employee Experience is intended to ingest, interpret, and operationalize employee experience data originating from survey results and operational data to deliver real time insights related to the experience of USBP recruits, applicants, and employees. These metrics inform HRM leadership of opportunities for process improvement in order to meet congressionally mandated hiring targets and retain a qualified workforce.","Real time insights related to the experience of USBP recruits, applicants, and employees.",Operation and Maintenance,Neither,12/1/2023,5/1/2022,10/1/2023,Developed with contracting resources.,024-000005282,No,No,No,Yes,Yes,User input is captured in feedback surveys.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,Yes,MASS,6-12 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"CBP Employee Experience is intended to ingest, interpret, and operationalize employee experience data originating from survey results and operational data to deliver real time insights related to the experience of USBP recruits, applicants, and employees. These metrics inform HRM leadership of opportunities for process improvement in order to meet congressionally mandated hiring targets and retain a qualified workforce. . Real time insights related to the experience of USBP recruits, applicants, and employees.","cbp employee experience is intended to ingest, interpret, and operationalize employee experience data originating from survey results and operational data to deliver real time insights related to the experience of usbp recruits, applicants, and employees. these metrics inform hrm leadership of opportunities for process improvement in order to meet congressionally mandated hiring targets and retain a qualified workforce. . real time insights related to the experience of usbp recruits, applicants, and employees."
Passive Body Scanner,Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,"PBS is intended to enhance situational awareness in pedestrian traveler processing to aid  CBP officers in observing potentially dangerous objects or contraband in a timely manner and pursuant to CBP’s border search authority.
","This algorithm highlights areas on a person where potential objects may be blocking the subject's expected body heat and displays these areas on live video image, monitored by a CBP officer. The highlighted areas may show the locations of carried objects, which could be potential weapons or contraband.
",Operation and Maintenance,"Rights-Impacting
",8/18/2022,9/29/2022,9/29/2023,Developed with contracting resources.,70B03C22A00000031,No,No,No,Yes,No,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Yes,Passive Body Scanner,6-12 months,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Planned or in-progress.,"Forthcoming. Office of Management and Budget (OMB) approved a compliance extension through November 30, 2025.  ",Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Rights-Impacting,0.5873015873015873,"PBS is intended to enhance situational awareness in pedestrian traveler processing to aid  CBP officers in observing potentially dangerous objects or contraband in a timely manner and pursuant to CBP’s border search authority. . This algorithm highlights areas on a person where potential objects may be blocking the subject's expected body heat and displays these areas on live video image, monitored by a CBP officer. The highlighted areas may show the locations of carried objects, which could be potential weapons or contraband. . Forthcoming. Office of Management and Budget (OMB) approved a compliance extension through November 30, 2025.","pbs is intended to enhance situational awareness in pedestrian traveler processing to aid cbp officers in observing potentially dangerous objects or contraband in a timely manner and pursuant to cbp’s border search authority. . this algorithm highlights areas on a person where potential objects may be blocking the subject's expected body heat and displays these areas on live video image, monitored by a cbp officer. the highlighted areas may show the locations of carried objects, which could be potential weapons or contraband. . forthcoming. office of management and budget (omb) approved a compliance extension through november 30, 2025."
Unmanned Aircraft Collision Avoidance (Skydio),Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,The platform operates on video feed only which in turn activates the obstacle avoidance on the aircraft where the AI capabilities are housed.  The system supports the streamlined intake process while maintaining the accuracy and reliability of identity verification.,"The pilot of the Skydio X2D will receive a visual alert on the hand controller, indicating a possible collision.",Operation and Maintenance,Neither,6/1/2022,6/1/2022,10/1/2022,Developed in-house.,Unknown,No,No,No,No,No,Still capture pictures and video files.,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,Yes,Skydio X2D,Less than 6 months,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The platform operates on video feed only which in turn activates the obstacle avoidance on the aircraft where the AI capabilities are housed.  The system supports the streamlined intake process while maintaining the accuracy and reliability of identity verification. . The pilot of the Skydio X2D will receive a visual alert on the hand controller, indicating a possible collision.","the platform operates on video feed only which in turn activates the obstacle avoidance on the aircraft where the ai capabilities are housed. the system supports the streamlined intake process while maintaining the accuracy and reliability of identity verification. . the pilot of the skydio x2d will receive a visual alert on the hand controller, indicating a possible collision."
CBP Translate,Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,"CBP Translate is used to facilitate clear communication between officers and non-English-speaking individuals during border interactions. The AI enhances efficiency by reducing language barriers, supports diverse languages for greater inclusivity, and ensures accurate communication to minimize misunderstandings during critical procedures.
","The system provides real-time translations of spoken or written communication, detects individuals' languages for accurate translation, and generates logs of interactions for review, streamlining processes and ensuring clear communication. This output is used to support necessary interpretation but sworn statements and official communications for law-enforcement purposes are moved to stardard translation services when necessary for formal documenation and questioning. 
",Operation and Maintenance,"Rights-Impacting
",Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Yes,Yes,Yes,Unknown,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,CBP Translate,Less than 6 months,Unknown,Other,Unknown,Unknown,Unknown,Yes – Agency requested an extension for this use case.,Planned or in-progress.,"Forthcoming. Office of Management and Budget (OMB) approved a compliance extension through November 30, 2025.  ",Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Rights-Impacting,0.42857142857142855,"CBP Translate is used to facilitate clear communication between officers and non-English-speaking individuals during border interactions. The AI enhances efficiency by reducing language barriers, supports diverse languages for greater inclusivity, and ensures accurate communication to minimize misunderstandings during critical procedures. . The system provides real-time translations of spoken or written communication, detects individuals' languages for accurate translation, and generates logs of interactions for review, streamlining processes and ensuring clear communication. This output is used to support necessary interpretation but sworn statements and official communications for law-enforcement purposes are moved to stardard translation services when necessary for formal documenation and questioning. . Forthcoming. Office of Management and Budget (OMB) approved a compliance extension through November 30, 2025.","cbp translate is used to facilitate clear communication between officers and non-english-speaking individuals during border interactions. the ai enhances efficiency by reducing language barriers, supports diverse languages for greater inclusivity, and ensures accurate communication to minimize misunderstandings during critical procedures. . the system provides real-time translations of spoken or written communication, detects individuals' languages for accurate translation, and generates logs of interactions for review, streamlining processes and ensuring clear communication. this output is used to support necessary interpretation but sworn statements and official communications for law-enforcement purposes are moved to stardard translation services when necessary for formal documenation and questioning. . forthcoming. office of management and budget (omb) approved a compliance extension through november 30, 2025."
Entity Resolution,Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,Detection research of force labor within supply chain utilizing analytical AI platform.,Detection of potential force labor within supply chain.,Operation and Maintenance,Neither,10/1/2021,10/1/2021,5/1/2023,Developed with contracting resources.,2014-14031000011,No,No,No,Yes,No,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,6-12 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,Detection research of force labor within supply chain utilizing analytical AI platform. . Detection of potential force labor within supply chain.,detection research of force labor within supply chain utilizing analytical ai platform. . detection of potential force labor within supply chain.
Supervised Traveler Identity Verification Services (Officer Initiated),Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,"The TVS Biometric matching service is a cloud-based facial biometric matching service that enables CBP to match a passenger’s identity against a trusted source, throughout the travel continuum which improves traveler facilitation and reduces manual identity verification",Leverages DHS facial matching technologies to provide a match or no match response,Operation and Maintenance,"Rights-Impacting
",4/1/2013,4/1/2016,9/1/2017,Developed with contracting resources.,024-000005265,Yes,No,Yes,Yes,Yes,Border Crossing Information,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Traveler Verification Service,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Yes,This AI system has been tested in operational or real-world environments and risks have been identified and mitigated. Information about risks and mitigations for this use case may be law enforcement sensitive; DHS continues to review details for potential future disclosure in accordance with applicable law and policy.,Yes – by the CAIO,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,The threshold for the biometric matching was tested extensively with a variety of face types for several months to establish a match threshold for the identification.,"Direct user testing,General solicitations of comments from the public",Yes,Rights-Impacting,0.7777777777777778,"The TVS Biometric matching service is a cloud-based facial biometric matching service that enables CBP to match a passenger’s identity against a trusted source, throughout the travel continuum which improves traveler facilitation and reduces manual identity verification . Leverages DHS facial matching technologies to provide a match or no match response . This AI system has been tested in operational or real-world environments and risks have been identified and mitigated. Information about risks and mitigations for this use case may be law enforcement sensitive; DHS continues to review details for potential future disclosure in accordance with applicable law and policy.","the tvs biometric matching service is a cloud-based facial biometric matching service that enables cbp to match a passenger’s identity against a trusted source, throughout the travel continuum which improves traveler facilitation and reduces manual identity verification . leverages dhs facial matching technologies to provide a match or no match response . this ai system has been tested in operational or real-world environments and risks have been identified and mitigated. information about risks and mitigations for this use case may be law enforcement sensitive; dhs continues to review details for potential future disclosure in accordance with applicable law and policy."
Semi-Supervised Traveler Identity Verification Services (Traveler Initiated),Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,"The TVS Biometric matching service is a cloud-based facial biometric matching service that enables CBP to match a passenger’s identity against a trusted source, throughout the travel continuum which improves traveler facilitation and reduces manual identity verification.",Leverages DHS facial matching technologies to provide a match or no match response,Operation and Maintenance,"Rights-Impacting
",6/1/2017,7/1/2023,2/1/2024,Developed with contracting resources.,024-000005265,No,No,Yes,Yes,Yes,Trusted Traveler Information,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Traveler Verification Service,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Yes,This AI system has been tested in operational or real-world environments and risks have been identified and mitigated. Information about risks and mitigations for this use case may be law enforcement sensitive; DHS continues to review details for potential future disclosure in accordance with applicable law and policy.,Yes – by the CAIO,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,The threshold for the biometric matching was tested extensively with a variety of face types for several months to establish a match threshold for the identification.,"Direct user testing,General solicitations of comments from the public",Yes,Rights-Impacting,0.746031746031746,"The TVS Biometric matching service is a cloud-based facial biometric matching service that enables CBP to match a passenger’s identity against a trusted source, throughout the travel continuum which improves traveler facilitation and reduces manual identity verification. . Leverages DHS facial matching technologies to provide a match or no match response . This AI system has been tested in operational or real-world environments and risks have been identified and mitigated. Information about risks and mitigations for this use case may be law enforcement sensitive; DHS continues to review details for potential future disclosure in accordance with applicable law and policy.","the tvs biometric matching service is a cloud-based facial biometric matching service that enables cbp to match a passenger’s identity against a trusted source, throughout the travel continuum which improves traveler facilitation and reduces manual identity verification. . leverages dhs facial matching technologies to provide a match or no match response . this ai system has been tested in operational or real-world environments and risks have been identified and mitigated. information about risks and mitigations for this use case may be law enforcement sensitive; dhs continues to review details for potential future disclosure in accordance with applicable law and policy."
3rd Party Traveler Identity Verification Services,Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,"The TVS Biometric Air Exit solution is a cloud-based facial biometric matching service that enables CBP, External Partners, and Other Government Agencies (OGA) to match a passenger’s identity against a trusted source, throughout the travel continuum which improves traveler facilitation and reduces manual identity verification.
","Leverages DHS facial matching technologies to provide a match or no match response.
",Operation and Maintenance,"Rights-Impacting
",4/1/2013,12/1/2015,5/1/2017,Developed with contracting resources.,024-000005265,No,No,Yes,Yes,Yes,Border Crossing Information.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Traveler Verification Service,Less than 6 months,No,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Yes,This AI system has been tested in operational or real-world environments and risks have been identified and mitigated. Information about risks and mitigations for this use case may be law enforcement sensitive; DHS continues to review details for potential future disclosure in accordance with applicable law and policy.,Yes – by the CAIO,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,The threshold for the biometric matching was tested extensively with a variety of face types for several months to establish a match threshold for the identification.,"Direct user testing,General solicitations of comments from the public",Yes,Rights-Impacting,0.746031746031746,"The TVS Biometric Air Exit solution is a cloud-based facial biometric matching service that enables CBP, External Partners, and Other Government Agencies (OGA) to match a passenger’s identity against a trusted source, throughout the travel continuum which improves traveler facilitation and reduces manual identity verification. . Leverages DHS facial matching technologies to provide a match or no match response. . This AI system has been tested in operational or real-world environments and risks have been identified and mitigated. Information about risks and mitigations for this use case may be law enforcement sensitive; DHS continues to review details for potential future disclosure in accordance with applicable law and policy.","the tvs biometric air exit solution is a cloud-based facial biometric matching service that enables cbp, external partners, and other government agencies (oga) to match a passenger’s identity against a trusted source, throughout the travel continuum which improves traveler facilitation and reduces manual identity verification. . leverages dhs facial matching technologies to provide a match or no match response. . this ai system has been tested in operational or real-world environments and risks have been identified and mitigated. information about risks and mitigations for this use case may be law enforcement sensitive; dhs continues to review details for potential future disclosure in accordance with applicable law and policy."
Mobile Traveler Identity Verification,Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,"The TVS Biometric matching service is a cloud-based facial biometric matching service that enables CBP to match a passenger’s identity against a trusted source, throughout the travel continuum which improves traveler facilitation and reduces manual identity verification",Provides a match or no match response,Operation and Maintenance,Neither,6/1/2017,9/1/2017,1/1/2022,Developed with contracting resources.,024-000005265,Yes,No,Yes,Yes,Yes,Border Crossing Information/ Trusted Traveler Information,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Traveler Verification Service,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"The TVS Biometric matching service is a cloud-based facial biometric matching service that enables CBP to match a passenger’s identity against a trusted source, throughout the travel continuum which improves traveler facilitation and reduces manual identity verification . Provides a match or no match response","the tvs biometric matching service is a cloud-based facial biometric matching service that enables cbp to match a passenger’s identity against a trusted source, throughout the travel continuum which improves traveler facilitation and reduces manual identity verification . provides a match or no match response"
Traveler Identity Verification Services (Vetting),Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,CBP's Traveler Identity Verification Services (Vetting) utilizes facial recognition technology to enhance threat identification by matching travelers' biometrics against records of concern. ,"When the system identifies a potential match to concerning records, CBP personnel conducts a manual facial comparison to determine whether the record is likely associated with the individual.",Operation and Maintenance,"Rights-Impacting
",12/1/2017,1/1/2018,12/1/2018,Developed with contracting resources.,024-000005265,No,No,Yes,Yes,Yes,Border Crossing Information,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Traveler Verification Service,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Yes,This AI system has been tested in operational or real-world environments and risks have been identified and mitigated. Information about risks and mitigations for this use case may be law enforcement sensitive; DHS continues to review details for potential future disclosure in accordance with applicable law and policy.,Yes – by the CAIO,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,The threshold for the biometric matching was tested extensively with a variety of face types for several months to establish a match threshold for the identification.,None of the above,Yes,Rights-Impacting,0.746031746031746,"CBP's Traveler Identity Verification Services (Vetting) utilizes facial recognition technology to enhance threat identification by matching travelers' biometrics against records of concern. . When the system identifies a potential match to concerning records, CBP personnel conducts a manual facial comparison to determine whether the record is likely associated with the individual. . This AI system has been tested in operational or real-world environments and risks have been identified and mitigated. Information about risks and mitigations for this use case may be law enforcement sensitive; DHS continues to review details for potential future disclosure in accordance with applicable law and policy.","cbp's traveler identity verification services (vetting) utilizes facial recognition technology to enhance threat identification by matching travelers' biometrics against records of concern. . when the system identifies a potential match to concerning records, cbp personnel conducts a manual facial comparison to determine whether the record is likely associated with the individual. . this ai system has been tested in operational or real-world environments and risks have been identified and mitigated. information about risks and mitigations for this use case may be law enforcement sensitive; dhs continues to review details for potential future disclosure in accordance with applicable law and policy."
Customs Broker License Exam - Proctor Support,Department of Homeland Security,DHS,CBP,Education & Workforce,None of the above.,"The model supports remote proctoring of the exam and ensure the integrity of the testing process, by ensuring the exam is conducted under secure conditions, preventing cheating or fraud, while also verifying the identity of exam takers to confirm they meet the necessary requirements.
","Integrity reports, identity confirmation, proctoring complicance feedback, and test results. 

",Operation and Maintenance,Neither,10/1/2022,10/2/2022,3/31/2023,Developed with contracting resources.,N/A - Aligned with the Proctoring contract.,No,No,Yes,Yes,No,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The model supports remote proctoring of the exam and ensure the integrity of the testing process, by ensuring the exam is conducted under secure conditions, preventing cheating or fraud, while also verifying the identity of exam takers to confirm they meet the necessary requirements. . Integrity reports, identity confirmation, proctoring complicance feedback, and test results.","the model supports remote proctoring of the exam and ensure the integrity of the testing process, by ensuring the exam is conducted under secure conditions, preventing cheating or fraud, while also verifying the identity of exam takers to confirm they meet the necessary requirements. . integrity reports, identity confirmation, proctoring complicance feedback, and test results."
ERNIE,Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,"The model enhances threat detection and prioritizes high-risk targets, improving operational efficiency and national security.",The model provides real-time risk assessments and alerts for potential threats detected by the Radiation Portal Monitors. It also provides prioritized recommendations for further screening based on the analysis of radiation data.,Operation and Maintenance,Both,10/1/2017,10/1/2017,Unknown,Developed with contracting resources.,Developed under CWMD contract.,No,No,No,Yes,Yes,Numerical data from RPM radiation detectors and ERNIE assessments.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Automated Radiological Data Integration System - Cloud (ARDIS-C),More than 12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Yes,This AI system has been tested in operational or real-world environments and risks have been identified and mitigated. Information about risks and mitigations for this use case may be law enforcement sensitive; DHS continues to review details for potential future disclosure in accordance with applicable law and policy.,Yes – by the CAIO,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5714285714285714,"The model enhances threat detection and prioritizes high-risk targets, improving operational efficiency and national security. . The model provides real-time risk assessments and alerts for potential threats detected by the Radiation Portal Monitors. It also provides prioritized recommendations for further screening based on the analysis of radiation data. . This AI system has been tested in operational or real-world environments and risks have been identified and mitigated. Information about risks and mitigations for this use case may be law enforcement sensitive; DHS continues to review details for potential future disclosure in accordance with applicable law and policy.","the model enhances threat detection and prioritizes high-risk targets, improving operational efficiency and national security. . the model provides real-time risk assessments and alerts for potential threats detected by the radiation portal monitors. it also provides prioritized recommendations for further screening based on the analysis of radiation data. . this ai system has been tested in operational or real-world environments and risks have been identified and mitigated. information about risks and mitigations for this use case may be law enforcement sensitive; dhs continues to review details for potential future disclosure in accordance with applicable law and policy."
Autonomous Surveillance Tower (AST),Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,"AST machine learning assisted system is augmenting the U.S. Border Patrol by enhancing the capabiltiies of individual users when carrying out the domain awareness mission. 
The expected benefit is to have ability of single person to monitor magnitude greater area than could be done with conventional CCTV or human surveillance. The ultimate outcome for the agency and the public is greater availability of the agents to solve and address more complex tasks and allow for better strategic/tactical deployment of existing resources and personnel.
","The AI provides alerts when it detects the presence of an IoI (i.e., persons, vehicles, animals) in the image frame.  With regard to persons, this computer vision application is trained to determine if the object in the image frame is a person with a certain of level of confidence and not another object that may be shaped similarly to a person.  After the alert of a detection, a trained agent or user, reviews the image to identify and classify the activity taking place.  The AI merely alerts to the presence of an item it was trained to detect. This is not a biometric system and does not identify or track specific individuals. ",Operation and Maintenance,Neither,10/1/2018,10/1/2019,1/1/2020,Developed with both contracting and in-house resources.,70B02C20D00000019,No,No,No,No,Other,Meta-data and data created by CBP. Generally comprised of agent adjudications of autonomous sensory inputs of items of interests by the system.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,No – agency does not have access to source code.,Yes,Autonomous Surveillance Towers,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"AST machine learning assisted system is augmenting the U.S. Border Patrol by enhancing the capabiltiies of individual users when carrying out the domain awareness mission. 
The expected benefit is to have ability of single person to monitor magnitude greater area than could be done with conventional CCTV or human surveillance. The ultimate outcome for the agency and the public is greater availability of the agents to solve and address more complex tasks and allow for better strategic/tactical deployment of existing resources and personnel. . The AI provides alerts when it detects the presence of an IoI (i.e., persons, vehicles, animals) in the image frame.  With regard to persons, this computer vision application is trained to determine if the object in the image frame is a person with a certain of level of confidence and not another object that may be shaped similarly to a person.  After the alert of a detection, a trained agent or user, reviews the image to identify and classify the activity taking place.  The AI merely alerts to the presence of an item it was trained to detect. This is not a biometric system and does not identify or track specific individuals.","ast machine learning assisted system is augmenting the u.s. border patrol by enhancing the capabiltiies of individual users when carrying out the domain awareness mission. the expected benefit is to have ability of single person to monitor magnitude greater area than could be done with conventional cctv or human surveillance. the ultimate outcome for the agency and the public is greater availability of the agents to solve and address more complex tasks and allow for better strategic/tactical deployment of existing resources and personnel. . the ai provides alerts when it detects the presence of an ioi (i.e., persons, vehicles, animals) in the image frame. with regard to persons, this computer vision application is trained to determine if the object in the image frame is a person with a certain of level of confidence and not another object that may be shaped similarly to a person. after the alert of a detection, a trained agent or user, reviews the image to identify and classify the activity taking place. the ai merely alerts to the presence of an item it was trained to detect. this is not a biometric system and does not identify or track specific individuals."
Automated Item of Interest Detection,Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,"The software analyzes images and video that are taken by operationally deployed equipment, which are then fed into CBP systems for review by USBP agents and personnel. It provides quick identification of people either crossing into the U.S. at a time and place other than designated for entry or those already inside the U.S. trying to elude capture, as well as the ability for human operators to quickly determine if subjects in an image are, in fact, human.",The system creates a layer which is overlaid over the image to produce a box around items of interest it has determined to be likely human beings.,Operation and Maintenance,Neither,Unknown,Unknown,Unknown,Developed with contracting resources.,70B06C24F00000475,No,No,No,Yes,No,All of the image data fed to the models are owned by USBP.,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Yes,I4 Viewer,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"The software analyzes images and video that are taken by operationally deployed equipment, which are then fed into CBP systems for review by USBP agents and personnel. It provides quick identification of people either crossing into the U.S. at a time and place other than designated for entry or those already inside the U.S. trying to elude capture, as well as the ability for human operators to quickly determine if subjects in an image are, in fact, human. . The system creates a layer which is overlaid over the image to produce a box around items of interest it has determined to be likely human beings.","the software analyzes images and video that are taken by operationally deployed equipment, which are then fed into cbp systems for review by usbp agents and personnel. it provides quick identification of people either crossing into the u.s. at a time and place other than designated for entry or those already inside the u.s. trying to elude capture, as well as the ability for human operators to quickly determine if subjects in an image are, in fact, human. . the system creates a layer which is overlaid over the image to produce a box around items of interest it has determined to be likely human beings."
CBP One,Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,"TVS uses facial recognition to compare live or uploaded images with CBP's database, enabling real-time identity verification. This automation streamlines border processes, enhances accuracy, and reduces fraud.","The system outputs include identity match confirmation, fraud alerts, and traveler status updates for clearance in processes like boarding or border crossing.
",Operation and Maintenance,Both,4/6/2023,4/6/2023,5/11/2023,Developed with contracting resources.,024-000005265,No,No,Yes,Yes,Yes,CBP One submission information.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Traveler Verification Service,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Yes,This AI system has been tested in operational or real-world environments and risks have been identified and mitigated. Information about risks and mitigations for this use case may be law enforcement sensitive; DHS continues to review details for potential future disclosure in accordance with applicable law and policy.,Yes – by the CAIO,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,The threshold for the biometric matching was tested extensively with a variety of face types for several months to establish a match threshold for the identification.,Direct user testing,Yes,Both,0.7301587301587301,"TVS uses facial recognition to compare live or uploaded images with CBP's database, enabling real-time identity verification. This automation streamlines border processes, enhances accuracy, and reduces fraud. . The system outputs include identity match confirmation, fraud alerts, and traveler status updates for clearance in processes like boarding or border crossing. . This AI system has been tested in operational or real-world environments and risks have been identified and mitigated. Information about risks and mitigations for this use case may be law enforcement sensitive; DHS continues to review details for potential future disclosure in accordance with applicable law and policy.","tvs uses facial recognition to compare live or uploaded images with cbp's database, enabling real-time identity verification. this automation streamlines border processes, enhances accuracy, and reduces fraud. . the system outputs include identity match confirmation, fraud alerts, and traveler status updates for clearance in processes like boarding or border crossing. . this ai system has been tested in operational or real-world environments and risks have been identified and mitigated. information about risks and mitigations for this use case may be law enforcement sensitive; dhs continues to review details for potential future disclosure in accordance with applicable law and policy."
Unified Processing/Mobile Intake,Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,The purpose is to facilitate the biometric identification of individuals as they are encountered by CBP for the purpose of expedited processing.,Leverages CBP facial matching technologies to provide a match or no match response,Operation and Maintenance,"Rights-Impacting
",12/1/2017,1/1/2018,3/1/2022,Developed with contracting resources.,024-000005265,No,No,Yes,Yes,Yes,Border Crossing Information,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Traveler Verification Service,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Yes,This AI system has been tested in operational or real-world environments and risks have been identified and mitigated. Information about risks and mitigations for this use case may be law enforcement sensitive; DHS continues to review details for potential future disclosure in accordance with applicable law and policy.,Yes – by the CAIO,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,The threshold for the biometric matching was tested extensively with a variety of face types for several months to establish a match threshold for the identification.,Direct user testing,Yes,Rights-Impacting,0.746031746031746,The purpose is to facilitate the biometric identification of individuals as they are encountered by CBP for the purpose of expedited processing. . Leverages CBP facial matching technologies to provide a match or no match response . This AI system has been tested in operational or real-world environments and risks have been identified and mitigated. Information about risks and mitigations for this use case may be law enforcement sensitive; DHS continues to review details for potential future disclosure in accordance with applicable law and policy.,the purpose is to facilitate the biometric identification of individuals as they are encountered by cbp for the purpose of expedited processing. . leverages cbp facial matching technologies to provide a match or no match response . this ai system has been tested in operational or real-world environments and risks have been identified and mitigated. information about risks and mitigations for this use case may be law enforcement sensitive; dhs continues to review details for potential future disclosure in accordance with applicable law and policy.
Cyber Threat Analysis (Recorded Future),Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,"Cyber Threat Analysis quickly populates query results when searching against adversary tactics, techniques, and procedures, establishing a threat scorecard. This service can also provide cyber risk scorecards for third party vendors, companies, and organizations.",Actionable intelligence supporting Security Operations Center (SOC) and Cyber Risk Management (CRM) investigations and reports.,Operation and Maintenance,Neither,4/1/2020,4/1/2020,Unknown,Unknown,Unknown,No,No,No,Unknown,No,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.",Unknown,No,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.36507936507936506,"Cyber Threat Analysis quickly populates query results when searching against adversary tactics, techniques, and procedures, establishing a threat scorecard. This service can also provide cyber risk scorecards for third party vendors, companies, and organizations. . Actionable intelligence supporting Security Operations Center (SOC) and Cyber Risk Management (CRM) investigations and reports.","cyber threat analysis quickly populates query results when searching against adversary tactics, techniques, and procedures, establishing a threat scorecard. this service can also provide cyber risk scorecards for third party vendors, companies, and organizations. . actionable intelligence supporting security operations center (soc) and cyber risk management (crm) investigations and reports."
Vault Access Log (SPVAA),Department of Homeland Security,DHS,CBP,Mission-Enabling,None of the above.,"The system will enhance monitoring and minimizes the risk of unauthorized access, contributing to stronger security protocols for handling seized property.",Leverages DHS facial matching technologies to provide a match or no match response.,Operation and Maintenance,Neither,12/1/2020,1/1/2021,8/1/2022,Developed with contracting resources.,024-000005265,No,No,Yes,Yes,No,Personal Identity Verification (PIV) card,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Traveler Verification Service,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The system will enhance monitoring and minimizes the risk of unauthorized access, contributing to stronger security protocols for handling seized property. . Leverages DHS facial matching technologies to provide a match or no match response.","the system will enhance monitoring and minimizes the risk of unauthorized access, contributing to stronger security protocols for handling seized property. . leverages dhs facial matching technologies to provide a match or no match response."
HTS Classifier,Department of Homeland Security,DHS,CBP,Law & Justice,None of the above.,"CBP’s HTS Classifier improves trade compliance and enhances cargo risk assessment by streamlining the classification of goods, enabling better integration with machine learning systems, and refining entity risk evaluations. It identifies potential threats linked to specific cargo types and prior violations by categorizing goods based on their descriptions and attributes. These improvements contribute to faster, more accurate classification and risk-based targeting, which strengthens security and facilitates trade.","The HTS Classifier produce outputs that map cargo commodity descriptions to their most probable tariff codes, enhancing classification accuracy. These outputs integrate seamlessly into broader threat-specific risk models, providing features to support predictive risk assessments in cargo security.",Operation and Maintenance,Neither,10/1/2020,10/1/2020,10/1/2020,Developed with contracting resources.,Unknown,No,No,Yes,Yes,Yes,"This model leverages data provided by carriers within the Automated Commercial Environment (ACE), as well as transformations of that data within the Automated Targeting System (ATS).","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Automated Targeting System (ATS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"CBP’s HTS Classifier improves trade compliance and enhances cargo risk assessment by streamlining the classification of goods, enabling better integration with machine learning systems, and refining entity risk evaluations. It identifies potential threats linked to specific cargo types and prior violations by categorizing goods based on their descriptions and attributes. These improvements contribute to faster, more accurate classification and risk-based targeting, which strengthens security and facilitates trade. . The HTS Classifier produce outputs that map cargo commodity descriptions to their most probable tariff codes, enhancing classification accuracy. These outputs integrate seamlessly into broader threat-specific risk models, providing features to support predictive risk assessments in cargo security.","cbp’s hts classifier improves trade compliance and enhances cargo risk assessment by streamlining the classification of goods, enabling better integration with machine learning systems, and refining entity risk evaluations. it identifies potential threats linked to specific cargo types and prior violations by categorizing goods based on their descriptions and attributes. these improvements contribute to faster, more accurate classification and risk-based targeting, which strengthens security and facilitates trade. . the hts classifier produce outputs that map cargo commodity descriptions to their most probable tariff codes, enhancing classification accuracy. these outputs integrate seamlessly into broader threat-specific risk models, providing features to support predictive risk assessments in cargo security."
CISAChat,Department of Homeland Security,DHS,CISA,Mission-Enabling,None of the above.,"Currently, multiple CISA program offices are using contractor staff to review pre-production content and other internal materials to develop summaries, key themes, and improve clarity. Leveraging a Generative AI solution improves internal agency Customer Experience (CX) and saves staff time. ",LLM generated response to the questions posed on uploaded content.  ,Implementation and Assessment,Neither,12/13/2023,1/1/2024,Unknown,Developed with both contracting and in-house resources.,CEEOSS-Task Order No.- 70RCSJ23FR0000001 DSS-70RCSJ24FC0000018 - IT O&M,No,No,No,Yes,Other,Current data used is pre-publication content that has already been approved.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Currently, multiple CISA program offices are using contractor staff to review pre-production content and other internal materials to develop summaries, key themes, and improve clarity. Leveraging a Generative AI solution improves internal agency Customer Experience (CX) and saves staff time. . LLM generated response to the questions posed on uploaded content.","currently, multiple cisa program offices are using contractor staff to review pre-production content and other internal materials to develop summaries, key themes, and improve clarity. leveraging a generative ai solution improves internal agency customer experience (cx) and saves staff time. . llm generated response to the questions posed on uploaded content."
Security Operation Center (SOC) Network Anomaly Detection,Department of Homeland Security,DHS,CISA,Mission-Enabling,None of the above.,"This use case delivers improved internal government tools for hunting and detection of malicious threat actors on federal civilian networks. It automates manual data fusion and correlation processes and highlights potential anomalies, allowing CISA analysts to focus more on hunting adversaries.
","An interface is provided for analysts to query cybersecurity data, and dashboards are provided with potential cybersecurity alerts, including anomalies detected through predictive models and rule-based heuristics.
",Operation and Maintenance,Neither,9/4/2020,9/4/2020,8/22/2023,Developed in-house.,Unknown,No,No,No,Yes,Yes,"Cybersecurity cloud, network and host logs, Cybersecurity threat intelligence (CTI)
","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Cyber Analytics and Data System Analytics Environment,More than 12 months,No,Yes,No,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"This use case delivers improved internal government tools for hunting and detection of malicious threat actors on federal civilian networks. It automates manual data fusion and correlation processes and highlights potential anomalies, allowing CISA analysts to focus more on hunting adversaries. . An interface is provided for analysts to query cybersecurity data, and dashboards are provided with potential cybersecurity alerts, including anomalies detected through predictive models and rule-based heuristics.","this use case delivers improved internal government tools for hunting and detection of malicious threat actors on federal civilian networks. it automates manual data fusion and correlation processes and highlights potential anomalies, allowing cisa analysts to focus more on hunting adversaries. . an interface is provided for analysts to query cybersecurity data, and dashboards are provided with potential cybersecurity alerts, including anomalies detected through predictive models and rule-based heuristics."
Automated Detection of Personally Identifiable Information (PII) in Cybersecurity Data,Department of Homeland Security,DHS,CISA,Mission-Enabling,None of the above.,"Automated PII Detection and Review Process uses analytics to identify and manage potential PII in submissions. If PII is flagged, the submission is sent to CISA analysts, who are guided by AI to review and confirm or reject the detection, redacting information if necessary. Privacy experts monitor the system and provide feedback. The system learns from this feedback, ensuring compliance with privacy regulations and improving efficiency by reducing false positives. Regular audits ensure the process remains trustworthy and effective.
","The system sends flagged data rows with potential PII to humans for review.
",Operation and Maintenance,Neither,2/1/2020,9/1/2020,12/1/2020,Developed with contracting resources.,70QS0124C00000002,No,No,Yes,Yes,Yes,"Cybersecurity indicators of compromise (IOCs), Cybersecurity threat intelligence (CTI)
","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Trusted Automated eXchange of Indicator Information server,Less than 6 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Automated PII Detection and Review Process uses analytics to identify and manage potential PII in submissions. If PII is flagged, the submission is sent to CISA analysts, who are guided by AI to review and confirm or reject the detection, redacting information if necessary. Privacy experts monitor the system and provide feedback. The system learns from this feedback, ensuring compliance with privacy regulations and improving efficiency by reducing false positives. Regular audits ensure the process remains trustworthy and effective. . The system sends flagged data rows with potential PII to humans for review.","automated pii detection and review process uses analytics to identify and manage potential pii in submissions. if pii is flagged, the submission is sent to cisa analysts, who are guided by ai to review and confirm or reject the detection, redacting information if necessary. privacy experts monitor the system and provide feedback. the system learns from this feedback, ensuring compliance with privacy regulations and improving efficiency by reducing false positives. regular audits ensure the process remains trustworthy and effective. . the system sends flagged data rows with potential pii to humans for review."
Confidence Scoring for Cybersecurity Threat Indicators,Department of Homeland Security,DHS,CISA,Mission-Enabling,None of the above.,"The confidence scores produced by this use case via an AI-driven decision tree process allow CISA's Automated Indicator Sharing (AIS) partners to contextualize the indicator information they are receiving so that they can ingest the data accordingly on their systems.
","A set of confidence scores is included along with the other fields in the indicator data set.
",Operation and Maintenance,Neither,6/1/2020,7/1/2020,7/1/2021,Developed with contracting resources.,70QS0124C00000002,No,Yes,Yes,Yes,Yes,"Cybersecurity indicators of compromise (IOCs), Cybersecurity threat intelligence (CTI)
","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Trusted Automated eXchange of Indicator Information Server,Less than 6 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,The confidence scores produced by this use case via an AI-driven decision tree process allow CISA's Automated Indicator Sharing (AIS) partners to contextualize the indicator information they are receiving so that they can ingest the data accordingly on their systems. . A set of confidence scores is included along with the other fields in the indicator data set.,the confidence scores produced by this use case via an ai-driven decision tree process allow cisa's automated indicator sharing (ais) partners to contextualize the indicator information they are receiving so that they can ingest the data accordingly on their systems. . a set of confidence scores is included along with the other fields in the indicator data set.
Report Analysis and Archive System (RAAS),Department of Homeland Security,DHS,CWMD,Mission-Enabling,None of the above.,"To assist users of the Test and Evaluation document archive in locating documents and portions of document that are relevant for their needs by specifying individual queries as natural language questions, responding with detailed, natural language answers, and facilitating attainment of related or more precise information through follow-up questions.",Intermediate outputs include context summaries to facilitate indexing of document passages. Output to the user includes summarization of content relevant to the questions and interests indicated by the conversation with the user.,Acquisition and/or Development,Neither,8/1/2023,10/1/2023,Unknown,Developed with contracting resources.,70RWMD20K00000016,No,Unknown,Unknown,Unknown,No,The data set available to the model is the set of CWMD Test and Evaluation Division test reports stored in the RAAS archive.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,Yes,Report Analysis and Archive System (RAAS),Less than 6 months,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"To assist users of the Test and Evaluation document archive in locating documents and portions of document that are relevant for their needs by specifying individual queries as natural language questions, responding with detailed, natural language answers, and facilitating attainment of related or more precise information through follow-up questions. . Intermediate outputs include context summaries to facilitate indexing of document passages. Output to the user includes summarization of content relevant to the questions and interests indicated by the conversation with the user.","to assist users of the test and evaluation document archive in locating documents and portions of document that are relevant for their needs by specifying individual queries as natural language questions, responding with detailed, natural language answers, and facilitating attainment of related or more precise information through follow-up questions. . intermediate outputs include context summaries to facilitate indexing of document passages. output to the user includes summarization of content relevant to the questions and interests indicated by the conversation with the user."
RelativityOne,Department of Homeland Security,DHS,DHS,Mission-Enabling,None of the above.,"RelativityOne is a document review platform used to gain efficiencies in document review in litigation, FOIA, and other arenas where large-scale document review and production is necessary. It is currently in use across several different DHS Components and Offices.",Review Center tailors the review process by predicting the categorization of documents based on the reviewer's inputs.,Operation and Maintenance,Neither,5/1/2021,5/1/2021,8/1/2022,Developed with contracting resources.,Purchased as COTS in several Components.,No,No,Yes,Yes,No,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,Yes,Multiple across DHS,6-12 months,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"RelativityOne is a document review platform used to gain efficiencies in document review in litigation, FOIA, and other arenas where large-scale document review and production is necessary. It is currently in use across several different DHS Components and Offices. . Review Center tailors the review process by predicting the categorization of documents based on the reviewer's inputs.","relativityone is a document review platform used to gain efficiencies in document review in litigation, foia, and other arenas where large-scale document review and production is necessary. it is currently in use across several different dhs components and offices. . review center tailors the review process by predicting the categorization of documents based on the reviewer's inputs."
OCFO Code Assist GPT,Department of Homeland Security,DHS,FEMA,Mission-Enabling,None of the above.,"1) Code Assist GPT is an internal facing GenaAI tool to augment the FEMA workforce in generating and troubleshooting existing queries in established query languages.  

2) The tool provides improved queries and rapid iteration to produce desired scripts/query language saving the end user 80-90% of the time versus custom query development.  The tool also opens other computer languages, as required, to support our data analytics community.""
","Code Assist GPT is an internal facing GenaAI tool to augment the FEMA workforce in generating and troubleshooting existing queries in established query languages.  The user simply enters the language they are trying to query (i.e. SQL, Java, COBOL, etc) and the Code Assist GPT then provides a proposed query based on the elements provided by the user.  If the user experiences an error or the query is not successful, the Code Assist GPT maintains the session as long as the user is still logged in and hasn't restarted the session, the user can continue to prompt the GPT to provide enhancements to the provided query until results are as expected.  At the end of the user session, all prompts/queries are removed and no data is stored outside of the active user session.
",Implementation and Assessment,Neither,6/1/2024,10/1/2024,Unknown,Developed with contracting resources.,70FA3124F00000027,No,No,No,Yes,No,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,FEMA OCFO GPT,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"1) Code Assist GPT is an internal facing GenaAI tool to augment the FEMA workforce in generating and troubleshooting existing queries in established query languages.  

2) The tool provides improved queries and rapid iteration to produce desired scripts/query language saving the end user 80-90% of the time versus custom query development.  The tool also opens other computer languages, as required, to support our data analytics community."" . Code Assist GPT is an internal facing GenaAI tool to augment the FEMA workforce in generating and troubleshooting existing queries in established query languages.  The user simply enters the language they are trying to query (i.e. SQL, Java, COBOL, etc) and the Code Assist GPT then provides a proposed query based on the elements provided by the user.  If the user experiences an error or the query is not successful, the Code Assist GPT maintains the session as long as the user is still logged in and hasn't restarted the session, the user can continue to prompt the GPT to provide enhancements to the provided query until results are as expected.  At the end of the user session, all prompts/queries are removed and no data is stored outside of the active user session.","1) code assist gpt is an internal facing genaai tool to augment the fema workforce in generating and troubleshooting existing queries in established query languages. 2) the tool provides improved queries and rapid iteration to produce desired scripts/query language saving the end user 80-90% of the time versus custom query development. the tool also opens other computer languages, as required, to support our data analytics community."" . code assist gpt is an internal facing genaai tool to augment the fema workforce in generating and troubleshooting existing queries in established query languages. the user simply enters the language they are trying to query (i.e. sql, java, cobol, etc) and the code assist gpt then provides a proposed query based on the elements provided by the user. if the user experiences an error or the query is not successful, the code assist gpt maintains the session as long as the user is still logged in and hasn't restarted the session, the user can continue to prompt the gpt to provide enhancements to the provided query until results are as expected. at the end of the user session, all prompts/queries are removed and no data is stored outside of the active user session."
Geospatial Damage Assessments,Department of Homeland Security,DHS,FEMA,Emergency Management,None of the above.,"We will integrate these automated tools for imagery-based damage assessments into RGO processes and systems to inform FEMA geospatial analysts.  FEMA geospatial analysts will review output and develop recommendations, so FEMA can quickly detect and characterize damaged and undamaged buildings, identify the concentrations of damage, identify and detect debris, and to support rapid response and recovery activities.","AI models generate points or polygons over areas where damage or debris is likely. This output is a recommendation meant to trigger investigation by human analysts via traditional damage assessments. In large scale incidents, imagery collection can be comprised of hundreds of thousands of miles of land and millions of structures. AI automation allows for the prioritization of imagery exploitation where damage is likely to have occurred, reducing the time required to identify impacts and summarize damage sustained. ",Implementation and Assessment,Neither,8/1/2022,8/1/2022,Unknown,Developed with contracting resources.,The PIID is specific to the disaster the use case is used for.,No,No,No,Yes,Yes,Geospatial Damage Assessments.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"We will integrate these automated tools for imagery-based damage assessments into RGO processes and systems to inform FEMA geospatial analysts.  FEMA geospatial analysts will review output and develop recommendations, so FEMA can quickly detect and characterize damaged and undamaged buildings, identify the concentrations of damage, identify and detect debris, and to support rapid response and recovery activities. . AI models generate points or polygons over areas where damage or debris is likely. This output is a recommendation meant to trigger investigation by human analysts via traditional damage assessments. In large scale incidents, imagery collection can be comprised of hundreds of thousands of miles of land and millions of structures. AI automation allows for the prioritization of imagery exploitation where damage is likely to have occurred, reducing the time required to identify impacts and summarize damage sustained.","we will integrate these automated tools for imagery-based damage assessments into rgo processes and systems to inform fema geospatial analysts. fema geospatial analysts will review output and develop recommendations, so fema can quickly detect and characterize damaged and undamaged buildings, identify the concentrations of damage, identify and detect debris, and to support rapid response and recovery activities. . ai models generate points or polygons over areas where damage or debris is likely. this output is a recommendation meant to trigger investigation by human analysts via traditional damage assessments. in large scale incidents, imagery collection can be comprised of hundreds of thousands of miles of land and millions of structures. ai automation allows for the prioritization of imagery exploitation where damage is likely to have occurred, reducing the time required to identify impacts and summarize damage sustained."
Hazard Mitigation Assistance Chatbot,Department of Homeland Security,DHS,FEMA,Emergency Management,None of the above.,"The intended purpose is to deploy an AI-powered internal chatbot that serves as a centralized, intuitive user interface for staff across all FEMA regions and headquarters. The intended outcomes include: Improving Efficiency and Consistency by providing quick access to accurate information, the chatbot streamlines work processes, reduces redundancy, and ensures consistent procedures across regions. Accelerate Onboarding of new hires become competent more quickly, reducing the time to full productivity and strengthening the workforce. Ensure Compliance and Accountability by being Transparent and accurate guidance promotes adherence to HMA policies, reducing errors and enhancing program integrity. Enhancing Decision-Making by providing access to precise citations and real-time data improves the quality of decisions, benefiting program outcomes. Supporting FEMA's mission by empowering staff to perform effectively, the chatbot enhances overall efficiency and service delivery, aligning with FEMA's goal of improving program delivery and serving the whole community.","The chatbot will provide accurate, uniform answers with citations, ensuring everyone is aligned and reducing the risk of misinterpretation of publicly available HMA data sourced from FEMA.gov. 
The chatbot will provide clear, plain-language explanation, highlighting eligibility criteria, funding priorities, and application processes for each HMA program. The chatbot will quickly retrieve relevant HMA policy documents, provides precise citations, and summarizes key points. The chatbot will compile publicly available of publicly available HMA data sourced from FEMA.gov., perform the analyses, and present it in a clear report with visualizations.",Acquisition and/or Development,Neither,9/5/2024,9/5/2024,Unknown,Developed with both contracting and in-house resources.,70FA6022A00000001,No,Unknown,Unknown,Unknown,Yes,"The agency-owned data included all HMA policy, training, and data available on FEMA.gov. This include OpenFEMA datasets, Legal/Policy Documents (i.e., federal legal and policy references, such as nondiscrimination clauses and presidential memorandums); Regulations (i.e., regulatory texts, including sections of the Code of Federal Regulations (CFR)); and Guides and Handbooks (i.e., FEMA-issued guides that provide frameworks and instructions, such as planning handbooks and operational guides).","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"The intended purpose is to deploy an AI-powered internal chatbot that serves as a centralized, intuitive user interface for staff across all FEMA regions and headquarters. The intended outcomes include: Improving Efficiency and Consistency by providing quick access to accurate information, the chatbot streamlines work processes, reduces redundancy, and ensures consistent procedures across regions. Accelerate Onboarding of new hires become competent more quickly, reducing the time to full productivity and strengthening the workforce. Ensure Compliance and Accountability by being Transparent and accurate guidance promotes adherence to HMA policies, reducing errors and enhancing program integrity. Enhancing Decision-Making by providing access to precise citations and real-time data improves the quality of decisions, benefiting program outcomes. Supporting FEMA's mission by empowering staff to perform effectively, the chatbot enhances overall efficiency and service delivery, aligning with FEMA's goal of improving program delivery and serving the whole community. . The chatbot will provide accurate, uniform answers with citations, ensuring everyone is aligned and reducing the risk of misinterpretation of publicly available HMA data sourced from FEMA.gov. 
The chatbot will provide clear, plain-language explanation, highlighting eligibility criteria, funding priorities, and application processes for each HMA program. The chatbot will quickly retrieve relevant HMA policy documents, provides precise citations, and summarizes key points. The chatbot will compile publicly available of publicly available HMA data sourced from FEMA.gov., perform the analyses, and present it in a clear report with visualizations.","the intended purpose is to deploy an ai-powered internal chatbot that serves as a centralized, intuitive user interface for staff across all fema regions and headquarters. the intended outcomes include: improving efficiency and consistency by providing quick access to accurate information, the chatbot streamlines work processes, reduces redundancy, and ensures consistent procedures across regions. accelerate onboarding of new hires become competent more quickly, reducing the time to full productivity and strengthening the workforce. ensure compliance and accountability by being transparent and accurate guidance promotes adherence to hma policies, reducing errors and enhancing program integrity. enhancing decision-making by providing access to precise citations and real-time data improves the quality of decisions, benefiting program outcomes. supporting fema's mission by empowering staff to perform effectively, the chatbot enhances overall efficiency and service delivery, aligning with fema's goal of improving program delivery and serving the whole community. . the chatbot will provide accurate, uniform answers with citations, ensuring everyone is aligned and reducing the risk of misinterpretation of publicly available hma data sourced from fema.gov. the chatbot will provide clear, plain-language explanation, highlighting eligibility criteria, funding priorities, and application processes for each hma program. the chatbot will quickly retrieve relevant hma policy documents, provides precise citations, and summarizes key points. the chatbot will compile publicly available of publicly available hma data sourced from fema.gov., perform the analyses, and present it in a clear report with visualizations."
Planning Assistant for Resilient Communities (PARC),Department of Homeland Security,DHS,FEMA,Emergency Management,None of the above.,"Hazard mitigation plans are not only a foundational step that communities can take to build their resilience but can be lengthy to produce and challenging for communities that lack resources to do so. PARC will specifically support State, Local, Tribal, and Territorial (SLTT) governments’ understanding of how to craft a plan that identifies risks and mitigation strategies as well as generate draft plan elements—from publicly-available, well-researched sources — that governments could customize to meet their needs. This capability could provide planners more time to allocate toward community engagement and stakeholder collaboration, and lead to more communities having the ability to submit grant applications for funding to become more resilient and reduce disaster risks.","The Beta Release GenAI Plan Generator (OpenAI GPT-4o) is an LLM that  generates sections of the Hazard Mitigation Plans based on user inputs. It processes prompts and user responses to create detailed, regulatory-compliant Hazard Mitigation Plan sections.  Beta Release PARC Assistant (OpenAI GPT-4o) prodices responses through an interactive chat assistant. The Assistant helps planners by answering questions related to the Hazard Mitigation Plan process, offering explanations of policy guidelines, and guiding users through FEMA’s regulatory requirements.",Acquisition and/or Development,Neither,10/27/2023,1/29/2024,Unknown,Developed with both contracting and in-house resources.,"70FA4020A0000000281, 70FA3020A00000008",No,Unknown,Unknown,Unknown,Yes,"The agency-owned data included: Legal/Policy Documents (i.e., federal legal and policy references, such as nondiscrimination clauses and presidential memorandums); Regulations (i.e., regulatory texts, including sections of the Code of Federal Regulations (CFR)); and Guides and Handbooks (i.e., FEMA-issued guides that provide frameworks and instructions, such as planning handbooks and operational guides).","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,6-12 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"Hazard mitigation plans are not only a foundational step that communities can take to build their resilience but can be lengthy to produce and challenging for communities that lack resources to do so. PARC will specifically support State, Local, Tribal, and Territorial (SLTT) governments’ understanding of how to craft a plan that identifies risks and mitigation strategies as well as generate draft plan elements—from publicly-available, well-researched sources — that governments could customize to meet their needs. This capability could provide planners more time to allocate toward community engagement and stakeholder collaboration, and lead to more communities having the ability to submit grant applications for funding to become more resilient and reduce disaster risks. . The Beta Release GenAI Plan Generator (OpenAI GPT-4o) is an LLM that  generates sections of the Hazard Mitigation Plans based on user inputs. It processes prompts and user responses to create detailed, regulatory-compliant Hazard Mitigation Plan sections.  Beta Release PARC Assistant (OpenAI GPT-4o) prodices responses through an interactive chat assistant. The Assistant helps planners by answering questions related to the Hazard Mitigation Plan process, offering explanations of policy guidelines, and guiding users through FEMA’s regulatory requirements.","hazard mitigation plans are not only a foundational step that communities can take to build their resilience but can be lengthy to produce and challenging for communities that lack resources to do so. parc will specifically support state, local, tribal, and territorial (sltt) governments’ understanding of how to craft a plan that identifies risks and mitigation strategies as well as generate draft plan elements—from publicly-available, well-researched sources — that governments could customize to meet their needs. this capability could provide planners more time to allocate toward community engagement and stakeholder collaboration, and lead to more communities having the ability to submit grant applications for funding to become more resilient and reduce disaster risks. . the beta release genai plan generator (openai gpt-4o) is an llm that generates sections of the hazard mitigation plans based on user inputs. it processes prompts and user responses to create detailed, regulatory-compliant hazard mitigation plan sections. beta release parc assistant (openai gpt-4o) prodices responses through an interactive chat assistant. the assistant helps planners by answering questions related to the hazard mitigation plan process, offering explanations of policy guidelines, and guiding users through fema’s regulatory requirements."
OCFO Response Augmentation Suite,Department of Homeland Security,DHS,FEMA,Mission-Enabling,None of the above.,"1) FEMA OCFO GPT - B is an internal facing GenAI tool to augment the FEMA workforce in generating initial responses to questions for the record and providing a preliminary response to the Program Office to use in their formal response to the request. 
2) FEMA OCFO GPT provides draft responses reducing the data gathering stage and providing additional time for analysis, response, and approval.  This has reduced the analyst initial level of effort versus individual research by 80% on initial surveys.

1) Travel Policy GPT is an internal facing GenAI tool to augment the FEMA workforce in generating initial responses to questions regarding FEMA/DHS Travel Policy, including the JTR, and providing a preliminary response to the travel specialist to use in their formal response to the queries. 
2) The tool provides improved responses saving the end user 80-90% of the time versus regular engagement with the Travel Service Center.  The tool also allows travelers to ask specific questions that require Travel Service Center engagement, limiting the needed triage and speeding resoultion times.

1) Fiscal Policy GPT is an internal facing GenAI tool to augment the FEMA workforce in providing preliminary responses to questions regarding FEMA/DHS Fiscal Policy.
2) The tool provides improved responses saving the end user 80-90% of the time versus regular engagement with the DHS/FEMA OCFO Policy.  The tool also allows internal users to ask specific questions that require Fiscal Policy engagement, limiting the needed triage and speeding resoultion times.","FEMA OCFO GPT is an internal facing GenAI tool to augment the FEMA workforce in generating initial responses to questions for the record and providing a preliminary response to the Program Office to use in their formal response to the request.  FEMA OCFO GPT leverages public facing and internal deliberative documents (hearing testimony, answers sent to previous questions, and other relevant documents) to assist in answering questions the Agency receives.  The tool generates a draft response that is then refined/updated prior to providing a formal response.  This tool is leveraged in the data gathering stage and does not replace any current analysts work or Agency leadership review prior to submittal via the formal request for information or question from an internal stakeholder or external stakeholder.

Travel Policy GPT is an internal facing GenAI tool to augment the FEMA workforce in generating initial responses to questions regarding FEMA/DHS Travel Policy, including the JTR, and providing a preliminary response to the travel specialist to use in their formal response to the queries.  Travel Policy GPT leverages the JTR and internal travel policy documents to assist in answering questions the FEMA Travel Service Center receives.  The tool generates a draft response that is then refined/updated prior to providing a formal response.  This tool is leveraged in the data gathering stage and does not replace any current analysts work or leadership review, as required, prior to submittal via the formal request for information or question from a traveler traveling on FEMA's behalf or in accordance with their duties to support the FEMA mission.

Fiscal Policy GPT is an internal facing GenAI tool to augment the FEMA workforce in providing preliminary responses to questions regarding FEMA/DHS Fiscal Policy.  Fiscal Policy GPT will leverage the DHS FMPM and FEMA Fiscal Policy documents.  The tool is planned to generate a draft response with references to assist FEMA internal workforce in compliance with established policy.  This tool is leveraged in the data gathering stage and does not replace good judgement or in any way impact decisions on compliance with established Fiscal Policy.  The tool will display a disclaimer to this effect that will be cleared with FEMA/DHS OGC, as appropriate.",Operation and Maintenance,Neither,7/1/2023,8/1/2023,2/1/2024,Developed with contracting resources.,70FA3124F00000027,No,No,No,Yes,Yes,"Budget Exhibits, Passback Materials, Hearing Testimony, Questions Received, Answers Provided

Travel Policy Documents (Joint Travel Regulation (JTR), DHS Travel Policy, FEMA Travel Policy)

Fiscal Policy Documents - Treasury Financial Manual (TFM), DHS Financial Policy Manual, FEMA Fiscal Policies","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,FEMA OCFO GPT,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"1) FEMA OCFO GPT - B is an internal facing GenAI tool to augment the FEMA workforce in generating initial responses to questions for the record and providing a preliminary response to the Program Office to use in their formal response to the request. 
2) FEMA OCFO GPT provides draft responses reducing the data gathering stage and providing additional time for analysis, response, and approval.  This has reduced the analyst initial level of effort versus individual research by 80% on initial surveys.

1) Travel Policy GPT is an internal facing GenAI tool to augment the FEMA workforce in generating initial responses to questions regarding FEMA/DHS Travel Policy, including the JTR, and providing a preliminary response to the travel specialist to use in their formal response to the queries. 
2) The tool provides improved responses saving the end user 80-90% of the time versus regular engagement with the Travel Service Center.  The tool also allows travelers to ask specific questions that require Travel Service Center engagement, limiting the needed triage and speeding resoultion times.

1) Fiscal Policy GPT is an internal facing GenAI tool to augment the FEMA workforce in providing preliminary responses to questions regarding FEMA/DHS Fiscal Policy.
2) The tool provides improved responses saving the end user 80-90% of the time versus regular engagement with the DHS/FEMA OCFO Policy.  The tool also allows internal users to ask specific questions that require Fiscal Policy engagement, limiting the needed triage and speeding resoultion times. . FEMA OCFO GPT is an internal facing GenAI tool to augment the FEMA workforce in generating initial responses to questions for the record and providing a preliminary response to the Program Office to use in their formal response to the request.  FEMA OCFO GPT leverages public facing and internal deliberative documents (hearing testimony, answers sent to previous questions, and other relevant documents) to assist in answering questions the Agency receives.  The tool generates a draft response that is then refined/updated prior to providing a formal response.  This tool is leveraged in the data gathering stage and does not replace any current analysts work or Agency leadership review prior to submittal via the formal request for information or question from an internal stakeholder or external stakeholder.

Travel Policy GPT is an internal facing GenAI tool to augment the FEMA workforce in generating initial responses to questions regarding FEMA/DHS Travel Policy, including the JTR, and providing a preliminary response to the travel specialist to use in their formal response to the queries.  Travel Policy GPT leverages the JTR and internal travel policy documents to assist in answering questions the FEMA Travel Service Center receives.  The tool generates a draft response that is then refined/updated prior to providing a formal response.  This tool is leveraged in the data gathering stage and does not replace any current analysts work or leadership review, as required, prior to submittal via the formal request for information or question from a traveler traveling on FEMA's behalf or in accordance with their duties to support the FEMA mission.

Fiscal Policy GPT is an internal facing GenAI tool to augment the FEMA workforce in providing preliminary responses to questions regarding FEMA/DHS Fiscal Policy.  Fiscal Policy GPT will leverage the DHS FMPM and FEMA Fiscal Policy documents.  The tool is planned to generate a draft response with references to assist FEMA internal workforce in compliance with established policy.  This tool is leveraged in the data gathering stage and does not replace good judgement or in any way impact decisions on compliance with established Fiscal Policy.  The tool will display a disclaimer to this effect that will be cleared with FEMA/DHS OGC, as appropriate.","1) fema ocfo gpt - b is an internal facing genai tool to augment the fema workforce in generating initial responses to questions for the record and providing a preliminary response to the program office to use in their formal response to the request. 2) fema ocfo gpt provides draft responses reducing the data gathering stage and providing additional time for analysis, response, and approval. this has reduced the analyst initial level of effort versus individual research by 80% on initial surveys. 1) travel policy gpt is an internal facing genai tool to augment the fema workforce in generating initial responses to questions regarding fema/dhs travel policy, including the jtr, and providing a preliminary response to the travel specialist to use in their formal response to the queries. 2) the tool provides improved responses saving the end user 80-90% of the time versus regular engagement with the travel service center. the tool also allows travelers to ask specific questions that require travel service center engagement, limiting the needed triage and speeding resoultion times. 1) fiscal policy gpt is an internal facing genai tool to augment the fema workforce in providing preliminary responses to questions regarding fema/dhs fiscal policy. 2) the tool provides improved responses saving the end user 80-90% of the time versus regular engagement with the dhs/fema ocfo policy. the tool also allows internal users to ask specific questions that require fiscal policy engagement, limiting the needed triage and speeding resoultion times. . fema ocfo gpt is an internal facing genai tool to augment the fema workforce in generating initial responses to questions for the record and providing a preliminary response to the program office to use in their formal response to the request. fema ocfo gpt leverages public facing and internal deliberative documents (hearing testimony, answers sent to previous questions, and other relevant documents) to assist in answering questions the agency receives. the tool generates a draft response that is then refined/updated prior to providing a formal response. this tool is leveraged in the data gathering stage and does not replace any current analysts work or agency leadership review prior to submittal via the formal request for information or question from an internal stakeholder or external stakeholder. travel policy gpt is an internal facing genai tool to augment the fema workforce in generating initial responses to questions regarding fema/dhs travel policy, including the jtr, and providing a preliminary response to the travel specialist to use in their formal response to the queries. travel policy gpt leverages the jtr and internal travel policy documents to assist in answering questions the fema travel service center receives. the tool generates a draft response that is then refined/updated prior to providing a formal response. this tool is leveraged in the data gathering stage and does not replace any current analysts work or leadership review, as required, prior to submittal via the formal request for information or question from a traveler traveling on fema's behalf or in accordance with their duties to support the fema mission. fiscal policy gpt is an internal facing genai tool to augment the fema workforce in providing preliminary responses to questions regarding fema/dhs fiscal policy. fiscal policy gpt will leverage the dhs fmpm and fema fiscal policy documents. the tool is planned to generate a draft response with references to assist fema internal workforce in compliance with established policy. this tool is leveraged in the data gathering stage and does not replace good judgement or in any way impact decisions on compliance with established fiscal policy. the tool will display a disclaimer to this effect that will be cleared with fema/dhs ogc, as appropriate."
Incident Management Workforce Deployment Model (depmod),Department of Homeland Security,DHS,FEMA,Mission-Enabling,None of the above.,"The model is able to extract patterns from big data, combine these patterns in a way that mirrors a real-world process, and simulate likely outcomes to yield predictions in the form of relatable, mission-centric, metrics.",The system primarily outputs predictions that can then (optionally) be summarized and communicated as recommendations.,Operation and Maintenance,Neither,4/1/2021,8/1/2021,2/1/2022,Developed with both contracting and in-house resources.,HHSN316201200112W,No,No,No,No,Yes,The Deployment Tracking System (DTS) is currently the primary data source. It provides a historical record of IM workforce deployments that can be validated for accuracy and used for model training and prediction.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The model is able to extract patterns from big data, combine these patterns in a way that mirrors a real-world process, and simulate likely outcomes to yield predictions in the form of relatable, mission-centric, metrics. . The system primarily outputs predictions that can then (optionally) be summarized and communicated as recommendations.","the model is able to extract patterns from big data, combine these patterns in a way that mirrors a real-world process, and simulate likely outcomes to yield predictions in the form of relatable, mission-centric, metrics. . the system primarily outputs predictions that can then (optionally) be summarized and communicated as recommendations."
Individual Assistance (IA) & Public Assistance (PA) Projections,Department of Homeland Security,DHS,FEMA,Government Services (includes Benefits and Service Delivery),None of the above.,"The purpose of these supervised learning models is two-fold:

1.	For informational purposes: the models will produce predictions for to-be-determined quantities of interest. These quantities are often of interest to Agency personnel in the field, region, and headquarters, as well as DHS, OMB, NSC, and the Whitehouse.
2.	For decisional purposes: in addition to being informative, the model’s predictions are likely to be used for decision making. This may include implicit use (e.g., seeing that there are a large number of households that require assistance may motivate FEMA CFO to consider requesting additional funding for the Disaster Relief Fund) or explicit use (e.g., the number of estimated housing units required may be used to determine the number of housing units to order just-in-time).

Anticipated benefits of using supervised learning models include:
1.	Increased predictive performance: supervised learning models offer means of quantifying and choosing models with the best possible predictive performance.
2.	Timely prediction: supervised learning models can produce reasonably high performance predictions based on limited data.
3.	Quantification of uncertainty: supervised learning models are able to provide prediction intervals and predictive distributions in addition to point predictions to give users a sense of how uncertainty is inherent in the prediction.
4.	Rigor: supervised learning models are well founded on statistical learning theory and provide a level of mathematical rigor that speaks to the reliability and limitations of methods.","These supervised learning models produce predictions, not recommendations and not decisions (though they can be used to inform human users in making recommendations and decisions).

At minimum, these supervised learning models will produce point predictions for the different quantities of interest for disaster declarations. Additionally supervised learning models may produce prediction intervals or predictive distributions as feasible and appropriate for the given prediction problem. Often these outputs will be shared via business intelligence tools (e.g., Tableau or PowerBI) for wide internal FEMA use. Some predictions may be shared to a more restricted audience through simpler means (e.g., an excel workbook) as appropriate.",Operation and Maintenance,Neither,8/1/2018,8/1/2018,6/1/2024,Developed in-house.,Unknown,Yes,No,No,No,Yes,Data from the National Emergency Management Information System (NEMIS) and Grants Manager,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The purpose of these supervised learning models is two-fold:

1.	For informational purposes: the models will produce predictions for to-be-determined quantities of interest. These quantities are often of interest to Agency personnel in the field, region, and headquarters, as well as DHS, OMB, NSC, and the Whitehouse.
2.	For decisional purposes: in addition to being informative, the model’s predictions are likely to be used for decision making. This may include implicit use (e.g., seeing that there are a large number of households that require assistance may motivate FEMA CFO to consider requesting additional funding for the Disaster Relief Fund) or explicit use (e.g., the number of estimated housing units required may be used to determine the number of housing units to order just-in-time).

Anticipated benefits of using supervised learning models include:
1.	Increased predictive performance: supervised learning models offer means of quantifying and choosing models with the best possible predictive performance.
2.	Timely prediction: supervised learning models can produce reasonably high performance predictions based on limited data.
3.	Quantification of uncertainty: supervised learning models are able to provide prediction intervals and predictive distributions in addition to point predictions to give users a sense of how uncertainty is inherent in the prediction.
4.	Rigor: supervised learning models are well founded on statistical learning theory and provide a level of mathematical rigor that speaks to the reliability and limitations of methods. . These supervised learning models produce predictions, not recommendations and not decisions (though they can be used to inform human users in making recommendations and decisions).

At minimum, these supervised learning models will produce point predictions for the different quantities of interest for disaster declarations. Additionally supervised learning models may produce prediction intervals or predictive distributions as feasible and appropriate for the given prediction problem. Often these outputs will be shared via business intelligence tools (e.g., Tableau or PowerBI) for wide internal FEMA use. Some predictions may be shared to a more restricted audience through simpler means (e.g., an excel workbook) as appropriate.","the purpose of these supervised learning models is two-fold: 1. for informational purposes: the models will produce predictions for to-be-determined quantities of interest. these quantities are often of interest to agency personnel in the field, region, and headquarters, as well as dhs, omb, nsc, and the whitehouse. 2. for decisional purposes: in addition to being informative, the model’s predictions are likely to be used for decision making. this may include implicit use (e.g., seeing that there are a large number of households that require assistance may motivate fema cfo to consider requesting additional funding for the disaster relief fund) or explicit use (e.g., the number of estimated housing units required may be used to determine the number of housing units to order just-in-time). anticipated benefits of using supervised learning models include: 1. increased predictive performance: supervised learning models offer means of quantifying and choosing models with the best possible predictive performance. 2. timely prediction: supervised learning models can produce reasonably high performance predictions based on limited data. 3. quantification of uncertainty: supervised learning models are able to provide prediction intervals and predictive distributions in addition to point predictions to give users a sense of how uncertainty is inherent in the prediction. 4. rigor: supervised learning models are well founded on statistical learning theory and provide a level of mathematical rigor that speaks to the reliability and limitations of methods. . these supervised learning models produce predictions, not recommendations and not decisions (though they can be used to inform human users in making recommendations and decisions). at minimum, these supervised learning models will produce point predictions for the different quantities of interest for disaster declarations. additionally supervised learning models may produce prediction intervals or predictive distributions as feasible and appropriate for the given prediction problem. often these outputs will be shared via business intelligence tools (e.g., tableau or powerbi) for wide internal fema use. some predictions may be shared to a more restricted audience through simpler means (e.g., an excel workbook) as appropriate."
Voice Analytics for Investigative Data,Department of Homeland Security,DHS,ICE,Law & Justice,None of the above.,"While conducting investigations, Homeland Security Investigations (HSI) encounters a significant volume of live and recorded content, requiring human language to be extracted and analyzed. Currently, this is achieved through manual processes that are extremely labor-intensive and expensive. To address this challenge, HSI has developed Voice Analytics, a cutting-edge technology that leverages machine learning and advanced speech processing techniques to perform multilingual speech transcription and translation on user-supplied audio files. This innovative solution enables HSI personnel to quickly identify new investigative leads and analyze the outputs against other law enforcement datasets, significantly improving the efficiency and effectiveness of their investigations compared to manual voice analytics processes.",This system outputs transcripted and translated audio files.,Implementation and Assessment,Neither,3/1/2020,7/1/2022,Unknown,Developed with contracting resources.,"70OCTD22FR00000002, 70OCTD22FR00000235, 70OCTD22FR00000009",No,No,Yes,No,Yes,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Repository for Analytics in a Virtualized Environment,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"While conducting investigations, Homeland Security Investigations (HSI) encounters a significant volume of live and recorded content, requiring human language to be extracted and analyzed. Currently, this is achieved through manual processes that are extremely labor-intensive and expensive. To address this challenge, HSI has developed Voice Analytics, a cutting-edge technology that leverages machine learning and advanced speech processing techniques to perform multilingual speech transcription and translation on user-supplied audio files. This innovative solution enables HSI personnel to quickly identify new investigative leads and analyze the outputs against other law enforcement datasets, significantly improving the efficiency and effectiveness of their investigations compared to manual voice analytics processes. . This system outputs transcripted and translated audio files.","while conducting investigations, homeland security investigations (hsi) encounters a significant volume of live and recorded content, requiring human language to be extracted and analyzed. currently, this is achieved through manual processes that are extremely labor-intensive and expensive. to address this challenge, hsi has developed voice analytics, a cutting-edge technology that leverages machine learning and advanced speech processing techniques to perform multilingual speech transcription and translation on user-supplied audio files. this innovative solution enables hsi personnel to quickly identify new investigative leads and analyze the outputs against other law enforcement datasets, significantly improving the efficiency and effectiveness of their investigations compared to manual voice analytics processes. . this system outputs transcripted and translated audio files."
Semantic Search and Summarization for Investigative Reports,Department of Homeland Security,DHS,ICE,Law & Justice,None of the above.,"Effective investigations require querying large volumes of unstructured reports to extract and summarize contextually-relevant information. Current lexical search methods are insufficient for finding such information. Semantic Search and Summarization enables contextually-relevant information extraction and summarization from large volumes of unstructured investigative reports. The output will enhance and optimize investigative analysis by accelerating person-of-interest identification, surfacing trends, and improving network or fraud detection.","The output consists of contextually relevant contents of reports that can be used in investigations to accelerate person-of-interest identification, surface trends, and improve network or fraud detection.",Implementation and Assessment,Neither,10/1/2023,10/1/2023,Unknown,Developed with contracting resources.,70OCTD22FR00000002; 70OCTD22FR00000235; 70OCTD22FR00000009,No,No,Yes,Yes,Yes,"HSI Report of Investigation (ROI) data. HSI ROIs document the findings and results of an investigation into a specific case or matter. ROIs typically include information about the investigation, such as individuals or organizations suspected of violating federal laws related to customs, immigration, or national security. ROIs may also include recommendations for further action, such as prosecution, deportation, or other enforcement measures.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Effective investigations require querying large volumes of unstructured reports to extract and summarize contextually-relevant information. Current lexical search methods are insufficient for finding such information. Semantic Search and Summarization enables contextually-relevant information extraction and summarization from large volumes of unstructured investigative reports. The output will enhance and optimize investigative analysis by accelerating person-of-interest identification, surfacing trends, and improving network or fraud detection. . The output consists of contextually relevant contents of reports that can be used in investigations to accelerate person-of-interest identification, surface trends, and improve network or fraud detection.","effective investigations require querying large volumes of unstructured reports to extract and summarize contextually-relevant information. current lexical search methods are insufficient for finding such information. semantic search and summarization enables contextually-relevant information extraction and summarization from large volumes of unstructured investigative reports. the output will enhance and optimize investigative analysis by accelerating person-of-interest identification, surfacing trends, and improving network or fraud detection. . the output consists of contextually relevant contents of reports that can be used in investigations to accelerate person-of-interest identification, surface trends, and improve network or fraud detection."
Virtual Agent Chatbot for Human Capital,Department of Homeland Security,DHS,ICE,Education & Workforce,None of the above.,"The intended purpose of the ServiceNow Virtual Agent natural language understanding (NLU) is to facilitate seamless interactions between users and the chatbot. It aims to provide automated support for common Human Capital related questions and inquiries, enhancing efficiency and user experience.  By understanding natural language, the virtual agent can provide relevant and accurate responses, making it easier for users to get the help they need.  Reduce the amount of submitted inquiries that need to be manually processed by OHC. Reduce the amounts of clicks it takes for the user to find the information they are looking for.",The natural language understanding (NLU) will point towards pre-defined Human Capital related categories that are most relevant based on the users’ inquiries. Specific responses/answers to user questions and resources associated with the questions if applicable.,Implementation and Assessment,Neither,5/9/2024,8/5/2024,Unknown,Developed with contracting resources.,70CTD023FR0000134,No,No,No,No,Yes,ICE Office of Human Capital team members compiled documentation to train the NLU model based on their subject matter expertise on human capital related questions and inquiries.  The data was compiled from knowledge base articles and frequently asked questions for human capital related inquiries.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,ServiceNow,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The intended purpose of the ServiceNow Virtual Agent natural language understanding (NLU) is to facilitate seamless interactions between users and the chatbot. It aims to provide automated support for common Human Capital related questions and inquiries, enhancing efficiency and user experience.  By understanding natural language, the virtual agent can provide relevant and accurate responses, making it easier for users to get the help they need.  Reduce the amount of submitted inquiries that need to be manually processed by OHC. Reduce the amounts of clicks it takes for the user to find the information they are looking for. . The natural language understanding (NLU) will point towards pre-defined Human Capital related categories that are most relevant based on the users’ inquiries. Specific responses/answers to user questions and resources associated with the questions if applicable.","the intended purpose of the servicenow virtual agent natural language understanding (nlu) is to facilitate seamless interactions between users and the chatbot. it aims to provide automated support for common human capital related questions and inquiries, enhancing efficiency and user experience. by understanding natural language, the virtual agent can provide relevant and accurate responses, making it easier for users to get the help they need. reduce the amount of submitted inquiries that need to be manually processed by ohc. reduce the amounts of clicks it takes for the user to find the information they are looking for. . the natural language understanding (nlu) will point towards pre-defined human capital related categories that are most relevant based on the users’ inquiries. specific responses/answers to user questions and resources associated with the questions if applicable."
Machine Learning Translation Technology Initiative,Department of Homeland Security,DHS,ICE,Law & Justice,None of the above.,"The solution will be used to facilitate non-critical communications only. When deployed to production, the solution should not be used alone, absent human quality control, when materials are vital to an individual’s rights or benefits, or when the source materials contain non-literal language (e.g., slang, metaphor), lack clear grammar or structure, contain abbreviations or acronyms, or are overly complex, technical, or wordy. If any interactions indicate the presence of legal or medical issues or a need for greater accuracy, ERO personnel are required to seek additional language assistance via bilingual staff or a professional language line. The solution was first spearheaded by U.S. Coast Guard for maritime usage with the support of DHS Science and Technology Directorate through the Silicon Valley Innovation Program (SVIP). ICE ERO received FY22 funding to initiate the proof of concept with the vendors vetted by USCG and DHS S&T through an interagency agreement. Solutions, in the form of mobile phone applications, are being developed under research and development phases and projected to completely meet ERO’s requirements by January 2026.","The Machine Learning Translation Technology Initiative will offer real-time communication/translation services to assist personnel in providing meaningful access to programs and services for limited English proficient (LEP) noncitizens. AI will be used to provide voice to text, text to voice, and voice to voice translations in at least 21 languages.",Acquisition and/or Development,Neither,7/1/2022,7/1/2022,Unknown,Developed with contracting resources.,Mylanguage Phase 3 - 70RSAT23T00000030; Kynamics Phase 4 - 70RSAT24T00000029,No,Unknown,Unknown,Unknown,No,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,More than 12 months,No,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"The solution will be used to facilitate non-critical communications only. When deployed to production, the solution should not be used alone, absent human quality control, when materials are vital to an individual’s rights or benefits, or when the source materials contain non-literal language (e.g., slang, metaphor), lack clear grammar or structure, contain abbreviations or acronyms, or are overly complex, technical, or wordy. If any interactions indicate the presence of legal or medical issues or a need for greater accuracy, ERO personnel are required to seek additional language assistance via bilingual staff or a professional language line. The solution was first spearheaded by U.S. Coast Guard for maritime usage with the support of DHS Science and Technology Directorate through the Silicon Valley Innovation Program (SVIP). ICE ERO received FY22 funding to initiate the proof of concept with the vendors vetted by USCG and DHS S&T through an interagency agreement. Solutions, in the form of mobile phone applications, are being developed under research and development phases and projected to completely meet ERO’s requirements by January 2026. . The Machine Learning Translation Technology Initiative will offer real-time communication/translation services to assist personnel in providing meaningful access to programs and services for limited English proficient (LEP) noncitizens. AI will be used to provide voice to text, text to voice, and voice to voice translations in at least 21 languages.","the solution will be used to facilitate non-critical communications only. when deployed to production, the solution should not be used alone, absent human quality control, when materials are vital to an individual’s rights or benefits, or when the source materials contain non-literal language (e.g., slang, metaphor), lack clear grammar or structure, contain abbreviations or acronyms, or are overly complex, technical, or wordy. if any interactions indicate the presence of legal or medical issues or a need for greater accuracy, ero personnel are required to seek additional language assistance via bilingual staff or a professional language line. the solution was first spearheaded by u.s. coast guard for maritime usage with the support of dhs science and technology directorate through the silicon valley innovation program (svip). ice ero received fy22 funding to initiate the proof of concept with the vendors vetted by uscg and dhs s&t through an interagency agreement. solutions, in the form of mobile phone applications, are being developed under research and development phases and projected to completely meet ero’s requirements by january 2026. . the machine learning translation technology initiative will offer real-time communication/translation services to assist personnel in providing meaningful access to programs and services for limited english proficient (lep) noncitizens. ai will be used to provide voice to text, text to voice, and voice to voice translations in at least 21 languages."
Policy Analyst Assistant,Department of Homeland Security,DHS,ICE,Government Services (includes Benefits and Service Delivery),None of the above.,"The Student and Exchange Visitor Program (SEVP) uses AI to help policy analysts quickly find and summarize information about visa rules for students and schools. This saves time and helps analysts respond faster to questions.

This project is intended purpose is to enable SEVP Policy staff to provides responses to policy related inquiries and conduct policy research and analysis: It will be a tool embedded in workflow processes. The processes augmented by the AI includes responses for questions asked by SEVP internal staff and responses given to schools and students by SEVP personnel. For this process, the source of information for the responses will be from authoritative, pre-vetted documents. This reduces the workload on the Policy staff and allows them to focus on more complex policy and guidance issue. For these processes, the AI will assist with researching internal documents and information from regulations, applicable legislation, and other federal guidance. It will assist the policy analyst with summaries, citations, and initial drafts. The benefits includes faster, more consistent responses from Policy and the ability to handle additional work without adding staff.",The outputs provides answers to frequently asked policy and guidance questions and summaries and drafts for more in-depth analysis of policy issues. All responses go to an SEVP staff member and are not the final work product.,Acquisition and/or Development,Neither,8/1/2024,10/14/2024,Unknown,Developed with contracting resources.,70CTD020FR0000006,No,Unknown,Unknown,Unknown,Yes,"We are not training the AI, but we are augmenting the AI using RAG (Retrieval Augmented Generation) dataset with SEVP policy documents. These will be used to generate responses and the data will also be to evaluate the answers given.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,Yes,SEVPAMS,Less than 6 months,Yes,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"The Student and Exchange Visitor Program (SEVP) uses AI to help policy analysts quickly find and summarize information about visa rules for students and schools. This saves time and helps analysts respond faster to questions.

This project is intended purpose is to enable SEVP Policy staff to provides responses to policy related inquiries and conduct policy research and analysis: It will be a tool embedded in workflow processes. The processes augmented by the AI includes responses for questions asked by SEVP internal staff and responses given to schools and students by SEVP personnel. For this process, the source of information for the responses will be from authoritative, pre-vetted documents. This reduces the workload on the Policy staff and allows them to focus on more complex policy and guidance issue. For these processes, the AI will assist with researching internal documents and information from regulations, applicable legislation, and other federal guidance. It will assist the policy analyst with summaries, citations, and initial drafts. The benefits includes faster, more consistent responses from Policy and the ability to handle additional work without adding staff. . The outputs provides answers to frequently asked policy and guidance questions and summaries and drafts for more in-depth analysis of policy issues. All responses go to an SEVP staff member and are not the final work product.","the student and exchange visitor program (sevp) uses ai to help policy analysts quickly find and summarize information about visa rules for students and schools. this saves time and helps analysts respond faster to questions. this project is intended purpose is to enable sevp policy staff to provides responses to policy related inquiries and conduct policy research and analysis: it will be a tool embedded in workflow processes. the processes augmented by the ai includes responses for questions asked by sevp internal staff and responses given to schools and students by sevp personnel. for this process, the source of information for the responses will be from authoritative, pre-vetted documents. this reduces the workload on the policy staff and allows them to focus on more complex policy and guidance issue. for these processes, the ai will assist with researching internal documents and information from regulations, applicable legislation, and other federal guidance. it will assist the policy analyst with summaries, citations, and initial drafts. the benefits includes faster, more consistent responses from policy and the ability to handle additional work without adding staff. . the outputs provides answers to frequently asked policy and guidance questions and summaries and drafts for more in-depth analysis of policy issues. all responses go to an sevp staff member and are not the final work product."
SEVP Response Center Chatbot - SID (SEVIS Interactive Dialog),Department of Homeland Security,DHS,ICE,Government Services (includes Benefits and Service Delivery),None of the above.,"AI provides SID the ability to understand voices and its deterministic question and answer workflow (1) enables SID to answer routine caller questions without a help desk agent, and (2) when a help desk agent is required, SID will create a ticket with a caller transcript to reduce the burden on the agent. This frees up the human agents to deal with more complex cases and issues with specific records.","SID answers frequently asked questions from callers. If the SID cannot answer a caller’s question, it turns the caller over to an agent in the response center. The chatbot captures the interaction with the caller and sends the information via an API to Student and Exchange Visitor Program Automated Management System (SEVPAMS).",Acquisition and/or Development,Neither,2/6/2023,3/6/2023,Unknown,Developed with contracting resources.,PIID 70CTD020FR0000006/70CTD022FR0000005,No,Unknown,Unknown,Unknown,Yes,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Unknown,Unknown,Yes,SEVPAMS,Less than 6 months,Yes,No,No,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"AI provides SID the ability to understand voices and its deterministic question and answer workflow (1) enables SID to answer routine caller questions without a help desk agent, and (2) when a help desk agent is required, SID will create a ticket with a caller transcript to reduce the burden on the agent. This frees up the human agents to deal with more complex cases and issues with specific records. . SID answers frequently asked questions from callers. If the SID cannot answer a caller’s question, it turns the caller over to an agent in the response center. The chatbot captures the interaction with the caller and sends the information via an API to Student and Exchange Visitor Program Automated Management System (SEVPAMS).","ai provides sid the ability to understand voices and its deterministic question and answer workflow (1) enables sid to answer routine caller questions without a help desk agent, and (2) when a help desk agent is required, sid will create a ticket with a caller transcript to reduce the burden on the agent. this frees up the human agents to deal with more complex cases and issues with specific records. . sid answers frequently asked questions from callers. if the sid cannot answer a caller’s question, it turns the caller over to an agent in the response center. the chatbot captures the interaction with the caller and sends the information via an api to student and exchange visitor program automated management system (sevpams)."
Investigative Prioritization Aggregator,Department of Homeland Security,DHS,ICE,Law & Justice,None of the above.,"The sheer volume of data associated with investigations often overwhelms human capabilities, making it challenging for HSI personnel to analyze evidence and identify key players in criminal networks. Currently, there is no effective mechanism to quantify the level of evidence related to a particular subject or entity, or to determine which actors within a network are the most influential. This is particularly critical in the context of the counter-opioid/fentanyl mission, where timely and accurate intelligence is essential.

To address this challenge, this project utilizes machine learning to assign point values to data, enabling the scoring of information associated with a given selector, such as a phone number or legal name. This scoring system helps to understand the importance of an entity to investigations and the potential consequences of removing or neutralizing that entity. By doing so, HSI personnel can focus on high-priority targets and associated criminal networks, ultimately enhancing their ability to disrupt and dismantle these threats.",The output is scored entity data (such as a phone number or legal name). This scoring system helps to understand the importance of an entity to investigations and the potential consequences of removing or neutralizing that entity.,Operation and Maintenance,Neither,2/1/2021,10/1/2022,2/1/2024,Developed with contracting resources.,196918HSI00013,No,No,Yes,Yes,Yes,Law Enforcement Sensitive (LES) investigative data.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Repository for Analytics in a Virtualized Environment,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The sheer volume of data associated with investigations often overwhelms human capabilities, making it challenging for HSI personnel to analyze evidence and identify key players in criminal networks. Currently, there is no effective mechanism to quantify the level of evidence related to a particular subject or entity, or to determine which actors within a network are the most influential. This is particularly critical in the context of the counter-opioid/fentanyl mission, where timely and accurate intelligence is essential.

To address this challenge, this project utilizes machine learning to assign point values to data, enabling the scoring of information associated with a given selector, such as a phone number or legal name. This scoring system helps to understand the importance of an entity to investigations and the potential consequences of removing or neutralizing that entity. By doing so, HSI personnel can focus on high-priority targets and associated criminal networks, ultimately enhancing their ability to disrupt and dismantle these threats. . The output is scored entity data (such as a phone number or legal name). This scoring system helps to understand the importance of an entity to investigations and the potential consequences of removing or neutralizing that entity.","the sheer volume of data associated with investigations often overwhelms human capabilities, making it challenging for hsi personnel to analyze evidence and identify key players in criminal networks. currently, there is no effective mechanism to quantify the level of evidence related to a particular subject or entity, or to determine which actors within a network are the most influential. this is particularly critical in the context of the counter-opioid/fentanyl mission, where timely and accurate intelligence is essential. to address this challenge, this project utilizes machine learning to assign point values to data, enabling the scoring of information associated with a given selector, such as a phone number or legal name. this scoring system helps to understand the importance of an entity to investigations and the potential consequences of removing or neutralizing that entity. by doing so, hsi personnel can focus on high-priority targets and associated criminal networks, ultimately enhancing their ability to disrupt and dismantle these threats. . the output is scored entity data (such as a phone number or legal name). this scoring system helps to understand the importance of an entity to investigations and the potential consequences of removing or neutralizing that entity."
Hurricane Score,Department of Homeland Security,DHS,ICE,Law & Justice,None of the above.,"The Immigration and Customs Enforcement (ICE) Enforcement and Removal Operations (ERO) division would like to explore technology that could help lead to time savings and eliminate human error due to overburdened officers.  An AI generated hurricane score could also minimize the risk that a factor is overlooked due to officers being overburdened.  The hurricane score may be one factor that is used to inform the decision-making process. The “Hurricane Score” models the potential risk (1-5) that a noncitizen who is released from detention with the requirement to check in with ICE through monitoring will abscond from the program, with a higher number indicating a higher risk of absconding.","The Hurricane score model determines a probability of absconding and outputs a number (1-5), with a higher number indicating a higher risk of absconding. This number is used as one of many factors an officer uses when reviewing an ATD-ISAP case to determine the appropriate level of case management and technology for a noncitizen.",Operation and Maintenance,"Rights-Impacting
",2/1/2017,2/1/2017,2/1/2019,Developed in-house.,Unknown,No,No,Yes,No,No,Inactive case data from individuals enrolled in the ATD-ISAP program.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,"There is a risk that the Absconder Model may include false positives and model bias. A false positive would occur when non-absconders are incorrectly classified with a score indicating a high probability of absconding. Model bias can arise if the training data is not representative of the full range of participants, leading to poor performance on certain patterns or characteristics. These risks are identified and if necessary remediated through error analysis during model testing and model performance monitoring. ",Yes – by the CAIO,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,It is reevaluated and retrained on an annual basis.,Direct user testing,Yes,Rights-Impacting,0.746031746031746,"The Immigration and Customs Enforcement (ICE) Enforcement and Removal Operations (ERO) division would like to explore technology that could help lead to time savings and eliminate human error due to overburdened officers.  An AI generated hurricane score could also minimize the risk that a factor is overlooked due to officers being overburdened.  The hurricane score may be one factor that is used to inform the decision-making process. The “Hurricane Score” models the potential risk (1-5) that a noncitizen who is released from detention with the requirement to check in with ICE through monitoring will abscond from the program, with a higher number indicating a higher risk of absconding. . The Hurricane score model determines a probability of absconding and outputs a number (1-5), with a higher number indicating a higher risk of absconding. This number is used as one of many factors an officer uses when reviewing an ATD-ISAP case to determine the appropriate level of case management and technology for a noncitizen. . There is a risk that the Absconder Model may include false positives and model bias. A false positive would occur when non-absconders are incorrectly classified with a score indicating a high probability of absconding. Model bias can arise if the training data is not representative of the full range of participants, leading to poor performance on certain patterns or characteristics. These risks are identified and if necessary remediated through error analysis during model testing and model performance monitoring.","the immigration and customs enforcement (ice) enforcement and removal operations (ero) division would like to explore technology that could help lead to time savings and eliminate human error due to overburdened officers. an ai generated hurricane score could also minimize the risk that a factor is overlooked due to officers being overburdened. the hurricane score may be one factor that is used to inform the decision-making process. the “hurricane score” models the potential risk (1-5) that a noncitizen who is released from detention with the requirement to check in with ice through monitoring will abscond from the program, with a higher number indicating a higher risk of absconding. . the hurricane score model determines a probability of absconding and outputs a number (1-5), with a higher number indicating a higher risk of absconding. this number is used as one of many factors an officer uses when reviewing an atd-isap case to determine the appropriate level of case management and technology for a noncitizen. . there is a risk that the absconder model may include false positives and model bias. a false positive would occur when non-absconders are incorrectly classified with a score indicating a high probability of absconding. model bias can arise if the training data is not representative of the full range of participants, leading to poor performance on certain patterns or characteristics. these risks are identified and if necessary remediated through error analysis during model testing and model performance monitoring."
ICE Mobile Check-in Application,Department of Homeland Security,DHS,ICE,Law & Justice,None of the above.,"The Check-In App is an opt-in capability that allows for low-risk adult noncitizens to securely check-in via an ICE Verified user account, verify their identity, identify their location, and satisfy check-in requirements thereby reducing in-person office visits and reducing manual processing work. AI-enabled capabilities within the ICE Check-In application give ICE the ability to scale its interactions to a greater number of noncitizens and reduce in-office presence at ICE facilities.","AI/ML is used for this face detection and tracking. Once captured, the photo is verified with a 1:1 verification using the CBP Traveler Verification Service. Verification of all requirements will satisfy check-in requirements. A failed verification will result in a review by an officer. The officer can request the user try again or request the user use the default in-person check-in process by making an appointment with their local ICE office.",Operation and Maintenance,"Rights-Impacting
",7/10/2023,10/2/2023,11/30/2023,Developed with contracting resources.,70CTD022D00000010,No,No,Yes,Yes,Yes,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,ICE Portal,6-12 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,"There is a risk that the face detection model testing will show the model performs poorly for certain performance metrics that cannot be remediated through software configuration or other reasonable means. In that scenario, ICE will either pursue using a different face detection model or pursue additional testing to uncover the root cause of poor performance.",Yes – by the CAIO,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,"None for liveness detection using Google ML Kit. For facial verification, ICE leverages CBP TVS and inherits their controls.",Direct user testing,Yes,Rights-Impacting,0.7301587301587301,"The Check-In App is an opt-in capability that allows for low-risk adult noncitizens to securely check-in via an ICE Verified user account, verify their identity, identify their location, and satisfy check-in requirements thereby reducing in-person office visits and reducing manual processing work. AI-enabled capabilities within the ICE Check-In application give ICE the ability to scale its interactions to a greater number of noncitizens and reduce in-office presence at ICE facilities. . AI/ML is used for this face detection and tracking. Once captured, the photo is verified with a 1:1 verification using the CBP Traveler Verification Service. Verification of all requirements will satisfy check-in requirements. A failed verification will result in a review by an officer. The officer can request the user try again or request the user use the default in-person check-in process by making an appointment with their local ICE office. . There is a risk that the face detection model testing will show the model performs poorly for certain performance metrics that cannot be remediated through software configuration or other reasonable means. In that scenario, ICE will either pursue using a different face detection model or pursue additional testing to uncover the root cause of poor performance.","the check-in app is an opt-in capability that allows for low-risk adult noncitizens to securely check-in via an ice verified user account, verify their identity, identify their location, and satisfy check-in requirements thereby reducing in-person office visits and reducing manual processing work. ai-enabled capabilities within the ice check-in application give ice the ability to scale its interactions to a greater number of noncitizens and reduce in-office presence at ice facilities. . ai/ml is used for this face detection and tracking. once captured, the photo is verified with a 1:1 verification using the cbp traveler verification service. verification of all requirements will satisfy check-in requirements. a failed verification will result in a review by an officer. the officer can request the user try again or request the user use the default in-person check-in process by making an appointment with their local ice office. . there is a risk that the face detection model testing will show the model performs poorly for certain performance metrics that cannot be remediated through software configuration or other reasonable means. in that scenario, ice will either pursue using a different face detection model or pursue additional testing to uncover the root cause of poor performance."
AI Assisted Compromise Email Detector (AACED),Department of Homeland Security,DHS,ICE,Mission-Enabling,None of the above.,"The use case was developed to assist ICE SOC in reviewing a collection of emails between ICE personnel and Microsoft that were part of ED 24-02.  The use case provides a faster mechanism to the SOC analysts to determine indicators of compromise, reducing the level of effort for these individuals’ analysis exponentially.  To assist the analysts, Named Entity Recognition (NER) was used to detect PII and other associated keywords to increase analyst productivity, and reduce time required to analyze emails.
",Outputs are named entities and generated text for specific questions. Chat interface for analyst to conduct Q&A with email as context.,Operation and Maintenance,Neither,5/1/2024,5/1/2024,6/1/2024,Developed in-house.,Unknown,No,No,Yes,No,Yes,Stored Agency emails used for validation.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The use case was developed to assist ICE SOC in reviewing a collection of emails between ICE personnel and Microsoft that were part of ED 24-02.  The use case provides a faster mechanism to the SOC analysts to determine indicators of compromise, reducing the level of effort for these individuals’ analysis exponentially.  To assist the analysts, Named Entity Recognition (NER) was used to detect PII and other associated keywords to increase analyst productivity, and reduce time required to analyze emails. . Outputs are named entities and generated text for specific questions. Chat interface for analyst to conduct Q&A with email as context.","the use case was developed to assist ice soc in reviewing a collection of emails between ice personnel and microsoft that were part of ed 24-02. the use case provides a faster mechanism to the soc analysts to determine indicators of compromise, reducing the level of effort for these individuals’ analysis exponentially. to assist the analysts, named entity recognition (ner) was used to detect pii and other associated keywords to increase analyst productivity, and reduce time required to analyze emails. . outputs are named entities and generated text for specific questions. chat interface for analyst to conduct q&a with email as context."
Intelligent Document Processing for Workflow Automation,Department of Homeland Security,DHS,ICE,Mission-Enabling,None of the above.,"Business units within ICE leverage these services to automate repeatable, time-consuming processes such as invoice processing, and form entry validation and extraction. This platform will provide Optical Character Recognition and machine learning models to verify, extract, and classify information from ICE forms. Using AI to provide information extraction for these processes saves ICE personnel a significant amount of time while improving data quality and enabling automation.
","This platform will provide Optical Character Recognition and machine learning models to verify, extract, and classify information from ICE forms.",Operation and Maintenance,Neither,1/1/2019,1/1/2019,6/1/2019,Developed with contracting resources.,70CTD021FR0000248; 70CDCR24FR0000048; 70CMSD21FR0000169; 70CTD021FR0000248; 70CTD023FR0000003; 70CTD020FR0000006; 70CTD022FR0000005,No,No,Yes,No,Yes,"ICE’s document understanding platforms provide out-of-the box models to verify and/or extract information from a variety of document types. These platforms also include the ability to create an ML feedback loop to tailor the models to try to improve the accuracy in extracting data fields from multiple document types. In this scenario, ICE would fine-tune the document understanding model using the documents submitted to the understanding workflow.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,"UiPath Suite, Azure AI Document Intelligence",More than 12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Business units within ICE leverage these services to automate repeatable, time-consuming processes such as invoice processing, and form entry validation and extraction. This platform will provide Optical Character Recognition and machine learning models to verify, extract, and classify information from ICE forms. Using AI to provide information extraction for these processes saves ICE personnel a significant amount of time while improving data quality and enabling automation. . This platform will provide Optical Character Recognition and machine learning models to verify, extract, and classify information from ICE forms.","business units within ice leverage these services to automate repeatable, time-consuming processes such as invoice processing, and form entry validation and extraction. this platform will provide optical character recognition and machine learning models to verify, extract, and classify information from ice forms. using ai to provide information extraction for these processes saves ice personnel a significant amount of time while improving data quality and enabling automation. . this platform will provide optical character recognition and machine learning models to verify, extract, and classify information from ice forms."
"Cybersecurity Threat Management, Detection, and Response",Department of Homeland Security,DHS,ICE,Mission-Enabling,None of the above.,"Machine Learning (ML) is used to analyze historical ICE cybersecurity data, create activity baselines, and detect anomalous behaviors for alerting. By integrating ML with other advanced analytics and correlation rules, AI identifies patterns indicative of security incidents, enabling SOC analysts to detect and respond to threats more quickly than previously possible.",The output of the analytics will be anomaly detection from patterns in security tool data correlated into alerts for SOC analysts.,Operation and Maintenance,Neither,5/21/2020,9/17/2020,3/16/2021,Developed in-house.,Unknown,No,No,No,Yes,No,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,Yes,"Xploitation Platform Leveraging Offensive Information Technology, Phishing Defense Solution, Audit Log Management Solution (ALMS), Endpoint Detection and Response, EMVEC",Less than 6 months,No,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Machine Learning (ML) is used to analyze historical ICE cybersecurity data, create activity baselines, and detect anomalous behaviors for alerting. By integrating ML with other advanced analytics and correlation rules, AI identifies patterns indicative of security incidents, enabling SOC analysts to detect and respond to threats more quickly than previously possible. . The output of the analytics will be anomaly detection from patterns in security tool data correlated into alerts for SOC analysts.","machine learning (ml) is used to analyze historical ice cybersecurity data, create activity baselines, and detect anomalous behaviors for alerting. by integrating ml with other advanced analytics and correlation rules, ai identifies patterns indicative of security incidents, enabling soc analysts to detect and respond to threats more quickly than previously possible. . the output of the analytics will be anomaly detection from patterns in security tool data correlated into alerts for soc analysts."
Translation and Transcription for Investigative Data,Department of Homeland Security,DHS,ICE,Law & Justice,None of the above.,"HSI investigators often encounter data from various sources, including legal and administrative processes, enforcement actions, and open-source materials, in languages other than English. To unlock the value of this data, it must be translated into English before further analysis can be conducted. The Translation and Transcription Service leverages neural machine translation (NMT) models for text translation and automatic speech recognition (ASR) and deep neural network (DNN) models with normalization for voice-to-text transcription. This innovative approach enables users to quickly triage large datasets and identify key information relevant to investigations. Any data deemed critical for court proceedings is then submitted to certified human translators for final review, ensuring that government resources are allocated efficiently and only used for necessary translations and transcriptions.
","The Translation and Transcription Service leverages neural machine translation (NMT) models for text translation and automatic speech recognition (ASR) and deep neural network (DNN) models with normalization for voice-to-text transcription.
",Operation and Maintenance,Neither,10/1/2022,10/1/2022,2/1/2024,Developed with contracting resources.,"70OCTD22FR00000002, 70OCTD22FR00000235, 70OCTD22FR00000009",No,No,Yes,Yes,Yes,"AI models within the use case are not trained on agency data. Open-source models (i) Whisper is trained on 680K hours of multilingual and multitask supervised data collected from the web, (ii) No Language Left Behind (NLLB) is trained on a combination of publicly available datasets (additional information available in Section 5 of Meta’s NLLB whitepaper: https://research.facebook.com/file/585831413174038/No-Language-Left-Behind--Scaling-Human-Centered-Machine-Translation.pdf). ","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,"RAVEn, Azure AI Translator",Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"HSI investigators often encounter data from various sources, including legal and administrative processes, enforcement actions, and open-source materials, in languages other than English. To unlock the value of this data, it must be translated into English before further analysis can be conducted. The Translation and Transcription Service leverages neural machine translation (NMT) models for text translation and automatic speech recognition (ASR) and deep neural network (DNN) models with normalization for voice-to-text transcription. This innovative approach enables users to quickly triage large datasets and identify key information relevant to investigations. Any data deemed critical for court proceedings is then submitted to certified human translators for final review, ensuring that government resources are allocated efficiently and only used for necessary translations and transcriptions. . The Translation and Transcription Service leverages neural machine translation (NMT) models for text translation and automatic speech recognition (ASR) and deep neural network (DNN) models with normalization for voice-to-text transcription.","hsi investigators often encounter data from various sources, including legal and administrative processes, enforcement actions, and open-source materials, in languages other than english. to unlock the value of this data, it must be translated into english before further analysis can be conducted. the translation and transcription service leverages neural machine translation (nmt) models for text translation and automatic speech recognition (asr) and deep neural network (dnn) models with normalization for voice-to-text transcription. this innovative approach enables users to quickly triage large datasets and identify key information relevant to investigations. any data deemed critical for court proceedings is then submitted to certified human translators for final review, ensuring that government resources are allocated efficiently and only used for necessary translations and transcriptions. . the translation and transcription service leverages neural machine translation (nmt) models for text translation and automatic speech recognition (asr) and deep neural network (dnn) models with normalization for voice-to-text transcription."
Biometric Check-in for ATD-ISAP (SmartLINK),Department of Homeland Security,DHS,ICE,Law & Justice,None of the above.,"ISAP Biometric Monitoring App is a technology option that allows participants to report in using a smartphone. This app verifies a participant’s identity, determine their location, and quickly collect status change information. The app adds functionality not available with telephonic and is less intrusive than a GPS unit. ISAP monitoring app limits in-person interactions of routine check-ins, allowing more time to be allocated to non-compliant participants, complex removal proceedings cases and docket management.","There are two outputs related to using ISAP Biometric Monitoring App. Either a participant “passes” (biometric match) or the photo is moved to a “pending review” status.  In either scenario, a human can evaluate the response.",Operation and Maintenance,"Rights-Impacting
",2/1/2018,2/1/2018,2/1/2018,Developed with contracting resources.,70CDCR20D00000011,No,No,Yes,Yes,No,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,Yes,ISAP IV (SmartLink) IATSS,Less than 6 months,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Yes,"Environmental variables may negatively impact model performance. At enrollment, participants meet with a Case Manager, who explains the program in the participant’s preferred language. Participants receive guidance on the use of the system and environmental parameters which may affect results (e.g. poor lighting, busy backgrounds). All nonmatches are reviewed by human examiner and considered for a redo before the check in is officially marked as a nonmatch.  ",Yes – by the CAIO,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,"There are two outputs related to using ISAP Biometric Monitoring App. Either a participant “passes” (biometric match) or the photo is moved to a “pending review” status.  In either scenario, a human can evaluate the response.",None of the above,No – it is not operationally practical to offer this.,Rights-Impacting,0.746031746031746,"ISAP Biometric Monitoring App is a technology option that allows participants to report in using a smartphone. This app verifies a participant’s identity, determine their location, and quickly collect status change information. The app adds functionality not available with telephonic and is less intrusive than a GPS unit. ISAP monitoring app limits in-person interactions of routine check-ins, allowing more time to be allocated to non-compliant participants, complex removal proceedings cases and docket management. . There are two outputs related to using ISAP Biometric Monitoring App. Either a participant “passes” (biometric match) or the photo is moved to a “pending review” status.  In either scenario, a human can evaluate the response. . Environmental variables may negatively impact model performance. At enrollment, participants meet with a Case Manager, who explains the program in the participant’s preferred language. Participants receive guidance on the use of the system and environmental parameters which may affect results (e.g. poor lighting, busy backgrounds). All nonmatches are reviewed by human examiner and considered for a redo before the check in is officially marked as a nonmatch.","isap biometric monitoring app is a technology option that allows participants to report in using a smartphone. this app verifies a participant’s identity, determine their location, and quickly collect status change information. the app adds functionality not available with telephonic and is less intrusive than a gps unit. isap monitoring app limits in-person interactions of routine check-ins, allowing more time to be allocated to non-compliant participants, complex removal proceedings cases and docket management. . there are two outputs related to using isap biometric monitoring app. either a participant “passes” (biometric match) or the photo is moved to a “pending review” status. in either scenario, a human can evaluate the response. . environmental variables may negatively impact model performance. at enrollment, participants meet with a case manager, who explains the program in the participant’s preferred language. participants receive guidance on the use of the system and environmental parameters which may affect results (e.g. poor lighting, busy backgrounds). all nonmatches are reviewed by human examiner and considered for a redo before the check in is officially marked as a nonmatch."
Email Analytics for Investigative Data,Department of Homeland Security,DHS,ICE,Law & Justice,None of the above.,"HSI personnel encounter large volumes of legally acquired, multilingual email data that must be prepared (ingested, triaged, translated, searched and filtered) before it can be analyzed to support investigations. The Email Analytics application eliminates manual data preparation processes, and leverages machine learning to conduct spam message classification, translation, and entity extraction, including names, organizations, or locations. It also utilizes HSI's AI-enabled translation capabilities (see related use case “Translation and Transcription for Investigative Data”) for translation of emails in other languages to English. The output reduces time and resources spent preparing data, increases the analytic utility of the data, and allows HSI personnel to more quickly conduct analysis on the information.","The output is email data that has undergone spam message classification, translation, and entity extraction, including names, organizations, or locations.",Operation and Maintenance,Neither,7/1/2019,8/1/2021,8/1/2021,Developed with contracting resources.,"70OCTD22FR00000002, 70OCTD22FR00000235, 70OCTD22FR00000009",No,No,Yes,Yes,Yes,Lawfully obtained email data.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Repository for Analytics in a Virtualized Environment,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"HSI personnel encounter large volumes of legally acquired, multilingual email data that must be prepared (ingested, triaged, translated, searched and filtered) before it can be analyzed to support investigations. The Email Analytics application eliminates manual data preparation processes, and leverages machine learning to conduct spam message classification, translation, and entity extraction, including names, organizations, or locations. It also utilizes HSI's AI-enabled translation capabilities (see related use case “Translation and Transcription for Investigative Data”) for translation of emails in other languages to English. The output reduces time and resources spent preparing data, increases the analytic utility of the data, and allows HSI personnel to more quickly conduct analysis on the information. . The output is email data that has undergone spam message classification, translation, and entity extraction, including names, organizations, or locations.","hsi personnel encounter large volumes of legally acquired, multilingual email data that must be prepared (ingested, triaged, translated, searched and filtered) before it can be analyzed to support investigations. the email analytics application eliminates manual data preparation processes, and leverages machine learning to conduct spam message classification, translation, and entity extraction, including names, organizations, or locations. it also utilizes hsi's ai-enabled translation capabilities (see related use case “translation and transcription for investigative data”) for translation of emails in other languages to english. the output reduces time and resources spent preparing data, increases the analytic utility of the data, and allows hsi personnel to more quickly conduct analysis on the information. . the output is email data that has undergone spam message classification, translation, and entity extraction, including names, organizations, or locations."
Mobile Device Analytics for Investigative Data,Department of Homeland Security,DHS,ICE,Law & Justice,None of the above.,"Mobile device analytics capabilities were developed to meet the growing need for investigators to quickly and effectively review and analyze the massive amounts of data extracted from mobile devices obtained in court-ordered seizures through a warrant or other investigative due process. The solution empowers investigators and analysts to identify and extract critical evidence, relationships, and networks from mobile device data, leveraging machine learning capabilities to determine locations of interest. Present capabilities are limited to the use of an algorithm for clustering of geo-location data, allowing investigators to identify 'stops' and 'overlaps' of locations from lawfully seized phones, where a 'stop' is a location a phone may have remained for a period of time and an 'overlap' is a location where multiple phones may have visited. By streamlining the analysis process, these capabilities aim to enhance the efficacy of HSI agents and analysts by providing advanced data processing to identify the most relevant information for lead generation in a shorter amount of time than manually processing, ultimately contributing to the disruption and dismantling of criminal networks and the protection of national security.","Present capabilities are limited to the use of an algorithm for clustering of geo-location data, allowing investigators to identify 'stops' and 'overlaps' of locations from lawfully seized phones, where a 'stop' is a location a phone may have remained for a period of time and an 'overlap' is a location where multiple phones may have visited.",Operation and Maintenance,Neither,6/1/2019,10/15/2021,9/1/2022,Developed with contracting resources.,"70OCTD22FR00000002, 70OCTD22FR00000235, 70OCTD22FR00000009",No,No,Yes,Yes,Yes,"Lawfully obtained mobile device extraction data, including location data.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Repository for Analytics in a Virtualized Environment,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Mobile device analytics capabilities were developed to meet the growing need for investigators to quickly and effectively review and analyze the massive amounts of data extracted from mobile devices obtained in court-ordered seizures through a warrant or other investigative due process. The solution empowers investigators and analysts to identify and extract critical evidence, relationships, and networks from mobile device data, leveraging machine learning capabilities to determine locations of interest. Present capabilities are limited to the use of an algorithm for clustering of geo-location data, allowing investigators to identify 'stops' and 'overlaps' of locations from lawfully seized phones, where a 'stop' is a location a phone may have remained for a period of time and an 'overlap' is a location where multiple phones may have visited. By streamlining the analysis process, these capabilities aim to enhance the efficacy of HSI agents and analysts by providing advanced data processing to identify the most relevant information for lead generation in a shorter amount of time than manually processing, ultimately contributing to the disruption and dismantling of criminal networks and the protection of national security. . Present capabilities are limited to the use of an algorithm for clustering of geo-location data, allowing investigators to identify 'stops' and 'overlaps' of locations from lawfully seized phones, where a 'stop' is a location a phone may have remained for a period of time and an 'overlap' is a location where multiple phones may have visited.","mobile device analytics capabilities were developed to meet the growing need for investigators to quickly and effectively review and analyze the massive amounts of data extracted from mobile devices obtained in court-ordered seizures through a warrant or other investigative due process. the solution empowers investigators and analysts to identify and extract critical evidence, relationships, and networks from mobile device data, leveraging machine learning capabilities to determine locations of interest. present capabilities are limited to the use of an algorithm for clustering of geo-location data, allowing investigators to identify 'stops' and 'overlaps' of locations from lawfully seized phones, where a 'stop' is a location a phone may have remained for a period of time and an 'overlap' is a location where multiple phones may have visited. by streamlining the analysis process, these capabilities aim to enhance the efficacy of hsi agents and analysts by providing advanced data processing to identify the most relevant information for lead generation in a shorter amount of time than manually processing, ultimately contributing to the disruption and dismantling of criminal networks and the protection of national security. . present capabilities are limited to the use of an algorithm for clustering of geo-location data, allowing investigators to identify 'stops' and 'overlaps' of locations from lawfully seized phones, where a 'stop' is a location a phone may have remained for a period of time and an 'overlap' is a location where multiple phones may have visited."
Identification Card and Travel Document Code Detection,Department of Homeland Security,DHS,ICE,Law & Justice,None of the above.,"The Identification Card and Travel Document Code Detection service leverages mobile phone cameras to quickly and accurately read 2D Data Matrix Codes on US Driver's Licenses and ID cards, as well as Machine Readable Zones (MRZ) on travel documents like Passports and Passport cards. This innovative solution utilizes machine learning to enhance the detection of these codes, allowing for seamless scanning and automatic population of detected information into corresponding text fields within HSI mobile apps. By streamlining the code recognition process, the service significantly reduces the time required for users to read cards or documents, eliminating the need for manual data entry and resulting in more efficient, in-person interactions with individuals.","The output is a dataset of structured identification data.
",Operation and Maintenance,Neither,6/1/2020,11/1/2020,6/1/2021,Developed with contracting resources.,"70OCTD22FR00000002, 70OCTD22FR00000235, 70OCTD22FR00000009",No,No,Yes,Yes,Yes,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Repository for Analytics in a Virtualized Environment,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The Identification Card and Travel Document Code Detection service leverages mobile phone cameras to quickly and accurately read 2D Data Matrix Codes on US Driver's Licenses and ID cards, as well as Machine Readable Zones (MRZ) on travel documents like Passports and Passport cards. This innovative solution utilizes machine learning to enhance the detection of these codes, allowing for seamless scanning and automatic population of detected information into corresponding text fields within HSI mobile apps. By streamlining the code recognition process, the service significantly reduces the time required for users to read cards or documents, eliminating the need for manual data entry and resulting in more efficient, in-person interactions with individuals. . The output is a dataset of structured identification data.","the identification card and travel document code detection service leverages mobile phone cameras to quickly and accurately read 2d data matrix codes on us driver's licenses and id cards, as well as machine readable zones (mrz) on travel documents like passports and passport cards. this innovative solution utilizes machine learning to enhance the detection of these codes, allowing for seamless scanning and automatic population of detected information into corresponding text fields within hsi mobile apps. by streamlining the code recognition process, the service significantly reduces the time required for users to read cards or documents, eliminating the need for manual data entry and resulting in more efficient, in-person interactions with individuals. . the output is a dataset of structured identification data."
Normalization Services,Department of Homeland Security,DHS,ICE,Law & Justice,None of the above.,"HSI utilizes artificial intelligence to enhance data accuracy and efficiency by verifying, validating, correcting, and normalizing various types of information, including addresses, phone numbers, names, and ID numbers. This process helps to eliminate data entry errors, detect intentional misidentification, and connect related information across multiple datasets, ultimately reducing the time and resources required for investigations. The machine learning-powered normalization services offered by HSI include converting ambiguous addresses into usable formats, identifying ID types from partial information, categorizing names with complex suffixes and family names, and standardizing phone numbers to the E164 format, including determining their originating county. By normalizing and improving the quality of investigative datasets, HSI is able to use more advanced tools to find correlations and leads that would have otherwise gone undetected without extensive manual effort.","The output includes normalized data that improves search capability during investigations. This includes normalizing data to update less well-defined addresses into usable addresses for analysis- (such as those using mile markers instead of a street number); inferring ID type based on user-provided ID value (such as distinguishing a SSN from a DL number without additional context); categorizing name parts while taking into account additional factors (including generational suffixes and multi-part family names); and validating and normalizing phone numbers to the E164 standard, including their identified county of origin.",Operation and Maintenance,Neither,6/1/2019,10/1/2019,4/1/2021,Developed with contracting resources.,"70OCTD22FR00000002, 70OCTD22FR00000235, 70OCTD22FR00000009",No,No,Yes,Yes,Yes,"Data holdings within HSI case files that require normalization (e.g. subpoenaed phone record). This includes, but is not limited to, evidentiary records containing phone numbers, names, addresses, and ID numbers.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Repository for Analytics in a Virtualized Environment,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"HSI utilizes artificial intelligence to enhance data accuracy and efficiency by verifying, validating, correcting, and normalizing various types of information, including addresses, phone numbers, names, and ID numbers. This process helps to eliminate data entry errors, detect intentional misidentification, and connect related information across multiple datasets, ultimately reducing the time and resources required for investigations. The machine learning-powered normalization services offered by HSI include converting ambiguous addresses into usable formats, identifying ID types from partial information, categorizing names with complex suffixes and family names, and standardizing phone numbers to the E164 format, including determining their originating county. By normalizing and improving the quality of investigative datasets, HSI is able to use more advanced tools to find correlations and leads that would have otherwise gone undetected without extensive manual effort. . The output includes normalized data that improves search capability during investigations. This includes normalizing data to update less well-defined addresses into usable addresses for analysis- (such as those using mile markers instead of a street number); inferring ID type based on user-provided ID value (such as distinguishing a SSN from a DL number without additional context); categorizing name parts while taking into account additional factors (including generational suffixes and multi-part family names); and validating and normalizing phone numbers to the E164 standard, including their identified county of origin.","hsi utilizes artificial intelligence to enhance data accuracy and efficiency by verifying, validating, correcting, and normalizing various types of information, including addresses, phone numbers, names, and id numbers. this process helps to eliminate data entry errors, detect intentional misidentification, and connect related information across multiple datasets, ultimately reducing the time and resources required for investigations. the machine learning-powered normalization services offered by hsi include converting ambiguous addresses into usable formats, identifying id types from partial information, categorizing names with complex suffixes and family names, and standardizing phone numbers to the e164 format, including determining their originating county. by normalizing and improving the quality of investigative datasets, hsi is able to use more advanced tools to find correlations and leads that would have otherwise gone undetected without extensive manual effort. . the output includes normalized data that improves search capability during investigations. this includes normalizing data to update less well-defined addresses into usable addresses for analysis- (such as those using mile markers instead of a street number); inferring id type based on user-provided id value (such as distinguishing a ssn from a dl number without additional context); categorizing name parts while taking into account additional factors (including generational suffixes and multi-part family names); and validating and normalizing phone numbers to the e164 standard, including their identified county of origin."
JES and Appropriations Insight,Department of Homeland Security,DHS,MGMT,Mission-Enabling,None of the above.,"Purpose: Converts scanned financial tables from PDFs into structured, machine-readable data while maintaining multi-year spending relationships.
Benefits: Eliminates manual data entry, reduces errors, and significantly speeds up the process of consolidating historical financial data from legacy documents.",Structured tables in place of free text to provide a machine-readable dataset.,Acquisition and/or Development,Neither,3/1/2024,4/1/2024,Unknown,Developed with contracting resources.,70RDAD22FR0000064,No,Unknown,Unknown,Unknown,Other,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"Purpose: Converts scanned financial tables from PDFs into structured, machine-readable data while maintaining multi-year spending relationships.
Benefits: Eliminates manual data entry, reduces errors, and significantly speeds up the process of consolidating historical financial data from legacy documents. . Structured tables in place of free text to provide a machine-readable dataset.","purpose: converts scanned financial tables from pdfs into structured, machine-readable data while maintaining multi-year spending relationships. benefits: eliminates manual data entry, reduces errors, and significantly speeds up the process of consolidating historical financial data from legacy documents. . structured tables in place of free text to provide a machine-readable dataset."
CFO Companion,Department of Homeland Security,DHS,MGMT,Mission-Enabling,None of the above.,"Provides authorized staff with an intuitive, conversational interface to query and analyze DHS CFO financial data and reports.
Benefits: Democratizes access to financial information, reduces time spent searching through documents, and enables quick self-service analytics without specialized database knowledge.",On-demand information retrieval via natural language processing.,Acquisition and/or Development,Neither,4/1/2024,4/8/2024,Unknown,Developed with contracting resources.,70RDAD22FR0000064,No,Unknown,Unknown,Unknown,Other,"Internal system and process information, such as the link to a visualization, or data attribute information tied to publicly available finanacial data.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,Yes,CFO Horizon,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"Provides authorized staff with an intuitive, conversational interface to query and analyze DHS CFO financial data and reports.
Benefits: Democratizes access to financial information, reduces time spent searching through documents, and enables quick self-service analytics without specialized database knowledge. . On-demand information retrieval via natural language processing.","provides authorized staff with an intuitive, conversational interface to query and analyze dhs cfo financial data and reports. benefits: democratizes access to financial information, reduces time spent searching through documents, and enables quick self-service analytics without specialized database knowledge. . on-demand information retrieval via natural language processing."
Climate Change Assessment Tool,Department of Homeland Security,DHS,MGMT,Mission-Enabling,None of the above.,"Our Nationis already experiencing the adverse impacts from climate change, from sea-level rise, extreme heat, flooding, and drought, to changes in migration patterns and harmful effects on workforce health. DHS needs to be able to better understand and predict where extreme weather conditions will affect DHS assets, including our facilities and our workforce, in the near and long-term. In support of the DHS Climate Action Plan, this AI pilot will enable the detection and streamliinng the assessment of cost/benefit data driven decisions like the location of facilities, prioritization of O&M projects and the health and safety of the DHS workforce. 

Benefits include cost savings, time savings and improved access to critical data and ability to make decisions, improved health and safety of the workforce.",Outputs include the prediction and early detection of extreme weather risks and an integrated understanding of the affects this will have on critical DHS resources. A secondary output is cost-benefit recommendations.,Acquisition and/or Development,Neither,3/1/2024,11/19/2024,Unknown,Developed with both contracting and in-house resources.,70RDAD23FR0000148,No,Unknown,Unknown,Unknown,Yes,"DHS Real Property/Facility Data, DHS Employee and Contractor Data, DHS Facility utilization (e.g. energy usage) Data","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Unknown,Unknown,Yes,Management Cube (MGMT Cube),More than 12 months,Yes,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"Our Nationis already experiencing the adverse impacts from climate change, from sea-level rise, extreme heat, flooding, and drought, to changes in migration patterns and harmful effects on workforce health. DHS needs to be able to better understand and predict where extreme weather conditions will affect DHS assets, including our facilities and our workforce, in the near and long-term. In support of the DHS Climate Action Plan, this AI pilot will enable the detection and streamliinng the assessment of cost/benefit data driven decisions like the location of facilities, prioritization of O&M projects and the health and safety of the DHS workforce. 

Benefits include cost savings, time savings and improved access to critical data and ability to make decisions, improved health and safety of the workforce. . Outputs include the prediction and early detection of extreme weather risks and an integrated understanding of the affects this will have on critical DHS resources. A secondary output is cost-benefit recommendations.","our nationis already experiencing the adverse impacts from climate change, from sea-level rise, extreme heat, flooding, and drought, to changes in migration patterns and harmful effects on workforce health. dhs needs to be able to better understand and predict where extreme weather conditions will affect dhs assets, including our facilities and our workforce, in the near and long-term. in support of the dhs climate action plan, this ai pilot will enable the detection and streamliinng the assessment of cost/benefit data driven decisions like the location of facilities, prioritization of o&m projects and the health and safety of the dhs workforce. benefits include cost savings, time savings and improved access to critical data and ability to make decisions, improved health and safety of the workforce. . outputs include the prediction and early detection of extreme weather risks and an integrated understanding of the affects this will have on critical dhs resources. a secondary output is cost-benefit recommendations."
Spending Analysis and Budget Execution Risk (SABER) Model,Department of Homeland Security,DHS,MGMT,Mission-Enabling,None of the above.,"Purpose: Predicts potential budget execution issues by analyzing historical spending patterns across various Treasury accounts and classifications.
Benefits: Early identification of spending anomalies allows proactive budget management and reduces the risk of under/overspending.","Warnings, flags for review and comparison via prediction and classification.",Acquisition and/or Development,Neither,1/22/2024,2/19/2024,Unknown,Developed with contracting resources.,70RDAD22FR0000064,No,Unknown,Unknown,Unknown,Other,Treasury data with slight report-centric fields added. The data is publicly available.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"Purpose: Predicts potential budget execution issues by analyzing historical spending patterns across various Treasury accounts and classifications.
Benefits: Early identification of spending anomalies allows proactive budget management and reduces the risk of under/overspending. . Warnings, flags for review and comparison via prediction and classification.","purpose: predicts potential budget execution issues by analyzing historical spending patterns across various treasury accounts and classifications. benefits: early identification of spending anomalies allows proactive budget management and reduces the risk of under/overspending. . warnings, flags for review and comparison via prediction and classification."
AdaptiveMFA,Department of Homeland Security,DHS,MGMT,Mission-Enabling,None of the above.,"Adaptive Multi-factor Authentication (AMFA) introduces additional intelligence into Identity flows by taking into account the authentication context data during the authentication. Using the data, DHS is able to adapt security and authentication policies to enhance the security to DHS systems.","The input includes: device, network, location, travel, IP, and external data from third parties and endpoint security integrations. The output are Risk ratings HIGH, MEDIUM, LOW for each authentication attempt which can be configigured to require stricted access comtrol policies.",Acquisition and/or Development,Neither,3/1/2024,3/30/2024,Unknown,Developed with contracting resources.,47QFRA24F0005,No,Unknown,Unknown,Unknown,No,The system collects signal data during authentication to dynamically build behaviors of the user.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,Yes,The DHS Next Generation Authentication as a Service (DHSAuthPortal) System,More than 12 months,Yes,No,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"Adaptive Multi-factor Authentication (AMFA) introduces additional intelligence into Identity flows by taking into account the authentication context data during the authentication. Using the data, DHS is able to adapt security and authentication policies to enhance the security to DHS systems. . The input includes: device, network, location, travel, IP, and external data from third parties and endpoint security integrations. The output are Risk ratings HIGH, MEDIUM, LOW for each authentication attempt which can be configigured to require stricted access comtrol policies.","adaptive multi-factor authentication (amfa) introduces additional intelligence into identity flows by taking into account the authentication context data during the authentication. using the data, dhs is able to adapt security and authentication policies to enhance the security to dhs systems. . the input includes: device, network, location, travel, ip, and external data from third parties and endpoint security integrations. the output are risk ratings high, medium, low for each authentication attempt which can be configigured to require stricted access comtrol policies."
ESEC Inquiry (STORM) Summarization,Department of Homeland Security,DHS,MGMT,Mission-Enabling,None of the above.,"ESEC-STORM AI employs Generative Artificial Intelligence (Gen-AI) technology to automate the creation of document summaries. This advanced system facilitates the automatic integration of these summaries into the System of Tracking, Operations, and Record Management (STORM), thereby optimizing the management of correspondence and information requests. ","When a user creates a new work package and uploads a letter, it activates a Power Apps workflow that creates a summary.",Acquisition and/or Development,Neither,3/1/2024,4/1/2024,12/13/2024,Developed in-house.,Unknown,No,No,No,No,Yes,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Dynamics Online As a Service; BIaaS,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"ESEC-STORM AI employs Generative Artificial Intelligence (Gen-AI) technology to automate the creation of document summaries. This advanced system facilitates the automatic integration of these summaries into the System of Tracking, Operations, and Record Management (STORM), thereby optimizing the management of correspondence and information requests. . When a user creates a new work package and uploads a letter, it activates a Power Apps workflow that creates a summary.","esec-storm ai employs generative artificial intelligence (gen-ai) technology to automate the creation of document summaries. this advanced system facilitates the automatic integration of these summaries into the system of tracking, operations, and record management (storm), thereby optimizing the management of correspondence and information requests. . when a user creates a new work package and uploads a letter, it activates a power apps workflow that creates a summary."
DHS-Chat,Department of Homeland Security,DHS,MGMT,Mission-Enabling,None of the above.,"This is a chatbot based on a Large Language Models (LLM) for internal DHS employee use - It's like chat-GPT for DHS but approved for use with non-classified but internal information (this includes FOUO (For Official Use Only) CUI (Controlled Unclassified Information) due to its improved security compared to publicly available chatbots. 

This tool is able to dynamically create written content through text prompts submitted by the user. Approved applications of this tool to DHS business include generating first drafts of documents that a human would subsequently review, conducting and synthesizing research on open-source information and internal documents, and developing briefing materials or preparing for meetings and events.",The internally available generative AI tool outputs text based on the users input.,Operation and Maintenance,Neither,10/1/2024,10/1/2024,12/12/2024,Developed in-house.,Unknown,No,No,No,Yes,No,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",No,(Pending),Less than 6 months,Yes,Other,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"This is a chatbot based on a Large Language Models (LLM) for internal DHS employee use - It's like chat-GPT for DHS but approved for use with non-classified but internal information (this includes FOUO (For Official Use Only) CUI (Controlled Unclassified Information) due to its improved security compared to publicly available chatbots. 

This tool is able to dynamically create written content through text prompts submitted by the user. Approved applications of this tool to DHS business include generating first drafts of documents that a human would subsequently review, conducting and synthesizing research on open-source information and internal documents, and developing briefing materials or preparing for meetings and events. . The internally available generative AI tool outputs text based on the users input.","this is a chatbot based on a large language models (llm) for internal dhs employee use - it's like chat-gpt for dhs but approved for use with non-classified but internal information (this includes fouo (for official use only) cui (controlled unclassified information) due to its improved security compared to publicly available chatbots. this tool is able to dynamically create written content through text prompts submitted by the user. approved applications of this tool to dhs business include generating first drafts of documents that a human would subsequently review, conducting and synthesizing research on open-source information and internal documents, and developing briefing materials or preparing for meetings and events. . the internally available generative ai tool outputs text based on the users input."
User and Entity Behavior Analytics (UEBA),Department of Homeland Security,DHS,MGMT,Mission-Enabling,None of the above.,"Enhanced Threat Detection: Identify patterns in user behaviors that deviate from normal baselines, signaling potential insider threats or security risks. Continuous Risk Assessment: Move from static vetting to a continuous vetting process by monitoring real-time activities and interactions with secure information.
Improved Incident Response: Enable rapid responses to high-risk behaviors, escalating alerts for timely intervention by security personnel.""
","Behavioral Baseline Modeling: Develop a baseline for each individual based on regular access patterns, network usage, and interactions with classified data or secure areas.
Anomaly Detection: Employ machine learning models to detect deviations from established baselines, such as unusual access times, atypical access to high-sensitivity resources, or excessive data downloads.
Risk Scoring: Assign a risk score to each user based on observed anomalies, factoring in historical behavior, job role, and access level, allowing security teams to prioritize investigations.
Automated Alerts & Reporting: Generate automated alerts for high-risk behaviors or patterns of concern and deliver timely reports to personnel security teams for further investigation.""
",Operation and Maintenance,Neither,10/1/2020,4/3/2021,7/8/2023,Developed with both contracting and in-house resources.,"PIID Pending, not yet issued.",No,No,Yes,Yes,Yes,"PII, SPII , Clearance and Background Investigation Data: Data from security clearances, background investigations, and adjudication records.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Vetting Identities for an Enterprise Workforce,More than 12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Enhanced Threat Detection: Identify patterns in user behaviors that deviate from normal baselines, signaling potential insider threats or security risks. Continuous Risk Assessment: Move from static vetting to a continuous vetting process by monitoring real-time activities and interactions with secure information.
Improved Incident Response: Enable rapid responses to high-risk behaviors, escalating alerts for timely intervention by security personnel."" . Behavioral Baseline Modeling: Develop a baseline for each individual based on regular access patterns, network usage, and interactions with classified data or secure areas.
Anomaly Detection: Employ machine learning models to detect deviations from established baselines, such as unusual access times, atypical access to high-sensitivity resources, or excessive data downloads.
Risk Scoring: Assign a risk score to each user based on observed anomalies, factoring in historical behavior, job role, and access level, allowing security teams to prioritize investigations.
Automated Alerts & Reporting: Generate automated alerts for high-risk behaviors or patterns of concern and deliver timely reports to personnel security teams for further investigation.""","enhanced threat detection: identify patterns in user behaviors that deviate from normal baselines, signaling potential insider threats or security risks. continuous risk assessment: move from static vetting to a continuous vetting process by monitoring real-time activities and interactions with secure information. improved incident response: enable rapid responses to high-risk behaviors, escalating alerts for timely intervention by security personnel."" . behavioral baseline modeling: develop a baseline for each individual based on regular access patterns, network usage, and interactions with classified data or secure areas. anomaly detection: employ machine learning models to detect deviations from established baselines, such as unusual access times, atypical access to high-sensitivity resources, or excessive data downloads. risk scoring: assign a risk score to each user based on observed anomalies, factoring in historical behavior, job role, and access level, allowing security teams to prioritize investigations. automated alerts & reporting: generate automated alerts for high-risk behaviors or patterns of concern and deliver timely reports to personnel security teams for further investigation."""
Text Analytics for Survey Responses (TASR),Department of Homeland Security,DHS,MGMT,Mission-Enabling,None of the above.,"The intended purpose of the AI is to perform topic modeling,  sentiment analysis, or other text classification tasks on responses provided to internal staff DHS Pulse Survey questions.

Text Analytics for Survey Responses (TASR) is an application for performing Natural Language Processing (NLP) and text analytics on survey responses. It is currently being applied by DHS Office of the Chief Human Capital Officer (OCHCO) to analyze and extract significant topics/themes from unstructured text responses to open-ended questions in the quarterly DHS Pulse Surveys. Results of extracted topics/themes are provided to DHS Leadership to better inform agency-wide efforts to meet employees’ basic needs and improve job satisfaction","The systems outputs include a set of topics inferred or surfaced from the raw text comment data, as well as sentiments or other classifications inferred from the data.",Operation and Maintenance,Neither,10/1/2022,10/1/2022,11/1/2022,Developed in-house.,Unknown,No,No,No,Yes,No,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The intended purpose of the AI is to perform topic modeling,  sentiment analysis, or other text classification tasks on responses provided to internal staff DHS Pulse Survey questions.

Text Analytics for Survey Responses (TASR) is an application for performing Natural Language Processing (NLP) and text analytics on survey responses. It is currently being applied by DHS Office of the Chief Human Capital Officer (OCHCO) to analyze and extract significant topics/themes from unstructured text responses to open-ended questions in the quarterly DHS Pulse Surveys. Results of extracted topics/themes are provided to DHS Leadership to better inform agency-wide efforts to meet employees’ basic needs and improve job satisfaction . The systems outputs include a set of topics inferred or surfaced from the raw text comment data, as well as sentiments or other classifications inferred from the data.","the intended purpose of the ai is to perform topic modeling, sentiment analysis, or other text classification tasks on responses provided to internal staff dhs pulse survey questions. text analytics for survey responses (tasr) is an application for performing natural language processing (nlp) and text analytics on survey responses. it is currently being applied by dhs office of the chief human capital officer (ochco) to analyze and extract significant topics/themes from unstructured text responses to open-ended questions in the quarterly dhs pulse surveys. results of extracted topics/themes are provided to dhs leadership to better inform agency-wide efforts to meet employees’ basic needs and improve job satisfaction . the systems outputs include a set of topics inferred or surfaced from the raw text comment data, as well as sentiments or other classifications inferred from the data."
Low Probability of False Alarm (Low-Pfa) Algorithm for on-person screening.,Department of Homeland Security,DHS,TSA,Transportation,None of the above.,"The purpose is to reduce alarm rates while providing gender neutral detection which would increase passenger throughput and experience.

Utilizes Machine Learning (ML) to improve detection performance while decreasing alarm rates and passengers touch rates. The algorithm is gender agnostic which no longer requires officers to select a passengers gender prior to being scanned. AIT throughput and AIT utilization have increased with this new algorithm. Note: Once the algorithm is trained, it is locked down and no longer learning.",The AI outputs target coordinates to the operator viewing station which is viewed as a bounding box on a representative human figure.,Implementation and Assessment,Both,9/1/2021,9/1/2022,Unknown,Developed with contracting resources.,Contract: HSTS0417DCT1087 Task Order: 70T04022F7672N011,Yes,No,No,No,No,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,More than 12 months,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Yes,"Security risks include false negatives allowing threats to get through the sterile side of the airport or high false alarm rates slowing operations. The program follows the Test & Evaluation (T&E) process documented in the TSA T&E Policy, T&E Guidebook, TSA Acquisition Qualification Policy, and TSA Third Party Testing Strategy, which includes independent test & evaluation. Various types of testing were conducted, including functional testing, developmental test & evaluation, independent test & evaluation, and integration/implementation testing. Tests were conducted on a variety of body types with varying concealment locations, concealment methods, and target types. Metrics include probability of detection, probability of false alarm, mean time between failure, mean down time, mean time to repair, and percentage of eligible passengers unable to be screened. All tests met or exceeded expectations. TSA performs regular checks to verify performance of the scanner (which includes the AI model) using Operational Test Kits (OTK). OTKs are used daily, whenever a scanner is moved, whenever a scanner is power-cycled, and as needed to verify performance. OTKs are created by each OEM and are specific to their scanners. They contain items that when concealed on a scanned person will result in a positive scan for a target. ",Yes – by the CAIO,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"The program follows the Test & Evaluation process documented in the TSA T&E Policy, T&E Guidebook, TSA Acquisition Qualification Policy, and TSA Third Party Testing Strategy, which includes independent test & evaluation.","Direct user testing,General solicitations of comments from the public",Yes,Both,0.7301587301587301,"The purpose is to reduce alarm rates while providing gender neutral detection which would increase passenger throughput and experience.

Utilizes Machine Learning (ML) to improve detection performance while decreasing alarm rates and passengers touch rates. The algorithm is gender agnostic which no longer requires officers to select a passengers gender prior to being scanned. AIT throughput and AIT utilization have increased with this new algorithm. Note: Once the algorithm is trained, it is locked down and no longer learning. . The AI outputs target coordinates to the operator viewing station which is viewed as a bounding box on a representative human figure. . Security risks include false negatives allowing threats to get through the sterile side of the airport or high false alarm rates slowing operations. The program follows the Test & Evaluation (T&E) process documented in the TSA T&E Policy, T&E Guidebook, TSA Acquisition Qualification Policy, and TSA Third Party Testing Strategy, which includes independent test & evaluation. Various types of testing were conducted, including functional testing, developmental test & evaluation, independent test & evaluation, and integration/implementation testing. Tests were conducted on a variety of body types with varying concealment locations, concealment methods, and target types. Metrics include probability of detection, probability of false alarm, mean time between failure, mean down time, mean time to repair, and percentage of eligible passengers unable to be screened. All tests met or exceeded expectations. TSA performs regular checks to verify performance of the scanner (which includes the AI model) using Operational Test Kits (OTK). OTKs are used daily, whenever a scanner is moved, whenever a scanner is power-cycled, and as needed to verify performance. OTKs are created by each OEM and are specific to their scanners. They contain items that when concealed on a scanned person will result in a positive scan for a target.","the purpose is to reduce alarm rates while providing gender neutral detection which would increase passenger throughput and experience. utilizes machine learning (ml) to improve detection performance while decreasing alarm rates and passengers touch rates. the algorithm is gender agnostic which no longer requires officers to select a passengers gender prior to being scanned. ait throughput and ait utilization have increased with this new algorithm. note: once the algorithm is trained, it is locked down and no longer learning. . the ai outputs target coordinates to the operator viewing station which is viewed as a bounding box on a representative human figure. . security risks include false negatives allowing threats to get through the sterile side of the airport or high false alarm rates slowing operations. the program follows the test & evaluation (t&e) process documented in the tsa t&e policy, t&e guidebook, tsa acquisition qualification policy, and tsa third party testing strategy, which includes independent test & evaluation. various types of testing were conducted, including functional testing, developmental test & evaluation, independent test & evaluation, and integration/implementation testing. tests were conducted on a variety of body types with varying concealment locations, concealment methods, and target types. metrics include probability of detection, probability of false alarm, mean time between failure, mean down time, mean time to repair, and percentage of eligible passengers unable to be screened. all tests met or exceeded expectations. tsa performs regular checks to verify performance of the scanner (which includes the ai model) using operational test kits (otk). otks are used daily, whenever a scanner is moved, whenever a scanner is power-cycled, and as needed to verify performance. otks are created by each oem and are specific to their scanners. they contain items that when concealed on a scanned person will result in a positive scan for a target."
Automated Target Recognition (ATR) Developments for Standard Screening,Department of Homeland Security,DHS,TSA,Transportation,None of the above.,"The purpose of this use case is to improve upon Automated Target Recognition (ATR) algorithms used to reduce privacy concerns because a TSO is no longer required to view Advanced Imaging Technology (AIT) images. The expected benefits are to increase detection, reduce false alarms, and improve efficiency and passenger experience.","The system reproduces the threat location, which is viewed as a bounding box on a representative human figure, for TSO resolution.",Acquisition and/or Development,Both,6/22/2017,6/22/2017,Unknown,Developed with contracting resources.,70T04023C7573N001,Yes,Unknown,Unknown,Unknown,No,TSA owns AIT image scans which are used for algorithm development. The scans contain threat images and are potentially sensitive.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,More than 12 months,No,No,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.4444444444444444,"The purpose of this use case is to improve upon Automated Target Recognition (ATR) algorithms used to reduce privacy concerns because a TSO is no longer required to view Advanced Imaging Technology (AIT) images. The expected benefits are to increase detection, reduce false alarms, and improve efficiency and passenger experience. . The system reproduces the threat location, which is viewed as a bounding box on a representative human figure, for TSO resolution.","the purpose of this use case is to improve upon automated target recognition (atr) algorithms used to reduce privacy concerns because a tso is no longer required to view advanced imaging technology (ait) images. the expected benefits are to increase detection, reduce false alarms, and improve efficiency and passenger experience. . the system reproduces the threat location, which is viewed as a bounding box on a representative human figure, for tso resolution."
Accessible Property Screening (APS) Checkpoint CT Prohibited Items (PI) Detection,Department of Homeland Security,DHS,TSA,Transportation,None of the above.,"This AI helps airplane luggage checks by the TSA officers scanning bags to have a continuous always watching partner to alert them to anything suspicious. 

Currently, a Transportation Security Officer (TSO) who is assigned to every X-ray equipment at an airport checkpoint visually inspects each image. This officer resolves the system generated explosive alarms as well as visually inspecting the image for the presence of non-explosive prohibited items such as guns and sharp objects (see TSA Travel site).
TSA is working on developing new Artificial Intelligence/Machine Learning (AI/ML) algorithms to automate the search for the non-explosive prohibited items (e.g. guns, knives, etc.).  Once a threat is found, the algorithm displays bounding boxes around the suspect item for the operator to then investigate and adjudicate.  
These AI solutions benefit the public by providing a consistent and uninterrupted level of threat detection as an added layer of security. The ML algorithms allow the TSA officers to be more flexible and to better prioritize their attention on important items to improve security.",AI system output is a set of 3-dimentional bounding boxes that is displayed on the X-ray image.  The bounding boxes are placed on top of objects or areas where the algorithm believes it has found a prohibited item (threat object).,Acquisition and/or Development,Both,10/1/2019,10/1/2019,Unknown,Developed with contracting resources.,70T04024F7573N010    - Smiths; 70T04023C7573N003   - SureScan (IDSS); 70T04023D7672N007   - Analogic,Yes,Unknown,Unknown,Unknown,No,"CT images of government scanned bags with threats, additionally have Stream of Commerce CT images collected at several airports. The images are in DICOS or vendor proprietary image format.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,Yes,CheckPoint Property Screening System (CPSS),Less than 6 months,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.4603174603174603,"This AI helps airplane luggage checks by the TSA officers scanning bags to have a continuous always watching partner to alert them to anything suspicious. 

Currently, a Transportation Security Officer (TSO) who is assigned to every X-ray equipment at an airport checkpoint visually inspects each image. This officer resolves the system generated explosive alarms as well as visually inspecting the image for the presence of non-explosive prohibited items such as guns and sharp objects (see TSA Travel site).
TSA is working on developing new Artificial Intelligence/Machine Learning (AI/ML) algorithms to automate the search for the non-explosive prohibited items (e.g. guns, knives, etc.).  Once a threat is found, the algorithm displays bounding boxes around the suspect item for the operator to then investigate and adjudicate.  
These AI solutions benefit the public by providing a consistent and uninterrupted level of threat detection as an added layer of security. The ML algorithms allow the TSA officers to be more flexible and to better prioritize their attention on important items to improve security. . AI system output is a set of 3-dimentional bounding boxes that is displayed on the X-ray image.  The bounding boxes are placed on top of objects or areas where the algorithm believes it has found a prohibited item (threat object).","this ai helps airplane luggage checks by the tsa officers scanning bags to have a continuous always watching partner to alert them to anything suspicious. currently, a transportation security officer (tso) who is assigned to every x-ray equipment at an airport checkpoint visually inspects each image. this officer resolves the system generated explosive alarms as well as visually inspecting the image for the presence of non-explosive prohibited items such as guns and sharp objects (see tsa travel site). tsa is working on developing new artificial intelligence/machine learning (ai/ml) algorithms to automate the search for the non-explosive prohibited items (e.g. guns, knives, etc.). once a threat is found, the algorithm displays bounding boxes around the suspect item for the operator to then investigate and adjudicate. these ai solutions benefit the public by providing a consistent and uninterrupted level of threat detection as an added layer of security. the ml algorithms allow the tsa officers to be more flexible and to better prioritize their attention on important items to improve security. . ai system output is a set of 3-dimentional bounding boxes that is displayed on the x-ray image. the bounding boxes are placed on top of objects or areas where the algorithm believes it has found a prohibited item (threat object)."
Walk-Through Metal Detector (WTMD) Alternative Automated Target Recognition (ATR) Developments,Department of Homeland Security,DHS,TSA,Transportation,None of the above.,Artificial intelligence (AI)-enhanced Millimeter wave (mmWave) detectors are used as an alternative to Walk-Through Metal Detectors (WTMDs) for passenger screening to detect both metallic and non-metallic threats and prohibited items on passengers at the security checkpoint. These detectors are an improvement over traditional walk-through metal detectors and provide increased security and a better passenger experience.,The AI outputs target coordinates to the operator viewing station which is viewed as a bounding box on a representative human figure.,Acquisition and/or Development,Both,9/30/2021,9/30/2021,Unknown,Developed with contracting resources.,70T04024C7573N002,Yes,Unknown,Unknown,Unknown,No,TSA owns AIT image scans which are used for algorithm development. The scans contain threat images and are potentially sensitive.,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,More than 12 months,No,Other,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.47619047619047616,Artificial intelligence (AI)-enhanced Millimeter wave (mmWave) detectors are used as an alternative to Walk-Through Metal Detectors (WTMDs) for passenger screening to detect both metallic and non-metallic threats and prohibited items on passengers at the security checkpoint. These detectors are an improvement over traditional walk-through metal detectors and provide increased security and a better passenger experience. . The AI outputs target coordinates to the operator viewing station which is viewed as a bounding box on a representative human figure.,artificial intelligence (ai)-enhanced millimeter wave (mmwave) detectors are used as an alternative to walk-through metal detectors (wtmds) for passenger screening to detect both metallic and non-metallic threats and prohibited items on passengers at the security checkpoint. these detectors are an improvement over traditional walk-through metal detectors and provide increased security and a better passenger experience. . the ai outputs target coordinates to the operator viewing station which is viewed as a bounding box on a representative human figure.
Conversation Training and Feedback Simulator,Department of Homeland Security,DHS,TSA,Education & Workforce,None of the above.,"TSA's Training and Development office is using an off-the-shelf, on-demand, employee learning/training platform which contains commercial training on many topics tailored to each user's interests and skill level to support their personalized training plans and goals. One available component of the platform uses a large language model (LLM) as part of an interactive, dialogue-style training functionality to communicate with and advise the learner to develop critical non-security sensitive soft skills such as customer service and communicating on difficult topics.",Text responses to questions or answers to a question,Acquisition and/or Development,Neither,6/1/2024,9/1/2024,Unknown,Developed with contracting resources.,70RDAD23FR0000044,No,Unknown,Unknown,Unknown,No,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,Yes,Percipio,Less than 6 months,No,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"TSA's Training and Development office is using an off-the-shelf, on-demand, employee learning/training platform which contains commercial training on many topics tailored to each user's interests and skill level to support their personalized training plans and goals. One available component of the platform uses a large language model (LLM) as part of an interactive, dialogue-style training functionality to communicate with and advise the learner to develop critical non-security sensitive soft skills such as customer service and communicating on difficult topics. . Text responses to questions or answers to a question","tsa's training and development office is using an off-the-shelf, on-demand, employee learning/training platform which contains commercial training on many topics tailored to each user's interests and skill level to support their personalized training plans and goals. one available component of the platform uses a large language model (llm) as part of an interactive, dialogue-style training functionality to communicate with and advise the learner to develop critical non-security sensitive soft skills such as customer service and communicating on difficult topics. . text responses to questions or answers to a question"
Answer Engine,Department of Homeland Security,DHS,TSA,Mission-Enabling,None of the above.,"TSA aims to enhance its capabilities in managing and analyzing complex data, ultimately contributing to more effective and efficient security operations and optimizing the TSA's operational workflows and support capabilities.","This platform is anticipated to harness the power of AI to provide intelligent, context-aware responses and insights.",Acquisition and/or Development,Neither,10/1/2024,10/1/2024,Unknown,Developed with contracting resources.,70T03024F7667N086,No,Unknown,Unknown,Unknown,No,SOP and Policy documents for conducting TSO duties.,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"TSA aims to enhance its capabilities in managing and analyzing complex data, ultimately contributing to more effective and efficient security operations and optimizing the TSA's operational workflows and support capabilities. . This platform is anticipated to harness the power of AI to provide intelligent, context-aware responses and insights.","tsa aims to enhance its capabilities in managing and analyzing complex data, ultimately contributing to more effective and efficient security operations and optimizing the tsa's operational workflows and support capabilities. . this platform is anticipated to harness the power of ai to provide intelligent, context-aware responses and insights."
Automated Field Data Collection,Department of Homeland Security,DHS,TSA,Transportation,None of the above.,"The AI will analyze screening environments via CCTV footage and extract passenger processing times of various steps within the screening processes. Enabling AI to extract and visualize this data will enable TSA to make data informed decisions while testing or deploying new screening equipment, identify anomalies, establish real-world rates and standards, and reduce or eliminate TSA’s need to deploy data collection teams, resulting in real-time data collection and significantly reduced computational time of findings.
","AI system outputs multiple decisions to include screening location performance, rates and standards of the end-to-end screening system, and passenger wait times.
",Acquisition and/or Development,Neither,4/1/2023,9/1/2024,Unknown,Developed with both contracting and in-house resources.,70T04024CSOP7573N001,Yes,Unknown,Unknown,Unknown,No,Screening process rates & standards.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,6-12 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"The AI will analyze screening environments via CCTV footage and extract passenger processing times of various steps within the screening processes. Enabling AI to extract and visualize this data will enable TSA to make data informed decisions while testing or deploying new screening equipment, identify anomalies, establish real-world rates and standards, and reduce or eliminate TSA’s need to deploy data collection teams, resulting in real-time data collection and significantly reduced computational time of findings. . AI system outputs multiple decisions to include screening location performance, rates and standards of the end-to-end screening system, and passenger wait times.","the ai will analyze screening environments via cctv footage and extract passenger processing times of various steps within the screening processes. enabling ai to extract and visualize this data will enable tsa to make data informed decisions while testing or deploying new screening equipment, identify anomalies, establish real-world rates and standards, and reduce or eliminate tsa’s need to deploy data collection teams, resulting in real-time data collection and significantly reduced computational time of findings. . ai system outputs multiple decisions to include screening location performance, rates and standards of the end-to-end screening system, and passenger wait times."
Plan of Day Staff Optimization,Department of Homeland Security,DHS,TSA,Transportation,None of the above.,"Plan of Day will automate TSA screening staff optimization.
","Staffing operations models prescribing when screening lanes should be opened/closed, when/where screening staff is required to absorb operational peaks, determining optimal gender and certification ratios, recommending when to schedule overtime/shift adjustments, drafting lane rotation plans, and informing national TSA staffing requirements as prescribed optimization plans deviate as airline schedules shift.
",Acquisition and/or Development,Neither,10/1/2017,10/1/2017,Unknown,Developed with both contracting and in-house resources.,"DHS, S&T Contract Number: 70RSAT21PI00001",Yes,Unknown,Unknown,Unknown,Yes,"STIP CAT data and PMIS passenger throughput data; PIMS historic throughput of screening equipment; Master Checkpoint List/Location Codes; Salesforce Master Airport List; HR data via eTAS, NFC, and OLC; AIMs data for airport logistic information; OAG for flight information; Google weather and traffic ","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Unknown,Unknown,No,Unknown,More than 12 months,Other,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"Plan of Day will automate TSA screening staff optimization. . Staffing operations models prescribing when screening lanes should be opened/closed, when/where screening staff is required to absorb operational peaks, determining optimal gender and certification ratios, recommending when to schedule overtime/shift adjustments, drafting lane rotation plans, and informing national TSA staffing requirements as prescribed optimization plans deviate as airline schedules shift.","plan of day will automate tsa screening staff optimization. . staffing operations models prescribing when screening lanes should be opened/closed, when/where screening staff is required to absorb operational peaks, determining optimal gender and certification ratios, recommending when to schedule overtime/shift adjustments, drafting lane rotation plans, and informing national tsa staffing requirements as prescribed optimization plans deviate as airline schedules shift."
TSA Contact Center Virtual Assistant,Department of Homeland Security,DHS,TSA,Government Services (includes Benefits and Service Delivery),None of the above.,"The three goals for TSA’s virtual assistant are:  1) Improve Ease / Efficiency – deliver timely, accurate responses to customers within / outside of TCC operational hours; 2) Improve Effectiveness – collect new customer insights to help drive agency improvements; 3) Fiscal Responsibility – leverage automation to meet increasing demand without the need for additional resources.","TSA’s virtual assistant, as a Predictive AI tool, will not be able to create and provide original content to customers.  It will be limited to providing responses to customer inquiries based on the existing content in the TCC’s knowledge library.  However, it will record transactional data related to these customer inquiries (e.g., customer inputs, assistant outputs, topic classifications, etc.).

TSA’s virtual assistant will create transactional data regarding customer inquiries in a consistent manner to our existing email and phone channels.  TSA uses this transactional data to guide customer experience improvement efforts.  This data offers TSA insights into:  what is working well, where the pain points are, where to improve public information, and what services are most requested.",Acquisition and/or Development,Neither,1/10/2024,8/1/2024,Unknown,Developed with both contracting and in-house resources.,70T01024F5600N002,Yes,Unknown,Unknown,Unknown,Yes,"TSA.gov FAQs (publicly available), TSA Contact Center (TCC) inquiry data (e.g. why customers contacted us; how we responded to them)(not pubicly available), TCC knowledge library (i.e. approved response language; currently used by TCC agents when responding to customers via email and phone channels)(not publicly available; but derived from TSA.gov FAQs).","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,Yes,Customer Service Center - Salesforce,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"The three goals for TSA’s virtual assistant are:  1) Improve Ease / Efficiency – deliver timely, accurate responses to customers within / outside of TCC operational hours; 2) Improve Effectiveness – collect new customer insights to help drive agency improvements; 3) Fiscal Responsibility – leverage automation to meet increasing demand without the need for additional resources. . TSA’s virtual assistant, as a Predictive AI tool, will not be able to create and provide original content to customers.  It will be limited to providing responses to customer inquiries based on the existing content in the TCC’s knowledge library.  However, it will record transactional data related to these customer inquiries (e.g., customer inputs, assistant outputs, topic classifications, etc.).

TSA’s virtual assistant will create transactional data regarding customer inquiries in a consistent manner to our existing email and phone channels.  TSA uses this transactional data to guide customer experience improvement efforts.  This data offers TSA insights into:  what is working well, where the pain points are, where to improve public information, and what services are most requested.","the three goals for tsa’s virtual assistant are: 1) improve ease / efficiency – deliver timely, accurate responses to customers within / outside of tcc operational hours; 2) improve effectiveness – collect new customer insights to help drive agency improvements; 3) fiscal responsibility – leverage automation to meet increasing demand without the need for additional resources. . tsa’s virtual assistant, as a predictive ai tool, will not be able to create and provide original content to customers. it will be limited to providing responses to customer inquiries based on the existing content in the tcc’s knowledge library. however, it will record transactional data related to these customer inquiries (e.g., customer inputs, assistant outputs, topic classifications, etc.). tsa’s virtual assistant will create transactional data regarding customer inquiries in a consistent manner to our existing email and phone channels. tsa uses this transactional data to guide customer experience improvement efforts. this data offers tsa insights into: what is working well, where the pain points are, where to improve public information, and what services are most requested."
Machine Learning Analysis Applied to Cyber Threat Hunt Data,Department of Homeland Security,DHS,TSA,Mission-Enabling,None of the above.,"Cyber threat hunts typically involve a vast amount of data. Machine learning models can quickly and efficiently process this data as well as more effectively identify anomalous activity than humans. 
This could improve the efficiency and quality of cyber threat hunts by detecting suspicious behavior more quickly and increasing the amount of data that can be analyzed during a hunt.","Currently it is a list of potential anomalies or outliers within the system, but development is still ongoing.",Acquisition and/or Development,Neither,1/1/2024,2/1/2024,Unknown,Developed with contracting resources.,GS-35F-525GA,No,Unknown,Unknown,Unknown,No,Data collected from TSA-owned assets through cyber threat hunt operations.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,Yes,System name is operationally sensitive.,Less than 6 months,No,No,Other,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"Cyber threat hunts typically involve a vast amount of data. Machine learning models can quickly and efficiently process this data as well as more effectively identify anomalous activity than humans. 
This could improve the efficiency and quality of cyber threat hunts by detecting suspicious behavior more quickly and increasing the amount of data that can be analyzed during a hunt. . Currently it is a list of potential anomalies or outliers within the system, but development is still ongoing.","cyber threat hunts typically involve a vast amount of data. machine learning models can quickly and efficiently process this data as well as more effectively identify anomalous activity than humans. this could improve the efficiency and quality of cyber threat hunts by detecting suspicious behavior more quickly and increasing the amount of data that can be analyzed during a hunt. . currently it is a list of potential anomalies or outliers within the system, but development is still ongoing."
Airport Throughput Predictive Model,Department of Homeland Security,DHS,TSA,Mission-Enabling,None of the above.,This project was to create a predictive model for the passenger volume using the Security Operations throughput count from checkpoints to help with airport staffing.,"Once a month the data is ingested, the predictive model is trained, and predictions of airport checkpoint throughput are made for the airports.",Operation and Maintenance,Neither,6/1/2023,6/1/2023,4/1/2024,Developed with both contracting and in-house resources.,"Performance Engineering Analytics – Contract#:47QRAD20D1175, Task Order:70T02023F7500N001",No,No,No,Yes,No,Secure Flight Passenger Data: passenger and airline reservation information received from airlines; PMIS Data: Secure checkpoint throughput counts by airport and checkpoint,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Secure Flight,More than 12 months,No,Other,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"This project was to create a predictive model for the passenger volume using the Security Operations throughput count from checkpoints to help with airport staffing. . Once a month the data is ingested, the predictive model is trained, and predictions of airport checkpoint throughput are made for the airports.","this project was to create a predictive model for the passenger volume using the security operations throughput count from checkpoints to help with airport staffing. . once a month the data is ingested, the predictive model is trained, and predictions of airport checkpoint throughput are made for the airports."
Credential Authentication Technology with Camera System (CAT-2) and AutoCAT (CAT-2 in an e-gate form factor),Department of Homeland Security,DHS,TSA,Transportation,None of the above.,"The Transportation Security Administration (TSA) uses AI-based, one-to-one (1:1) facial matching technologies at some checkpoints to assist human reviewers with traveler identity verification. The purpose and expected benefits of the technology include increased speed and accuracy of identity verification at the checkpoint while improving detection of imposters.","The system produces an recommendation to the Transportation Security Officer (TSO) to indicate if person presenting the identity document is similar to the face on the photo ID document.  In the event of a non-match, the TSO is responsible for additional identity verification steps to verify the identity of the traveler.",Operation and Maintenance,"Rights-Impacting
",1/1/2022,7/1/2023,9/1/2023,Developed with contracting resources.,70T04023D7672N0001,Yes,No,Yes,Yes,No,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,STIP/CAT,6-12 months,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Yes,"In the event of a non-match, the traveler may make a second attempt or TSA may perform additional identity verification steps to verify the identity of the traveler. This process may add between 20 seconds or a few minutes to the identity verification and security screening process. This is mitigated through T&E and continuous monitoring.  ",Yes – by the CAIO,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,Test and Evaluation and continuous monitoring.,Direct user testing,Yes,Rights-Impacting,0.7777777777777778,"The Transportation Security Administration (TSA) uses AI-based, one-to-one (1:1) facial matching technologies at some checkpoints to assist human reviewers with traveler identity verification. The purpose and expected benefits of the technology include increased speed and accuracy of identity verification at the checkpoint while improving detection of imposters. . The system produces an recommendation to the Transportation Security Officer (TSO) to indicate if person presenting the identity document is similar to the face on the photo ID document.  In the event of a non-match, the TSO is responsible for additional identity verification steps to verify the identity of the traveler. . In the event of a non-match, the traveler may make a second attempt or TSA may perform additional identity verification steps to verify the identity of the traveler. This process may add between 20 seconds or a few minutes to the identity verification and security screening process. This is mitigated through T&E and continuous monitoring.","the transportation security administration (tsa) uses ai-based, one-to-one (1:1) facial matching technologies at some checkpoints to assist human reviewers with traveler identity verification. the purpose and expected benefits of the technology include increased speed and accuracy of identity verification at the checkpoint while improving detection of imposters. . the system produces an recommendation to the transportation security officer (tso) to indicate if person presenting the identity document is similar to the face on the photo id document. in the event of a non-match, the tso is responsible for additional identity verification steps to verify the identity of the traveler. . in the event of a non-match, the traveler may make a second attempt or tsa may perform additional identity verification steps to verify the identity of the traveler. this process may add between 20 seconds or a few minutes to the identity verification and security screening process. this is mitigated through t&e and continuous monitoring."
PreCheck Touchless Identity Solution (TIS),Department of Homeland Security,DHS,TSA,Transportation,None of the above.,"TSA is using Facial Comparison to verify a passenger’s identity at its security checkpoints and CMAP locations using the CBP Traveler Verification Service (TVS). This process streamlines passenger and crewmember identity verification, increasing the speed of security checks while maintaining a high degree of safety for all passengers and crewmembers.""","TSA is leveraging CBP's TVS system technology as an optional process for passengers or crew members traveling via certain airports who wish to further expedite their TSA PreCheck or crew member ID verification process. This additional TSA PreCheck feature is voluntary, and passengers may opt-out of the process at any time and instead choose the standard identity verification by a Transportation Security Officer (TSO). Crew members that wish to opt-out will be sent to the security checkpoint to process through screening.",Operation and Maintenance,"Rights-Impacting
",4/1/2016,4/1/2017,10/1/2018,Developed with contracting resources.,024-000005265,Yes,No,Yes,Yes,Other,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,CBP TVS,Less than 6 months,Other,Other,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Yes,"The key risk is the degradation of the TVS verification to degrade overtime based on the parameters of assessment for comparing images to templates. This was mitigated by testing the threshold for the biometric matching extensively with a variety of face types for several months to establish a match threshold for the identification. The algorithms have been trained on hundreds of millions of data points from a combination of public and private data sources. The underlying data covers a wide variety of demographics (e.g., age, race, gender, etc.) representative of the general population. During model training and refinement shortcomings (e.g., certain combinations of flesh tones and lighting, facial hair variations, etc.) were compensated for. The algorithms are industry leading as measured by NIST. This mitigates the risk of poor performance and algorithmic discrimination. ",Yes – by the CAIO,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,The threshold for the biometric matching was tested extensively with a variety of face types for several months to establish a match threshold for the identification.,"Direct user testing,General solicitations of comments from the public",Yes,Rights-Impacting,0.8253968253968254,"TSA is using Facial Comparison to verify a passenger’s identity at its security checkpoints and CMAP locations using the CBP Traveler Verification Service (TVS). This process streamlines passenger and crewmember identity verification, increasing the speed of security checks while maintaining a high degree of safety for all passengers and crewmembers."" . TSA is leveraging CBP's TVS system technology as an optional process for passengers or crew members traveling via certain airports who wish to further expedite their TSA PreCheck or crew member ID verification process. This additional TSA PreCheck feature is voluntary, and passengers may opt-out of the process at any time and instead choose the standard identity verification by a Transportation Security Officer (TSO). Crew members that wish to opt-out will be sent to the security checkpoint to process through screening. . The key risk is the degradation of the TVS verification to degrade overtime based on the parameters of assessment for comparing images to templates. This was mitigated by testing the threshold for the biometric matching extensively with a variety of face types for several months to establish a match threshold for the identification. The algorithms have been trained on hundreds of millions of data points from a combination of public and private data sources. The underlying data covers a wide variety of demographics (e.g., age, race, gender, etc.) representative of the general population. During model training and refinement shortcomings (e.g., certain combinations of flesh tones and lighting, facial hair variations, etc.) were compensated for. The algorithms are industry leading as measured by NIST. This mitigates the risk of poor performance and algorithmic discrimination.","tsa is using facial comparison to verify a passenger’s identity at its security checkpoints and cmap locations using the cbp traveler verification service (tvs). this process streamlines passenger and crewmember identity verification, increasing the speed of security checks while maintaining a high degree of safety for all passengers and crewmembers."" . tsa is leveraging cbp's tvs system technology as an optional process for passengers or crew members traveling via certain airports who wish to further expedite their tsa precheck or crew member id verification process. this additional tsa precheck feature is voluntary, and passengers may opt-out of the process at any time and instead choose the standard identity verification by a transportation security officer (tso). crew members that wish to opt-out will be sent to the security checkpoint to process through screening. . the key risk is the degradation of the tvs verification to degrade overtime based on the parameters of assessment for comparing images to templates. this was mitigated by testing the threshold for the biometric matching extensively with a variety of face types for several months to establish a match threshold for the identification. the algorithms have been trained on hundreds of millions of data points from a combination of public and private data sources. the underlying data covers a wide variety of demographics (e.g., age, race, gender, etc.) representative of the general population. during model training and refinement shortcomings (e.g., certain combinations of flesh tones and lighting, facial hair variations, etc.) were compensated for. the algorithms are industry leading as measured by nist. this mitigates the risk of poor performance and algorithmic discrimination."
Adaptive Risk Model for Inspected Small Passenger Vessels,Department of Homeland Security,DHS,USCG,Mission-Enabling,None of the above.,"The Small Passenger Vessels (SPV) Safety Task Force leveraged machine learning and subject matter expert input to create an adaptable, quantitative analysis tool that reliably identifies the key drivers of marine casualties and calculates a risk score for each vessel within the largest segment of the U.S.-inspected fleet. A vessel risk score is computed with an analytic model which leverages logistic regression and basic machine learning. The effort directly improved marine inspector allocation, targeted risk and enhanced the governance necessary to increase passenger safety across the country.",Numerical score that compares vessels predicted safety risk relative to each other.,Operation and Maintenance,Neither,11/1/2019,11/1/2019,1/1/2021,Developed in-house.,Unknown,No,No,No,No,Yes,"Commercial vessel profiles including: engineering, life saving, propulsion, fire protection, manning, operating routes, plan review, and USCG inspection activity details.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The Small Passenger Vessels (SPV) Safety Task Force leveraged machine learning and subject matter expert input to create an adaptable, quantitative analysis tool that reliably identifies the key drivers of marine casualties and calculates a risk score for each vessel within the largest segment of the U.S.-inspected fleet. A vessel risk score is computed with an analytic model which leverages logistic regression and basic machine learning. The effort directly improved marine inspector allocation, targeted risk and enhanced the governance necessary to increase passenger safety across the country. . Numerical score that compares vessels predicted safety risk relative to each other.","the small passenger vessels (spv) safety task force leveraged machine learning and subject matter expert input to create an adaptable, quantitative analysis tool that reliably identifies the key drivers of marine casualties and calculates a risk score for each vessel within the largest segment of the u.s.-inspected fleet. a vessel risk score is computed with an analytic model which leverages logistic regression and basic machine learning. the effort directly improved marine inspector allocation, targeted risk and enhanced the governance necessary to increase passenger safety across the country. . numerical score that compares vessels predicted safety risk relative to each other."
Intelligent Document Processing (IDP) for I-539 Form Digitization,Department of Homeland Security,DHS,USCIS,Mission-Enabling,None of the above.,"IDP for the I-539 makes use of an AI-enhanced tool to identify, categorize, and create separate images for each document type submitted as part of the 539 benefit application. Prior to implementation of this use case, all pages of a 539 application were scanned and stored as a single document in the content management system.  The benefit is reduced case processing time for adjudicators by identifying and classifying supporting documents for ease of use. An additional benefit is to bring digital images into compliance with NARA standards.","Input - one digital file comprised of all pages of a 539 benefit application. Output - multiple digital files comprised of the individual documents submitted as part of the 539 benefit application. These will include the 539 form, any other USCIS forms, and image files of other supporting documents such as Passports, Driver's license, Marriage Certificate, Bank Statement, etc. All pages of the original digital file are accounted for and stored. Any pages not identified by the tool are referred to a human for document type resolution.",Implementation and Assessment,Neither,9/30/2023,10/1/2023,Unknown,Developed with contracting resources.,70SBUR23Q00000186,No,No,Yes,Yes,Other,"CMS/STACKS test data is used to train the model. This data is comprised of digital images of blank USCIS forms and common supporting forms (Marriage License, Driver's License, Passport, ect.) generated using fake information such as Mickey Mouse and Donald Duck in place of PII.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,Yes,Content Management Services (CMS),Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"IDP for the I-539 makes use of an AI-enhanced tool to identify, categorize, and create separate images for each document type submitted as part of the 539 benefit application. Prior to implementation of this use case, all pages of a 539 application were scanned and stored as a single document in the content management system.  The benefit is reduced case processing time for adjudicators by identifying and classifying supporting documents for ease of use. An additional benefit is to bring digital images into compliance with NARA standards. . Input - one digital file comprised of all pages of a 539 benefit application. Output - multiple digital files comprised of the individual documents submitted as part of the 539 benefit application. These will include the 539 form, any other USCIS forms, and image files of other supporting documents such as Passports, Driver's license, Marriage Certificate, Bank Statement, etc. All pages of the original digital file are accounted for and stored. Any pages not identified by the tool are referred to a human for document type resolution.","idp for the i-539 makes use of an ai-enhanced tool to identify, categorize, and create separate images for each document type submitted as part of the 539 benefit application. prior to implementation of this use case, all pages of a 539 application were scanned and stored as a single document in the content management system. the benefit is reduced case processing time for adjudicators by identifying and classifying supporting documents for ease of use. an additional benefit is to bring digital images into compliance with nara standards. . input - one digital file comprised of all pages of a 539 benefit application. output - multiple digital files comprised of the individual documents submitted as part of the 539 benefit application. these will include the 539 form, any other uscis forms, and image files of other supporting documents such as passports, driver's license, marriage certificate, bank statement, etc. all pages of the original digital file are accounted for and stored. any pages not identified by the tool are referred to a human for document type resolution."
Large Language Models for an Officer Training Tool,Department of Homeland Security,DHS,USCIS,Mission-Enabling,None of the above.,"The LLM for an Officer Training tool, will use GenAI to improve the way the agency trains immigration officer personnel. The tool will generate dynamic, personalized training materials that adapt to officers’ specific needs and ensure the best possible knowledge and training on a wide range of current policies and laws relevant to their jobs.","The AI outputs human-like conversation in a text format, similar to other AI chatbots, but specially trained and tuned for RAIO Officer training.",Implementation and Assessment,Neither,10/20/2023,11/1/2023,Unknown,Developed with contracting resources.,70SBUR25F00000003,No,No,No,Yes,No,"The USCIS Officer Training Tool does use Proprietary/Private but Not Sensitive data, including training materials, internal guidance documents, and policies that are proprietary to the USCIS or the training program fall under this category. While not publicly accessible, this data is not considered sensitive but should still be treated with a degree of confidentiality to maintain the integrity of the training program.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The LLM for an Officer Training tool, will use GenAI to improve the way the agency trains immigration officer personnel. The tool will generate dynamic, personalized training materials that adapt to officers’ specific needs and ensure the best possible knowledge and training on a wide range of current policies and laws relevant to their jobs. . The AI outputs human-like conversation in a text format, similar to other AI chatbots, but specially trained and tuned for RAIO Officer training.","the llm for an officer training tool, will use genai to improve the way the agency trains immigration officer personnel. the tool will generate dynamic, personalized training materials that adapt to officers’ specific needs and ensure the best possible knowledge and training on a wide range of current policies and laws relevant to their jobs. . the ai outputs human-like conversation in a text format, similar to other ai chatbots, but specially trained and tuned for raio officer training."
User Entity and Behavior Analytics (UEBA) for Security Operations (SecOps) Anomaly Identification,Department of Homeland Security,DHS,USCIS,Mission-Enabling,None of the above.,"UEBA's purpose is to review USCIS system logs to determine when an entity is performing actions that are anomalous. An entity can be classified as a workstation, server or an internal USCIS system account. The UEBA ingests logs from systems to perform analytics based off of models that are manually created and maintained. UEBA uses the models to apply a risk score to the entity which the risk score is then used to create a case (or ticket) for Security Operations analyst review. The AI reviews the action of the analyst to adjust the risk scoring for future events. Output would assist in prioritizing cyber events for further manual investigation.",Output of the Machine Learning is an alert with all artifacts for the SOC to investigate. The alert is used as a recommendation to prioritize specific investigations in the SOC ticket queue.,Acquisition and/or Development,Neither,8/18/2023,8/18/2023,Unknown,Developed with contracting resources.,70SBUR24F00000168,No,Unknown,Unknown,Unknown,No,Data used to tune models is USCIS internal system logs.,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Unknown,Unknown,Yes,ESS,6-12 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"UEBA's purpose is to review USCIS system logs to determine when an entity is performing actions that are anomalous. An entity can be classified as a workstation, server or an internal USCIS system account. The UEBA ingests logs from systems to perform analytics based off of models that are manually created and maintained. UEBA uses the models to apply a risk score to the entity which the risk score is then used to create a case (or ticket) for Security Operations analyst review. The AI reviews the action of the analyst to adjust the risk scoring for future events. Output would assist in prioritizing cyber events for further manual investigation. . Output of the Machine Learning is an alert with all artifacts for the SOC to investigate. The alert is used as a recommendation to prioritize specific investigations in the SOC ticket queue.","ueba's purpose is to review uscis system logs to determine when an entity is performing actions that are anomalous. an entity can be classified as a workstation, server or an internal uscis system account. the ueba ingests logs from systems to perform analytics based off of models that are manually created and maintained. ueba uses the models to apply a risk score to the entity which the risk score is then used to create a case (or ticket) for security operations analyst review. the ai reviews the action of the analyst to adjust the risk scoring for future events. output would assist in prioritizing cyber events for further manual investigation. . output of the machine learning is an alert with all artifacts for the soc to investigate. the alert is used as a recommendation to prioritize specific investigations in the soc ticket queue."
I-765 - USCIS Face Capture Mobile App,Department of Homeland Security,DHS,USCIS,Law & Justice,None of the above.,This will allow the user to complete the biometric verification requirement without having to attend an appointment at am Applicant Support Center. This reduces the burden on the beneficiary as well as reducing demands on USCIS Applicant Service Center resources.,"Face detection locates human faces in visual media such as digital images or video. When a face is detected it has an associated position, size, and orientation; and it can be searched for landmarks such as the eyes and nose which are returned as numerical values. 

ELIS Photo Validation service sends response back to user based on the pre-defined quality checks if the uploaded photo meets USCIS requirements. I-765 - USCIS Facial Recognition through IDENT (1:1 Face Recognition/Validation) returns a match or no match response from IDENT.",Acquisition and/or Development,"Rights-Impacting
",10/1/2023,10/1/2023,Unknown,Developed with contracting resources.,70SBUR23F00000138,Yes,Unknown,Unknown,Unknown,Other,"N/A - The Agency does not own any data used to train, fine-tune, or evaluate the use case.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Unknown,Unknown,No,Unknown,6-12 months,Yes,No,Other,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Rights-Impacting,0.47619047619047616,"This will allow the user to complete the biometric verification requirement without having to attend an appointment at am Applicant Support Center. This reduces the burden on the beneficiary as well as reducing demands on USCIS Applicant Service Center resources. . Face detection locates human faces in visual media such as digital images or video. When a face is detected it has an associated position, size, and orientation; and it can be searched for landmarks such as the eyes and nose which are returned as numerical values. 

ELIS Photo Validation service sends response back to user based on the pre-defined quality checks if the uploaded photo meets USCIS requirements. I-765 - USCIS Facial Recognition through IDENT (1:1 Face Recognition/Validation) returns a match or no match response from IDENT.","this will allow the user to complete the biometric verification requirement without having to attend an appointment at am applicant support center. this reduces the burden on the beneficiary as well as reducing demands on uscis applicant service center resources. . face detection locates human faces in visual media such as digital images or video. when a face is detected it has an associated position, size, and orientation; and it can be searched for landmarks such as the eyes and nose which are returned as numerical values. elis photo validation service sends response back to user based on the pre-defined quality checks if the uploaded photo meets uscis requirements. i-765 - uscis facial recognition through ident (1:1 face recognition/validation) returns a match or no match response from ident."
Text Analytics Data Science Sentence Similarity Model,Department of Homeland Security,DHS,USCIS,Law & Justice,None of the above.,"Text Analytics augments the tedious and time-consuming manual process to identify potential fraud, national security, and/or public safety concerns and enables the identification of such concerns across jurisdictional boundaries. It increases the integrity of immigration programs, strengthens officers’ confidence in their work, and contributes to the reduction in customer wait times.","Text Analytics does not make predictions, recommendations, or decisions. It is merely a research tool that identifies potential patterns, while remaining agnostic as to whether those patterns identify potential fraud, national security, and/or public safety concerns. Instead, trained staff evaluate the patterns to determine whether they identify potential concerns and then validate and/or invalidate those potential concerns through the course of their investigations or adjudications.",Operation and Maintenance,"Rights-Impacting
",2/1/2019,2/1/2019,11/1/2019,Developed with contracting resources.,70SBUR22F00000116,No,No,Yes,Yes,Yes,"Text Analytics stores information extracted from benefit forms and supporting documents, focusing on the narrative portions of those documents.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Text Analytics,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Yes,"There is small risk of false positives or false negatives due to the model. The risk is mitigated through a manual review of any information produced from the tool. Text Analytics is a decision support tool. Text Analytics does not make recommendations of fraud or benefit / adjudication decisions, any decisions made from information stored in the tool is conducted through a manual review by a USCIS employee.",Yes – by the CAIO,"Established Process of Machine Learning Operations: Alongside automated testing and drift detection, model re-training and re-deployments are supported by continuous integration pipelines that are managed by machine learning and data engineers on the platform, adapting work done by data science team into repeatable scripts for re-training and re-testing a model once deployed.",No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,Yes,Rights-Impacting,0.7142857142857143,"Text Analytics augments the tedious and time-consuming manual process to identify potential fraud, national security, and/or public safety concerns and enables the identification of such concerns across jurisdictional boundaries. It increases the integrity of immigration programs, strengthens officers’ confidence in their work, and contributes to the reduction in customer wait times. . Text Analytics does not make predictions, recommendations, or decisions. It is merely a research tool that identifies potential patterns, while remaining agnostic as to whether those patterns identify potential fraud, national security, and/or public safety concerns. Instead, trained staff evaluate the patterns to determine whether they identify potential concerns and then validate and/or invalidate those potential concerns through the course of their investigations or adjudications. . There is small risk of false positives or false negatives due to the model. The risk is mitigated through a manual review of any information produced from the tool. Text Analytics is a decision support tool. Text Analytics does not make recommendations of fraud or benefit / adjudication decisions, any decisions made from information stored in the tool is conducted through a manual review by a USCIS employee.","text analytics augments the tedious and time-consuming manual process to identify potential fraud, national security, and/or public safety concerns and enables the identification of such concerns across jurisdictional boundaries. it increases the integrity of immigration programs, strengthens officers’ confidence in their work, and contributes to the reduction in customer wait times. . text analytics does not make predictions, recommendations, or decisions. it is merely a research tool that identifies potential patterns, while remaining agnostic as to whether those patterns identify potential fraud, national security, and/or public safety concerns. instead, trained staff evaluate the patterns to determine whether they identify potential concerns and then validate and/or invalidate those potential concerns through the course of their investigations or adjudications. . there is small risk of false positives or false negatives due to the model. the risk is mitigated through a manual review of any information produced from the tool. text analytics is a decision support tool. text analytics does not make recommendations of fraud or benefit / adjudication decisions, any decisions made from information stored in the tool is conducted through a manual review by a uscis employee."
Biometrics Enrollment Tool (BET) Fingerprint Maximization,Department of Homeland Security,DHS,USCIS,Law & Justice,None of the above.,"BET assists in determining if the fingerprint taken is good enough quality to pass the FBI fingerprint check process. It provides immediate feedback when a set of prints is likely to be rejected by the FBI by incorporating machine learning models into the BET application. The FBI will not disclose their quality grading criteria for fingerprints, leaving BET with the responsibility of determining quality to prevent unnecessary secondary encounters with applicants.","Numerical Fingerprint Quality score, which is compared against  fingerprint quality thresholds (per finger and per set of fingerprints) to align with FBI specifications",Operation and Maintenance,Neither,6/1/2021,5/23/2023,1/1/2024,Developed with contracting resources.,70SBUR23F00000126,No,No,Yes,Yes,Yes,"Internal data from BET data capture into Databricks lakehouse, numerical values representing fingerprint quality scores determined by the BET system outside of the AI workflow.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Biometrics Enrollment Tool,Less than 6 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"BET assists in determining if the fingerprint taken is good enough quality to pass the FBI fingerprint check process. It provides immediate feedback when a set of prints is likely to be rejected by the FBI by incorporating machine learning models into the BET application. The FBI will not disclose their quality grading criteria for fingerprints, leaving BET with the responsibility of determining quality to prevent unnecessary secondary encounters with applicants. . Numerical Fingerprint Quality score, which is compared against  fingerprint quality thresholds (per finger and per set of fingerprints) to align with FBI specifications","bet assists in determining if the fingerprint taken is good enough quality to pass the fbi fingerprint check process. it provides immediate feedback when a set of prints is likely to be rejected by the fbi by incorporating machine learning models into the bet application. the fbi will not disclose their quality grading criteria for fingerprints, leaving bet with the responsibility of determining quality to prevent unnecessary secondary encounters with applicants. . numerical fingerprint quality score, which is compared against fingerprint quality thresholds (per finger and per set of fingerprints) to align with fbi specifications"
ELIS Evidence Classifier Machine Learning (ML) Tagging Solution,Department of Homeland Security,DHS,USCIS,Government Services (includes Benefits and Service Delivery),None of the above.,"To enable end users to navigate directly to the page(s) containing evidence documents of interest instead of sifting through large PDF documents. Evidence tagging intends to accelerate case processing by identifying specific types of documents (e.g., I-589, passport photo spread, marriage certificate) and applying a metadata tag to that document object in ELIS. This way, when a user opens a case with potentially hundreds of pages of evidence documents, rather than scrolling through them one at a time to find a specific document of interest, they have clickable ""bookmarks"" in the UI generated from these tags that will jump directly to the corresponding page.","Tagged evidence. The system inputs an image (scanned document from Lockbox) and outputs either a specific label, such as ""Border Crossing Card - Front,"" or no label if that document is not recognized as one of the classes.",Operation and Maintenance,Neither,11/20/2019,11/20/2019,9/1/2020,Developed with contracting resources.,70SBUR23F00000120,No,No,Yes,No,Yes,Applicant submitted scanned evidence documents sent to ELIS and Global for processing.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,ELIS,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"To enable end users to navigate directly to the page(s) containing evidence documents of interest instead of sifting through large PDF documents. Evidence tagging intends to accelerate case processing by identifying specific types of documents (e.g., I-589, passport photo spread, marriage certificate) and applying a metadata tag to that document object in ELIS. This way, when a user opens a case with potentially hundreds of pages of evidence documents, rather than scrolling through them one at a time to find a specific document of interest, they have clickable ""bookmarks"" in the UI generated from these tags that will jump directly to the corresponding page. . Tagged evidence. The system inputs an image (scanned document from Lockbox) and outputs either a specific label, such as ""Border Crossing Card - Front,"" or no label if that document is not recognized as one of the classes.","to enable end users to navigate directly to the page(s) containing evidence documents of interest instead of sifting through large pdf documents. evidence tagging intends to accelerate case processing by identifying specific types of documents (e.g., i-589, passport photo spread, marriage certificate) and applying a metadata tag to that document object in elis. this way, when a user opens a case with potentially hundreds of pages of evidence documents, rather than scrolling through them one at a time to find a specific document of interest, they have clickable ""bookmarks"" in the ui generated from these tags that will jump directly to the corresponding page. . tagged evidence. the system inputs an image (scanned document from lockbox) and outputs either a specific label, such as ""border crossing card - front,"" or no label if that document is not recognized as one of the classes."
Automated Name and Date of Birth (DOB) Harvesting from Existing Records,Department of Homeland Security,DHS,USCIS,Government Services (includes Benefits and Service Delivery),None of the above.,To reduce the amount of adjudicative time spent manually harvesting aliases and dates of birth (DOBs) from identity history summary (idHS) report attached to the ELIS case as part of the Manual Name Harvesting Task during case processing.,"Eliminates need for manual review by extracting unique names and DOBs from IdHS documents and when names/DOBs already in ELIS, ANH will not suggest any names.This is still a human in the loop process. The ELIS user performing MNH tasks prompted to make a decision if the suggested names and DOBs are related to case hence are able to accept/reject the suggestions.",Operation and Maintenance,Neither,7/10/2020,7/10/2020,6/27/2022,Developed with contracting resources.,70SBUR23F00000120,No,No,Yes,No,Yes,"1. To train this deep learning model, we’ve gathered the text from about 8,000 IdHS records.
2. Typically, annotation of these entities would be done by hand. But in this case, we’re able to leverage the manual name harvesting that’s already been completed by ELIS users and therefore systematically tag our training data.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,ELIS,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"To reduce the amount of adjudicative time spent manually harvesting aliases and dates of birth (DOBs) from identity history summary (idHS) report attached to the ELIS case as part of the Manual Name Harvesting Task during case processing. . Eliminates need for manual review by extracting unique names and DOBs from IdHS documents and when names/DOBs already in ELIS, ANH will not suggest any names.This is still a human in the loop process. The ELIS user performing MNH tasks prompted to make a decision if the suggested names and DOBs are related to case hence are able to accept/reject the suggestions.","to reduce the amount of adjudicative time spent manually harvesting aliases and dates of birth (dobs) from identity history summary (idhs) report attached to the elis case as part of the manual name harvesting task during case processing. . eliminates need for manual review by extracting unique names and dobs from idhs documents and when names/dobs already in elis, anh will not suggest any names.this is still a human in the loop process. the elis user performing mnh tasks prompted to make a decision if the suggested names and dobs are related to case hence are able to accept/reject the suggestions."
Automated Realtime Global Organization Specialist (ARGOS) for Company Registration Submissions to E-Verify,Department of Homeland Security,DHS,USCIS,Mission-Enabling,None of the above.,"ARGOS sentiment analysis produces a risk score and keyword extraction identifies the keword category of interest to the VAC MPAs (management and program analyst) for the aggregated open-source information to help quickly identify any pertinent information to aid the MPAs in their open-source investigation of company applications.  This saves potentially thousands of MPA man hours in open-source investigation and creates a single source-of-truth for each MPAs investigation of a company application.  This, in turn, allows for quicker application processing and, if risk of company fraud exists, much faster referral processing time quickening the next-step referral to FDNS for further investigations.",Responses back to a user dashboard accessible internally only by VAC Management and Program Analyst (MPA) personnel.  Keywords relating to the MPA's work interest are extracted if present and risk scores are assigned to the open-source collected information. The data is presented to the MPA on the GUI (graphical user interface) dashboard.,Operation and Maintenance,"Rights-Impacting
",6/16/2021,6/16/2021,8/12/2023,Developed with contracting resources.,70SBUR24F00000163,No,No,No,No,Other,The fine-tuned dataset is collected from open-source queries from the Bing API connected to the ARGOS system.  This is publicly available data that doesn't contain any PII.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,ARGOS (Automated Real-Time Global Organization Specialist),Less than 6 months,Yes,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,"Data Collection Bias: search engine prioritization of certain content may skew results. Lack of Domain-Specific Accuracy: model tested on company data across different industries resulted in inconsistent performance. Limited Generalization to Unseen Data: model’s performance on validation datasets lower than on training data, indicating potential overfitting. Misinterpretation of Sentiment: instances of sarcasm/irony not recognized. All risks identified in testing and evaluation phases. ",Yes – by the CAIO,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"Bias monitoring, fine-tuning data balancing, and statistical precision deviation monitoring have been put in place.",None of the above,Yes,Rights-Impacting,0.746031746031746,"ARGOS sentiment analysis produces a risk score and keyword extraction identifies the keword category of interest to the VAC MPAs (management and program analyst) for the aggregated open-source information to help quickly identify any pertinent information to aid the MPAs in their open-source investigation of company applications.  This saves potentially thousands of MPA man hours in open-source investigation and creates a single source-of-truth for each MPAs investigation of a company application.  This, in turn, allows for quicker application processing and, if risk of company fraud exists, much faster referral processing time quickening the next-step referral to FDNS for further investigations. . Responses back to a user dashboard accessible internally only by VAC Management and Program Analyst (MPA) personnel.  Keywords relating to the MPA's work interest are extracted if present and risk scores are assigned to the open-source collected information. The data is presented to the MPA on the GUI (graphical user interface) dashboard. . Data Collection Bias: search engine prioritization of certain content may skew results. Lack of Domain-Specific Accuracy: model tested on company data across different industries resulted in inconsistent performance. Limited Generalization to Unseen Data: model’s performance on validation datasets lower than on training data, indicating potential overfitting. Misinterpretation of Sentiment: instances of sarcasm/irony not recognized. All risks identified in testing and evaluation phases.","argos sentiment analysis produces a risk score and keyword extraction identifies the keword category of interest to the vac mpas (management and program analyst) for the aggregated open-source information to help quickly identify any pertinent information to aid the mpas in their open-source investigation of company applications. this saves potentially thousands of mpa man hours in open-source investigation and creates a single source-of-truth for each mpas investigation of a company application. this, in turn, allows for quicker application processing and, if risk of company fraud exists, much faster referral processing time quickening the next-step referral to fdns for further investigations. . responses back to a user dashboard accessible internally only by vac management and program analyst (mpa) personnel. keywords relating to the mpa's work interest are extracted if present and risk scores are assigned to the open-source collected information. the data is presented to the mpa on the gui (graphical user interface) dashboard. . data collection bias: search engine prioritization of certain content may skew results. lack of domain-specific accuracy: model tested on company data across different industries resulted in inconsistent performance. limited generalization to unseen data: model’s performance on validation datasets lower than on training data, indicating potential overfitting. misinterpretation of sentiment: instances of sarcasm/irony not recognized. all risks identified in testing and evaluation phases."
ELIS Card Photo Validation via myUSCIS,Department of Homeland Security,DHS,USCIS,Government Services (includes Benefits and Service Delivery),None of the above.,"USCIS uses a system called ELIS to manage immigration requests, and it includes a Photo Validation Service to check if ID photos meet requirements. This helps ensure photos are correct before making ID cards, saving time and avoiding delays.

The photo validation service uses a combination of computer vision techniques and machine learning models to validate photographs and ensure they meet the requirements, so these photos can be used in card production.",Response back to user based on the pre-defined quality checks if the uploaded photo meets USCIS requirements. Users still have the option to ignore the warnings and upload the photo.,Operation and Maintenance,Neither,12/9/2020,12/9/2020,3/15/2022,Developed with contracting resources.,70SBUR23F00000120,No,No,Yes,Yes,Yes,"Applicant submitted scanned or digital photographs sent to ELIS for processing, used for creating custom models or fine-tuning open source models and evaluation.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,ELIS,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"USCIS uses a system called ELIS to manage immigration requests, and it includes a Photo Validation Service to check if ID photos meet requirements. This helps ensure photos are correct before making ID cards, saving time and avoiding delays.

The photo validation service uses a combination of computer vision techniques and machine learning models to validate photographs and ensure they meet the requirements, so these photos can be used in card production. . Response back to user based on the pre-defined quality checks if the uploaded photo meets USCIS requirements. Users still have the option to ignore the warnings and upload the photo.","uscis uses a system called elis to manage immigration requests, and it includes a photo validation service to check if id photos meet requirements. this helps ensure photos are correct before making id cards, saving time and avoiding delays. the photo validation service uses a combination of computer vision techniques and machine learning models to validate photographs and ensure they meet the requirements, so these photos can be used in card production. . response back to user based on the pre-defined quality checks if the uploaded photo meets uscis requirements. users still have the option to ignore the warnings and upload the photo."
I-765 - USCIS Facial Recognition through IDENT (1:1 Face Recognition/Validation),Department of Homeland Security,DHS,USCIS,Law & Justice,None of the above.,This will allow the user to complete the biometric verification requirement without having to attend an appointment at am Applicant Support Center. This reduces the burden on the beneficiary as well as reducing demands on USCIS Applicant Service Center resources.,Match or no match response from IDENT.,Operation and Maintenance,"Rights-Impacting
",10/3/2023,10/3/2023,11/12/2024,Developed with contracting resources.,70SBUR23F00000126,Yes,No,Yes,Yes,No,"N/A - Agency does not own any data used to train, fine-tune, or evaluate the use case.","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Customer Profile Management System,Less than 6 months,No,No,Other,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,Potential mismatch of face images and/or bias based on demographic data held by USCIS.   USCIS is in the process of developing a reporting mechanism for false negative matches that will highlight any disparate impacts on various demographics. USCIS will produce an internal annual overview of the facial recognition results and any data points USCIS has about demographics. The use case has been found to not lead to or perpetuate unlawful discrimination or bias.,Yes – by the CAIO,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,The development of the face matching AI by the vendor began several years ago and it has been continuously updated since. . The algorithms are industry leading as measured by NIST’s Face Recognition Technology Evaluation (FRTE) n:1 benchmark. All testing met or exceeded expectations. [Meets § 5(c)(iv)(B)],"General solicitations of comments from the public,Other",Yes,Rights-Impacting,0.7619047619047619,This will allow the user to complete the biometric verification requirement without having to attend an appointment at am Applicant Support Center. This reduces the burden on the beneficiary as well as reducing demands on USCIS Applicant Service Center resources. . Match or no match response from IDENT. . Potential mismatch of face images and/or bias based on demographic data held by USCIS.   USCIS is in the process of developing a reporting mechanism for false negative matches that will highlight any disparate impacts on various demographics. USCIS will produce an internal annual overview of the facial recognition results and any data points USCIS has about demographics. The use case has been found to not lead to or perpetuate unlawful discrimination or bias.,this will allow the user to complete the biometric verification requirement without having to attend an appointment at am applicant support center. this reduces the burden on the beneficiary as well as reducing demands on uscis applicant service center resources. . match or no match response from ident. . potential mismatch of face images and/or bias based on demographic data held by uscis. uscis is in the process of developing a reporting mechanism for false negative matches that will highlight any disparate impacts on various demographics. uscis will produce an internal annual overview of the facial recognition results and any data points uscis has about demographics. the use case has been found to not lead to or perpetuate unlawful discrimination or bias.
Person-Centric Identity Services Deduplication Model,Department of Homeland Security,DHS,USCIS,Law & Justice,None of the above.,"Using Machine Learning allows us to improve entity resolution as compared to rule based system.

PCIS offers the ability to see a person's immigration history organized in one place. Specific benefits do or will include: an organized summary view of the identity with the individual's latest photo from PCIS; full immigration history including receipts associated with the applicant, regardless of case management system; mailing, physical, and safe history of the individual organized in reverse chronological order, allowing users to easily find the most recent address; and all identifiers associated with the applicant, including A-Numbers, FINs, SSNs, SSNs, ELIS account numbers, passport numbers, etc.","Numerical likelihood score which is used to determine if the record belongs to the individual. Likelihood scores are subjected to a high threshold (.98, maximum 1) to assess whether the record belongs to the individual.",Operation and Maintenance,"Rights-Impacting
",1/1/2020,1/1/2020,2/1/2023,Developed with contracting resources.,47QTCA22D00AB,No,No,Yes,Yes,Yes,"USCIS-only data derived from 7 form-processing source systems including C3, ELIS, CPMS, GLOBAL, CIS2, AR-11, CAMINO.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Person Centric Identity Services,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Yes,"There is small risk of false positives or negatives, which are identified and sent to Manual Resolution Queue. The queue is processed by authorized and trained personnel. Human review is still done for the actual benefit or request being sought. AI is used to identify the person seeking the benefit or request.  ",Yes – by the CAIO,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,N/A - The model is agnostic to demographics.,Direct user testing,No – it is not operationally practical to offer this.,Rights-Impacting,0.7619047619047619,"Using Machine Learning allows us to improve entity resolution as compared to rule based system.

PCIS offers the ability to see a person's immigration history organized in one place. Specific benefits do or will include: an organized summary view of the identity with the individual's latest photo from PCIS; full immigration history including receipts associated with the applicant, regardless of case management system; mailing, physical, and safe history of the individual organized in reverse chronological order, allowing users to easily find the most recent address; and all identifiers associated with the applicant, including A-Numbers, FINs, SSNs, SSNs, ELIS account numbers, passport numbers, etc. . Numerical likelihood score which is used to determine if the record belongs to the individual. Likelihood scores are subjected to a high threshold (.98, maximum 1) to assess whether the record belongs to the individual. . There is small risk of false positives or negatives, which are identified and sent to Manual Resolution Queue. The queue is processed by authorized and trained personnel. Human review is still done for the actual benefit or request being sought. AI is used to identify the person seeking the benefit or request.","using machine learning allows us to improve entity resolution as compared to rule based system. pcis offers the ability to see a person's immigration history organized in one place. specific benefits do or will include: an organized summary view of the identity with the individual's latest photo from pcis; full immigration history including receipts associated with the applicant, regardless of case management system; mailing, physical, and safe history of the individual organized in reverse chronological order, allowing users to easily find the most recent address; and all identifiers associated with the applicant, including a-numbers, fins, ssns, ssns, elis account numbers, passport numbers, etc. . numerical likelihood score which is used to determine if the record belongs to the individual. likelihood scores are subjected to a high threshold (.98, maximum 1) to assess whether the record belongs to the individual. . there is small risk of false positives or negatives, which are identified and sent to manual resolution queue. the queue is processed by authorized and trained personnel. human review is still done for the actual benefit or request being sought. ai is used to identify the person seeking the benefit or request."
Person-Centric Identity Services A-Number Management Model,Department of Homeland Security,DHS,USCIS,Law & Justice,None of the above.,"The aim of this use case is to leverage machine learning to test the accuracy of PCIS to identify and manage associations between individuals and their assigned A-numbers, which is a unique 7, 8, or 9 digit number assigned to a noncitizen by DHS.  The A-Number plays a critical role in surfacing of a person and all their associated records from across PCIS.","Numerical likelihood score which is used to determine the validity of the A# presented. Likelihood scores are subjected to a high threshold (.98, maximum 1) to assess whether the A# presented belongs to the individual.",Operation and Maintenance,Neither,1/1/2022,1/1/2022,7/1/2022,Developed with contracting resources.,47QTCA22D00AB,No,No,Yes,Yes,Yes,"USCIS-only data derived from 7 form-processing source systems including C3, ELIS, CPMS, GLOBAL, CIS2, AR-11, CAMINO.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Person Centric Identity Services,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The aim of this use case is to leverage machine learning to test the accuracy of PCIS to identify and manage associations between individuals and their assigned A-numbers, which is a unique 7, 8, or 9 digit number assigned to a noncitizen by DHS.  The A-Number plays a critical role in surfacing of a person and all their associated records from across PCIS. . Numerical likelihood score which is used to determine the validity of the A# presented. Likelihood scores are subjected to a high threshold (.98, maximum 1) to assess whether the A# presented belongs to the individual.","the aim of this use case is to leverage machine learning to test the accuracy of pcis to identify and manage associations between individuals and their assigned a-numbers, which is a unique 7, 8, or 9 digit number assigned to a noncitizen by dhs. the a-number plays a critical role in surfacing of a person and all their associated records from across pcis. . numerical likelihood score which is used to determine the validity of the a# presented. likelihood scores are subjected to a high threshold (.98, maximum 1) to assess whether the a# presented belongs to the individual."
Identity Match Option (IMO) Tool for Record Compilation,Department of Homeland Security,DHS,USCIS,Mission-Enabling,None of the above.,"USCIS uses the IMO tool to aid in person-centric research and analytics. More specifically, IMO is used to derive a single identity across multiple systems for each applicant or beneficiary who interacts with USCIS.","A user friendly dashboard is created to display results and shows the data pattern but does not allow for any prediction or decision making. IMO is a COTS product offered by Informatica. The product has a variety of “transformations” that can be used together to build a workflow based solution. There are a number of different algorithms (Soundex, Jaro, Hamming distance, etc.) which are available for use in performing identity resolution.",Operation and Maintenance,Neither,1/3/2017,3/13/2017,1/3/2018,Developed with contracting resources.,70SBUR21F00000194,No,No,No,No,No,"N/A - The Agency does not own any of the data used to train, fine-tune, and/or evaluate performance of the model(s) used in this use case.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"USCIS uses the IMO tool to aid in person-centric research and analytics. More specifically, IMO is used to derive a single identity across multiple systems for each applicant or beneficiary who interacts with USCIS. . A user friendly dashboard is created to display results and shows the data pattern but does not allow for any prediction or decision making. IMO is a COTS product offered by Informatica. The product has a variety of “transformations” that can be used together to build a workflow based solution. There are a number of different algorithms (Soundex, Jaro, Hamming distance, etc.) which are available for use in performing identity resolution.","uscis uses the imo tool to aid in person-centric research and analytics. more specifically, imo is used to derive a single identity across multiple systems for each applicant or beneficiary who interacts with uscis. . a user friendly dashboard is created to display results and shows the data pattern but does not allow for any prediction or decision making. imo is a cots product offered by informatica. the product has a variety of “transformations” that can be used together to build a workflow based solution. there are a number of different algorithms (soundex, jaro, hamming distance, etc.) which are available for use in performing identity resolution."
Criminal Investigations (OBIM),Department of Homeland Security,DHS,USSS,Law & Justice,None of the above.,"The intended purpose of this AI is so that USSS INV personnel may submit available photographs or video stills of these unknown persons as probe images (facial images or templates searched against the gallery of an FRS) to other government agencies for comparison against their image galleries. The agencies will query their image galleries of known persons and may provide lists of potential matches. USSS INV personnel may use the potential matches to produce investigative leads which will assist in the further identification of victims or suspects. Additionally, USSS INV personnel may request another government agency to conduct a one-to-one comparison of two photographs or video stills  for investigative use.",The system will query image galleries of known persons and may provide lists of potential matches. USSS INV personnel may use the potential matches to produce investigative leads which will assist in the further identification of victims or suspects.,Operation and Maintenance,"Rights-Impacting
",10/30/2023,1/1/2016,1/1/2017,Developed with both contracting and in-house resources.,RUIM-25-00019,No,No,Yes,Yes,Yes,OBIM is the data steward for the data in the IDENT system. The data owners are the 45 US and international organizations that collect and submit to IDENT - including DHS Components.,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Automated Biometric Identification System (IDENT),Less than 6 months,Yes,Other,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Yes,"The product was developed by the NEC Corporation using AI and Deep Machine Learning to train the algorithm. However, the current NEC product that is used by OBIM in the production environment does not use AI to continue to train the NEC algorithm on production data. The fact that OBIM/NEC do not use AI on production data to continue to train the algorithm significantly limits the risks associated with the use of AI and ML on the face candidate list process.",Yes – by the CAIO,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"OBIM is currently working with the DHS CIO's office and DHS Science and Technology (S&T) to deeply evaluate the NEC P1 version algorithm that is currently employed by OBIM for the face candidate service. The NEC NeoFace P1 algorithm has also already been included in the National Institute of Standards and Technology (NIST) face recognition vendor testing and evaluation process. 
",None of the above,Yes,Rights-Impacting,0.746031746031746,"The intended purpose of this AI is so that USSS INV personnel may submit available photographs or video stills of these unknown persons as probe images (facial images or templates searched against the gallery of an FRS) to other government agencies for comparison against their image galleries. The agencies will query their image galleries of known persons and may provide lists of potential matches. USSS INV personnel may use the potential matches to produce investigative leads which will assist in the further identification of victims or suspects. Additionally, USSS INV personnel may request another government agency to conduct a one-to-one comparison of two photographs or video stills  for investigative use. . The system will query image galleries of known persons and may provide lists of potential matches. USSS INV personnel may use the potential matches to produce investigative leads which will assist in the further identification of victims or suspects. . The product was developed by the NEC Corporation using AI and Deep Machine Learning to train the algorithm. However, the current NEC product that is used by OBIM in the production environment does not use AI to continue to train the NEC algorithm on production data. The fact that OBIM/NEC do not use AI on production data to continue to train the algorithm significantly limits the risks associated with the use of AI and ML on the face candidate list process.","the intended purpose of this ai is so that usss inv personnel may submit available photographs or video stills of these unknown persons as probe images (facial images or templates searched against the gallery of an frs) to other government agencies for comparison against their image galleries. the agencies will query their image galleries of known persons and may provide lists of potential matches. usss inv personnel may use the potential matches to produce investigative leads which will assist in the further identification of victims or suspects. additionally, usss inv personnel may request another government agency to conduct a one-to-one comparison of two photographs or video stills for investigative use. . the system will query image galleries of known persons and may provide lists of potential matches. usss inv personnel may use the potential matches to produce investigative leads which will assist in the further identification of victims or suspects. . the product was developed by the nec corporation using ai and deep machine learning to train the algorithm. however, the current nec product that is used by obim in the production environment does not use ai to continue to train the nec algorithm on production data. the fact that obim/nec do not use ai on production data to continue to train the algorithm significantly limits the risks associated with the use of ai and ml on the face candidate list process."
GitHub Copilot for Code Modernization,Department of Commerce,DOC,BEA - Bureau of Economic Analysis,Mission-Enabling,Unknown,Accelerate and improve the quality of code modernization projects at BEA,"First-draft of code converted from legacy languages into modern, supported, target language. Code is then reviewed and edited by humans prior to use in an application.",Operation and Maintenance,Neither,10/1/2023,1/1/2024,12/1/2024,Developed with contracting resources.,1331L524F13210282,No,No,No,Yes,Yes,N/A--We are doing no model training.,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,BEA-ITS,Less than 6 months,Yes,Other,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Yes,If the code produced by this tool is not sufficiently reviewed and tested then the data outputs calculated by the final product may not be accurate and the quality of inputs to BEA's PFEI statistics may be compromised.  Key risk identified through completion of a Risk Analysis performed by the ITSO and CIO team.,Planned or in-progress,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.","Other – Immediate human intervention is not practicable; however, an appropriate fail-safe has been implemented. ","N/A; COTS tool used for code conversion, no individual's information is input into a model.","N/A; COTS tool used for code conversion, no individual's information is input into a model.","N/A; COTS tool used for code conversion, no individual's information is input into a model.",Neither,0.7936507936507936,"Accelerate and improve the quality of code modernization projects at BEA . First-draft of code converted from legacy languages into modern, supported, target language. Code is then reviewed and edited by humans prior to use in an application. . If the code produced by this tool is not sufficiently reviewed and tested then the data outputs calculated by the final product may not be accurate and the quality of inputs to BEA's PFEI statistics may be compromised.  Key risk identified through completion of a Risk Analysis performed by the ITSO and CIO team.","accelerate and improve the quality of code modernization projects at bea . first-draft of code converted from legacy languages into modern, supported, target language. code is then reviewed and edited by humans prior to use in an application. . if the code produced by this tool is not sufficiently reviewed and tested then the data outputs calculated by the final product may not be accurate and the quality of inputs to bea's pfei statistics may be compromised. key risk identified through completion of a risk analysis performed by the itso and cio team."
Azure OpenAI Chat Prototype,Department of Commerce,DOC,BEA - Bureau of Economic Analysis,Mission-Enabling,Unknown,"Provide staff with the ability to chat with an LLM model, comparable to ChatGPT, that is contained within BEA's network boundary.",Text and data anlysis,Initiated,Neither,10/12023,Unknown,Unknown,Developed with contracting resources.,1331L524F13210282,No,No,No,No,Yes,N/A--We are doing no model training.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,If the outputs from the chat tool are not accurate and are not reviewed sufficiently by the human writing the prompt then the quality of the end document will be compromised.  ,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,N/A; still in initiation phase,None of the above,No – it is not operationally practical to offer this.,Neither,0.746031746031746,"Provide staff with the ability to chat with an LLM model, comparable to ChatGPT, that is contained within BEA's network boundary. . Text and data anlysis . If the outputs from the chat tool are not accurate and are not reviewed sufficiently by the human writing the prompt then the quality of the end document will be compromised.","provide staff with the ability to chat with an llm model, comparable to chatgpt, that is contained within bea's network boundary. . text and data anlysis . if the outputs from the chat tool are not accurate and are not reviewed sufficiently by the human writing the prompt then the quality of the end document will be compromised."
Sentinel One Purple AI,Department of Commerce,DOC,BEA - Bureau of Economic Analysis,Mission-Enabling,Unknown,Faster and more throughout incident response within the BEA cybersecurity program,Log analysis,Initiated,Neither,7/1/2024,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Yes,N/A--We are doing no model training.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,More than 12 months,Yes,Other,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,No,If the outputs of the AI analysis of BEA logs is not accurate then the timeliness and accuracy of BEA's incident response program will suffer,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,N/A; still in initiation phase,None of the above,No – it is not operationally practical to offer this.,Neither,0.746031746031746,Faster and more throughout incident response within the BEA cybersecurity program . Log analysis . If the outputs of the AI analysis of BEA logs is not accurate then the timeliness and accuracy of BEA's incident response program will suffer,faster and more throughout incident response within the bea cybersecurity program . log analysis . if the outputs of the ai analysis of bea logs is not accurate then the timeliness and accuracy of bea's incident response program will suffer
Automated Change Detection,Department of Commerce,DOC,CENSUS - U.S. Census Bureau,Mission-Enabling (internal agency support),None of the above.,"As part of the Census Bureau's efforts to maintain and update the Geographic Frame ahead of the 2030 Decennial Census, the Geography Division is using AI to identify changes in the built environment throughout the nation twice a year. With the areas of change we are extracting our own geographic features (roads, building footprints) to support improving coverage across the nation.",We use Google Earth Engine to analyze change in construction using Sentinel-2 imagery and recommend where to use high resolution imagery. The object detection provides roads and building footprints to improve our feature coverage.,Implementation and Assessment,Neither,1/4/2021,5/24/2023,8/1/2023,Developed with both contracting and in-house resources.,47QTCA19D00MP,Yes,No,No,No,Yes,"The high resolution imagery for training our building footprints and road predictions is from Maxar, USG Plus through National Geospatial-Intelligence Agency Global Enhanced GEOINT Delivery system. We are evaluating extracted features produced from the high resolution imagery and fine-tuning the output with staff to correct any inconsistencies.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Google Cloud Platform and Enterprise Data Lake,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"As part of the Census Bureau's efforts to maintain and update the Geographic Frame ahead of the 2030 Decennial Census, the Geography Division is using AI to identify changes in the built environment throughout the nation twice a year. With the areas of change we are extracting our own geographic features (roads, building footprints) to support improving coverage across the nation. . We use Google Earth Engine to analyze change in construction using Sentinel-2 imagery and recommend where to use high resolution imagery. The object detection provides roads and building footprints to improve our feature coverage.","as part of the census bureau's efforts to maintain and update the geographic frame ahead of the 2030 decennial census, the geography division is using ai to identify changes in the built environment throughout the nation twice a year. with the areas of change we are extracting our own geographic features (roads, building footprints) to support improving coverage across the nation. . we use google earth engine to analyze change in construction using sentinel-2 imagery and recommend where to use high resolution imagery. the object detection provides roads and building footprints to improve our feature coverage."
School staff information extraction from web page text,Department of Commerce,DOC,CENSUS - U.S. Census Bureau,Education & Workforce,None of the above.,This AI use case examines publicly available information on the web related to staff rosters at public and private schools. The goal is to improve quality and coverage of Teacher and Principal frame.,The AI in this use case predicts whether a link on a webpage leads to a staff directory and extracts person names and teacher titles from webpage text.,Implementation and Assessment,Neither,1/15/2021,1/15/2021,5/10/2023,Developed with both contracting and in-house resources.,Brite Group,No,No,Yes,No,Yes,School employment lists and web scraped data.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,This AI use case examines publicly available information on the web related to staff rosters at public and private schools. The goal is to improve quality and coverage of Teacher and Principal frame. . The AI in this use case predicts whether a link on a webpage leads to a staff directory and extracts person names and teacher titles from webpage text.,this ai use case examines publicly available information on the web related to staff rosters at public and private schools. the goal is to improve quality and coverage of teacher and principal frame. . the ai in this use case predicts whether a link on a webpage leads to a staff directory and extracts person names and teacher titles from webpage text.
Census Bureau Demographic Frame Person-Place Model,Department of Commerce,DOC,CENSUS - U.S. Census Bureau,Mission-Enabling (internal agency support),None of the above.,"The Demographic Frame’s Person-Place Model (PPM) uses machine learning to assign individuals to their most accurate residential locations. This model addresses a critical need for accurate “person-place” associations, enabling the Census Bureau to support a range of essential use cases—from demographic analyses to population estimates. By leveraging machine learning, the PPM generates predicted probabilities for each person-address record, refining location accuracy while accommodating diverse data sources. This innovation strengthens the Census Bureau’s mission to deliver reliable, high-quality data, benefiting both governmental operations and the general public by enhancing data precision, policy-making, and service delivery.","The Person-Place Model (PPM) in the Demographic Frame outputs predicted probabilities indicating the likelihood that a person resides at a specific address. These predictions are generated by a machine learning model that analyzes historical and current data to produce a probability score for each person-address record. These scores do not constitute a final decision or assignment; instead, they provide a flexible basis for determining residential locations based on the desired level of confidence or specific application needs. This output can be used as input to downstream processes, providing robust, data-driven estimates to support various Census Bureau use cases and enhance the accuracy of demographic analyses.",Acquisition and/or Development,Neither,10/28/2021,10/28/2021,Unknown,Developed with both contracting and in-house resources.,1331L523D13OS0003,No,Unknown,Unknown,Unknown,Yes,American Community Survey data,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"The Demographic Frame’s Person-Place Model (PPM) uses machine learning to assign individuals to their most accurate residential locations. This model addresses a critical need for accurate “person-place” associations, enabling the Census Bureau to support a range of essential use cases—from demographic analyses to population estimates. By leveraging machine learning, the PPM generates predicted probabilities for each person-address record, refining location accuracy while accommodating diverse data sources. This innovation strengthens the Census Bureau’s mission to deliver reliable, high-quality data, benefiting both governmental operations and the general public by enhancing data precision, policy-making, and service delivery. . The Person-Place Model (PPM) in the Demographic Frame outputs predicted probabilities indicating the likelihood that a person resides at a specific address. These predictions are generated by a machine learning model that analyzes historical and current data to produce a probability score for each person-address record. These scores do not constitute a final decision or assignment; instead, they provide a flexible basis for determining residential locations based on the desired level of confidence or specific application needs. This output can be used as input to downstream processes, providing robust, data-driven estimates to support various Census Bureau use cases and enhance the accuracy of demographic analyses.","the demographic frame’s person-place model (ppm) uses machine learning to assign individuals to their most accurate residential locations. this model addresses a critical need for accurate “person-place” associations, enabling the census bureau to support a range of essential use cases—from demographic analyses to population estimates. by leveraging machine learning, the ppm generates predicted probabilities for each person-address record, refining location accuracy while accommodating diverse data sources. this innovation strengthens the census bureau’s mission to deliver reliable, high-quality data, benefiting both governmental operations and the general public by enhancing data precision, policy-making, and service delivery. . the person-place model (ppm) in the demographic frame outputs predicted probabilities indicating the likelihood that a person resides at a specific address. these predictions are generated by a machine learning model that analyzes historical and current data to produce a probability score for each person-address record. these scores do not constitute a final decision or assignment; instead, they provide a flexible basis for determining residential locations based on the desired level of confidence or specific application needs. this output can be used as input to downstream processes, providing robust, data-driven estimates to support various census bureau use cases and enhance the accuracy of demographic analyses."
Race and Ethnicity Autocoding,Department of Commerce,DOC,CENSUS - U.S. Census Bureau,Mission-Enabling (internal agency support),None of the above.,This AI use case aims to develop an automated methodology for coding race and ethnicity write-in entries. The goal is to improve quality and efficiency in the coding of race and ethnicity write-in entries.,The AI produces a predicted set of standardized race and ethnicity concept codes that align with the write-in response.,Acquisition and/or Development,Neither,12/1/2021,2/15/2022,Unknown,Developed in-house.,Unknown,No,Unknown,Unknown,Unknown,Yes,Response and clerical data.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,This AI use case aims to develop an automated methodology for coding race and ethnicity write-in entries. The goal is to improve quality and efficiency in the coding of race and ethnicity write-in entries. . The AI produces a predicted set of standardized race and ethnicity concept codes that align with the write-in response.,this ai use case aims to develop an automated methodology for coding race and ethnicity write-in entries. the goal is to improve quality and efficiency in the coding of race and ethnicity write-in entries. . the ai produces a predicted set of standardized race and ethnicity concept codes that align with the write-in response.
Information extraction for web scraped data for Group Quarters frame enhancement,Department of Commerce,DOC,CENSUS - U.S. Census Bureau,Mission-Enabling (internal agency support),None of the above.,This AI use case examines publicly available information on the web related to a variety of Group Quarters (GQ) types. The goal is to improve quality and coverage of GQ frame.,"Prediction label that corresponds to one of the following : extracted capacity of the GQ, extracted address component of the GQ, and extracted concepts, such as dates and building names, identified from pretrained NER models.",Implementation and Assessment,Neither,12/1/2021,5/16/2022,8/16/2024,Developed with both contracting and in-house resources.,Brite Group,No,No,No,No,Yes,Address lists and web scraped data.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"This AI use case examines publicly available information on the web related to a variety of Group Quarters (GQ) types. The goal is to improve quality and coverage of GQ frame. . Prediction label that corresponds to one of the following : extracted capacity of the GQ, extracted address component of the GQ, and extracted concepts, such as dates and building names, identified from pretrained NER models.","this ai use case examines publicly available information on the web related to a variety of group quarters (gq) types. the goal is to improve quality and coverage of gq frame. . prediction label that corresponds to one of the following : extracted capacity of the gq, extracted address component of the gq, and extracted concepts, such as dates and building names, identified from pretrained ner models."
Current Population Survey (CPS) Name Screening Tool,Department of Commerce,DOC,CENSUS - U.S. Census Bureau,Mission-Enabling (internal agency support),None of the above.,This AI use case processes and classifies entries in survey data collect to identify if the name provided is a valid entry. The goal is to support and enhance processing and future data collection efforts to efficiently and accurately identify responses with invalid names.,Classifications for name entries.,Acquisition and/or Development,Neither,3/10/2023,3/29/2023,Unknown,Developed with both contracting and in-house resources.,Brite Group,No,Unknown,Unknown,Unknown,Yes,Sample of survey response data from the Current Population Survey (CPS),"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,This AI use case processes and classifies entries in survey data collect to identify if the name provided is a valid entry. The goal is to support and enhance processing and future data collection efforts to efficiently and accurately identify responses with invalid names. . Classifications for name entries.,this ai use case processes and classifies entries in survey data collect to identify if the name provided is a valid entry. the goal is to support and enhance processing and future data collection efforts to efficiently and accurately identify responses with invalid names. . classifications for name entries.
Linkage and Matching Program (LaMP),Department of Commerce,DOC,CENSUS - U.S. Census Bureau,Mission-Enabling (internal agency support),None of the above.,"This use case involves probabilistic record linkage to link multiple person-level datasets. The predicted links are used for downstream processing to enable research regarding economic, demographic, and other topics and improve quality and efficiency of multiple Census operations.",The AI produces a prediction regarding whether two records match. Output from the model is pairs of records above a minimum threshold indicating a match along with the score describing the similarity between the two records in the pair.,Acquisition and/or Development,Neither,3/31/2023,8/1/2024,Unknown,Developed with both contracting and in-house resources.,"Brite Group, Harmonia",No,Unknown,Unknown,Unknown,Yes,Survey response data from decennial census operations.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"This use case involves probabilistic record linkage to link multiple person-level datasets. The predicted links are used for downstream processing to enable research regarding economic, demographic, and other topics and improve quality and efficiency of multiple Census operations. . The AI produces a prediction regarding whether two records match. Output from the model is pairs of records above a minimum threshold indicating a match along with the score describing the similarity between the two records in the pair.","this use case involves probabilistic record linkage to link multiple person-level datasets. the predicted links are used for downstream processing to enable research regarding economic, demographic, and other topics and improve quality and efficiency of multiple census operations. . the ai produces a prediction regarding whether two records match. output from the model is pairs of records above a minimum threshold indicating a match along with the score describing the similarity between the two records in the pair."
Census Research Exploration and Analysis Tool (CREAT),Department of Commerce,DOC,CENSUS - U.S. Census Bureau,Education & Workforce,None of the above.,"The Census Research Exploration and Analysis Tool (CREAT) is an experimental data tool from the Center for Economic Studies (CES) at the US Census Bureau that uses natural language processing to analyze, categorize, and sort the economic research contained in the CES working paper series. The goal of this project is to help CES researchers, managers, and other internal stakeholders explore connections among existing research, form new collaborations, and separate research into discrete topics. Working papers are sortable by author, tag, and keyword. Natural language processing (NLP) techniques are used to extract tags (using spaCy) and keywords (using KeyBERT) from the paper's text, as well as to determine working paper similarity (using Doc2Vec). This NLP usage highlights the value of these tools and techniques for making connections across economic research publications.","keywords, tags, and recommendations based on similarity.",Implementation and Assessment,Neither,11/1/2023,12/1/2023,3/1/2024,Developed in-house.,Unknown,No,Yes,No,No,No,CES working paper metadata,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The Census Research Exploration and Analysis Tool (CREAT) is an experimental data tool from the Center for Economic Studies (CES) at the US Census Bureau that uses natural language processing to analyze, categorize, and sort the economic research contained in the CES working paper series. The goal of this project is to help CES researchers, managers, and other internal stakeholders explore connections among existing research, form new collaborations, and separate research into discrete topics. Working papers are sortable by author, tag, and keyword. Natural language processing (NLP) techniques are used to extract tags (using spaCy) and keywords (using KeyBERT) from the paper's text, as well as to determine working paper similarity (using Doc2Vec). This NLP usage highlights the value of these tools and techniques for making connections across economic research publications. . keywords, tags, and recommendations based on similarity.","the census research exploration and analysis tool (creat) is an experimental data tool from the center for economic studies (ces) at the us census bureau that uses natural language processing to analyze, categorize, and sort the economic research contained in the ces working paper series. the goal of this project is to help ces researchers, managers, and other internal stakeholders explore connections among existing research, form new collaborations, and separate research into discrete topics. working papers are sortable by author, tag, and keyword. natural language processing (nlp) techniques are used to extract tags (using spacy) and keywords (using keybert) from the paper's text, as well as to determine working paper similarity (using doc2vec). this nlp usage highlights the value of these tools and techniques for making connections across economic research publications. . keywords, tags, and recommendations based on similarity."
Dr. NAICS LLM,Department of Commerce,DOC,CENSUS - U.S. Census Bureau,Mission-Enabling (internal agency support),Searching for information using AI.,The core problem the Dr. NAICS LLM addresses is the ability to accurately and consistently place a NAICS code for a given business description. Because NAICS is such a large classification system with a lot of nuance it is sometimes difficult for businesses to identify the correct business description within it. The AI makes suggestions to improve that classification decision when reading incoming emails and voicemails from the general public.,The output is a set of recommendations for which NAICS codes best align with a text description of a business activity. ,Implementation and Assessment,Neither,11/27/2023,11/27/2023,9/30/2025,Developed with both contracting and in-house resources.,1333LC23C00000030 and 1333LC23C00000026,No,No,No,No,No,Unknown,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",No,Unknown,No,Unknown,Less than 6 months,Other,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,The core problem the Dr. NAICS LLM addresses is the ability to accurately and consistently place a NAICS code for a given business description. Because NAICS is such a large classification system with a lot of nuance it is sometimes difficult for businesses to identify the correct business description within it. The AI makes suggestions to improve that classification decision when reading incoming emails and voicemails from the general public. . The output is a set of recommendations for which NAICS codes best align with a text description of a business activity.,the core problem the dr. naics llm addresses is the ability to accurately and consistently place a naics code for a given business description. because naics is such a large classification system with a lot of nuance it is sometimes difficult for businesses to identify the correct business description within it. the ai makes suggestions to improve that classification decision when reading incoming emails and voicemails from the general public. . the output is a set of recommendations for which naics codes best align with a text description of a business activity.
Automating Multilingual Census Data Processing: An AI and Transformer-Based Pipeline for Efficient Language Detection and Translation for Short-Text,Department of Commerce,DOC,CENSUS - U.S. Census Bureau,Mission-Enabling (internal agency support),None of the above.,"The purpose of this AI-driven pipeline is to tackle the challenge of processing non-English, short-text responses in large-scale surveys and censuses, especially brief entries for race and ethnicity. Traditional language detection and translation systems often struggle with these minimal text responses, impacting data accuracy and inclusivity. This AI solution is designed to automate the entire multilingual data processing workflow, from language detection to translation, named entity recognition, and validation, using AI and transformer-based models along with natural language processing techniques. By achieving high accuracy even with limited context, the system reduces the need for human translators, increases processing speed, and guarantees fair and accurate representation of diverse populations. This innovation not only supports the agency’s mission to collect inclusive, representative data but also benefits the public by contributing to more precise demographic insights, ultimately aiding in resource allocation and policy making.","The AI system outputs a series of automated decisions and validated translations for short-text responses, such as race and ethnicity write-ins, within large-scale survey data. Specifically, it provides accurate language detection, corrects potential spelling errors, generates contextually accurate translations, and validates these translations against standardized labels using semantic similarity analysis. The system then selects the most accurate translation for each input, guaranteeing precise categorization and extraction of demographic data. These outputs streamline survey data processing by reducing the need for human intervention, allowing for real-time or near real-time responses that meet high standards of accuracy and inclusivity",Acquisition and/or Development,Neither,6/3/2024,7/8/2024,9/30/2025,Developed in-house.,Unknown,No,Unknown,Unknown,Unknown,Unknown,"The agency used agency-owned decennial paradata to train, fine-tune, and evaluate the AI model’s performance. This paradata, collected from previous census responses, provided important interaction data that allowed the model to improve its language detection and translation accuracy, especially for short-text responses typical in race and ethnicity write-ins.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"The purpose of this AI-driven pipeline is to tackle the challenge of processing non-English, short-text responses in large-scale surveys and censuses, especially brief entries for race and ethnicity. Traditional language detection and translation systems often struggle with these minimal text responses, impacting data accuracy and inclusivity. This AI solution is designed to automate the entire multilingual data processing workflow, from language detection to translation, named entity recognition, and validation, using AI and transformer-based models along with natural language processing techniques. By achieving high accuracy even with limited context, the system reduces the need for human translators, increases processing speed, and guarantees fair and accurate representation of diverse populations. This innovation not only supports the agency’s mission to collect inclusive, representative data but also benefits the public by contributing to more precise demographic insights, ultimately aiding in resource allocation and policy making. . The AI system outputs a series of automated decisions and validated translations for short-text responses, such as race and ethnicity write-ins, within large-scale survey data. Specifically, it provides accurate language detection, corrects potential spelling errors, generates contextually accurate translations, and validates these translations against standardized labels using semantic similarity analysis. The system then selects the most accurate translation for each input, guaranteeing precise categorization and extraction of demographic data. These outputs streamline survey data processing by reducing the need for human intervention, allowing for real-time or near real-time responses that meet high standards of accuracy and inclusivity","the purpose of this ai-driven pipeline is to tackle the challenge of processing non-english, short-text responses in large-scale surveys and censuses, especially brief entries for race and ethnicity. traditional language detection and translation systems often struggle with these minimal text responses, impacting data accuracy and inclusivity. this ai solution is designed to automate the entire multilingual data processing workflow, from language detection to translation, named entity recognition, and validation, using ai and transformer-based models along with natural language processing techniques. by achieving high accuracy even with limited context, the system reduces the need for human translators, increases processing speed, and guarantees fair and accurate representation of diverse populations. this innovation not only supports the agency’s mission to collect inclusive, representative data but also benefits the public by contributing to more precise demographic insights, ultimately aiding in resource allocation and policy making. . the ai system outputs a series of automated decisions and validated translations for short-text responses, such as race and ethnicity write-ins, within large-scale survey data. specifically, it provides accurate language detection, corrects potential spelling errors, generates contextually accurate translations, and validates these translations against standardized labels using semantic similarity analysis. the system then selects the most accurate translation for each input, guaranteeing precise categorization and extraction of demographic data. these outputs streamline survey data processing by reducing the need for human intervention, allowing for real-time or near real-time responses that meet high standards of accuracy and inclusivity"
FAQ for SMaRT,Department of Commerce,DOC,CENSUS - U.S. Census Bureau,Government Services (includes Benefits and Service Delivery),Searching for information using AI.,"The purpose of the FAQ search tool is to find the correct Census Bureau-provided answers to common questions from the public about the 2025 Census Test. The 2025 Census Test is a proposed test as part of the Small-Scale Response Test (SmaRT) program in decennial. The purpose of this program is to perform experiments using different methods to improve self-response rates and data collection. The SmaRT program accomplishes this using iterative tests with relatively small sample sizes. Four installments of the SmaRT program were fielded before the 2020 Census. Three small-scale tests have occurred so far this decade with the fourth test planned for 2025. There is an online questionnaire for each of the tests, with a census.gov landing page to access the online questionnaire. In the first three tests this decade, there were approximately 15 FAQs on the landing page that the user could read. For the 2025 Census Test, we would like to offer over 50 FAQs to answer the public’s questions via a search tool.
Current public-facing search tools at the Census Bureau use exact match only. Depending on what users type to search, the correct FAQ might not appear because users do not always use the same terminology as the Census Bureau. This FAQ search tool built with machine-learning models will use semantic similarity (i.e., synonyms or misspellings) to find the correct answers. The answers it provides back to the public are census-approved answers, and not generative AI answers. If the 2025 Census Test is approved, this search feature will be on the landing page for the survey. Implementing this search tool is considered a proof-of-concept for the use of such a search feature. If it is successful, the search tool could be used in the 2026 Census Test on its landing page, or the 2028 Dress Rehearsal. Ultimately, if it is successful, the public will benefit by finding answers to common questions more easily and more accurately.","Description of what the AI system outputs, whether it’s a prediction, recommendation, decision, etc.
Utilizing transfer learning from Large Language Models (LLMs) we utilize them to encode our Census approved FAQs to then compare to a user query. This user query is then encoded as an embedding from the same LLM model and then we take a distance calculation using cosine similarity to rank our reference materials. We then showcase the top 3 results depending if a threshold is met of .25 cosine similarity or above. It is a semantic search tool that ranks reference material.",Acquisition and/or Development,Neither,7/8/2024,9/9/2024,7/11/2025,Developed in-house.,Unknown,No,Unknown,Unknown,Unknown,Unknown,We did not fine tune the LLM because it was just used for transfer learning. We created a question-and-answer pairing dataset to estimate search accuracy. We also selected FAQs that had a high use rate in the 2020 Census to populate the 2025 FAQ list.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"The purpose of the FAQ search tool is to find the correct Census Bureau-provided answers to common questions from the public about the 2025 Census Test. The 2025 Census Test is a proposed test as part of the Small-Scale Response Test (SmaRT) program in decennial. The purpose of this program is to perform experiments using different methods to improve self-response rates and data collection. The SmaRT program accomplishes this using iterative tests with relatively small sample sizes. Four installments of the SmaRT program were fielded before the 2020 Census. Three small-scale tests have occurred so far this decade with the fourth test planned for 2025. There is an online questionnaire for each of the tests, with a census.gov landing page to access the online questionnaire. In the first three tests this decade, there were approximately 15 FAQs on the landing page that the user could read. For the 2025 Census Test, we would like to offer over 50 FAQs to answer the public’s questions via a search tool.
Current public-facing search tools at the Census Bureau use exact match only. Depending on what users type to search, the correct FAQ might not appear because users do not always use the same terminology as the Census Bureau. This FAQ search tool built with machine-learning models will use semantic similarity (i.e., synonyms or misspellings) to find the correct answers. The answers it provides back to the public are census-approved answers, and not generative AI answers. If the 2025 Census Test is approved, this search feature will be on the landing page for the survey. Implementing this search tool is considered a proof-of-concept for the use of such a search feature. If it is successful, the search tool could be used in the 2026 Census Test on its landing page, or the 2028 Dress Rehearsal. Ultimately, if it is successful, the public will benefit by finding answers to common questions more easily and more accurately. . Description of what the AI system outputs, whether it’s a prediction, recommendation, decision, etc.
Utilizing transfer learning from Large Language Models (LLMs) we utilize them to encode our Census approved FAQs to then compare to a user query. This user query is then encoded as an embedding from the same LLM model and then we take a distance calculation using cosine similarity to rank our reference materials. We then showcase the top 3 results depending if a threshold is met of .25 cosine similarity or above. It is a semantic search tool that ranks reference material.","the purpose of the faq search tool is to find the correct census bureau-provided answers to common questions from the public about the 2025 census test. the 2025 census test is a proposed test as part of the small-scale response test (smart) program in decennial. the purpose of this program is to perform experiments using different methods to improve self-response rates and data collection. the smart program accomplishes this using iterative tests with relatively small sample sizes. four installments of the smart program were fielded before the 2020 census. three small-scale tests have occurred so far this decade with the fourth test planned for 2025. there is an online questionnaire for each of the tests, with a census.gov landing page to access the online questionnaire. in the first three tests this decade, there were approximately 15 faqs on the landing page that the user could read. for the 2025 census test, we would like to offer over 50 faqs to answer the public’s questions via a search tool. current public-facing search tools at the census bureau use exact match only. depending on what users type to search, the correct faq might not appear because users do not always use the same terminology as the census bureau. this faq search tool built with machine-learning models will use semantic similarity (i.e., synonyms or misspellings) to find the correct answers. the answers it provides back to the public are census-approved answers, and not generative ai answers. if the 2025 census test is approved, this search feature will be on the landing page for the survey. implementing this search tool is considered a proof-of-concept for the use of such a search feature. if it is successful, the search tool could be used in the 2026 census test on its landing page, or the 2028 dress rehearsal. ultimately, if it is successful, the public will benefit by finding answers to common questions more easily and more accurately. . description of what the ai system outputs, whether it’s a prediction, recommendation, decision, etc. utilizing transfer learning from large language models (llms) we utilize them to encode our census approved faqs to then compare to a user query. this user query is then encoded as an embedding from the same llm model and then we take a distance calculation using cosine similarity to rank our reference materials. we then showcase the top 3 results depending if a threshold is met of .25 cosine similarity or above. it is a semantic search tool that ranks reference material."
Statistical package syntax development and debugging,Department of Commerce,DOC,CENSUS - U.S. Census Bureau,Education & Workforce,None of the above.,"I. Interaction Terms Generation: I utilized ChatGPT to generate SPSS syntax for creating interaction terms in a 97 X 4 matrix (race by item). This was integral for conducting moderated multiple regression analyses, aiming to establish differential prediction based on continuous supervisor scores and binary supervisor status (1 = supervisor, 0 = non-supervisor).
II. Regression Analyses: The AI facilitated the development of large amounts of complex statistical analysis syntax for both binary logistic regression and multiple regression analyses, incorporating the interaction terms effectively.
III. Racial Codes Creation: Used the AI to write syntax that would generate unique racial codes based on participants' self-identifications from a ""select all that apply"" question, allowing for a nuanced analysis of racial group memberships.
IV. Consistency Comparison: Used AI to generate command syntax (for statistical software) to compare self-identified race and other protected class standings between two separate surveys, assessing consistency. The AI-generated syntax marked consistency (1) versus inconsistency (0) for each protected class.",The output was SPSS syntax and recommendations about configuring the syntax,Implementation and Assessment,Neither,9/16/2024,9/16/2024,9/16/2024,Developed in-house.,Unknown,No,No,No,No,No,none,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,Unknown,No,Unknown,Less than 6 months,Other,Other,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"I. Interaction Terms Generation: I utilized ChatGPT to generate SPSS syntax for creating interaction terms in a 97 X 4 matrix (race by item). This was integral for conducting moderated multiple regression analyses, aiming to establish differential prediction based on continuous supervisor scores and binary supervisor status (1 = supervisor, 0 = non-supervisor).
II. Regression Analyses: The AI facilitated the development of large amounts of complex statistical analysis syntax for both binary logistic regression and multiple regression analyses, incorporating the interaction terms effectively.
III. Racial Codes Creation: Used the AI to write syntax that would generate unique racial codes based on participants' self-identifications from a ""select all that apply"" question, allowing for a nuanced analysis of racial group memberships.
IV. Consistency Comparison: Used AI to generate command syntax (for statistical software) to compare self-identified race and other protected class standings between two separate surveys, assessing consistency. The AI-generated syntax marked consistency (1) versus inconsistency (0) for each protected class. . The output was SPSS syntax and recommendations about configuring the syntax","i. interaction terms generation: i utilized chatgpt to generate spss syntax for creating interaction terms in a 97 x 4 matrix (race by item). this was integral for conducting moderated multiple regression analyses, aiming to establish differential prediction based on continuous supervisor scores and binary supervisor status (1 = supervisor, 0 = non-supervisor). ii. regression analyses: the ai facilitated the development of large amounts of complex statistical analysis syntax for both binary logistic regression and multiple regression analyses, incorporating the interaction terms effectively. iii. racial codes creation: used the ai to write syntax that would generate unique racial codes based on participants' self-identifications from a ""select all that apply"" question, allowing for a nuanced analysis of racial group memberships. iv. consistency comparison: used ai to generate command syntax (for statistical software) to compare self-identified race and other protected class standings between two separate surveys, assessing consistency. the ai-generated syntax marked consistency (1) versus inconsistency (0) for each protected class. . the output was spss syntax and recommendations about configuring the syntax"
DSD Python Code Translation,Department of Commerce,DOC,CENSUS - U.S. Census Bureau,Mission-Enabling (internal agency support),None of the above.,The AI purpose for this effort is to assist the translation of SAS to Python code for the DSD data processing and supporting systems. The effort is in support of the Census mission to migration to enterprise-wide solutions and reducing dependency on costly proprietary software products.,The expected AI final system outputs are the translated Python code that replicates the SAS code. Comparison of the system outputs will be performed until the expected results are achieved.,Acquisition and/or Development,Neither,10/2/2024,10/28/2024,Unknown,Developed with contracting resources.,1333LC,No,Unknown,Unknown,Unknown,No,Unknown,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,Less than 6 months,No,No,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,The AI purpose for this effort is to assist the translation of SAS to Python code for the DSD data processing and supporting systems. The effort is in support of the Census mission to migration to enterprise-wide solutions and reducing dependency on costly proprietary software products. . The expected AI final system outputs are the translated Python code that replicates the SAS code. Comparison of the system outputs will be performed until the expected results are achieved.,the ai purpose for this effort is to assist the translation of sas to python code for the dsd data processing and supporting systems. the effort is in support of the census mission to migration to enterprise-wide solutions and reducing dependency on costly proprietary software products. . the expected ai final system outputs are the translated python code that replicates the sas code. comparison of the system outputs will be performed until the expected results are achieved.
Global Business Navigator Chatbot,Department of Commerce,DOC,ITA - International Trade Administration,Diplomacy & Trade,Searching for information using AI.,ITA’s Global Business Navigator ChatbotBeta uses artificial intelligence (AI) to answer your questions on the exporting process and the resources available for exporters.,"Deliver concise, accurate responses to frequently asked export questions
Guide users to relevant resources within Export Solutions content
Refer more complex inquiries to trade specialists for personalized support
Provide multilingual assistance, broadening accessibility to diverse user groups",Implementation and Assessment,Neither,2/2/2024,4/18/2024,Unknown,Developed with both contracting and in-house resources.,"1331L524A13ES0005 (MS EA)
1331L523P13500087 (Data Svsc)
1331L524P13500090 (AI COE)",Yes,Yes,No,Yes,No,"In formulating responses to user queries, GBN is restricted to referencing only the approx. 160 pages of web content in the trade.gov/export-solutions section of trade.gov.
To fine tune the chatbot, ITA will explore using temporarily retained AI responses to customer questions, in order to evaluate the chatbot's performance.
","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Microsoft Azure ,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,No,"Key risks for using the AI chatbot on Trade.gov include data privacy and security, accuracy and reliability, and compliance with legal and regulatory standards. They were identified through project scoping, stakeholder consultations, and legal assessments. Mitigations involve strict cybersecurity protocols, continuous testing, and adherence to legal regulations.",Yes – by the CAIO,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,"The agency has ongoing monitoring and regular audits are performed to identify and address any emerging biases, ensuring equitable service for all users.",Other,Yes,Neither,0.8253968253968254,"ITA’s Global Business Navigator ChatbotBeta uses artificial intelligence (AI) to answer your questions on the exporting process and the resources available for exporters. . Deliver concise, accurate responses to frequently asked export questions
Guide users to relevant resources within Export Solutions content
Refer more complex inquiries to trade specialists for personalized support
Provide multilingual assistance, broadening accessibility to diverse user groups . Key risks for using the AI chatbot on Trade.gov include data privacy and security, accuracy and reliability, and compliance with legal and regulatory standards. They were identified through project scoping, stakeholder consultations, and legal assessments. Mitigations involve strict cybersecurity protocols, continuous testing, and adherence to legal regulations.","ita’s global business navigator chatbotbeta uses artificial intelligence (ai) to answer your questions on the exporting process and the resources available for exporters. . deliver concise, accurate responses to frequently asked export questions guide users to relevant resources within export solutions content refer more complex inquiries to trade specialists for personalized support provide multilingual assistance, broadening accessibility to diverse user groups . key risks for using the ai chatbot on trade.gov include data privacy and security, accuracy and reliability, and compliance with legal and regulatory standards. they were identified through project scoping, stakeholder consultations, and legal assessments. mitigations involve strict cybersecurity protocols, continuous testing, and adherence to legal regulations."
Generative AI Tools Pilot - Global Markets,Department of Commerce,DOC,ITA - International Trade Administration,Diplomacy & Trade,Improving the quality of written communications using AI tools.,"Accelerate and streamline report production.
Enable analysts to focus on higher-level, strategic tasks rather than repetitive reporting.
Reduce costs and improve efficiency, leading to better client service and faster decision-making.","Generate initial drafts of market research reports incorporating public data sources
Provide data-driven insights to identify top markets and recommend strategic directions
Integrate with existing workflows, supporting analysts in refining and finalizing comprehensive, actionable reports",Acquisition and/or Development,Neither,2/3/2024,Unknown,Unknown,Developed with contracting resources.,Planned or in Progress,No,Unknown,No,Yes,No,TBD,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,TBD,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Planned or in-progress.,N/A - Users will be required to take all reasonable action to validate response from the generative AI tool,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,N/A - The generative AI tool will not be leveraged in a way that results in this type of disparity,Direct user testing,No – it is not operationally practical to offer this.,Neither,0.8253968253968254,"Accelerate and streamline report production.
Enable analysts to focus on higher-level, strategic tasks rather than repetitive reporting.
Reduce costs and improve efficiency, leading to better client service and faster decision-making. . Generate initial drafts of market research reports incorporating public data sources
Provide data-driven insights to identify top markets and recommend strategic directions
Integrate with existing workflows, supporting analysts in refining and finalizing comprehensive, actionable reports . N/A - Users will be required to take all reasonable action to validate response from the generative AI tool","accelerate and streamline report production. enable analysts to focus on higher-level, strategic tasks rather than repetitive reporting. reduce costs and improve efficiency, leading to better client service and faster decision-making. . generate initial drafts of market research reports incorporating public data sources provide data-driven insights to identify top markets and recommend strategic directions integrate with existing workflows, supporting analysts in refining and finalizing comprehensive, actionable reports . n/a - users will be required to take all reasonable action to validate response from the generative ai tool"
Generative AI Tools Pilot - Enterprise & Solutions Architecture,Department of Commerce,DOC,ITA - International Trade Administration,Mission-Enabling,Improving the quality of written communications using AI tools.,"Reduce manual effort in creating and managing architecture documentation
Accelerate solution design
Enhance overall operational efficiency and service quality","Assist in drafting architectural documents and reference models based on standardized templates
Summarize complex data sets and highlight key insights for faster decision-making
Generate initial technical documents 
Aid in compliance checks by referencing and applying relevant frameworks and standards",Acquisition and/or Development,Neither,5/17/2024,Unknown,Unknown,Developed with contracting resources.,Planned or in Progress,No,No,No,Yes,No,TBD,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,TBD,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Planned or in-progress.,N/A - Users will be required to take all reasonable action to validate response from the generative AI tool,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,N/A - The generative AI tool will not be leveraged in a way that results in this type of disparity,Direct user testing,No – it is not operationally practical to offer this.,Neither,0.8095238095238095,"Reduce manual effort in creating and managing architecture documentation
Accelerate solution design
Enhance overall operational efficiency and service quality . Assist in drafting architectural documents and reference models based on standardized templates
Summarize complex data sets and highlight key insights for faster decision-making
Generate initial technical documents 
Aid in compliance checks by referencing and applying relevant frameworks and standards . N/A - Users will be required to take all reasonable action to validate response from the generative AI tool",reduce manual effort in creating and managing architecture documentation accelerate solution design enhance overall operational efficiency and service quality . assist in drafting architectural documents and reference models based on standardized templates summarize complex data sets and highlight key insights for faster decision-making generate initial technical documents aid in compliance checks by referencing and applying relevant frameworks and standards . n/a - users will be required to take all reasonable action to validate response from the generative ai tool
Streamline Spectrum Activities,Department of Commerce,DOC,National Telecommunications and Information Administration,Other,None of the above.,Streamline NTIA Office of Spectrum Management (OSM) processes and enhance Federal spectrum management. This includes using Azure and CoPilot capabilities to reduce the level of effort and improve the timeliness of OSM and Federal spectrum management processes.,"OSM CoPilot provides General office productivity outputs. Additional outputs include orchestration of current spectrum management functionality, speeding processes by eliminating clicks and keystrokes while under the supervision of  spectrum stakeholders.",Acquisition and/or Development,Neither,12/10/2024,12/10/2024,Unknown,Unknown,Unknown,No,No,No,Unknown,Other,NTIA Manual of Regulations and Procedures for Federal Radio Frequency Management,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.36507936507936506,"Streamline NTIA Office of Spectrum Management (OSM) processes and enhance Federal spectrum management. This includes using Azure and CoPilot capabilities to reduce the level of effort and improve the timeliness of OSM and Federal spectrum management processes. . OSM CoPilot provides General office productivity outputs. Additional outputs include orchestration of current spectrum management functionality, speeding processes by eliminating clicks and keystrokes while under the supervision of  spectrum stakeholders.","streamline ntia office of spectrum management (osm) processes and enhance federal spectrum management. this includes using azure and copilot capabilities to reduce the level of effort and improve the timeliness of osm and federal spectrum management processes. . osm copilot provides general office productivity outputs. additional outputs include orchestration of current spectrum management functionality, speeding processes by eliminating clicks and keystrokes while under the supervision of spectrum stakeholders."
Grammarly,Department of Commerce,DOC,NIST - National Institute of Science and Technology,Education & Workforce,Improving the quality of written communications using AI tools.,"Improving the quality of written communications, idea generation, refine tone and style. ","Grammar, punctuation, style improvements, content generation",Operation and Maintenance,Neither,2/3/2023,6/16/2023,1/17/2024,Developed with contracting resources.,NNG15SC82B,No,No,No,Yes,No,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,Yes,GrammarlyGo,Unknown,No,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Yes,"Risks to NIST were determined through Assessment and Authorization process by NIST Security Control Assessors. They documented that the most significant risk to NIST is moderate-level data going to a cloud vendor that does not have a FedRAMP authorized package and is not currently in process to become FedRAMP. Grammarly does not meet Department of Commerce Security and Privacy Assessment and Authorization Handbook requirements. There are deficiencies restricting staff to US only personnel, specific documentation about use of FIPS 140-2 validated encryption, TIC 2.0 architecture implementation, and Grammarly is unable to send logs to DOC SIEM tools.

This is mitigated in multiple ways:
•	Grammarly has multiple independent assessments, including a SOC 2 Type II assessment.
•	Grammarly provided other internal security documentation for review.
•	NIST data is only used by Grammarly to perform grammar recommendations. All data is purged from Grammarly servers within 3 hours.
•	NIST data is not used for training purposes.
•	Grammarly uses Amazon Web services for its infrastructure, which has FedRAMP Moderate authorization.
•	Grammarly uses Microsoft Azure AI for AI recommendations, which has FedRAMP High authorization.

Other key risks are as follows: 

•	Logins to Grammarly are logged and viewable by administrators, but the actual usage of Grammarly by the user is not logged. This is mitigated by Grammarly’s limited built-in data loss prevention (DLP) that is enabled. This prevents the following data types from going up to Grammarly for analysis: email addresses, URLs, phone numbers, credit card numbers, and Social Security numbers.
",Yes – by another appropriate agency office that was not directly involved in the system’s development,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",Yes - All individual decisions or actions are automated and may be rendered without direct human interaction.,The model provides grammar and phrasing recommendations. All staff are presumed to be English subject matter experts and do not need to accept any proposed response from Grammarly. Staff are advised that they are ultimately responsible for the content of their responses.,None of the above,Yes,Neither,0.7777777777777778,"Improving the quality of written communications, idea generation, refine tone and style. . Grammar, punctuation, style improvements, content generation . Risks to NIST were determined through Assessment and Authorization process by NIST Security Control Assessors. They documented that the most significant risk to NIST is moderate-level data going to a cloud vendor that does not have a FedRAMP authorized package and is not currently in process to become FedRAMP. Grammarly does not meet Department of Commerce Security and Privacy Assessment and Authorization Handbook requirements. There are deficiencies restricting staff to US only personnel, specific documentation about use of FIPS 140-2 validated encryption, TIC 2.0 architecture implementation, and Grammarly is unable to send logs to DOC SIEM tools.

This is mitigated in multiple ways:
•	Grammarly has multiple independent assessments, including a SOC 2 Type II assessment.
•	Grammarly provided other internal security documentation for review.
•	NIST data is only used by Grammarly to perform grammar recommendations. All data is purged from Grammarly servers within 3 hours.
•	NIST data is not used for training purposes.
•	Grammarly uses Amazon Web services for its infrastructure, which has FedRAMP Moderate authorization.
•	Grammarly uses Microsoft Azure AI for AI recommendations, which has FedRAMP High authorization.

Other key risks are as follows: 

•	Logins to Grammarly are logged and viewable by administrators, but the actual usage of Grammarly by the user is not logged. This is mitigated by Grammarly’s limited built-in data loss prevention (DLP) that is enabled. This prevents the following data types from going up to Grammarly for analysis: email addresses, URLs, phone numbers, credit card numbers, and Social Security numbers.","improving the quality of written communications, idea generation, refine tone and style. . grammar, punctuation, style improvements, content generation . risks to nist were determined through assessment and authorization process by nist security control assessors. they documented that the most significant risk to nist is moderate-level data going to a cloud vendor that does not have a fedramp authorized package and is not currently in process to become fedramp. grammarly does not meet department of commerce security and privacy assessment and authorization handbook requirements. there are deficiencies restricting staff to us only personnel, specific documentation about use of fips 140-2 validated encryption, tic 2.0 architecture implementation, and grammarly is unable to send logs to doc siem tools. this is mitigated in multiple ways: • grammarly has multiple independent assessments, including a soc 2 type ii assessment. • grammarly provided other internal security documentation for review. • nist data is only used by grammarly to perform grammar recommendations. all data is purged from grammarly servers within 3 hours. • nist data is not used for training purposes. • grammarly uses amazon web services for its infrastructure, which has fedramp moderate authorization. • grammarly uses microsoft azure ai for ai recommendations, which has fedramp high authorization. other key risks are as follows: • logins to grammarly are logged and viewable by administrators, but the actual usage of grammarly by the user is not logged. this is mitigated by grammarly’s limited built-in data loss prevention (dlp) that is enabled. this prevents the following data types from going up to grammarly for analysis: email addresses, urls, phone numbers, credit card numbers, and social security numbers."
Science Data Portal Autosuggest Search ,Department of Commerce,DOC,NIST - National Institute of Science and Technology,Science & Space,Searching for information using AI.,"Assist end users is discovery,  access, and re-use of scientific,  engineering and technical information provided by bureau","Dynamically generated terms,  filtered and ranked for frequency",Operation and Maintenance,Neither,10/1/2021,10/1/2021,2/1/2022,Developed in-house.,Unknown,No,Yes,No,Yes,Yes,NIST public data repository metadata corpus,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,Yes – source code is publicly available.,No,https://data.nist.gov,More than 12 months,No,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,No,Risk is low to no risk in use,Yes – by another appropriate agency office that was not directly involved in the system’s development,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,Direct user testing,Yes,Neither,0.7936507936507936,"Assist end users is discovery,  access, and re-use of scientific,  engineering and technical information provided by bureau . Dynamically generated terms,  filtered and ranked for frequency . Risk is low to no risk in use","assist end users is discovery, access, and re-use of scientific, engineering and technical information provided by bureau . dynamically generated terms, filtered and ranked for frequency . risk is low to no risk in use"
Coastal Change Analysis Program (C-CAP),Department of Commerce,DOC,NOAA - National Oceanic and Atmospheric Administration,Science & Space,None of the above.,Develop high resolution land cover data for the United States.,,Implementation and Assessment,Neither,7/7/1905,2015-2022,Ongoing,Developed with both contracting and in-house resources.,Unknown,Unknown,Unknown,No,No,Yes,Existing NOAA C-CAP and other U.S. government authoritative geospatial datasets.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,Yes – source code is publicly available.,Unknown,Unknown,Unknown,Unknown,Yes,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,Develop high resolution land cover data for the United States.,develop high resolution land cover data for the united states.
Operational satellite product quality monitoring AI-ready training data,Department of Commerce,DOC,NOAA - National Oceanic and Atmospheric Administration,Other,None of the above.,"NOAA NDE and NESDIS Common Cloud Framework (NCCF) are current and future NOAA satellite product generation environments. There are hundreds of products based on a variety of national/international satellites (US, Europe and Japan) that are generated in near real time. The anomaly detections of the satellite product quality are still manpower intensive and in a traditional way. Nowadays, with the AI technology getting mature, the AI use case can be extended to this area of anomaly detection. In the traditional way, the maintenance programmers eyeball the product maps with 24/7 support which is stressful. On the other hand, AI has the strong capability of pattern recognition with high accuracy. The AI-based technique will relieve manpower and automate the alert system.","Detection/Identification of the satellite product anomalies, e.g. the bounding boxes locate the pre-defined error types. ",Acquisition and/or Development,Neither,11/6/2022,3/29/2024,Unknown,Developed with contracting resources.,1332KP21DNEEB0011,No,No,No,No,No,A variety of operational satellite products,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,More than 12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Planned or in-progress.,Lack of funding for Phase II development and transition from research to operations. ,Yes – by an agency AI oversight board not directly involved in the system’s development,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,"Test, assess and feedback collection",Direct user testing,Yes,Neither,0.7619047619047619,"NOAA NDE and NESDIS Common Cloud Framework (NCCF) are current and future NOAA satellite product generation environments. There are hundreds of products based on a variety of national/international satellites (US, Europe and Japan) that are generated in near real time. The anomaly detections of the satellite product quality are still manpower intensive and in a traditional way. Nowadays, with the AI technology getting mature, the AI use case can be extended to this area of anomaly detection. In the traditional way, the maintenance programmers eyeball the product maps with 24/7 support which is stressful. On the other hand, AI has the strong capability of pattern recognition with high accuracy. The AI-based technique will relieve manpower and automate the alert system. . Detection/Identification of the satellite product anomalies, e.g. the bounding boxes locate the pre-defined error types. . Lack of funding for Phase II development and transition from research to operations.","noaa nde and nesdis common cloud framework (nccf) are current and future noaa satellite product generation environments. there are hundreds of products based on a variety of national/international satellites (us, europe and japan) that are generated in near real time. the anomaly detections of the satellite product quality are still manpower intensive and in a traditional way. nowadays, with the ai technology getting mature, the ai use case can be extended to this area of anomaly detection. in the traditional way, the maintenance programmers eyeball the product maps with 24/7 support which is stressful. on the other hand, ai has the strong capability of pattern recognition with high accuracy. the ai-based technique will relieve manpower and automate the alert system. . detection/identification of the satellite product anomalies, e.g. the bounding boxes locate the pre-defined error types. . lack of funding for phase ii development and transition from research to operations."
LightningCast: AI for lightning prediction,Department of Commerce,DOC,NOAA - National Oceanic and Atmospheric Administration,Other,None of the above.,"LightningCast provides prognostic information on lightning, before the threat is realized, thereby allowing for actions that protect life and property",Probability that lightning will be observed at a given location in the next 60 minutes,Implementation and Assessment,Neither,9/1/2019,9/1/2020,Scheduled for FY2025,Developed in-house.,Unknown,No,No,No,No,No,Data from the Geostationary Operational Environmental Satellites,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,Yes – source code is publicly available.,No,Unknown,More than 12 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Planned or in-progress.,,Yes – by another appropriate agency office that was not directly involved in the system’s development,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Unknown,Direct user testing,No – it is not operationally practical to offer this.,Neither,0.7301587301587301,"LightningCast provides prognostic information on lightning, before the threat is realized, thereby allowing for actions that protect life and property . Probability that lightning will be observed at a given location in the next 60 minutes","lightningcast provides prognostic information on lightning, before the threat is realized, thereby allowing for actions that protect life and property . probability that lightning will be observed at a given location in the next 60 minutes"
CyberSecurity Hardening & Automation,Department of Commerce,DOC,NOAA - National Oceanic and Atmospheric Administration,Other,None of the above.,"Promptly analyze cybersecurity architecture, documentation, inventories, and vulnerabilities for remediation, automation opportunities, continuous monitoring, responding to data calls, and architectural risk assessments (e.g. Security Impact Analyses [SIAs]).","Risk Assessment, documentation generation, responses to data calls",Initiated,Neither,1/1/2024,6/1/2024,Unknown,Developed in-house.,Unknown,No,No,No,Unknown,Unknown,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.30158730158730157,"Promptly analyze cybersecurity architecture, documentation, inventories, and vulnerabilities for remediation, automation opportunities, continuous monitoring, responding to data calls, and architectural risk assessments (e.g. Security Impact Analyses [SIAs]). . Risk Assessment, documentation generation, responses to data calls","promptly analyze cybersecurity architecture, documentation, inventories, and vulnerabilities for remediation, automation opportunities, continuous monitoring, responding to data calls, and architectural risk assessments (e.g. security impact analyses [sias]). . risk assessment, documentation generation, responses to data calls"
AI based Community Radiative Transfer Model,Department of Commerce,DOC,NOAA - National Oceanic and Atmospheric Administration,Other,None of the above.,Fast radiative transfer calculations for increasing satellite data needed by weathers forecasts and the generation of satellite products.,simulated satellite radiances and sensitivities (Jacobian) of satellite data,Acquisition and/or Development,Neither,7/1/2024,7/1/2024,1/1/2027,Developed in-house.,Unknown,Yes,No,No,No,No,Meteorological data such as profiles of temperature and water vapor,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,No,Unknown,More than 12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Planned or in-progress.,Lack of funding,Yes – by another appropriate agency office that was not directly involved in the system’s development,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"Test, assess and feedback collection",Direct user testing,No – it is not operationally practical to offer this.,Neither,0.7301587301587301,Fast radiative transfer calculations for increasing satellite data needed by weathers forecasts and the generation of satellite products. . simulated satellite radiances and sensitivities (Jacobian) of satellite data . Lack of funding,fast radiative transfer calculations for increasing satellite data needed by weathers forecasts and the generation of satellite products. . simulated satellite radiances and sensitivities (jacobian) of satellite data . lack of funding
Operational Enterprise Cloud Mask,Department of Commerce,DOC,NOAA - National Oceanic and Atmospheric Administration,Science & Space,None of the above.,Used to create internal ECM algorithm classifiers to optimize cloud detection over different surface types and atmospheric conditions,Cloud detection ,Implementation and Assessment,Neither,Pre-2021,Pre-2021,Pre-2021,Developed in-house.,Unknown,No,Unknown,No,No,No,Data from operational satellites,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,Yes – source code is publicly available.,No,Unknown,More than 12 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Planned or in-progress.,,Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Unknown,Direct user testing,No – it is not operationally practical to offer this.,Neither,0.7142857142857143,Used to create internal ECM algorithm classifiers to optimize cloud detection over different surface types and atmospheric conditions . Cloud detection,used to create internal ecm algorithm classifiers to optimize cloud detection over different surface types and atmospheric conditions . cloud detection
AI retrieval for patent search,Department of Commerce,DOC,USPTO - United States Patent and Trade Office,Mission-Enabling,None of the above.,"Augmentation for next generation patent search tool to assist examiners identify relevant documents and additional areas to search.  System takes input from published or unpublished applications and provides recommendations on further prior art areas to search, giving the user the ability to sort by similarity to concepts of their choosing.","System takes input from published or unpublished applications and provides recommendations on further prior art areas to search, giving the user the ability to sort by similarity to concepts of their choosing.",Operation and Maintenance,Neither,11/7/2018,10/31/2019,10/15/2021,Developed with both contracting and in-house resources.,1333BJ24C00280005,No,No,Yes,No,No,"The models are trained on Patent Publication information, including US Patent Publications, that are publicly available.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,PPL-Patent Search Artificial Intelligence (PTOC-00-060-00),Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"Augmentation for next generation patent search tool to assist examiners identify relevant documents and additional areas to search.  System takes input from published or unpublished applications and provides recommendations on further prior art areas to search, giving the user the ability to sort by similarity to concepts of their choosing. . System takes input from published or unpublished applications and provides recommendations on further prior art areas to search, giving the user the ability to sort by similarity to concepts of their choosing.","augmentation for next generation patent search tool to assist examiners identify relevant documents and additional areas to search. system takes input from published or unpublished applications and provides recommendations on further prior art areas to search, giving the user the ability to sort by similarity to concepts of their choosing. . system takes input from published or unpublished applications and provides recommendations on further prior art areas to search, giving the user the ability to sort by similarity to concepts of their choosing."
AI use for CPC classification,Department of Commerce,DOC,USPTO - United States Patent and Trade Office,Mission-Enabling,Searching for information using AI.,"Automatically assign patent utility classification (CPC) symbols to patent utility documents to reduce classification time, provide agency cost savings, and improve consistency of classification practices.",Predictions of CPC symbols relevant for each utility patent document inputted,Acquisition and/or Development,Neither,10/25/2017,10/9/2019,Unknown,Developed with contracting resources.,1333BJ21D00280001 (ID/IQ contract number),Yes,No,No,Yes,Other,The use case uses USPTO owned classification (CPC) data on patent utility documents since 1976; USPTO owned patent document text data; agency published public data of CPC scheme and definitions.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,OD/BD master system,More than 12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,No,unable to meet performance quality metrics- will be identified by benchmark evaluation and future operation-environment testing,"Does not apply, use case is neither safety or rights impacting.","use case is not safety impacting or rights impacting; however, a monitoring process for AI performance is under development","use case does not impact safety or rights; however – Other,  Immediate human intervention is not practicable; however, an appropriate fail-safe has been implemented. ",Planned performance monitoring to evaluate performance across technologies and patent applications,"direct user testing
outreach to relevant labor organizations
other",No – it is not operationally practical to offer this.,Neither,0.873015873015873,"Automatically assign patent utility classification (CPC) symbols to patent utility documents to reduce classification time, provide agency cost savings, and improve consistency of classification practices. . Predictions of CPC symbols relevant for each utility patent document inputted . unable to meet performance quality metrics- will be identified by benchmark evaluation and future operation-environment testing","automatically assign patent utility classification (cpc) symbols to patent utility documents to reduce classification time, provide agency cost savings, and improve consistency of classification practices. . predictions of cpc symbols relevant for each utility patent document inputted . unable to meet performance quality metrics- will be identified by benchmark evaluation and future operation-environment testing"
Enriched Citation,Department of Commerce,DOC,USPTO - United States Patent and Trade Office,Government Services (includes Benefits and Service Delivery),Summarizing the key points of a lengthy report using AI.,"Data dissemination system that identifies which references, or prior art, were cited in specific patent application office actions, including: bibliographic information of the reference, the claims that the prior art was cited against, and the relevant sections that the examiner relied upon. Use of AI to extract summarized information from unstructured text is providing operational cost savings. ",System extracts information from unstructured office actions and provides the information through a structured public facing API.,Operation and Maintenance,Neither,9/18/2017,11/7/2017,3/21/2019,Unknown,Unknown,No,No,Unknown,Unknown,Unknown,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,No – Agency did not request an extension for this use case.,No,N/A - Not Safety or Rights impacting,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",Yes - All individual decisions or actions are automated and may be rendered without direct human interaction.,N/A - Not Safety or Rights impacting,Direct user testing,Yes,Neither,0.4603174603174603,"Data dissemination system that identifies which references, or prior art, were cited in specific patent application office actions, including: bibliographic information of the reference, the claims that the prior art was cited against, and the relevant sections that the examiner relied upon. Use of AI to extract summarized information from unstructured text is providing operational cost savings. . System extracts information from unstructured office actions and provides the information through a structured public facing API. . N/A - Not Safety or Rights impacting","data dissemination system that identifies which references, or prior art, were cited in specific patent application office actions, including: bibliographic information of the reference, the claims that the prior art was cited against, and the relevant sections that the examiner relied upon. use of ai to extract summarized information from unstructured text is providing operational cost savings. . system extracts information from unstructured office actions and provides the information through a structured public facing api. . n/a - not safety or rights impacting"
Inventor Search Assistant (iSAT),Department of Commerce,DOC,USPTO - United States Patent and Trade Office,Government Services (includes Benefits and Service Delivery),Searching for information using AI.,"Service to help inventors ""get started"" identifying relevant documents, figures, and classification codes used to conduct a novelty search.  Expected benefit was to provide continuity in service access to public search capabilities during COVID-19 related closure of in-person search assistance services. ","System takes a user entered short description of invention and provides a user selectable set of recommended documents, figures, and classification areas.",Retired,Neither,10/3/2019,2/3/2020,6/15/2021,Unknown,Unknown,No,Yes,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,No – Agency did not request an extension for this use case.,No,N/A - Not Safety or Rights impacting,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,N/A - Not Safety or Rights impacting,Post-transaction customer feedback collections,Yes,Neither,0.4603174603174603,"Service to help inventors ""get started"" identifying relevant documents, figures, and classification codes used to conduct a novelty search.  Expected benefit was to provide continuity in service access to public search capabilities during COVID-19 related closure of in-person search assistance services. . System takes a user entered short description of invention and provides a user selectable set of recommended documents, figures, and classification areas. . N/A - Not Safety or Rights impacting","service to help inventors ""get started"" identifying relevant documents, figures, and classification codes used to conduct a novelty search. expected benefit was to provide continuity in service access to public search capabilities during covid-19 related closure of in-person search assistance services. . system takes a user entered short description of invention and provides a user selectable set of recommended documents, figures, and classification areas. . n/a - not safety or rights impacting"
Patent Design Image Search (DesignVision),Department of Commerce,DOC,USPTO - United States Patent and Trade Office,Mission-Enabling,Searching for information using AI.,A SaaS AI driven image search tool to support the examination of design applications. ,"The tool compares uploaded images from design applications and yields image results based on similarity thereby providing access, search, and retrieval of information from Industrial Design collections.",Implementation and Assessment,Neither,2022-11-16 (first formal action related to this project),2024-02-15 (contract kick-off); 2024-08-23 (signed Acquisition Plan),2024-10-16 (pilot licenses assigned),Developed with contracting resources.,1333BJ24C00150001,No,No,Yes,No,No,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Yes,PTO-PPL-DV-DesignVision (PPL-PEC-02-00),Less than 6 months,No,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"A SaaS AI driven image search tool to support the examination of design applications. . The tool compares uploaded images from design applications and yields image results based on similarity thereby providing access, search, and retrieval of information from Industrial Design collections.","a saas ai driven image search tool to support the examination of design applications. . the tool compares uploaded images from design applications and yields image results based on similarity thereby providing access, search, and retrieval of information from industrial design collections."
USPTO Virtual Assistant (Public-facing chatbot),Department of Commerce,DOC,USPTO - United States Patent and Trade Office,Government Services (includes Benefits and Service Delivery),None of the above.,Answers questions about Trademarks and Patents,Answers to Trademark and patent questions that the public has.,Operation and Maintenance,Neither,4/1/2021,5/1/2021,12/8/2022,Developed in-house.,Unknown,Yes,Yes,No,Yes,Other,The model for Chatbot Virtual Assistant is trained on data pertaining to Trademark or Patent information that is publicly available on Trademark and Patent web pages.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Chatbot-C,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Yes,Minimal risks were identified for Chatbot Virtual Assistant as the data that is dissiminated to the users are from preapproved dataset. Chatbot cannot answer questions outside of what it is trained on.,Yes – by the CAIO,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,The greeting message clearly states not to use PII or demographic information  ,General solicitations of comments from the public,No – it is not operationally practical to offer this.,Neither,0.8571428571428571,Answers questions about Trademarks and Patents . Answers to Trademark and patent questions that the public has. . Minimal risks were identified for Chatbot Virtual Assistant as the data that is dissiminated to the users are from preapproved dataset. Chatbot cannot answer questions outside of what it is trained on.,answers questions about trademarks and patents . answers to trademark and patent questions that the public has. . minimal risks were identified for chatbot virtual assistant as the data that is dissiminated to the users are from preapproved dataset. chatbot cannot answer questions outside of what it is trained on.
Patent Automatic Document Code Determination,Department of Commerce,DOC,USPTO - United States Patent and Trade Office,Mission-Enabling,Inputting large amounts of data from paper forms into a digital system using AI.,Quality control of customer applied document labels,Document Labels,Implementation and Assessment,Neither,2/1/2023,2/1/2023,To Be Determined (calendar 2025),Developed with both contracting and in-house resources.,280-EIPL-0762 ,No,No,No,No,Yes,Publicly available open data of previous documents and their labels,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,PE2E-DAV,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Yes,,Unknown,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,Other,Yes,Neither,0.7619047619047619,Quality control of customer applied document labels . Document Labels,quality control of customer applied document labels . document labels
Patents - Skill Group Matching,Department of Commerce,DOC,USPTO - United States Patent and Trade Office,Mission-Enabling,None of the above.,AI will permit adoption of Skill Groups system for delineating subject matter of patent applications by automating the allocation of Skill Group designations to applications.  Benefits:  Cost reduction from eliminating dependence on outdated US Patent Classification (USPC) system; cost savings from reducing Skill Group designation workload; improvement in availability of metrics data; improvements in ability to manage unexamined patent inventory and workforce allocation/hiring,"Prediction of the correct Skill Group describing the technology/subject matter claimed in a patent application, along with a confidence score indicating the likelihood of each possible Skill Group.",Acquisition and/or Development,Neither,2022-08 (estimated),2022-09-13 Technical kickoff (no new acquisition - supported by existing contract),Unknown,Developed with both contracting and in-house resources.,280-EIPL-0762 (OY2),Yes,No,No,Unknown,Yes,Full text of published patent documents; USPC classifications thereof; USPC-to-Skill Group correspondence table,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"AI will permit adoption of Skill Groups system for delineating subject matter of patent applications by automating the allocation of Skill Group designations to applications.  Benefits:  Cost reduction from eliminating dependence on outdated US Patent Classification (USPC) system; cost savings from reducing Skill Group designation workload; improvement in availability of metrics data; improvements in ability to manage unexamined patent inventory and workforce allocation/hiring . Prediction of the correct Skill Group describing the technology/subject matter claimed in a patent application, along with a confidence score indicating the likelihood of each possible Skill Group.","ai will permit adoption of skill groups system for delineating subject matter of patent applications by automating the allocation of skill group designations to applications. benefits: cost reduction from eliminating dependence on outdated us patent classification (uspc) system; cost savings from reducing skill group designation workload; improvement in availability of metrics data; improvements in ability to manage unexamined patent inventory and workforce allocation/hiring . prediction of the correct skill group describing the technology/subject matter claimed in a patent application, along with a confidence score indicating the likelihood of each possible skill group."
Groundwater Modeling,Department of Energy,DOE,LM HQ - Office of Legacy Management (LM),Science & Space,Creating visual representations of data sets for reports and presentations using AI.,Predicts flow of ground water containments underground.,Parameter estimation for return to groundwater models,Operation and Maintenance,Neither,1/1/2003,2/1/2003,10/1/2003,Developed with both contracting and in-house resources., IDIQ Contract # 89303020DLM000001.,No,Yes,No,No,Other,LM maintains historically collected groundwater data to train the model.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",No,"Yes – agency has access to source code, but it is not public.",Yes,Legacy Management General Support System,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,Predicts flow of ground water containments underground. . Parameter estimation for return to groundwater models,predicts flow of ground water containments underground. . parameter estimation for return to groundwater models
Applications of Natural Language Processing and Similarity Measures for Similarity Ranking,Department of Energy,DOE,EHSS HQ -  Office of Environment Health Safety & Security (EHSS),Other,None of the above.,"(1) Demonstrate Capabilities of AI (2) Cost savings by sharing algorithms, approaches and lessons learned of those evaluated and deployed in the ES&H DAMaL Tools.","Custom output (e.g., ranking of records)",Operation and Maintenance,Neither,10/1/2021,10/1/2021,10/1/2021,Developed in-house.,Unknown,No,No,No,No,No,Approach is NOT supervised ML.,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,Yes,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,No,,Planned or in-progress,Unknown,Unknown,Unknown,Unknown,No – it is not operationally practical to offer this.,Neither,0.6825396825396826,"(1) Demonstrate Capabilities of AI (2) Cost savings by sharing algorithms, approaches and lessons learned of those evaluated and deployed in the ES&H DAMaL Tools. . Custom output (e.g., ranking of records)","(1) demonstrate capabilities of ai (2) cost savings by sharing algorithms, approaches and lessons learned of those evaluated and deployed in the es&h damal tools. . custom output (e.g., ranking of records)"
"Data Analytics and Machine Learning (DAMaL) Tools to enhance the analysis of Environment, Safety and Health (ES&H) data:  Classification, Robotic Process Automation and Data Visualization",Department of Energy,DOE,EHSS HQ -  Office of Environment Health Safety & Security (EHSS),Other,None of the above.,"(1) Data Visualization using AI (2) Improve efficiency of obtaining insights from data (e.g., ES&H data)",Dynamic data visualization charts,Operation and Maintenance,Safety-impacting,8/1/2018,8/1/2018,8/1/2018,Developed with both contracting and in-house resources.,"GSA Contract #: GS-00F-0004T 
Contract ID # Support to EHSS: 89243321FAU400002",No,No,No,Yes,No,"Utilizes ORPS data for training and applies labels to non-ORPS ES&H Data (e.g., CAIRS, FPRS, DOE OPEXShare, etc.)","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Reporting and  Analytical System (RAS),Less than 6 months,No,Yes,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Yes,Key risk from using the AI is providing misleading information to ES&H practitioners and subject matter experts.,Planned or in-progress,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,Unknown,Unknown,Yes,Safety-Impacting,0.7777777777777778,"(1) Data Visualization using AI (2) Improve efficiency of obtaining insights from data (e.g., ES&H data) . Dynamic data visualization charts . Key risk from using the AI is providing misleading information to ES&H practitioners and subject matter experts.","(1) data visualization using ai (2) improve efficiency of obtaining insights from data (e.g., es&h data) . dynamic data visualization charts . key risk from using the ai is providing misleading information to es&h practitioners and subject matter experts."
"Data Analytics and Machine Learning (DAMaL) Tools for Analysis of Environment, Safety and Health (ES&H) data:  Similarity Based Information Retrieval",Department of Energy,DOE,EHSS HQ -  Office of Environment Health Safety & Security (EHSS),Other,None of the above.,"(1) Searching for Information using AI (2) Improve efficiency of finding relevant data (e.g., ES&H data)",Ranking of records,Operation and Maintenance,Safety-impacting,10/1/2021,8/1/2018,10/1/2021,Developed with both contracting and in-house resources.,"GSA Contract #: GS-00F-0004T 
Contract ID # Support to EHSS: 89243321FAU400002",No,No,No,Yes,No,"ES&H Data (e.g., ORPS, CAIRS, FPRS, DOE OPEXShare). Approach utilized is NOT supervised ML.","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Reporting and  Analytical System (RAS),Less than 6 months,No,Yes,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Yes,"Performance evaluation in operational environment: The AI use case has been tested in an operational environment before being fully implemented as a solution, or has been tested in simulated or controlled environment using operational or synthetic data.",Planned or in-progress,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,Unknown,Unknown,Yes,Safety-Impacting,0.7619047619047619,"(1) Searching for Information using AI (2) Improve efficiency of finding relevant data (e.g., ES&H data) . Ranking of records . Performance evaluation in operational environment: The AI use case has been tested in an operational environment before being fully implemented as a solution, or has been tested in simulated or controlled environment using operational or synthetic data.","(1) searching for information using ai (2) improve efficiency of finding relevant data (e.g., es&h data) . ranking of records . performance evaluation in operational environment: the ai use case has been tested in an operational environment before being fully implemented as a solution, or has been tested in simulated or controlled environment using operational or synthetic data."
"Data Analytics and Machine Learning (DAMaL) Tools to enhance the analysis of Environment, Safety and Health (ES&H) data:  Unsupervised Machine Learning Text Clustering",Department of Energy,DOE,EHSS HQ -  Office of Environment Health Safety & Security (EHSS),Other,None of the above.,"(1) Grouping Reports using AI (i.e., Clustering) (2) Improve efficiency of grouping and highlighting important data (e.g., ES&H data)",Clustering of records with top terms and topics,Operation and Maintenance,Safety-impacting,8/1/2018,8/1/2018,8/1/2018,Developed with both contracting and in-house resources.,"GSA Contract #: GS-00F-0004T 
Contract ID # Support to EHSS: 89243321FAU400002",No,No,No,Yes,No,"ES&H Data (e.g., ORPS, CAIRS, FPRS, DOE OPEXShare). Approach utilized is Unsupervised ML","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Reporting and  Analytical System (RAS),Less than 6 months,No,Yes,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Yes,Key risk from using the AI is providing misleading information to ES&H practitioners and subject matter experts.,Planned or in-progress,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,Unknown,Unknown,Yes,Safety-Impacting,0.7777777777777778,"(1) Grouping Reports using AI (i.e., Clustering) (2) Improve efficiency of grouping and highlighting important data (e.g., ES&H data) . Clustering of records with top terms and topics . Key risk from using the AI is providing misleading information to ES&H practitioners and subject matter experts.","(1) grouping reports using ai (i.e., clustering) (2) improve efficiency of grouping and highlighting important data (e.g., es&h data) . clustering of records with top terms and topics . key risk from using the ai is providing misleading information to es&h practitioners and subject matter experts."
Fast AI for real-time/online diagnostics and calibration; AI-based autonomous system control/operation optimization (including anomaly/failure detection),Department of Energy,DOE,SLAC - SLAC National Accelerator Laboratory (SC43 OIM),Science & Space,None of the above.,"AI/ML to improve efficiency and quality of accelerator-based scientific user facility operation, including for light sources. Benefits include higher science throughput in scientific user facilities, greater energy efficiency, new scientific capabilities at Office of Science user facilities.
","Prediction of acclerator beam behavior, decisions about controllable variables to adjust to improve beam behavior.
",Implementation and Assessment,Neither,10/1/2019,10/1/2019,9/30/2023,Developed in-house.,Unknown,No,No,No,No,No,"Training data are collected from physics simulations and the accelerator as it runs.
","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,Yes – source code is publicly available.,No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"AI/ML to improve efficiency and quality of accelerator-based scientific user facility operation, including for light sources. Benefits include higher science throughput in scientific user facilities, greater energy efficiency, new scientific capabilities at Office of Science user facilities. . Prediction of acclerator beam behavior, decisions about controllable variables to adjust to improve beam behavior.","ai/ml to improve efficiency and quality of accelerator-based scientific user facility operation, including for light sources. benefits include higher science throughput in scientific user facilities, greater energy efficiency, new scientific capabilities at office of science user facilities. . prediction of acclerator beam behavior, decisions about controllable variables to adjust to improve beam behavior."
"AI for system design optimization (e.g., detector, accelerator)",Department of Energy,DOE,SLAC - SLAC National Accelerator Laboratory (SC43 OIM),Science & Space,None of the above.,"AI/ML models a relatively small data sets in a high dimensional parameter space and provides accurate predictions of future data points. The prediction capability in turn improves the convergence efficiency of the optimization algorithm, which is used for time consuming design optimization.
","The AI system is a non-parametric data fitting model and its output is predicted function values of any new solution in the parameter space and the uncertainties. 
",Operation and Maintenance,Neither,10/1/2017,10/1/2017,9/30/2020,Developed in-house.,Unknown,No,No,No,No,No,"Training data are collected at the time of running the optimization algorithm. 
","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,Yes – source code is publicly available.,No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"AI/ML models a relatively small data sets in a high dimensional parameter space and provides accurate predictions of future data points. The prediction capability in turn improves the convergence efficiency of the optimization algorithm, which is used for time consuming design optimization. . The AI system is a non-parametric data fitting model and its output is predicted function values of any new solution in the parameter space and the uncertainties.","ai/ml models a relatively small data sets in a high dimensional parameter space and provides accurate predictions of future data points. the prediction capability in turn improves the convergence efficiency of the optimization algorithm, which is used for time consuming design optimization. . the ai system is a non-parametric data fitting model and its output is predicted function values of any new solution in the parameter space and the uncertainties."
PARSGPT,Department of Energy,DOE,PM HQ -  Office of Project Management (PM),Mission-Enabling (internal agency support) ,None of the above.,The value-add is derived from providing an accessible way for PM Analysts to safely interact with LLM technology.,Free form text response to questions (Chatbot),Operation and Maintenance,Neither,8/18/2023,8/23/2023,9/10/2023,Developed in-house.,Unknown,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Project Assessment and Reporting System (PARS),Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,The value-add is derived from providing an accessible way for PM Analysts to safely interact with LLM technology. . Free form text response to questions (Chatbot),the value-add is derived from providing an accessible way for pm analysts to safely interact with llm technology. . free form text response to questions (chatbot)
CrowdStrike,Department of Energy,DOE,LM HQ - Office of Legacy Management (LM),Other,Identifying unusual patterns in system logs from a single incident report using AI.,"Predicts malicious behavior, provides decisions and blocks malicious behavior.","Prediction, Recommendation, Decision",Operation and Maintenance,Neither,6/15/2023,6/15/2023,6/15/2023,Developed in-house.,Unknown,No,No,No,No,No,LMs client and server logs data set,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,Legacy Management General Support System,Less than 6 months,Other,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,Privacy Loss,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,Yes - All individual decisions or actions are automated and may be rendered without direct human interaction.,Manual Review,None of the above,No – it is not operationally practical to offer this.,Neither,0.7619047619047619,"Predicts malicious behavior, provides decisions and blocks malicious behavior. . Prediction, Recommendation, Decision . Privacy Loss","predicts malicious behavior, provides decisions and blocks malicious behavior. . prediction, recommendation, decision . privacy loss"
SCMC AWS Data Infrastructure,Department of Energy,DOE,KCNSC - Kansas City National Security Campus (KCFO),Mission-Enabling (internal agency support) ,Creating visual representations of data sets for reports and presentations using AI.,Automation of multiple manual processes and improvement of data quality and governance.,Initially the system will use M.L. to identify  inconsistent data and make preditions of how that data should be organized. ,Implementation and Assessment,Neither,5/1/2024,8/1/2024,Unknown,Developed with contracting resources.,Procured under SCMC Agreement # WS181016,No,No,No,No,Yes,Non production related procurement data is currently being collected and managed by the SCMC. This is the data set that wil be used by the system.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",No,No – agency does not have access to source code.,Yes,AWS US EAST/WEST ,Less than 6 months,No,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,No,Not applicable to the risks defined above,Planned or in-progress,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,Not applicable to the scenario defined above,Other,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.7619047619047619,Automation of multiple manual processes and improvement of data quality and governance. . Initially the system will use M.L. to identify  inconsistent data and make preditions of how that data should be organized. . Not applicable to the risks defined above,automation of multiple manual processes and improvement of data quality and governance. . initially the system will use m.l. to identify inconsistent data and make preditions of how that data should be organized. . not applicable to the risks defined above
ServiceNow Predictive Intelligence,Department of Energy,DOE,LANL - Los Alamos National Lab (LAFO),Mission-Enabling (internal agency support) ,Prioritizing and categorizing incoming emails using AI.,"Provide better and more consistent classification of ticket data entered into ServiceNow
","Field classification data
",Operation and Maintenance,Neither,10/1/2023,10/1/2023,12/1/2023,Developed in-house.,Unknown,No,No,No,No,No,"Existing ticket data is used to train the model with data and training servers stored within FedRAMP High data centers where ServiceNow is hosted
","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",No,No – agency does not have access to source code.,Yes,ServiceNow ATO,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,Unknown,Unknown,Yes,Neither,0.6190476190476191,Provide better and more consistent classification of ticket data entered into ServiceNow . Field classification data,provide better and more consistent classification of ticket data entered into servicenow . field classification data
NLCOO AI for Lessons Learned tool ,Department of Energy,DOE,LANL - Los Alamos National Lab (LAFO),Mission-Enabling (internal agency support) ,Searching for information using AI.,"Better search to gain insights from exisitng lessons learned to improve how we do work.
","Search list of relevant documents
",Operation and Maintenance,Neither,2/27/2024,3/16/2024,3/16/2024,Developed in-house.,Unknown,No,No,No,No,No,"Not trained on any agency or LANL data
","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,Yes – source code is publicly available.,Yes,"Authorization to Operate Amazon Web Service Federal Risk and Authorization Management Program Infrastructure as a Service Cloud Service Provider, Amazon Web Service Information System Security Plan, SD-006-CP-002/L2",Less than 6 months,No,Yes,No,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,Unknown,Yes,Neither,0.6031746031746031,Better search to gain insights from exisitng lessons learned to improve how we do work. . Search list of relevant documents,better search to gain insights from exisitng lessons learned to improve how we do work. . search list of relevant documents
CAISY,Department of Energy,DOE,HC HQ -  Office of the Chief Human Capital Officer (HC),Education & Workforce,None of the above.,"1. CAISY provides interactive, scenario-based content powered by AI where a learner can practice new skills in a safe space. This AI simulation content allows learners to choose a role, practice specific skills by responding to AI prompts, and receive adaptive, personalized feedback to guide their development
2. The user  is introduced by an avatar that is generated with AI text-to-video. After the introduction, the learner can either interact by typing or using speech-to-text (STT) and text-to-speech (TTS) services for more immersive and natural interaction.
When the conversation is over, the learner will receive a rating and evaluation
",speech-to-text (STT) and text-to-speech (TTS) services for more immersive and natural interaction.,Operation and Maintenance,Neither,1/4/2024,8/21/2024,8/21/2024,Developed with contracting resources.,Unknown,No,No,No,No,No,None of the above,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",No,No – agency does not have access to source code.,Yes,PERCIPIO/CAISY,Less than 6 months,No,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"1. CAISY provides interactive, scenario-based content powered by AI where a learner can practice new skills in a safe space. This AI simulation content allows learners to choose a role, practice specific skills by responding to AI prompts, and receive adaptive, personalized feedback to guide their development
2. The user  is introduced by an avatar that is generated with AI text-to-video. After the introduction, the learner can either interact by typing or using speech-to-text (STT) and text-to-speech (TTS) services for more immersive and natural interaction.
When the conversation is over, the learner will receive a rating and evaluation . speech-to-text (STT) and text-to-speech (TTS) services for more immersive and natural interaction.","1. caisy provides interactive, scenario-based content powered by ai where a learner can practice new skills in a safe space. this ai simulation content allows learners to choose a role, practice specific skills by responding to ai prompts, and receive adaptive, personalized feedback to guide their development 2. the user is introduced by an avatar that is generated with ai text-to-video. after the introduction, the learner can either interact by typing or using speech-to-text (stt) and text-to-speech (tts) services for more immersive and natural interaction. when the conversation is over, the learner will receive a rating and evaluation . speech-to-text (stt) and text-to-speech (tts) services for more immersive and natural interaction."
ServiceNow Virtual Agent,Department of Energy,DOE,LANL - Los Alamos National Lab (LAFO),Mission-Enabling (internal agency support) ,Searching for information using AI.,"Provide troubleshooting assistance via chat to employees seeking assistance on IT-related issues
","Chat responses
",Operation and Maintenance,Neither,10/1/2023,1/1/2024,6/1/2024,Developed in-house.,Unknown,No,No,No,No,No,"Existing ticket data is used to train the model with data and training servers stored within FedRAMP High data centers where ServiceNow is hosted
","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",Unknown,No – agency does not have access to source code.,Yes,ServiceNow ATO,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,Unknown,Unknown,Yes,Neither,0.5873015873015873,Provide troubleshooting assistance via chat to employees seeking assistance on IT-related issues . Chat responses,provide troubleshooting assistance via chat to employees seeking assistance on it-related issues . chat responses
DOE Technical Standards,Department of Energy,DOE,EHSS HQ -  Office of Environment Health Safety & Security (EHSS),Mission-Enabling (internal agency support) ,Improving the quality of written communications using AI tools.,"Enhance quality and accuracy of technical standards, supporting DOE's commitment to safety excellence. ",Recommendations and feedback for improvement,Operation and Maintenance,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,Unknown,No,Unknown,No,Unknown,Unknown,No,No – agency does not have access to source code.,No,Unknown,Unknown,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,"Standards must align with regulatory requirements, which are subject to updates that an AI chatbot might not capture effectively. AI chatbot might misinterpret technical terminology or regulatory requirements, requiring human review. ",Unknown,Unknown,Unknown,Unknown,Unknown,No – it is not operationally practical to offer this.,Neither,0.5238095238095238,"Enhance quality and accuracy of technical standards, supporting DOE's commitment to safety excellence. . Recommendations and feedback for improvement . Standards must align with regulatory requirements, which are subject to updates that an AI chatbot might not capture effectively. AI chatbot might misinterpret technical terminology or regulatory requirements, requiring human review.","enhance quality and accuracy of technical standards, supporting doe's commitment to safety excellence. . recommendations and feedback for improvement . standards must align with regulatory requirements, which are subject to updates that an ai chatbot might not capture effectively. ai chatbot might misinterpret technical terminology or regulatory requirements, requiring human review."
Text To Speech Audio Generation,Department of Energy,DOE,KCNSC - Kansas City National Security Campus (KCFO),Education & Workforce,Improving the quality of written communications using AI tools.,AI tool is used to generate audio files to be used in training content.Allows for faster development of training to meet company needs.  This leads to a cost savings in both initial development and future modifications. ,MP4 Audio Files,Operation and Maintenance,Neither,Unknown,Unknown,9/16/2022,Unknown,Unknown,No,No,No,No,No,"We are not training the AI model, project is COTS",Unknown,No,No – agency does not have access to source code.,Yes,"N/A - System is used only for GREEN DATA, no OUO or CUI is allowed and does not require a System Security Plan to be named",Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Unknown,No – Agency did not request an extension for this use case.,No,No Risks Identified,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,Post transaction customer feedback collections,Yes,Neither,0.6031746031746031,AI tool is used to generate audio files to be used in training content.Allows for faster development of training to meet company needs.  This leads to a cost savings in both initial development and future modifications. . MP4 Audio Files . No Risks Identified,ai tool is used to generate audio files to be used in training content.allows for faster development of training to meet company needs. this leads to a cost savings in both initial development and future modifications. . mp4 audio files . no risks identified
Lex Natural Language Interface ,Department of Energy,DOE,NREL - National Renewable Energy Laboratory (EE),Mission-Enabling (internal agency support) ,Collaborating in real-time using AI-assisted tools in word processors.,"Create an efficient and user-friendly system that enables users to query project data, such as funding, AUs (allocation units), project focus, and fiscal years, with natural language prompts.
","The system executes a query against a postgres database and displays an LLM-generated textual summary of returned query records.  The system also displays the LLM generated queries. 
",Acquisition and/or Development,Neither,9/25/2024,10/1/2024,11/1/2024,Developed in-house.,Unknown,No,No,No,No,Yes,"Structured data from a PostgreSQL database containing information about HPC (High-Performance Computing) projects.
","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,"NREL Stratus AWS Compute Environment, NREL Azure System",Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Planned or in-progress.,"Misinterpretation of data due to model errors could lead to missed opportunities for users in receiving accurate project information. Inaccurate query results or inefficient use of resources may lead to financial costs associated with misinformed decision-making.
",Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"Process to detect bias and fairness are in testing and being regulary evaluated by model provideers. There are built-in guardrails in place to prevent abuse
",None of the above,Yes,Neither,0.7301587301587301,"Create an efficient and user-friendly system that enables users to query project data, such as funding, AUs (allocation units), project focus, and fiscal years, with natural language prompts. . The system executes a query against a postgres database and displays an LLM-generated textual summary of returned query records.  The system also displays the LLM generated queries. . Misinterpretation of data due to model errors could lead to missed opportunities for users in receiving accurate project information. Inaccurate query results or inefficient use of resources may lead to financial costs associated with misinformed decision-making.","create an efficient and user-friendly system that enables users to query project data, such as funding, aus (allocation units), project focus, and fiscal years, with natural language prompts. . the system executes a query against a postgres database and displays an llm-generated textual summary of returned query records. the system also displays the llm generated queries. . misinterpretation of data due to model errors could lead to missed opportunities for users in receiving accurate project information. inaccurate query results or inefficient use of resources may lead to financial costs associated with misinformed decision-making."
Scopus AI,Department of Energy,DOE,NE INL - NE Idaho National Laboratory (NE),Science & Space,None of the above.,Assist researchers by reducing time to find applicable research while increasing quality and accuracy of identified hits.,"Research citations, abstraacts and other summaries.",Operation and Maintenance,Neither,7/1/2024,8/6/2024,8/6/2024,Developed with contracting resources.,"Scopus AI was purcahsed, added as part of the Elsevier/Scopus subcontract.",No,No,No,No,No,"None. Scopus AI uses publicly available journal abstracts, no agency data is used.","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,No,"Key functional risks include fabricated & inaccurate answers as well as model & output bias.  Mitigated operational risks include data privacy, fraud and exploit risks.",Yes – by another appropriate agency office that was not directly involved in the system’s development,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,"Not using PII, commercial solution",None of the above,No – it is not operationally practical to offer this.,Neither,0.7777777777777778,"Assist researchers by reducing time to find applicable research while increasing quality and accuracy of identified hits. . Research citations, abstraacts and other summaries. . Key functional risks include fabricated & inaccurate answers as well as model & output bias.  Mitigated operational risks include data privacy, fraud and exploit risks.","assist researchers by reducing time to find applicable research while increasing quality and accuracy of identified hits. . research citations, abstraacts and other summaries. . key functional risks include fabricated & inaccurate answers as well as model & output bias. mitigated operational risks include data privacy, fraud and exploit risks."
EnerGPT,Department of Energy,DOE,IM-50 - Architecture Engineering Technology and Innovation (IM),Mission-Enabling (internal agency support) ,None of the above.,EnerGPT aims to enhance user productivity and reduce time spent on redundant tasks . ,EnerGPT generates answers to user questions. ,Acquisition and/or Development,Neither,7/8/2024,7/18/2024,Unknown,Developed with both contracting and in-house resources.,89303019AIM000005,No,Unknown,Unknown,Unknown,No,"Publicly available government documentation was used to evaluate the model, an example document is the Administration Cybersecurity Priorities for the FY 2026 Budget PDF (https://www.whitehouse.gov/wp-content/uploads/2024/07/FY26-Cybersecurity-Priorities-Memo_Signed.pdf)  published by the White House. ","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Unknown,Unknown,Yes,OCIO Google Cloud Platform,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,EnerGPT aims to enhance user productivity and reduce time spent on redundant tasks . . EnerGPT generates answers to user questions.,energpt aims to enhance user productivity and reduce time spent on redundant tasks . . energpt generates answers to user questions.
MAPPRITE,Department of Energy,DOE,EHSS HQ -  Office of Environment Health Safety & Security (EHSS),Mission-Enabling (internal agency support) ,Searching for information using AI.,"(1) The implemented AI will help automate data mining, ingesting, and indexing of existing disparate organizational data sources information for relevant safeguards and security (S&S) support information; and the (2) expected benefits will be to help improve EHSS-51 business workflows for researching potentially relevant S&S support data available such that the information will be accessible and searchable by policy subject matter specialists for awareness and additional context for strategic decision-making and policy management  ","In its full implementation phase, the application's AI output will provide S&S policy [support] data available such that the information will be accessible and searchable by policy subject matter specialists for strategic decision-making and policy management without having to manually search through hundreds of sources for relevant information.",Acquisition and/or Development,Neither,6/22/2023,7/15/2024,Unknown,Developed with both contracting and in-house resources.,Leveraged existing contract vehicle with DOE laboratory,No,No,Yes,No,No,"Department of Energy Directives. Requirement source documents, such as statutes, regulations and standards were also provided to the development team to assist with ingesting content to the AI model via AWS Kendra.  ","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Provisional Authorization to Operate (P-ATO) for the Methodology for Analyzing and Prioritizing Policy Requirements and Integrating Them for Effectiveness (MAPPRITE),6-12 months,Other,Other,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"(1) The implemented AI will help automate data mining, ingesting, and indexing of existing disparate organizational data sources information for relevant safeguards and security (S&S) support information; and the (2) expected benefits will be to help improve EHSS-51 business workflows for researching potentially relevant S&S support data available such that the information will be accessible and searchable by policy subject matter specialists for awareness and additional context for strategic decision-making and policy management . In its full implementation phase, the application's AI output will provide S&S policy [support] data available such that the information will be accessible and searchable by policy subject matter specialists for strategic decision-making and policy management without having to manually search through hundreds of sources for relevant information.","(1) the implemented ai will help automate data mining, ingesting, and indexing of existing disparate organizational data sources information for relevant safeguards and security (s&s) support information; and the (2) expected benefits will be to help improve ehss-51 business workflows for researching potentially relevant s&s support data available such that the information will be accessible and searchable by policy subject matter specialists for awareness and additional context for strategic decision-making and policy management . in its full implementation phase, the application's ai output will provide s&s policy [support] data available such that the information will be accessible and searchable by policy subject matter specialists for strategic decision-making and policy management without having to manually search through hundreds of sources for relevant information."
MI8 Collimators Surogate Model,Department of Energy,DOE,FNAL - Fermi National Accelerator (SC43 OIM),Science & Space,None of the above.,This project is working to create a ML surogate model of the exisitng MI8 collimation system. The purpose of the ML model is to aid in the tuning of the collimation system for acclerator operations and to help find more optimal settings in a timely manner. We hope to extend these techniques to other sub-systems and also a new MI8 collimation system being installed.,The ML outputs of the system are predictions of collimation system performance given collimation system settings and beam charecteristics.,Acquisition and/or Development,Neither,10/1/2023,10/1/2024,10/1/2024,Developed in-house.,Unknown,No,No,No,No,No,Accelerator operations machine data,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,Yes – source code is publicly available.,No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,None of the above,Unknown,Neither,0.5396825396825397,This project is working to create a ML surogate model of the exisitng MI8 collimation system. The purpose of the ML model is to aid in the tuning of the collimation system for acclerator operations and to help find more optimal settings in a timely manner. We hope to extend these techniques to other sub-systems and also a new MI8 collimation system being installed. . The ML outputs of the system are predictions of collimation system performance given collimation system settings and beam charecteristics.,this project is working to create a ml surogate model of the exisitng mi8 collimation system. the purpose of the ml model is to aid in the tuning of the collimation system for acclerator operations and to help find more optimal settings in a timely manner. we hope to extend these techniques to other sub-systems and also a new mi8 collimation system being installed. . the ml outputs of the system are predictions of collimation system performance given collimation system settings and beam charecteristics.
Machine Learning components within Splunk Enterprise Security,Department of Energy,DOE,SLAC - SLAC National Accelerator Laboratory (SC43 OIM),Mission-Enabling (internal agency support) ,Identifying unusual patterns in system logs from a single incident report using AI.,"Allows for big data analysis and identification of trends in large datasets. SLAC expects to be able to perform data clustering, trend analysis, classification, and regression by using Splunk
","Prediction and clustering models
",Operation and Maintenance,Neither,8/1/2019,8/1/2019,9/1/2020,Developed with both contracting and in-house resources.,SLAC-0000238366,No,No,No,No,No,"Splunk's machine learning model is trained on data generated by activity at SLAC
","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Allows for big data analysis and identification of trends in large datasets. SLAC expects to be able to perform data clustering, trend analysis, classification, and regression by using Splunk . Prediction and clustering models","allows for big data analysis and identification of trends in large datasets. slac expects to be able to perform data clustering, trend analysis, classification, and regression by using splunk . prediction and clustering models"
Machine Learning components within CrowdStrike,Department of Energy,DOE,SLAC - SLAC National Accelerator Laboratory (SC43 OIM),Mission-Enabling (internal agency support) ,Identifying unusual patterns in system logs from a single incident report using AI.,"CrowdStrike uses machine learning to review security events in order to create notifications of detections and incidents for the SLAC Cybersecurity team. From the use of CrowdStrike's machine learning components, SLAC receives the benefit of visibility to analyze possible security events
","CrowdStrike Detections or Incidents
",Operation and Maintenance,Neither,1/27/2021,1/27/2021,7/7/2021,Developed with contracting resources.,"SLAC-0000208966
",No,No,No,No,No,"CrowdStrike's machine learning model is trained on data generated by activity at SLAC
","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",No,No – agency does not have access to source code.,Yes,CrowdStrike Falcon,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"CrowdStrike uses machine learning to review security events in order to create notifications of detections and incidents for the SLAC Cybersecurity team. From the use of CrowdStrike's machine learning components, SLAC receives the benefit of visibility to analyze possible security events . CrowdStrike Detections or Incidents","crowdstrike uses machine learning to review security events in order to create notifications of detections and incidents for the slac cybersecurity team. from the use of crowdstrike's machine learning components, slac receives the benefit of visibility to analyze possible security events . crowdstrike detections or incidents"
AskOEDI,Department of Energy,DOE,NREL - National Renewable Energy Laboratory (EE),Mission-Enabling (internal agency support) ,Collaborating in real-time using AI-assisted tools in word processors.,Making data more accessible and user-friendly for the public. ,The systems leverages Retrieval-Augmented Generation to find semantically relevent content which the AI (LLM) summarizes for the end user as a method to describe relevent content within the data catalog. ,Operation and Maintenance,Neither,9/30/2024,10/1/2024,10/30/2024,Developed in-house.,Unknown,No,Yes,No,No,Yes,As this is summarizing data from the OEDI data repository the related datasets are described by the catalog which is also used to validate the AI responses:  https://data.openei.org/,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,NREL Stratus AWS Compute Environment and NREL Azure System,Less than 6 months,Yes,No,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Unknown,No – Agency did not request an extension for this use case.,Planned or in-progress.,"As the use case for the AI has been limited to summarization of existing metadata the key risks were identified as LLM hallucinations or LLM generated text which would cause reputational harm to DOE.   Techniques such as red-teaming, adversarial testing, and prompt-injection testing were used to evaluate constraint of hallucinations and LLM.  ",Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Process to detect bias and fairness are in testing and being regulary evaluated by model provideers. There are built-in guardrails in place to prevent abuse.,None of the above,No – it is not operationally practical to offer this.,Neither,0.746031746031746,"Making data more accessible and user-friendly for the public. . The systems leverages Retrieval-Augmented Generation to find semantically relevent content which the AI (LLM) summarizes for the end user as a method to describe relevent content within the data catalog. . As the use case for the AI has been limited to summarization of existing metadata the key risks were identified as LLM hallucinations or LLM generated text which would cause reputational harm to DOE.   Techniques such as red-teaming, adversarial testing, and prompt-injection testing were used to evaluate constraint of hallucinations and LLM.","making data more accessible and user-friendly for the public. . the systems leverages retrieval-augmented generation to find semantically relevent content which the ai (llm) summarizes for the end user as a method to describe relevent content within the data catalog. . as the use case for the ai has been limited to summarization of existing metadata the key risks were identified as llm hallucinations or llm generated text which would cause reputational harm to doe. techniques such as red-teaming, adversarial testing, and prompt-injection testing were used to evaluate constraint of hallucinations and llm."
Hanford Search,Department of Energy,DOE,RL - Hanford - Richland Operations Office - Hanford (EM),Mission-Enabling (internal agency support) ,Searching for information using AI.,"The purpose of the Hanford  Search is to provide similar functionality to our Hanford Search application without needing multiple applications. The benefits of the AI are that there is a single interface with multiple uses and the AI can provide better, more relevant search results.  Additionally, users can ask questions in natural language instead of needing to input specific search criteria.  
","The system outputs text respones from user prompts requesting information on grounded data related to Hanford Search Index
",Acquisition and/or Development,Neither,6/24/2024,6/23/2024,Unknown,Developed with contracting resources.,"89303320DEM000031
",No,No,No,Yes,Yes,"Our Data does not train the models.
","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,No – agency does not have access to source code.,Yes,Microsoft Azure OpenAI,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,"Risks identified in this AI were incorrect or halucination responses from the AI. Users are trained and informed to validate the AI responses before use. They were identified through discimination of information of risk from other ChatGPT applications along with developer and user testing.
",Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"The AI does not take into account any demographic information when generating responses, and does not use any demographic information about employees specifically to generate results. Our AI use case should have have any demographic disparity. If a users were to find an issue with a response, they can provide feedback via a feedback mechanism with each response and provide a summary of their feedback. Developers would then address the issue and look to mitigate any issues found.
",Other,No – it is not operationally practical to offer this.,Neither,0.7619047619047619,"The purpose of the Hanford  Search is to provide similar functionality to our Hanford Search application without needing multiple applications. The benefits of the AI are that there is a single interface with multiple uses and the AI can provide better, more relevant search results.  Additionally, users can ask questions in natural language instead of needing to input specific search criteria. . The system outputs text respones from user prompts requesting information on grounded data related to Hanford Search Index . Risks identified in this AI were incorrect or halucination responses from the AI. Users are trained and informed to validate the AI responses before use. They were identified through discimination of information of risk from other ChatGPT applications along with developer and user testing.","the purpose of the hanford search is to provide similar functionality to our hanford search application without needing multiple applications. the benefits of the ai are that there is a single interface with multiple uses and the ai can provide better, more relevant search results. additionally, users can ask questions in natural language instead of needing to input specific search criteria. . the system outputs text respones from user prompts requesting information on grounded data related to hanford search index . risks identified in this ai were incorrect or halucination responses from the ai. users are trained and informed to validate the ai responses before use. they were identified through discimination of information of risk from other chatgpt applications along with developer and user testing."
AI Chat Bot for IT User Services,Department of Energy,DOE,PPPL - Princeton Plasma Physics Laboratory (SC43 OIM),Mission-Enabling (internal agency support) ,None of the above.,"Interactive RAG chat bot model trained on existing, updated and new IT resource documentation in the user space. Platform will be used as an informative method for users to handle tier 1 IT issues and help guide to the correct place.",AI output will be recommendations and instructions based on training data from IT administrators in more user friendly responses,Initiated,Neither,5/1/2024,9/1/2024,Unknown,Developed in-house.,Unknown,No,No,No,No,No,Help Desk Knowledge Base Article and other supporting documentation in the user space,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Unknown,No – Agency did not request an extension for this use case.,Planned or in-progress.,No key risks identified,Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"RAG Model will be significantly grounded in factual data, limiting creativity in responses",Unknown,Yes,Neither,0.6666666666666666,"Interactive RAG chat bot model trained on existing, updated and new IT resource documentation in the user space. Platform will be used as an informative method for users to handle tier 1 IT issues and help guide to the correct place. . AI output will be recommendations and instructions based on training data from IT administrators in more user friendly responses . No key risks identified","interactive rag chat bot model trained on existing, updated and new it resource documentation in the user space. platform will be used as an informative method for users to handle tier 1 it issues and help guide to the correct place. . ai output will be recommendations and instructions based on training data from it administrators in more user friendly responses . no key risks identified"
Hanford Popfon Search,Department of Energy,DOE,RL - Hanford - Richland Operations Office - Hanford (EM),Mission-Enabling (internal agency support) ,Searching for information using AI.,"The purpose of the Hanford Popfon Search is to provide similar functionality to our employee look-up application without needing multiple applications. The benefits of the AI are that there is a single interface with multiple uses and the previous application, which is older in architecture, can be retired, providing a safer, more secure, and cost effective alternative. Additionally, users can ask questions in natural language instead of needing to input specific search criteria.  
","The system outputs text respones from user prompts requesting information on grounded data related to employee contact and organization information
",Acquisition and/or Development,Neither,5/7/2024,6/22/2024,Unknown,Developed with contracting resources.,89303320DEM000031,No,No,No,Yes,Yes,Our Data does not train the models.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,No – agency does not have access to source code.,Yes,Microsoft Azure OpenAI,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,Risks identified in this AI were incorrect or halucination responses from the AI. Users are trained and informed to validate the AI responses before use. They were identified through discimination of information of risk from other ChatGPT applications along with developer and user testing.,Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"The AI does not take into account any demographic information when generating responses, and does not use any demographic information about employees specifically to generate results. Our AI use case should have have any demographic disparity. If a users were to find an issue with a response, they can provide feedback via a feedback mechanism with each response and provide a summary of their feedback. Developers would then address the issue and look to mitigate any issues found.
",Other,No – it is not operationally practical to offer this.,Neither,0.7619047619047619,"The purpose of the Hanford Popfon Search is to provide similar functionality to our employee look-up application without needing multiple applications. The benefits of the AI are that there is a single interface with multiple uses and the previous application, which is older in architecture, can be retired, providing a safer, more secure, and cost effective alternative. Additionally, users can ask questions in natural language instead of needing to input specific search criteria. . The system outputs text respones from user prompts requesting information on grounded data related to employee contact and organization information . Risks identified in this AI were incorrect or halucination responses from the AI. Users are trained and informed to validate the AI responses before use. They were identified through discimination of information of risk from other ChatGPT applications along with developer and user testing.","the purpose of the hanford popfon search is to provide similar functionality to our employee look-up application without needing multiple applications. the benefits of the ai are that there is a single interface with multiple uses and the previous application, which is older in architecture, can be retired, providing a safer, more secure, and cost effective alternative. additionally, users can ask questions in natural language instead of needing to input specific search criteria. . the system outputs text respones from user prompts requesting information on grounded data related to employee contact and organization information . risks identified in this ai were incorrect or halucination responses from the ai. users are trained and informed to validate the ai responses before use. they were identified through discimination of information of risk from other chatgpt applications along with developer and user testing."
AI for Intelligent Automation,Department of Energy,DOE,NE INL - NE Idaho National Laboratory (NE),Mission-Enabling (internal agency support) ,None of the above.,"Improve the timeliness and quality of manual work processes through automation where generative AI can make comparisons and decisions using INL procedures and controlled documents, with humans performing final validation and approval.",Completion of forms for human validation and approval.,Initiated,Neither,10/1/2024,Unknown,Unknown,Developed in-house.,Unknown,No,No,Yes,Yes,Yes,"At this time, INL plans to use non-CUI data with this solution, including the employee handbook, approved controlled documents, and other material that will assist workers in completing processes and activities.  RAG (mini RAG preferred) is the method that will be used for integration with the AI solution.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,No – agency does not have access to source code.,Yes,"CS-SSP-123,Cybersecurity System Security Plan for Azure Platform",6-12 months,Other,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,Key functional risks include fabricated & inaccurate answers as well as model & output bias.  Mitigated operational risks include model exploit.,Yes – by another appropriate agency office that was not directly involved in the system’s development,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"Using minimal sensitive information, as identified in ATU, built on commercial solution",None of the above,No – it is not operationally practical to offer this.,Neither,0.7301587301587301,"Improve the timeliness and quality of manual work processes through automation where generative AI can make comparisons and decisions using INL procedures and controlled documents, with humans performing final validation and approval. . Completion of forms for human validation and approval. . Key functional risks include fabricated & inaccurate answers as well as model & output bias.  Mitigated operational risks include model exploit.","improve the timeliness and quality of manual work processes through automation where generative ai can make comparisons and decisions using inl procedures and controlled documents, with humans performing final validation and approval. . completion of forms for human validation and approval. . key functional risks include fabricated & inaccurate answers as well as model & output bias. mitigated operational risks include model exploit."
Hanford Ai Liaison,Department of Energy,DOE,RL - Hanford - Richland Operations Office - Hanford (EM),Mission-Enabling (internal agency support) ,None of the above.,"The purpose of the Hanford Ai Liaison tool is to provide a prompt to text tool that will allow users to ask questions against the chat and receive generative responses based on model training. The benefits of this AI are that it provides a mitigation for users that were using similar funtionality in external ChatGPT tools that were available to AI users and then blocked. It also provides a safe, secure enviroment for a ChatGPT experience where our prompts will not be used to train models in external system and our prompt and document data does not leave our accedidation boundary.","The system outputs text respones from user prompts using generative, pre-trained, transformer logic large language modeling (GPT LLM)",Operation and Maintenance,Neither,4/23/2024,5/14/2024,10/28/2024,Developed with contracting resources.,89303320DEM000031,No,No,No,Yes,No,No agency-owned data is used in this model. Our Data does not train the models.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",Yes,No – agency does not have access to source code.,Yes,Microsoft Azure OpenAI,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Planned or in-progress.,Risks identified in this AI were incorrect or halucination responses from the AI. Users are trained and informed to validate the AI responses before use. They were identified through discimination of information of risk from other ChatGPT applications along with developer and user testing.,Yes – by another appropriate agency office that was not directly involved in the system’s development,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"The AI does not take into account any demographic information when generating responses, and does not use any demographic information about employees specifically to generate results. Our AI use case should have have any demographic disparity. If a users were to find an issue with a response, they can provide feedback via a feedback mechanism with each response and provide a summary of their feedback. Developers would then address the issue and look to mitigate any issues found.",Other,No – it is not operationally practical to offer this.,Neither,0.7777777777777778,"The purpose of the Hanford Ai Liaison tool is to provide a prompt to text tool that will allow users to ask questions against the chat and receive generative responses based on model training. The benefits of this AI are that it provides a mitigation for users that were using similar funtionality in external ChatGPT tools that were available to AI users and then blocked. It also provides a safe, secure enviroment for a ChatGPT experience where our prompts will not be used to train models in external system and our prompt and document data does not leave our accedidation boundary. . The system outputs text respones from user prompts using generative, pre-trained, transformer logic large language modeling (GPT LLM) . Risks identified in this AI were incorrect or halucination responses from the AI. Users are trained and informed to validate the AI responses before use. They were identified through discimination of information of risk from other ChatGPT applications along with developer and user testing.","the purpose of the hanford ai liaison tool is to provide a prompt to text tool that will allow users to ask questions against the chat and receive generative responses based on model training. the benefits of this ai are that it provides a mitigation for users that were using similar funtionality in external chatgpt tools that were available to ai users and then blocked. it also provides a safe, secure enviroment for a chatgpt experience where our prompts will not be used to train models in external system and our prompt and document data does not leave our accedidation boundary. . the system outputs text respones from user prompts using generative, pre-trained, transformer logic large language modeling (gpt llm) . risks identified in this ai were incorrect or halucination responses from the ai. users are trained and informed to validate the ai responses before use. they were identified through discimination of information of risk from other chatgpt applications along with developer and user testing."
Tritium Accountancy for Fusione Energy,Department of Energy,DOE,SRS - SRNL - Savannah River Site - Savannah River National Laboratory (EM),Science & Space,None of the above.,"Adaptive learning, improved uncertainty on accountancy, and prediction for commercial systems.",Real-time tritium data with reduced uncertaintingy for a continually operating fusion power plant. A framework to be used in full systems.,Initiated,Neither,10/1/2023,10/1/2023,Unknown,Developed in-house.,No-Cost,No,Yes,No,No,No,Sensor measurements of hydrogen and deuterium. Subject matter experts.,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,6-12 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,Still working on AI algorithm assessment tests to identify optimal choice,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,Other,Yes,Neither,0.7301587301587301,"Adaptive learning, improved uncertainty on accountancy, and prediction for commercial systems. . Real-time tritium data with reduced uncertaintingy for a continually operating fusion power plant. A framework to be used in full systems. . Still working on AI algorithm assessment tests to identify optimal choice","adaptive learning, improved uncertainty on accountancy, and prediction for commercial systems. . real-time tritium data with reduced uncertaintingy for a continually operating fusion power plant. a framework to be used in full systems. . still working on ai algorithm assessment tests to identify optimal choice"
ServiceNow Now Assist,Department of Energy,DOE,NE INL - NE Idaho National Laboratory (NE),Mission-Enabling (internal agency support) ,None of the above.,"Improve the timeliness and quality of transactional service desk requests through improved incident search, resolution, feedback as well as AI-assisted coding of  workflows.","Outputs are consistent with commercial service desk products, including the search, creation and closure of service requests.",Initiated,Neither,10/1/2024,10/1/2024,Unknown,Developed with both contracting and in-house resources.,Unknown,No,No,Yes,Yes,Yes,"This use case will utilize service desk incident, problem and knowledge management sources for training and use.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,No – agency does not have access to source code.,Yes,"CS-CAR-157, Cybersecurity System Security Plan for ServiceNow Platform",6-12 months,Yes,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,"Key functional risks include fabricated & inaccurate answers as well as model & output bias.  Mitigated operational risks include data privacy, fraud and exploit risks.",Yes – by another appropriate agency office that was not directly involved in the system’s development,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,"Using minimal PII to identify employee (name, id#), commercial solution",None of the above,No – it is not operationally practical to offer this.,Neither,0.7301587301587301,"Improve the timeliness and quality of transactional service desk requests through improved incident search, resolution, feedback as well as AI-assisted coding of  workflows. . Outputs are consistent with commercial service desk products, including the search, creation and closure of service requests. . Key functional risks include fabricated & inaccurate answers as well as model & output bias.  Mitigated operational risks include data privacy, fraud and exploit risks.","improve the timeliness and quality of transactional service desk requests through improved incident search, resolution, feedback as well as ai-assisted coding of workflows. . outputs are consistent with commercial service desk products, including the search, creation and closure of service requests. . key functional risks include fabricated & inaccurate answers as well as model & output bias. mitigated operational risks include data privacy, fraud and exploit risks."
Enhancing ClimRR Capabilities to Better Support Electric Utility Applications and Technical Assistance,Department of Energy,DOE,GDO HQ -  Grid Deployment Office (GDO),Science & Space,None of the above.,"The AI, CALLM (Climate Action through Large Language Models), addresses the problem of communicating complex climate projections and scientific literature to a broad audience, particularly electric sector stakeholders. It simplifies this information to help these stakeholders identify climate resilience solutions.

Expected benefits include improved communication of climate science, empowering stakeholders to directly address climate change impacts, and accelerating the scalability of ClimRR (presumably an existing Argonne tool) to serve a wider range of users. This ultimately leads to more effective climate resilience planning and potentially cost savings through better informed decision-making.","The AI output of CALLM is information synthesized from complex climate projections and scientific literature, presented in a simplified and accessible format. This output helps users understand potential climate impacts and identify appropriate climate resilience solutions. The information is grounded in vetted data and published research to minimize inaccuracies and hallucinations common in large language models. The output could range from summaries of climate-related risks, to lists of potential adaptation strategies tailored to specific situations, depending on the user's input and the function of the system it's integrated with (like ClimRR).",Initiated,Neither,8/26/2024,10/1/2024,Unknown,Developed in-house.,Unknown,No,Unknown,No,Unknown,No,"The Climate Action through Large Language Models (CALLM) tool utilizes vetted climate data and published climate resilience literature to train, fine-tune, and evaluate its performance. The specific datasets are not detailed here, but the approach emphasizes the use of established climate science information to ground the model's responses and mitigate inaccuracies.",Unknown,Unknown,Unknown,No,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.30158730158730157,"The AI, CALLM (Climate Action through Large Language Models), addresses the problem of communicating complex climate projections and scientific literature to a broad audience, particularly electric sector stakeholders. It simplifies this information to help these stakeholders identify climate resilience solutions.

Expected benefits include improved communication of climate science, empowering stakeholders to directly address climate change impacts, and accelerating the scalability of ClimRR (presumably an existing Argonne tool) to serve a wider range of users. This ultimately leads to more effective climate resilience planning and potentially cost savings through better informed decision-making. . The AI output of CALLM is information synthesized from complex climate projections and scientific literature, presented in a simplified and accessible format. This output helps users understand potential climate impacts and identify appropriate climate resilience solutions. The information is grounded in vetted data and published research to minimize inaccuracies and hallucinations common in large language models. The output could range from summaries of climate-related risks, to lists of potential adaptation strategies tailored to specific situations, depending on the user's input and the function of the system it's integrated with (like ClimRR).","the ai, callm (climate action through large language models), addresses the problem of communicating complex climate projections and scientific literature to a broad audience, particularly electric sector stakeholders. it simplifies this information to help these stakeholders identify climate resilience solutions. expected benefits include improved communication of climate science, empowering stakeholders to directly address climate change impacts, and accelerating the scalability of climrr (presumably an existing argonne tool) to serve a wider range of users. this ultimately leads to more effective climate resilience planning and potentially cost savings through better informed decision-making. . the ai output of callm is information synthesized from complex climate projections and scientific literature, presented in a simplified and accessible format. this output helps users understand potential climate impacts and identify appropriate climate resilience solutions. the information is grounded in vetted data and published research to minimize inaccuracies and hallucinations common in large language models. the output could range from summaries of climate-related risks, to lists of potential adaptation strategies tailored to specific situations, depending on the user's input and the function of the system it's integrated with (like climrr)."
WCD-AI,Department of Energy,DOE,ANL - Argonne National Laboratory (SC43 OIM),Mission-Enabling (internal agency support) ,None of the above.,Recommend keyword-based search for relevant Lessons Learned published on DOE OPEXShare.,Recommended keywords for search based on user-authored Work Control Document.,Operation and Maintenance,Neither,6/1/2020,7/1/2020,2/1/2021,Developed in-house.,Unknown,No,No,No,No,Yes,Existing Work Control Document records in database system.,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Argonne National Laboratory Information Technology Infrastructure,Less than 6 months,No,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,AI use case provides recommendation of keywords for user to perform a manual search. Negligible risk.,Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Users can provide feedback.,Other,No – it is not operationally practical to offer this.,Neither,0.7777777777777778,Recommend keyword-based search for relevant Lessons Learned published on DOE OPEXShare. . Recommended keywords for search based on user-authored Work Control Document. . AI use case provides recommendation of keywords for user to perform a manual search. Negligible risk.,recommend keyword-based search for relevant lessons learned published on doe opexshare. . recommended keywords for search based on user-authored work control document. . ai use case provides recommendation of keywords for user to perform a manual search. negligible risk.
OpenText for Records Management (File share auto-classification),Department of Energy,DOE,EE HQ - EE Headquarters (EE),Mission-Enabling (internal agency support) ,None of the above.,Reduce time required to classify legacy records accumulated over 20 years.,OpenText will categorize each file located on the network drives.,Acquisition and/or Development,Neither,10/1/2022,1/2/2023,Unknown,Developed with contracting resources.,other direct costs,No,No,Yes,Yes,No,Trained using existing  internal EERE records currated by subject matter experts.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",No,No – agency does not have access to source code.,Yes,EE HQ LAN (aka SCORE),Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,Reduce time required to classify legacy records accumulated over 20 years. . OpenText will categorize each file located on the network drives.,reduce time required to classify legacy records accumulated over 20 years. . opentext will categorize each file located on the network drives.
OPQ-AI,Department of Energy,DOE,ANL - Argonne National Laboratory (SC43 OIM),Mission-Enabling (internal agency support) ,None of the above.,"Significantly reduces person-hours for manual review of incoming people registrations to match with existing database records by recommending most likely matches, if an existing record is identified that matches with the registration details.",Recommendation of matched person record that already exists or that a new person record should be created.,Operation and Maintenance,Neither,12/1/2019,5/1/2023,12/1/2023,Developed in-house.,Unknown,No,No,No,No,Yes,Data includes read-only access internal person/HR records existing in current database systems.,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Argonne National Laboratory Information Technology Infrastructure,Less than 6 months,No,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,"Mis-match of person records, requires human intervention to re-assign records or remove incorrect records.",Yes – by an agency AI oversight board not directly involved in the system’s development,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,None. No demographic disparities are forseen.,Other,Yes,Neither,0.7777777777777778,"Significantly reduces person-hours for manual review of incoming people registrations to match with existing database records by recommending most likely matches, if an existing record is identified that matches with the registration details. . Recommendation of matched person record that already exists or that a new person record should be created. . Mis-match of person records, requires human intervention to re-assign records or remove incorrect records.","significantly reduces person-hours for manual review of incoming people registrations to match with existing database records by recommending most likely matches, if an existing record is identified that matches with the registration details. . recommendation of matched person record that already exists or that a new person record should be created. . mis-match of person records, requires human intervention to re-assign records or remove incorrect records."
Funding Finder,Department of Energy,DOE,IM-50 - Architecture Engineering Technology and Innovation (IM),Mission-Enabling (internal agency support) ,None of the above.,The Funding Finder will aggregate FOAs from different DOE sources and enables users to ask questions when identifying opportunities and developing proposals. ,Answers to questions about DOE FOAs.,Acquisition and/or Development,Neither,3/10/2024,10/1/2023,Unknown,Developed with both contracting and in-house resources.,89303019AIM000005,No,Unknown,Unknown,Unknown,No,"The Funding Finder tool is in development and is utilizing publicly available DOE websites and FOAs.  Example websites include ""https://infrastructure-exchange.energy.gov/"", ""https://eere-exchange.energy.gov/"", ""https://arpa-e-foa.energy.gov/"", ""https://48c-exchange.energy.gov/"", ""https://oced-exchange.energy.gov/"", ""https://ie-exchange.energy.gov/"".","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Unknown,Unknown,Yes,OCIO Google Cloud Platform,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,The Funding Finder will aggregate FOAs from different DOE sources and enables users to ask questions when identifying opportunities and developing proposals. . Answers to questions about DOE FOAs.,the funding finder will aggregate foas from different doe sources and enables users to ask questions when identifying opportunities and developing proposals. . answers to questions about doe foas.
PDF Analyzer,Department of Energy,DOE,IM-50 - Architecture Engineering Technology and Innovation (IM),Mission-Enabling (internal agency support) ,None of the above.,PDF Analyzer will enable teams across the DOE to upload large PDFs and ask questions and generate content related to those PDFs.  ,PDF Analyzer will output the answers to a user's question along with the relevant sections of the PDF that the answer is based on. ,Acquisition and/or Development,Neither,10/1/2024,10/14/2024,Unknown,Developed with both contracting and in-house resources.,89303019AIM000005,No,Unknown,Unknown,Unknown,No,"For initial evaluation, the development team used publicly available documents like the  Feasibility Study - Nyngan Scandium Project PDF (https://scandiummining.com/site/assets/files/5775/feasbility_study-nyngan_scandium_project.pdf)  to test the tool.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,Yes,OCIO Google Cloud Platform,Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,PDF Analyzer will enable teams across the DOE to upload large PDFs and ask questions and generate content related to those PDFs. . PDF Analyzer will output the answers to a user's question along with the relevant sections of the PDF that the answer is based on.,pdf analyzer will enable teams across the doe to upload large pdfs and ask questions and generate content related to those pdfs. . pdf analyzer will output the answers to a user's question along with the relevant sections of the pdf that the answer is based on.
Hanford  Service Ticket Lookup,Department of Energy,DOE,RL - Hanford - Richland Operations Office - Hanford (EM),Mission-Enabling (internal agency support) ,Searching for information using AI.,"The purpose of the Hanford Service Ticket Lookup is to provide a single interface with for customers to ask questions and get to service tickets without having to navigate extensive menu's, tool bars, and search functions. Eventually, this will include service tickets from multiple platforms, providing the customer with a single interface to do all things service request related. Additionally, users can ask questions in natural language instead of needing to input specific search criteria.  
","The system outputs text respones from user prompts requesting information on grounded data related to Service Ticket Requests
",Acquisition and/or Development,Neither,5/7/2024,6/22/2024,Unknown,Developed with contracting resources.,"89303320DEM000031
",No,No,No,Yes,Yes,"Our Data does not train the models.
","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,No – agency does not have access to source code.,Yes,Microsoft Azure OpenAI,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,"Risks identified in this AI were incorrect or halucination responses from the AI. Users are trained and informed to validate the AI responses before use. They were identified through discimination of information of risk from other ChatGPT applications along with developer and user testing.
",Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"The AI does not take into account any demographic information when generating responses, and does not use any demographic information about employees specifically to generate results. Our AI use case should have have any demographic disparity. If a users were to find an issue with a response, they can provide feedback via a feedback mechanism with each response and provide a summary of their feedback. Developers would then address the issue and look to mitigate any issues found.
",Other,No – it is not operationally practical to offer this.,Neither,0.7619047619047619,"The purpose of the Hanford Service Ticket Lookup is to provide a single interface with for customers to ask questions and get to service tickets without having to navigate extensive menu's, tool bars, and search functions. Eventually, this will include service tickets from multiple platforms, providing the customer with a single interface to do all things service request related. Additionally, users can ask questions in natural language instead of needing to input specific search criteria. . The system outputs text respones from user prompts requesting information on grounded data related to Service Ticket Requests . Risks identified in this AI were incorrect or halucination responses from the AI. Users are trained and informed to validate the AI responses before use. They were identified through discimination of information of risk from other ChatGPT applications along with developer and user testing.","the purpose of the hanford service ticket lookup is to provide a single interface with for customers to ask questions and get to service tickets without having to navigate extensive menu's, tool bars, and search functions. eventually, this will include service tickets from multiple platforms, providing the customer with a single interface to do all things service request related. additionally, users can ask questions in natural language instead of needing to input specific search criteria. . the system outputs text respones from user prompts requesting information on grounded data related to service ticket requests . risks identified in this ai were incorrect or halucination responses from the ai. users are trained and informed to validate the ai responses before use. they were identified through discimination of information of risk from other chatgpt applications along with developer and user testing."
INL AI Virtual Assitant (AiVA),Department of Energy,DOE,NE INL - NE Idaho National Laboratory (NE),Mission-Enabling (internal agency support) ,Improving the quality of written communications using AI tools.,"This chatbot uses commercial chatgpt-like capability to answer questions, provide coaching on processes, summarize and improve communications, and produce code in a variety of formats.  INL has been authorized and has planned activiites in 2025 to begin adding internal INL non-CUI data using RAG.  Examples include the employee handbook and approved controlled documents.","Outputs are consistent with commercial chatbot products, such as ChatGPT.",Operation and Maintenance,Neither,1/1/2024,1/1/2024,5/13/2024,Developed in-house.,Unknown,No,No,No,Yes,Yes,"At this time, INL plans to use non-CUI data with this solution, including the employee handbook, approved controlled documents, and other material that will assist workers in completing processes and activities.  RAG (mini RAG preferred) is the method that will be used for integration with the AI solution.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,No – agency does not have access to source code.,Yes,"CS-SSP-123,Cybersecurity System Security Plan for Azure Platform",Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,"Key functional risks are similar to use of ChatGPT, including fabricated & inaccurate answers, model & output bias, and potential copyright risks.  Mitigated operational risks include data privacy, fraud and exploit risks.  Additional considerations include ethics and professional responsibilities, as associated with any new technology/process differentiator.",Yes – by another appropriate agency office that was not directly involved in the system’s development,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"Not using PII, models and interface are commercial",None of the above,No – it is not operationally practical to offer this.,Neither,0.7301587301587301,"This chatbot uses commercial chatgpt-like capability to answer questions, provide coaching on processes, summarize and improve communications, and produce code in a variety of formats.  INL has been authorized and has planned activiites in 2025 to begin adding internal INL non-CUI data using RAG.  Examples include the employee handbook and approved controlled documents. . Outputs are consistent with commercial chatbot products, such as ChatGPT. . Key functional risks are similar to use of ChatGPT, including fabricated & inaccurate answers, model & output bias, and potential copyright risks.  Mitigated operational risks include data privacy, fraud and exploit risks.  Additional considerations include ethics and professional responsibilities, as associated with any new technology/process differentiator.","this chatbot uses commercial chatgpt-like capability to answer questions, provide coaching on processes, summarize and improve communications, and produce code in a variety of formats. inl has been authorized and has planned activiites in 2025 to begin adding internal inl non-cui data using rag. examples include the employee handbook and approved controlled documents. . outputs are consistent with commercial chatbot products, such as chatgpt. . key functional risks are similar to use of chatgpt, including fabricated & inaccurate answers, model & output bias, and potential copyright risks. mitigated operational risks include data privacy, fraud and exploit risks. additional considerations include ethics and professional responsibilities, as associated with any new technology/process differentiator."
Unleashing AI Transformer Models on FPGAs for Accelerating LHC and Particle Physics,Department of Energy,DOE,FNAL - Fermi National Accelerator (SC43 OIM),Science & Space,None of the above.,"This effort focuses on Transformer models for representation learning on Field Programmable Gate Arrays
(FPGA), in order to seamlessly integrate AI capabilities into particle physics experiments, specifically
focusing on the CMS level-1 (L1) trigger at the High-Luminosity LHC (HL-LHC) and real-time magnet
quench detection. While conventional methods for event identification have limitations, modern AI and
machine learning techniques offer superior alternatives.","This AI system has two fold use cases, represnetation learning for LHC Trigger and multi-modal magnet quench detection algorithms.",Acquisition and/or Development,Neither,10/1/2023,10/1/2024,10/1/2027,Developed in-house.,Unknown,No,No,No,No,No,research datasets from scientific experiments,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,None of the above,Unknown,Neither,0.5238095238095238,"This effort focuses on Transformer models for representation learning on Field Programmable Gate Arrays
(FPGA), in order to seamlessly integrate AI capabilities into particle physics experiments, specifically
focusing on the CMS level-1 (L1) trigger at the High-Luminosity LHC (HL-LHC) and real-time magnet
quench detection. While conventional methods for event identification have limitations, modern AI and
machine learning techniques offer superior alternatives. . This AI system has two fold use cases, represnetation learning for LHC Trigger and multi-modal magnet quench detection algorithms.","this effort focuses on transformer models for representation learning on field programmable gate arrays (fpga), in order to seamlessly integrate ai capabilities into particle physics experiments, specifically focusing on the cms level-1 (l1) trigger at the high-luminosity lhc (hl-lhc) and real-time magnet quench detection. while conventional methods for event identification have limitations, modern ai and machine learning techniques offer superior alternatives. . this ai system has two fold use cases, represnetation learning for lhc trigger and multi-modal magnet quench detection algorithms."
Hanford Procedure Search,Department of Energy,DOE,RL - Hanford - Richland Operations Office - Hanford (EM),Mission-Enabling (internal agency support) ,Searching for information using AI.,"The purpose of the Hanford  Search is to provide greater service in our customers search for relevant proceedures, which is a main look-up for many of our employees. The benefits of the AI are that there is a single interface with multiple uses and the AI can provide better, more relevant search results.  Additionally, users can ask questions in natural language instead of needing to input specific search criteria.  
","The system outputs text respones from user prompts requesting information on grounded data related to the Hanford Procedure System
",Acquisition and/or Development,Neither,6/24/2024,6/23/2024,Unknown,Developed with contracting resources.,"89303320DEM000031
",No,No,No,Yes,Yes,"Our Data does not train the models.
","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,No – agency does not have access to source code.,Yes,Microsoft Azure OpenAI,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,"Risks identified in this AI were incorrect or halucination responses from the AI. Users are trained and informed to validate the AI responses before use. They were identified through discimination of information of risk from other ChatGPT applications along with developer and user testing.
",Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"The AI does not take into account any demographic information when generating responses, and does not use any demographic information about employees specifically to generate results. Our AI use case should have have any demographic disparity. If a users were to find an issue with a response, they can provide feedback via a feedback mechanism with each response and provide a summary of their feedback. Developers would then address the issue and look to mitigate any issues found.
",Other,No – it is not operationally practical to offer this.,Neither,0.7619047619047619,"The purpose of the Hanford  Search is to provide greater service in our customers search for relevant proceedures, which is a main look-up for many of our employees. The benefits of the AI are that there is a single interface with multiple uses and the AI can provide better, more relevant search results.  Additionally, users can ask questions in natural language instead of needing to input specific search criteria. . The system outputs text respones from user prompts requesting information on grounded data related to the Hanford Procedure System . Risks identified in this AI were incorrect or halucination responses from the AI. Users are trained and informed to validate the AI responses before use. They were identified through discimination of information of risk from other ChatGPT applications along with developer and user testing.","the purpose of the hanford search is to provide greater service in our customers search for relevant proceedures, which is a main look-up for many of our employees. the benefits of the ai are that there is a single interface with multiple uses and the ai can provide better, more relevant search results. additionally, users can ask questions in natural language instead of needing to input specific search criteria. . the system outputs text respones from user prompts requesting information on grounded data related to the hanford procedure system . risks identified in this ai were incorrect or halucination responses from the ai. users are trained and informed to validate the ai responses before use. they were identified through discimination of information of risk from other chatgpt applications along with developer and user testing."
LLM EV ,Department of Energy,DOE,NREL - National Renewable Energy Laboratory (EE),Mission-Enabling (internal agency support) ,Collaborating in real-time using AI-assisted tools in word processors.,"Making NREL EV-specific data more accessible for researchers and accelerating research in this field. 
","Outputs analysis results from these studies: https://www.sciencedirect.com/science/article/pii/S2666546824000971 
",Acquisition and/or Development,Neither,6/21/2024,7/1/2024,10/1/2024,Developed in-house.,Unknown,No,No,No,No,No,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",Yes,Yes – source code is publicly available.,Yes,"NREL Stratus AWS Compute Environment, NREL Azure System",Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Planned or in-progress.,"Misrepresentation of local legal policies of EV permitting, in open access datasets. ",Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"Initial spacial validation
",None of the above,Yes,Neither,0.7301587301587301,"Making NREL EV-specific data more accessible for researchers and accelerating research in this field. . Outputs analysis results from these studies: https://www.sciencedirect.com/science/article/pii/S2666546824000971 . Misrepresentation of local legal policies of EV permitting, in open access datasets.","making nrel ev-specific data more accessible for researchers and accelerating research in this field. . outputs analysis results from these studies: https://www.sciencedirect.com/science/article/pii/s2666546824000971 . misrepresentation of local legal policies of ev permitting, in open access datasets."
AskPRIMR,Department of Energy,DOE,NREL - National Renewable Energy Laboratory (EE),Mission-Enabling (internal agency support) ,Collaborating in real-time using AI-assisted tools in word processors.,"Making data more accessible and user-friendly for the public. 
","The systems leverages Retrieval-Augmented Generation to find semantically relevent content which the AI (LLM) summarizes for the end user as a method to describe relevent content within the data catalog. 
",Operation and Maintenance,Neither,9/30/2024,10/1/2024,10/30/2024,Developed in-house.,"NA
",No,Yes,No,No,Yes,"As this is summarizing data from the PRIMRE data repository the related datasets are described by the catalog which is also used to validate the AI responses:  https://openei.org/wiki/PRIMRE
","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,"Amazon Web Services, NREL Azure System",Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Planned or in-progress.,"As the use case for the AI has been limited to summarization of existing metadata the key risks were identified as LLM hallucinations or LLM generated text which would cause reputational harm to DOE.   Techniques such as red-teaming, adversarial testing, and prompt-injection testing were used to evaluate constraint of hallucinations and LLM.  
",Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"Process to detect bias and fairness are in testing and being regulary evaluated by model provideers. There are built-in guardrails in place to prevent abuse
",Unknown,No – it is not operationally practical to offer this.,Neither,0.7619047619047619,"Making data more accessible and user-friendly for the public. . The systems leverages Retrieval-Augmented Generation to find semantically relevent content which the AI (LLM) summarizes for the end user as a method to describe relevent content within the data catalog. . As the use case for the AI has been limited to summarization of existing metadata the key risks were identified as LLM hallucinations or LLM generated text which would cause reputational harm to DOE.   Techniques such as red-teaming, adversarial testing, and prompt-injection testing were used to evaluate constraint of hallucinations and LLM.","making data more accessible and user-friendly for the public. . the systems leverages retrieval-augmented generation to find semantically relevent content which the ai (llm) summarizes for the end user as a method to describe relevent content within the data catalog. . as the use case for the ai has been limited to summarization of existing metadata the key risks were identified as llm hallucinations or llm generated text which would cause reputational harm to doe. techniques such as red-teaming, adversarial testing, and prompt-injection testing were used to evaluate constraint of hallucinations and llm."
Coupa,Department of Energy,DOE,KCNSC - Kansas City National Security Campus (KCFO),Mission-Enabling (internal agency support) ,Creating visual representations of data sets for reports and presentations using AI.,AI functionality within Coupa drives better search functionailty and drives efficiency of sourcing and procurment processes.,Predictive analysis ,Operation and Maintenance,Neither,6/30/2020,11/30/2020,9/16/2022,Developed with contracting resources.,Procured under SCMC Agreement # WS200305,No,No,No,No,Yes,Non production related procurement data is currently being collected and managed by the SCMC. This is the data set that is used by the system.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",No,No – agency does not have access to source code.,Yes,COUPA BSM FOR FEDERAL,Less than 6 months,No,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,No,Not applicable to the risks defined above,Planned or in-progress,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,Not applicable to the scenario defined above,Other,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.7777777777777778,AI functionality within Coupa drives better search functionailty and drives efficiency of sourcing and procurment processes. . Predictive analysis . Not applicable to the risks defined above,ai functionality within coupa drives better search functionailty and drives efficiency of sourcing and procurment processes. . predictive analysis . not applicable to the risks defined above
Yurts AI search function,Department of Energy,DOE,SLAC - SLAC National Accelerator Laboratory (SC43 OIM),Mission-Enabling (internal agency support) ,Searching for information using AI.,"Locating relevant information across multiple data sources at SLAC. The expected benefit SLAC intends to receive from the use of Yurts is improved data accessibility for the userbase at SLAC
","Data summaries and recommendations
",Implementation and Assessment,Neither,6/2/2023,2/27/2024,Unknown,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,No,"The Yurts.ai model that's implemented at SLAC is fed SLAC-internal documentation for training and use
","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,Locating relevant information across multiple data sources at SLAC. The expected benefit SLAC intends to receive from the use of Yurts is improved data accessibility for the userbase at SLAC . Data summaries and recommendations,locating relevant information across multiple data sources at slac. the expected benefit slac intends to receive from the use of yurts is improved data accessibility for the userbase at slac . data summaries and recommendations
"Interactive platform to help review and create ""Promoting Inclusive and Equitable Research"" Plans",Department of Energy,DOE,PPPL - Princeton Plasma Physics Laboratory (SC43 OIM),Mission-Enabling (internal agency support) ,None of the above.,"A trained and informed model that can help to review PIER plans for accuracy, consistency and also to help integrate specific PPPL DEIA goals and initiatives, aligned with input from the user, helping to clearly define the goals of the research plan.",AI output will be revision suggestions to a user submitted PIER plan in order to align with PPPL specific goals and initiatives. It will also help to guide the user to create a more consistent plan with previously submitted/approved plans.,Initiated,Neither,5/1/2024,9/1/2024,Unknown,Developed in-house.,Unknown,No,No,No,No,No,"PPPL specific PIER plan guidelines, previously submitted and approved PIER plans, public DOE guidance and other leadership data to help refine plans that align with laboratory strategic goals.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Planned or in-progress.,No key risks identified,Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"RAG Model will be significantly grounded in factual data, limiting creativity in responses",Unknown,Yes,Neither,0.6825396825396826,"A trained and informed model that can help to review PIER plans for accuracy, consistency and also to help integrate specific PPPL DEIA goals and initiatives, aligned with input from the user, helping to clearly define the goals of the research plan. . AI output will be revision suggestions to a user submitted PIER plan in order to align with PPPL specific goals and initiatives. It will also help to guide the user to create a more consistent plan with previously submitted/approved plans. . No key risks identified","a trained and informed model that can help to review pier plans for accuracy, consistency and also to help integrate specific pppl deia goals and initiatives, aligned with input from the user, helping to clearly define the goals of the research plan. . ai output will be revision suggestions to a user submitted pier plan in order to align with pppl specific goals and initiatives. it will also help to guide the user to create a more consistent plan with previously submitted/approved plans. . no key risks identified"
Energy Wizard,Department of Energy,DOE,NREL - National Renewable Energy Laboratory (EE),Mission-Enabling (internal agency support) ,Collaborating in real-time using AI-assisted tools in word processors.,Making NREL data more accessible for researchers and accelerating research. ,The systems leverages Retrieval-Augmented Generation to find semantically relevent content which the AI (LLM) summarizes for the end user as a method to describe relevent content within the selected publications and research profiles. ,Operation and Maintenance,Neither,6/21/2024,7/1/2024,9/23/2024,Developed in-house.,Unknown,No,Yes,No,No,Yes,As this is summarizing data from the OEDI data repository the related datasets are described by the catalog which is also used to validate the AI responses:  https://data.openei.org/,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,No – agency does not have access to source code.,Yes,"NREL Stratus AWS Compute Environment, NREL Azure System, and NREL Google Cloud Platform",Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Planned or in-progress., Intellectual Property and Copyright Concerns. Bias and Fairness,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,"Process to detect bias and fairness are in testing and being regulary evaluated by model provideers. There are built-in guardrails in place to prevent abuse
",None of the above,Yes,Neither,0.746031746031746,Making NREL data more accessible for researchers and accelerating research. . The systems leverages Retrieval-Augmented Generation to find semantically relevent content which the AI (LLM) summarizes for the end user as a method to describe relevent content within the selected publications and research profiles. . Intellectual Property and Copyright Concerns. Bias and Fairness,making nrel data more accessible for researchers and accelerating research. . the systems leverages retrieval-augmented generation to find semantically relevent content which the ai (llm) summarizes for the end user as a method to describe relevent content within the selected publications and research profiles. . intellectual property and copyright concerns. bias and fairness
LANL AI Portal,Department of Energy,DOE,LANL - Los Alamos National Lab (LAFO),Mission-Enabling (internal agency support) ,None of the above.,"Democratized access to open-source/open-weights Large Language Models (LLMs) for general purpose office productivity, research of AI models, software development, operational streamlining, code development
","Interactive text chat replies from user prompts, summaries of documents user submitted for Retrieval Augmented Generation (RAG),  Replies to API queries from enterprise and scientific applications
",Implementation and Assessment,Neither,11/6/2023,3/21/2024,3/21/2024,Developed in-house.,Unknown,No,No,No,No,No,"Not trained in-house, using open-source/open-weights models. Reliant on model provider transparency 
","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,"Authorization to Operate Amazon Web Service Federal Risk and Authorization Management Program Infrastructure as a Service Cloud Service Provider, Amazon Web Service Information System Security Plan, SD-006-CP-002/L2",Less than 6 months,No,Yes,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Unknown,Unknown,Yes,Neither,0.6507936507936508,"Democratized access to open-source/open-weights Large Language Models (LLMs) for general purpose office productivity, research of AI models, software development, operational streamlining, code development . Interactive text chat replies from user prompts, summaries of documents user submitted for Retrieval Augmented Generation (RAG),  Replies to API queries from enterprise and scientific applications","democratized access to open-source/open-weights large language models (llms) for general purpose office productivity, research of ai models, software development, operational streamlining, code development . interactive text chat replies from user prompts, summaries of documents user submitted for retrieval augmented generation (rag), replies to api queries from enterprise and scientific applications"
ChatGPT Enterprise,Department of Energy,DOE,LANL - Los Alamos National Lab (LAFO),Mission-Enabling (internal agency support) ,None of the above.,"General business productivity, research of AI models
","Interactive text chat replies from user prompts, summaries of open/public/unrestricted documents user submitted for Retrieval Augmented Generation (RAG) through CustomGPTs
",Operation and Maintenance,Neither,11/6/2023,4/16/2024,4/18/2024,Developed in-house.,Unknown,No,No,No,No,No,"Not trained on any agency or LANL data
","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,Other,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Unknown,Unknown,Yes,Neither,0.6507936507936508,"General business productivity, research of AI models . Interactive text chat replies from user prompts, summaries of open/public/unrestricted documents user submitted for Retrieval Augmented Generation (RAG) through CustomGPTs","general business productivity, research of ai models . interactive text chat replies from user prompts, summaries of open/public/unrestricted documents user submitted for retrieval augmented generation (rag) through customgpts"
Argo,Department of Energy,DOE,ANL - Argonne National Laboratory (SC43 OIM),Mission-Enabling (internal agency support) ,None of the above.,"Enables anyone with the Argonne community to leverage text-based generative AI with their Argonne-specific information and data, including sensitive research or operational data up to and including CUI.",Large language model responses (prediction-based) following user prompting.,Operation and Maintenance,Neither,4/1/2023,5/1/2023,12/1/2023,Developed in-house.,Unknown,No,No,No,No,Yes,"N/A – we are using existing pre-trained large language models, no training required.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Argonne National Laboratory Information Technology Infrastructure,Less than 6 months,No,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,None of the above,Unknown,Neither,0.5396825396825397,"Enables anyone with the Argonne community to leverage text-based generative AI with their Argonne-specific information and data, including sensitive research or operational data up to and including CUI. . Large language model responses (prediction-based) following user prompting.","enables anyone with the argonne community to leverage text-based generative ai with their argonne-specific information and data, including sensitive research or operational data up to and including cui. . large language model responses (prediction-based) following user prompting."
AI Chat Bot for Facility Sustainability Practices,Department of Energy,DOE,PPPL - Princeton Plasma Physics Laboratory (SC43 OIM),Mission-Enabling (internal agency support) ,None of the above.,"Interactive RAG chat bot model trained on facility recycling, composting and trashing guidelines to inform users how to handle niche cases for sustainably getting rid of unwanted items. This should reduce confusion when throwing items out and also increase the amount of properly recycled items at PPPL.","AI output will be recommendations and instructions based on training data from facility data on recycling, trash and composting",Initiated,Neither,5/1/2024,9/1/2024,Unknown,Developed in-house.,Unknown,No,No,No,No,No,"Facility documentation on proper recycling, trash and composting guidelines. Location data for areas where specific items can be thrown away. Dynamic training on publicly maintained websites to interpret updated guidelines within PPPL and in the complex, as well as data on upcoming events.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Planned or in-progress.,No key risks identified,Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"RAG Model will be significantly grounded in factual data, limiting creativity in responses",Unknown,Yes,Neither,0.6825396825396826,"Interactive RAG chat bot model trained on facility recycling, composting and trashing guidelines to inform users how to handle niche cases for sustainably getting rid of unwanted items. This should reduce confusion when throwing items out and also increase the amount of properly recycled items at PPPL. . AI output will be recommendations and instructions based on training data from facility data on recycling, trash and composting . No key risks identified","interactive rag chat bot model trained on facility recycling, composting and trashing guidelines to inform users how to handle niche cases for sustainably getting rid of unwanted items. this should reduce confusion when throwing items out and also increase the amount of properly recycled items at pppl. . ai output will be recommendations and instructions based on training data from facility data on recycling, trash and composting . no key risks identified"
AskGDR,Department of Energy,DOE,NREL - National Renewable Energy Laboratory (EE),Mission-Enabling (internal agency support) ,Collaborating in real-time using AI-assisted tools in word processors.,"Making data more accessible and user-friendly for the public. 
","The systems leverages Retrieval-Augmented Generation to find semantically relevent content which the AI (LLM) summarizes for the end user as a method to describe relevent content within the data catalog. 
",Operation and Maintenance,Neither,9/30/2024,10/1/2024,10/30/2024,Developed in-house.,"NA
",No,Yes,No,No,Yes,"As this is summarizing data from the GDR data repository the related datasets are described by the catalog which is also used to validate the AI responses:  https://gdr.openei.org/
","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,NREL Stratus AWS Compute Environment,Less than 6 months,Yes,No,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Planned or in-progress.,"As the use case for the AI has been limited to summarization of existing metadata the key risks were identified as LLM hallucinations or LLM generated text which would cause reputational harm to DOE.   Techniques such as red-teaming, adversarial testing, and prompt-injection testing were used to evaluate constraint of hallucinations and LLM.  
",Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"Process to detect bias and fairness are in testing and being regulary evaluated by model provideers. There are built-in guardrails in place to prevent abuse
",Unknown,No – it is not operationally practical to offer this.,Neither,0.7619047619047619,"Making data more accessible and user-friendly for the public. . The systems leverages Retrieval-Augmented Generation to find semantically relevent content which the AI (LLM) summarizes for the end user as a method to describe relevent content within the data catalog. . As the use case for the AI has been limited to summarization of existing metadata the key risks were identified as LLM hallucinations or LLM generated text which would cause reputational harm to DOE.   Techniques such as red-teaming, adversarial testing, and prompt-injection testing were used to evaluate constraint of hallucinations and LLM.","making data more accessible and user-friendly for the public. . the systems leverages retrieval-augmented generation to find semantically relevent content which the ai (llm) summarizes for the end user as a method to describe relevent content within the data catalog. . as the use case for the ai has been limited to summarization of existing metadata the key risks were identified as llm hallucinations or llm generated text which would cause reputational harm to doe. techniques such as red-teaming, adversarial testing, and prompt-injection testing were used to evaluate constraint of hallucinations and llm."
Topic Modeling for Energy.gov ,Department of Energy,DOE,PA HQ -  Office of Public Affairs (PA),Mission-Enabling (internal agency support) ,None of the above.,"Minimize time spent through manual effort of reading and tagging 100,000 Energy.gov webpages.",A list of five tags that best categorize an Energy.gov webpage. ,Acquisition and/or Development,Neither,5/15/2024,7/16/2024,Unknown,Developed with contracting resources.,89303019AIM000005,No,Unknown,Unknown,Unknown,No,Web content from Energy.gov is being used for this use case. ,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Unknown,Unknown,Yes,OCIO Google Cloud Platform,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"Minimize time spent through manual effort of reading and tagging 100,000 Energy.gov webpages. . A list of five tags that best categorize an Energy.gov webpage.","minimize time spent through manual effort of reading and tagging 100,000 energy.gov webpages. . a list of five tags that best categorize an energy.gov webpage."
In-Situ Monitoring of Additively Manufactured Parts,Department of Energy,DOE,SRS - SRNL - Savannah River Site - Savannah River National Laboratory (EM),Science & Space,None of the above.,Identify print defects during the printing process.,Layerwise annotated images of defects in print bed that can be registered to post print analysis or trigger a fixing process or a full stop to save on material.,Implementation and Assessment,Neither,10/1/2022,10/1/2022,10/1/2022,Developed with contracting resources.,Government use license,No,No,No,No,No,Print data,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,ORNL Software,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,No,Annotation,Planned or in-progress,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,Unknown,Other,Yes,Neither,0.7619047619047619,Identify print defects during the printing process. . Layerwise annotated images of defects in print bed that can be registered to post print analysis or trigger a fixing process or a full stop to save on material. . Annotation,identify print defects during the printing process. . layerwise annotated images of defects in print bed that can be registered to post print analysis or trigger a fixing process or a full stop to save on material. . annotation
Boston Dynamics Spot Robotics,Department of Energy,DOE,Y-12 - Consolidated Nuclear Security Y-12 (NPO),Mission-Enabling (internal agency support) ,None of the above.,Automation of the robots movements,"Development and testing of robotic use for security and emergency responses, helping to decide on the ""best"" path for the robot to move",Acquisition and/or Development,Neither,2/10/2021,10/1/2021,Unknown,Developed with both contracting and in-house resources.,PO 4300174099 (renewed annually),No,No,No,No,No,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,Public acceptance of robotics integration,Yes – by another appropriate agency office that was not directly involved in the system’s development,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,Unknown,"Direct user testing, Post transaction customer feedback collections",Yes,Neither,0.7301587301587301,"Automation of the robots movements . Development and testing of robotic use for security and emergency responses, helping to decide on the ""best"" path for the robot to move . Public acceptance of robotics integration","automation of the robots movements . development and testing of robotic use for security and emergency responses, helping to decide on the ""best"" path for the robot to move . public acceptance of robotics integration"
Elastic Stack Technology (ELK),Department of Energy,DOE,Y-12 - Consolidated Nuclear Security Y-12 (NPO),Mission-Enabling (internal agency support) ,Searching for information using AI.,Enable intelligent media cataloging to assist with data discovery,"Intelligent collection content searching using ElasticSearch, Logstash, and Kibana",Acquisition and/or Development,Neither,10/1/2022,12/1/2022,Unknown,Developed with both contracting and in-house resources.,PO 4300182430 (renewed annually),No,No,No,No,No,Knowledge Preservation Management (KPM) Media,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,No – agency does not have access to source code.,Yes,Elastic Stack Software,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,No,Not identifying the right media for target searches,Planned or in-progress,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,Manual review by Human in the Loop resolves discrepancies,"Direct user testing, Post transaction customer feedback collections",Yes,Neither,0.7142857142857143,"Enable intelligent media cataloging to assist with data discovery . Intelligent collection content searching using ElasticSearch, Logstash, and Kibana . Not identifying the right media for target searches","enable intelligent media cataloging to assist with data discovery . intelligent collection content searching using elasticsearch, logstash, and kibana . not identifying the right media for target searches"
Raytheon Multimedia Monitoring System (M3S),Department of Energy,DOE,Y-12 - Consolidated Nuclear Security Y-12 (NPO),Mission-Enabling (internal agency support) ,Transcribing and summarizing a recorded meeting or interview using AI.,Enable off-cloud transcription of pre-recorded video media to improve data discoverability,Off-cloud voice to text transcription technology for pre-recorded videos,Acquisition and/or Development,Neither,10/1/2017,12/1/2017,Unknown,Developed with both contracting and in-house resources.,PO 4300183978 (renewed annually),No,No,No,No,No,Knowledge Preservation Management (KPM) Media,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,No – agency does not have access to source code.,Yes,The Multimedia Monitoring System,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,No,"Incomplete or incorrect transcription of videos, negatively impacting the discoverability of this media",Planned or in-progress,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,Manual review by Human in the Loop resolves discrepancies,"Direct user testing, Post transaction customer feedback collections",Yes,Neither,0.7142857142857143,"Enable off-cloud transcription of pre-recorded video media to improve data discoverability . Off-cloud voice to text transcription technology for pre-recorded videos . Incomplete or incorrect transcription of videos, negatively impacting the discoverability of this media","enable off-cloud transcription of pre-recorded video media to improve data discoverability . off-cloud voice to text transcription technology for pre-recorded videos . incomplete or incorrect transcription of videos, negatively impacting the discoverability of this media"
Cognitive Prescreen Tool (CPT),Department of Energy,DOE,Y-12 - Consolidated Nuclear Security Y-12 (NPO),Mission-Enabling (internal agency support) ,None of the above.,"Serve as a recommender to Derivative Classifier to assist with document review to improve process accuracy and efficiency, in that order.",Sensitive information detection bound to DOE classification guidance to help reduce IOSC and prevent information loss,Acquisition and/or Development,Neither,10/1/2016,10/1/2019,Unknown,Developed in-house.,Unknown,No,No,No,Unknown,No,Classification Guides from the CNS Classification Office,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,No – agency does not have access to source code.,Yes,Automated Classification System,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,No,IOSC potential as information detection models are tuned,Yes – by another appropriate agency office that was not directly involved in the system’s development,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Manual review by Human in the Loop resolves discrepancies,"Direct user testing, Post transaction customer feedback collections",Yes,Neither,0.6984126984126984,"Serve as a recommender to Derivative Classifier to assist with document review to improve process accuracy and efficiency, in that order. . Sensitive information detection bound to DOE classification guidance to help reduce IOSC and prevent information loss . IOSC potential as information detection models are tuned","serve as a recommender to derivative classifier to assist with document review to improve process accuracy and efficiency, in that order. . sensitive information detection bound to doe classification guidance to help reduce iosc and prevent information loss . iosc potential as information detection models are tuned"
Machine Learning for Linac Improved Performance,Department of Energy,DOE,FNAL - Fermi National Accelerator (SC43 OIM),Science & Space,None of the above.,"Daily fluctuations in the Ion Source conditions as well af the effect of environmental changes to RF systems and cavities affect Linac beam. Results include increased beam loss resulting in increased beamline component irradiation, decreased beam intensity to downstream machines affecting Accelerator Complex deliverables, drifts in Linac beam energy directly affecting Booster losses. These drifts are not easily predictable since we do not have environmental control on the RF gallery, not enough instrumentaion in the Ion Source or Linac proper. To counter these effects, we are developing AI-based optimization and modeling, including Bayesian Optimization and surrogate model-based optimization, with the ultimate goal of (near) real-time RF compensation.",Outputs are proposed changes to RF system parameters (cavity phase settings and/or field gradients) to counter the effect of daily drift and to stabilize the output energy.,Acquisition and/or Development,Neither,10/1/2020,10/1/2020,10/1/2024,Developed in-house.,Unknown,No,No,No,No,No,Accelerator operations machine data as well as accelerator simulation,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,None of the above,Unknown,Neither,0.5238095238095238,"Daily fluctuations in the Ion Source conditions as well af the effect of environmental changes to RF systems and cavities affect Linac beam. Results include increased beam loss resulting in increased beamline component irradiation, decreased beam intensity to downstream machines affecting Accelerator Complex deliverables, drifts in Linac beam energy directly affecting Booster losses. These drifts are not easily predictable since we do not have environmental control on the RF gallery, not enough instrumentaion in the Ion Source or Linac proper. To counter these effects, we are developing AI-based optimization and modeling, including Bayesian Optimization and surrogate model-based optimization, with the ultimate goal of (near) real-time RF compensation. . Outputs are proposed changes to RF system parameters (cavity phase settings and/or field gradients) to counter the effect of daily drift and to stabilize the output energy.","daily fluctuations in the ion source conditions as well af the effect of environmental changes to rf systems and cavities affect linac beam. results include increased beam loss resulting in increased beamline component irradiation, decreased beam intensity to downstream machines affecting accelerator complex deliverables, drifts in linac beam energy directly affecting booster losses. these drifts are not easily predictable since we do not have environmental control on the rf gallery, not enough instrumentaion in the ion source or linac proper. to counter these effects, we are developing ai-based optimization and modeling, including bayesian optimization and surrogate model-based optimization, with the ultimate goal of (near) real-time rf compensation. . outputs are proposed changes to rf system parameters (cavity phase settings and/or field gradients) to counter the effect of daily drift and to stabilize the output energy."
Next-Generation Beam Cooling and Control with Optical Stochastic Cooling,Department of Energy,DOE,FNAL - Fermi National Accelerator (SC43 OIM),Science & Space,None of the above.,This effort focuses on enhanced real-time control of the structure of circulating particle beams. The additional performance and capabilities provided may enable substantially greater operational flexibility and science reach at current and future DOE accelerator facilities.,The AI system will continuously infer the state of a circulating beam distribution and then use this inference in the execution of an RL-based control policy. The primary means of control is an advanced optical stochastic cooling system.,Acquisition and/or Development,Neither,10/1/2021,10/1/2021,10/1/2024,Developed in-house.,Unknown,No,No,No,No,No,Large-scale simulation data is being used to train the diagnostic and control systems. Online training with experimental data may also be leveraged once the system is operational.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,None of the above,Unknown,Neither,0.5238095238095238,This effort focuses on enhanced real-time control of the structure of circulating particle beams. The additional performance and capabilities provided may enable substantially greater operational flexibility and science reach at current and future DOE accelerator facilities. . The AI system will continuously infer the state of a circulating beam distribution and then use this inference in the execution of an RL-based control policy. The primary means of control is an advanced optical stochastic cooling system.,this effort focuses on enhanced real-time control of the structure of circulating particle beams. the additional performance and capabilities provided may enable substantially greater operational flexibility and science reach at current and future doe accelerator facilities. . the ai system will continuously infer the state of a circulating beam distribution and then use this inference in the execution of an rl-based control policy. the primary means of control is an advanced optical stochastic cooling system.
high level synthesis for machine learning (previously hls4ml),Department of Energy,DOE,FNAL - Fermi National Accelerator (SC43 OIM),Science & Space,None of the above.,hls4ml is used to implement specialized AI algorithms in embedded hardware.  This is valuable across a wide range of scientific applications for enabling real-time processing capabilitles.  This can accelerate sientific discovery and time to science thus enabling large cost savings and DOE scientific prestige.,It can be an AI algorithm from prediction to data compression to control (decision making).,Acquisition and/or Development,Neither,10/1/2021,6/1/2021,10/1/2024,Developed in-house.,Unknown,No,No,No,No,No,research datasets from scientific experiments,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",Yes,Yes – source code is publicly available.,No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,None of the above,Unknown,Neither,0.5396825396825397,hls4ml is used to implement specialized AI algorithms in embedded hardware.  This is valuable across a wide range of scientific applications for enabling real-time processing capabilitles.  This can accelerate sientific discovery and time to science thus enabling large cost savings and DOE scientific prestige. . It can be an AI algorithm from prediction to data compression to control (decision making).,hls4ml is used to implement specialized ai algorithms in embedded hardware. this is valuable across a wide range of scientific applications for enabling real-time processing capabilitles. this can accelerate sientific discovery and time to science thus enabling large cost savings and doe scientific prestige. . it can be an ai algorithm from prediction to data compression to control (decision making).
Streamining intelligent detectors for sPHENIX/EIC,Department of Energy,DOE,FNAL - Fermi National Accelerator (SC43 OIM),Science & Space,None of the above.,AI tools are developed for embedded inference in real-time processing systems for scientific experiments  such as sPHENIX and upcoming EIC. This can accelerate sientific discovery and time to science thus enabling large cost savings and DOE scientific prestige.,It can be an AI algorithm from prediction to data compression to control (decision making).,Acquisition and/or Development,Neither,10/1/2021,10/1/2021,10/1/2024,Developed in-house.,Unknown,No,No,No,No,No,research datasets from scientific experiments,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,None of the above,Unknown,Neither,0.5238095238095238,AI tools are developed for embedded inference in real-time processing systems for scientific experiments  such as sPHENIX and upcoming EIC. This can accelerate sientific discovery and time to science thus enabling large cost savings and DOE scientific prestige. . It can be an AI algorithm from prediction to data compression to control (decision making).,ai tools are developed for embedded inference in real-time processing systems for scientific experiments such as sphenix and upcoming eic. this can accelerate sientific discovery and time to science thus enabling large cost savings and doe scientific prestige. . it can be an ai algorithm from prediction to data compression to control (decision making).
In-pixel AI for future tracking detectors,Department of Energy,DOE,FNAL - Fermi National Accelerator (SC43 OIM),Science & Space,None of the above.,AI algorithms are implemented in on-detector electronics in order to reduce data size and enable processing at high rates.,"A recommendation of whether to save data based on AI classifier. Or, a fast inference of track parameters to be used for fast selection",Acquisition and/or Development,Neither,10/1/2022,10/1/2022,10/1/2025,Developed in-house.,Unknown,No,No,No,No,No,Accelerator operations machine data,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,Yes – source code is publicly available.,No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,None of the above,Unknown,Neither,0.5396825396825397,"AI algorithms are implemented in on-detector electronics in order to reduce data size and enable processing at high rates. . A recommendation of whether to save data based on AI classifier. Or, a fast inference of track parameters to be used for fast selection","ai algorithms are implemented in on-detector electronics in order to reduce data size and enable processing at high rates. . a recommendation of whether to save data based on ai classifier. or, a fast inference of track parameters to be used for fast selection"
SONIC: AI acceleration as a service,Department of Energy,DOE,FNAL - Fermi National Accelerator (SC43 OIM),Science & Space,None of the above.,SONIC is used to accelerate AI workloads on coprocessors in scientific experiments.  This can accelerate sientific discovery and time to science thus enabling large cost savings and DOE scientific prestige.,It can be an AI algorithm from prediction to data compression to control (decision making).,Acquisition and/or Development,Neither,10/1/2020,10/1/2020,10/1/2024,Developed in-house.,Unknown,No,No,No,No,No,research datasets from scientific experiments,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,Yes – source code is publicly available.,No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,None of the above,Unknown,Neither,0.5396825396825397,SONIC is used to accelerate AI workloads on coprocessors in scientific experiments.  This can accelerate sientific discovery and time to science thus enabling large cost savings and DOE scientific prestige. . It can be an AI algorithm from prediction to data compression to control (decision making).,sonic is used to accelerate ai workloads on coprocessors in scientific experiments. this can accelerate sientific discovery and time to science thus enabling large cost savings and doe scientific prestige. . it can be an ai algorithm from prediction to data compression to control (decision making).
READS: Real-time Edge AI for Distributed Systems,Department of Energy,DOE,FNAL - Fermi National Accelerator (SC43 OIM),Science & Space,None of the above.,"READS has two sub-projects. The first project created the means to stream live Main Injector and Recycler accelerator beam loss monitor data. This data is then fed to an AI model deployed on an FPGA so that it can infer, in realtime, the origin of beam loss, either Main Injector or Recycler, for each beam loss monitor in the tunnel enclosure. The second project aimed to improve upon traditional resonant beam extraction regulation techiniques using AI for use in the Fermilab Delivery Ring and Mu2e.",The ML outputs of the system are inferences as to the origin of beam loss in the Main Injector acclerator enclosure and also suggested regulation ramps to best improve the Spill Duty Factor in the Delivery Ring for Mu2e,Acquisition and/or Development,Neither,10/1/2019,6/1/2019,10/1/2024,Developed in-house.,Unknown,No,No,No,No,No,research datasets from scientific experiments,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,Yes – source code is publicly available.,No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,None of the above,Unknown,Neither,0.5396825396825397,"READS has two sub-projects. The first project created the means to stream live Main Injector and Recycler accelerator beam loss monitor data. This data is then fed to an AI model deployed on an FPGA so that it can infer, in realtime, the origin of beam loss, either Main Injector or Recycler, for each beam loss monitor in the tunnel enclosure. The second project aimed to improve upon traditional resonant beam extraction regulation techiniques using AI for use in the Fermilab Delivery Ring and Mu2e. . The ML outputs of the system are inferences as to the origin of beam loss in the Main Injector acclerator enclosure and also suggested regulation ramps to best improve the Spill Duty Factor in the Delivery Ring for Mu2e","reads has two sub-projects. the first project created the means to stream live main injector and recycler accelerator beam loss monitor data. this data is then fed to an ai model deployed on an fpga so that it can infer, in realtime, the origin of beam loss, either main injector or recycler, for each beam loss monitor in the tunnel enclosure. the second project aimed to improve upon traditional resonant beam extraction regulation techiniques using ai for use in the fermilab delivery ring and mu2e. . the ml outputs of the system are inferences as to the origin of beam loss in the main injector acclerator enclosure and also suggested regulation ramps to best improve the spill duty factor in the delivery ring for mu2e"
Extreme data reduction for the edge,Department of Energy,DOE,FNAL - Fermi National Accelerator (SC43 OIM),Science & Space,None of the above.,AI tools are developed for embedded inference in real-time processing systems for scientific experiments. This can accelerate sientific discovery and time to science thus enabling large cost savings and DOE scientific prestige.,It can be an AI algorithm from prediction to data compression to control (decision making).,Acquisition and/or Development,Neither,10/1/2021,6/1/2021,10/1/2024,Developed in-house.,Unknown,No,No,No,No,No,research datasets from scientific experiments,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,None of the above,Unknown,Neither,0.5238095238095238,AI tools are developed for embedded inference in real-time processing systems for scientific experiments. This can accelerate sientific discovery and time to science thus enabling large cost savings and DOE scientific prestige. . It can be an AI algorithm from prediction to data compression to control (decision making).,ai tools are developed for embedded inference in real-time processing systems for scientific experiments. this can accelerate sientific discovery and time to science thus enabling large cost savings and doe scientific prestige. . it can be an ai algorithm from prediction to data compression to control (decision making).
Machine Learning for Accelerator Operations Using Big Data Analytics / L-CAPE,Department of Energy,DOE,FNAL - Fermi National Accelerator (SC43 OIM),Science & Space,None of the above.,"ML models are deployed for the FNAL's Linac.to detect, label andact upon faults.  The usage of ML will jimporve our fault labelingand detection.  This will allow for improved operatioal efficeincy, fault statistics, and preventitive maintenance. To my knowledge this is the first global accelerator operations ML system.",The ML outputs to a dashboard withfault labels and downtime predictiojns.The model will also try and predict dwwontime and possible actions.,Acquisition and/or Development,Neither,10/1/2020,1/1/2020,10/1/2024,Developed in-house.,Unknown,No,No,No,No,No,my own simulated data; research datasets from scientific experiments,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case's development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,No,None / under evaluation,Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,Yes,Neither,0.6825396825396826,"ML models are deployed for the FNAL's Linac.to detect, label andact upon faults.  The usage of ML will jimporve our fault labelingand detection.  This will allow for improved operatioal efficeincy, fault statistics, and preventitive maintenance. To my knowledge this is the first global accelerator operations ML system. . The ML outputs to a dashboard withfault labels and downtime predictiojns.The model will also try and predict dwwontime and possible actions. . None / under evaluation","ml models are deployed for the fnal's linac.to detect, label andact upon faults. the usage of ml will jimporve our fault labelingand detection. this will allow for improved operatioal efficeincy, fault statistics, and preventitive maintenance. to my knowledge this is the first global accelerator operations ml system. . the ml outputs to a dashboard withfault labels and downtime predictiojns.the model will also try and predict dwwontime and possible actions. . none / under evaluation"
DOIChatGPT API Management Instance,Department of the Interior,DOI,OCIO,Mission-Enabling (internal agency support),None of the above.,"An Azure OpenAI instance secured behind an Azure API Management instance hosted within our internal network. This will create a backbone of secure access to Azure OpenAI permitted by APIM subscription keys that also supports cost tracking per project key.

This is just a service layer to enable other project activities.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter ,Implementation and Assessment,Neither,Unknown,Unknown,7/9/2024,Developed in-house.,Unknown,No,No,No,Unknown,Yes,Unknown,Unknown,Yes,"Yes – agency has access to source code, but it is not public.",Unknown,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"An Azure OpenAI instance secured behind an Azure API Management instance hosted within our internal network. This will create a backbone of secure access to Azure OpenAI permitted by APIM subscription keys that also supports cost tracking per project key.

This is just a service layer to enable other project activities. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter",an azure openai instance secured behind an azure api management instance hosted within our internal network. this will create a backbone of secure access to azure openai permitted by apim subscription keys that also supports cost tracking per project key. this is just a service layer to enable other project activities. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
DOIChatGPT API Management Development Environment,Department of the Interior,DOI,OCIO,Mission-Enabling (internal agency support),None of the above., This will be a public facing version of the DOIChatGPT APIM and Azure OpenAI that is used for training and proof of concept validation (with sanitized data) prior to migrating to production DOIChatGPT APIM.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter ,Implementation and Assessment,Neither,Unknown,Unknown,8/21/2024,Developed in-house.,Unknown,No,No,No,Unknown,Yes,Unknown,Unknown,Yes,"Yes – agency has access to source code, but it is not public.",Unknown,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,This will be a public facing version of the DOIChatGPT APIM and Azure OpenAI that is used for training and proof of concept validation (with sanitized data) prior to migrating to production DOIChatGPT APIM. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,this will be a public facing version of the doichatgpt apim and azure openai that is used for training and proof of concept validation (with sanitized data) prior to migrating to production doichatgpt apim. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Adobe Firefly Generative Images,Department of the Interior,DOI,NPS,Mission-Enabling (internal agency support),None of the above.,  Adobe's Firefly generative image AI model was used within Photoshop to create artistic sketches of different nature scenes. These sketches were used in the StoryMap created by San Juan Island National Historical Park as artistic and narrative elements. The images were generated in Photoshop and then manually edited to ensure accuracy. Using this AI model created results that were quicker and cheaper compared to traditional artistic methods. The StoryMap with the sketches can be viewed here: https://arcg.is/1uCy0e,Imagery,Operation and Maintenance,Neither,Unknown,Unknown,Unknown,Data not reported by submitter and will be updated once additional information is collected,Data not reported by submitter and will be updated once additional information is collected,No,Yes,Unknown,Unknown,Yes,Photographs taken by NPS staff were used to compare the accuracy of images output by the AI model,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",Unknown,Unknown,Less than 6 months,Yes,Yes,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,Adobe's Firefly generative image AI model was used within Photoshop to create artistic sketches of different nature scenes. These sketches were used in the StoryMap created by San Juan Island National Historical Park as artistic and narrative elements. The images were generated in Photoshop and then manually edited to ensure accuracy. Using this AI model created results that were quicker and cheaper compared to traditional artistic methods. The StoryMap with the sketches can be viewed here: https://arcg.is/1uCy0e . Imagery,adobe's firefly generative image ai model was used within photoshop to create artistic sketches of different nature scenes. these sketches were used in the storymap created by san juan island national historical park as artistic and narrative elements. the images were generated in photoshop and then manually edited to ensure accuracy. using this ai model created results that were quicker and cheaper compared to traditional artistic methods. the storymap with the sketches can be viewed here: https://arcg.is/1ucy0e . imagery
Use of AI to Enhance Flash Flood Forecast Tool,Department of the Interior,DOI,NPS,Mission-Enabling (internal agency support),None of the above.,"  University of Illinois created a model to predict rainfall on a watershed scale in Great Smoky Mountains National Park.  They are now working with IBM to create a system that will use this model to provide forecasts of flooding events with a goal of a 24+ hour lead time.  The developers are using AI to improve on the Quantitative Precipitation Forecast of the National Weather Service to thus improve on the accuracy of their tool.  The NPS is cooperating as an end-user of the flood forecast tool, but not as a developer or direct user of the AI.",,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Data not reported by submitter and will be updated once additional information is collected,Data not reported by submitter and will be updated once additional information is collected,No,Unknown,Unknown,Unknown,Yes,Data not reported by submitter and will be updated once additional information is collected,Data not reported by submitter and will be updated once additional information is collected,Yes,"Yes – agency has access to source code, but it is not public.",Unknown,Unknown,Less than 6 months,Yes,Yes,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.36507936507936506,"University of Illinois created a model to predict rainfall on a watershed scale in Great Smoky Mountains National Park.  They are now working with IBM to create a system that will use this model to provide forecasts of flooding events with a goal of a 24+ hour lead time.  The developers are using AI to improve on the Quantitative Precipitation Forecast of the National Weather Service to thus improve on the accuracy of their tool.  The NPS is cooperating as an end-user of the flood forecast tool, but not as a developer or direct user of the AI.","university of illinois created a model to predict rainfall on a watershed scale in great smoky mountains national park. they are now working with ibm to create a system that will use this model to provide forecasts of flooding events with a goal of a 24+ hour lead time. the developers are using ai to improve on the quantitative precipitation forecast of the national weather service to thus improve on the accuracy of their tool. the nps is cooperating as an end-user of the flood forecast tool, but not as a developer or direct user of the ai."
Data Extraction Using MS Power Automate AI Functionality,Department of the Interior,DOI,ONRR,Mission-Enabling,None of the above.,Train pre-built Machine Learning model to extract data from different document formats and convert into a consistent structured format to improve business operations.,Improved business documentation. ,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,No,Yes,No,Yes,Document copies received from Bureau of Indian Affairs (BIA),Unknown,No,No – agency does not have access to source code.,Unknown,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,Train pre-built Machine Learning model to extract data from different document formats and convert into a consistent structured format to improve business operations. . Improved business documentation.,train pre-built machine learning model to extract data from different document formats and convert into a consistent structured format to improve business operations. . improved business documentation.
CESU project to detect of bird nests using deep learning to support annual colonial bird monitoring,Department of the Interior,DOI,NPS,Mission-Enabling (internal agency support),None of the above.,"This work is already being accomplished via a cooperative agreement with a university (Florida International University)

The National Park Service (NPS), has been monitoring colonial nesting birds monthly via low level aerial photography from a helicopter platform in Biscayne National Park since 2010. These photographs are processed to determine the number of active nests for specific focal species: Double-crested Cormorants, Great Egrets, Great White Herons, Great Blue Herons, White Ibises, and Roseate Spoonbills. NPS now has 13 years of data which is composed of 64,350 uniquely identified nests and 57,793 birds representing.

This monitoring is time consuming and needs to be accomplished consistently and accurately. Currently, the aerial photos are processed by individuals. Which takes substantial time and effort. We propose to explore the creation of automatic script that would allow the active nest to be identified using software. The goal of this project is to explore software programs and create automatic script that can identify potential bird nests on the photograph and to highlight this area on the photograph. By developing a robust object detection model, researchers can monitor bird colonies and their populations more efficiently, e.g. by consistently identifying nests with software which will allow the tracking of nesting patterns with less person effort needed to accomplish this task. It will also help to control observer detection variation over time.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Data not reported by submitter and will be updated once additional information is collected,Data not reported by submitter and will be updated once additional information is collected,No,Unknown,Unknown,Unknown,Yes,Data not reported by submitter and will be updated once additional information is collected,Data not reported by submitter and will be updated once additional information is collected,Yes,"Yes – agency has access to source code, but it is not public.",Unknown,Unknown,Less than 6 months,Yes,Yes,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.38095238095238093,"This work is already being accomplished via a cooperative agreement with a university (Florida International University)

The National Park Service (NPS), has been monitoring colonial nesting birds monthly via low level aerial photography from a helicopter platform in Biscayne National Park since 2010. These photographs are processed to determine the number of active nests for specific focal species: Double-crested Cormorants, Great Egrets, Great White Herons, Great Blue Herons, White Ibises, and Roseate Spoonbills. NPS now has 13 years of data which is composed of 64,350 uniquely identified nests and 57,793 birds representing.

This monitoring is time consuming and needs to be accomplished consistently and accurately. Currently, the aerial photos are processed by individuals. Which takes substantial time and effort. We propose to explore the creation of automatic script that would allow the active nest to be identified using software. The goal of this project is to explore software programs and create automatic script that can identify potential bird nests on the photograph and to highlight this area on the photograph. By developing a robust object detection model, researchers can monitor bird colonies and their populations more efficiently, e.g. by consistently identifying nests with software which will allow the tracking of nesting patterns with less person effort needed to accomplish this task. It will also help to control observer detection variation over time. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","this work is already being accomplished via a cooperative agreement with a university (florida international university) the national park service (nps), has been monitoring colonial nesting birds monthly via low level aerial photography from a helicopter platform in biscayne national park since 2010. these photographs are processed to determine the number of active nests for specific focal species: double-crested cormorants, great egrets, great white herons, great blue herons, white ibises, and roseate spoonbills. nps now has 13 years of data which is composed of 64,350 uniquely identified nests and 57,793 birds representing. this monitoring is time consuming and needs to be accomplished consistently and accurately. currently, the aerial photos are processed by individuals. which takes substantial time and effort. we propose to explore the creation of automatic script that would allow the active nest to be identified using software. the goal of this project is to explore software programs and create automatic script that can identify potential bird nests on the photograph and to highlight this area on the photograph. by developing a robust object detection model, researchers can monitor bird colonies and their populations more efficiently, e.g. by consistently identifying nests with software which will allow the tracking of nesting patterns with less person effort needed to accomplish this task. it will also help to control observer detection variation over time. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Use of AI for developing bioacoustic and remote camera imagery wildlife species classifiers for noni,Department of the Interior,DOI,NPS,Mission-Enabling (internal agency support),None of the above.,"University of Illinois created a model to predict rainfall on a watershed scale in Great Smoky Mountains National Park. They are now working with IBM to create a system that will use this model to provide forecasts of flooding events with a goal of a 24+ hour lead time. The developers are using AI to improve on the Quantitative Precipitation Forecast of the National Weather Service to thus improve on the accuracy of their tool. The NPS is cooperating as an end-user of the flood forecast tool, but not as a developer or direct user of the AI.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Data not reported by submitter and will be updated once additional information is collected,Data not reported by submitter and will be updated once additional information is collected,No,Unknown,Unknown,Unknown,Yes,Data not reported by submitter and will be updated once additional information is collected,Data not reported by submitter and will be updated once additional information is collected,Yes,"Yes – agency has access to source code, but it is not public.",Unknown,Unknown,Less than 6 months,Yes,Yes,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.38095238095238093,"University of Illinois created a model to predict rainfall on a watershed scale in Great Smoky Mountains National Park. They are now working with IBM to create a system that will use this model to provide forecasts of flooding events with a goal of a 24+ hour lead time. The developers are using AI to improve on the Quantitative Precipitation Forecast of the National Weather Service to thus improve on the accuracy of their tool. The NPS is cooperating as an end-user of the flood forecast tool, but not as a developer or direct user of the AI. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","university of illinois created a model to predict rainfall on a watershed scale in great smoky mountains national park. they are now working with ibm to create a system that will use this model to provide forecasts of flooding events with a goal of a 24+ hour lead time. the developers are using ai to improve on the quantitative precipitation forecast of the national weather service to thus improve on the accuracy of their tool. the nps is cooperating as an end-user of the flood forecast tool, but not as a developer or direct user of the ai. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Machine Learning Model Optimization,Department of the Interior,DOI,ONRR,Mission-Enabling,None of the above.," Alteryx machine learning model optimization and fitting helps users select which model is optimal to fit to an underlying dataset. It also can help refine the model, recommending variable removal if autocorrelation issues are detected in the dataset.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter ,Implementation and Assessment,Neither,Unknown,Unknown,4/13/2023,Developed in-house.,Unknown,No,No,No,Unknown,Yes,"Internal data, subscription sourced data, and publicly available data.",Unknown,No,No – agency does not have access to source code.,Unknown,Unknown,Less than 6 months,Yes,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"Alteryx machine learning model optimization and fitting helps users select which model is optimal to fit to an underlying dataset. It also can help refine the model, recommending variable removal if autocorrelation issues are detected in the dataset. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","alteryx machine learning model optimization and fitting helps users select which model is optimal to fit to an underlying dataset. it also can help refine the model, recommending variable removal if autocorrelation issues are detected in the dataset. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Internal Controls Testing - SAM.gov certifications,Department of the Interior,DOI,OS - Office of the Secretary of the Interior,Mission-Enabling,None of the above.,"  Federal financial assistance regulation and DOI PGM policy requires awarding officers check SAM.gov, a Federal public website, to ensure that the entity funds are to be award to is eligible to receive Federal funds and that the POCs on the awards are not debarred. The use case involves using a large language AI model to assess and score SAM.gov pdf documents against the date the award was made.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter ,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,No,Unknown,Unknown,Yes,PDF documents,Unknown,Yes,"Yes – agency has access to source code, but it is not public.",Unknown,Unknown,Unknown,Yes,Yes,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3492063492063492,"Federal financial assistance regulation and DOI PGM policy requires awarding officers check SAM.gov, a Federal public website, to ensure that the entity funds are to be award to is eligible to receive Federal funds and that the POCs on the awards are not debarred. The use case involves using a large language AI model to assess and score SAM.gov pdf documents against the date the award was made. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","federal financial assistance regulation and doi pgm policy requires awarding officers check sam.gov, a federal public website, to ensure that the entity funds are to be award to is eligible to receive federal funds and that the pocs on the awards are not debarred. the use case involves using a large language ai model to assess and score sam.gov pdf documents against the date the award was made. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Potential Applications of AI Models to UAS Post-fire Mapping Data Analysis and Image Processing,Department of the Interior,DOI,OS - Office of the Secretary of the Interior,Mission-Enabling,None of the above.," Evaluating AI Models for UAS Post-Fire Mapping Analysis Focusing on the Following Issues...
Post-Fire Damage Assessment:  Efficiently allocate resources for future rehabilitation efforts.
Vegetation Recovery Prediction:  Assess burn severity to estimate ecosystem regeneration speed.
Soil Erosion Risk Mapping:  Identify vulnerable areas and guide erosion control measures.
Habitat Restoration Planning:  Prioritize areas for reforestation, invasive species removal, and soil stabilization.
Water Quality Monitoring:  Predict sediment runoff, nutrient levels, and contaminants for future water management.
Structural Evolution Analysis and Prediction:  Analyze fire spread and growth to aid risk assessment and response planning.
Environmental Impact Assessment: Aid in planning rehabilitation efforts and assessing ecosystem resilience.
Risk Communication and Public Awareness: Inform communities about rehabilitation efforts and safety precautions.
Ongoing Post-Fire Rehabilitation Treatment Effects:  Assist BAER teams in assessing burned landscapes and monitoring ecological effects.
Cross-Disciplinary Integration:  Bridge communication gaps between experts in various fields for wildfire risk assessment.
Literature Reanalysis:  Assist researchers in developing insights, identifying gaps, and informing future research.
Assessing Damage and Recovery:  Identify fire-damaged objects (e.g., infrastructure, trees) to prioritize rehabilitation.
Monitoring Ecosystem Recovery:  Identify plant species, soil erosion patterns, and ecological indicators for restoration planning.
Identifying Hazards and Safety Risks:  Identify post-fire hazards (e.g., unstable terrain, debris) to ensure safety during rehabilitation.
Identifying Cultural and Archeological Objects: Identify relevant objects (e.g., artifacts, disturbances) for avoidance during rehabilitation.
Customized Rehabilitation Plans:  Detect specific objects (e.g., utility poles, contaminated soil) for tailored rehabilitation plans.",see description ,Initiated,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,No,Yes,   UAS-derived ,Unknown,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3333333333333333,"Evaluating AI Models for UAS Post-Fire Mapping Analysis Focusing on the Following Issues...
Post-Fire Damage Assessment:  Efficiently allocate resources for future rehabilitation efforts.
Vegetation Recovery Prediction:  Assess burn severity to estimate ecosystem regeneration speed.
Soil Erosion Risk Mapping:  Identify vulnerable areas and guide erosion control measures.
Habitat Restoration Planning:  Prioritize areas for reforestation, invasive species removal, and soil stabilization.
Water Quality Monitoring:  Predict sediment runoff, nutrient levels, and contaminants for future water management.
Structural Evolution Analysis and Prediction:  Analyze fire spread and growth to aid risk assessment and response planning.
Environmental Impact Assessment: Aid in planning rehabilitation efforts and assessing ecosystem resilience.
Risk Communication and Public Awareness: Inform communities about rehabilitation efforts and safety precautions.
Ongoing Post-Fire Rehabilitation Treatment Effects:  Assist BAER teams in assessing burned landscapes and monitoring ecological effects.
Cross-Disciplinary Integration:  Bridge communication gaps between experts in various fields for wildfire risk assessment.
Literature Reanalysis:  Assist researchers in developing insights, identifying gaps, and informing future research.
Assessing Damage and Recovery:  Identify fire-damaged objects (e.g., infrastructure, trees) to prioritize rehabilitation.
Monitoring Ecosystem Recovery:  Identify plant species, soil erosion patterns, and ecological indicators for restoration planning.
Identifying Hazards and Safety Risks:  Identify post-fire hazards (e.g., unstable terrain, debris) to ensure safety during rehabilitation.
Identifying Cultural and Archeological Objects: Identify relevant objects (e.g., artifacts, disturbances) for avoidance during rehabilitation.
Customized Rehabilitation Plans:  Detect specific objects (e.g., utility poles, contaminated soil) for tailored rehabilitation plans. . see description","evaluating ai models for uas post-fire mapping analysis focusing on the following issues... post-fire damage assessment: efficiently allocate resources for future rehabilitation efforts. vegetation recovery prediction: assess burn severity to estimate ecosystem regeneration speed. soil erosion risk mapping: identify vulnerable areas and guide erosion control measures. habitat restoration planning: prioritize areas for reforestation, invasive species removal, and soil stabilization. water quality monitoring: predict sediment runoff, nutrient levels, and contaminants for future water management. structural evolution analysis and prediction: analyze fire spread and growth to aid risk assessment and response planning. environmental impact assessment: aid in planning rehabilitation efforts and assessing ecosystem resilience. risk communication and public awareness: inform communities about rehabilitation efforts and safety precautions. ongoing post-fire rehabilitation treatment effects: assist baer teams in assessing burned landscapes and monitoring ecological effects. cross-disciplinary integration: bridge communication gaps between experts in various fields for wildfire risk assessment. literature reanalysis: assist researchers in developing insights, identifying gaps, and informing future research. assessing damage and recovery: identify fire-damaged objects (e.g., infrastructure, trees) to prioritize rehabilitation. monitoring ecosystem recovery: identify plant species, soil erosion patterns, and ecological indicators for restoration planning. identifying hazards and safety risks: identify post-fire hazards (e.g., unstable terrain, debris) to ensure safety during rehabilitation. identifying cultural and archeological objects: identify relevant objects (e.g., artifacts, disturbances) for avoidance during rehabilitation. customized rehabilitation plans: detect specific objects (e.g., utility poles, contaminated soil) for tailored rehabilitation plans. . see description"
Pilot: Using Machine Learning to Harvest Data for Standardized Species Conservation,Department of the Interior,DOI,FWS,Science & Space,None of the above.,"The U.S. Fish and Wildlife Service is in the process of reimagining and creating a new database to hold, depict, and disseminate information about the species we manage. This effort will greatly expand and standardize the data related to species and their conservation. We have worked with contractors to develop a preliminary tool that explores the possibiliy of using machine learning and large language modelling to harvest species information related to their biological and physical needs into standardized pick-lists. This includes a way to then vet the harvested data. As part of this we are evaluating the infrastructure needed to containerize tool like this in our environment.","we aim to integrate  summmarized data, using some sort of tailored AI and framework or API, into our species workflows in ECOSPhere as it relates to species information.  

fill in worflows for reporting and analytics",Initiated,Neither,2/14/2024,Unknown,Unknown,Developed in-house.,Unknown,No,No,No,No,Yes,Literature provided by field offices related to species information. Evalutaed with sample data taken from published literature in USFWS library Zotero.,Unknown,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,6-12 months,Yes,Yes,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"The U.S. Fish and Wildlife Service is in the process of reimagining and creating a new database to hold, depict, and disseminate information about the species we manage. This effort will greatly expand and standardize the data related to species and their conservation. We have worked with contractors to develop a preliminary tool that explores the possibiliy of using machine learning and large language modelling to harvest species information related to their biological and physical needs into standardized pick-lists. This includes a way to then vet the harvested data. As part of this we are evaluating the infrastructure needed to containerize tool like this in our environment. . we aim to integrate  summmarized data, using some sort of tailored AI and framework or API, into our species workflows in ECOSPhere as it relates to species information.  

fill in worflows for reporting and analytics","the u.s. fish and wildlife service is in the process of reimagining and creating a new database to hold, depict, and disseminate information about the species we manage. this effort will greatly expand and standardize the data related to species and their conservation. we have worked with contractors to develop a preliminary tool that explores the possibiliy of using machine learning and large language modelling to harvest species information related to their biological and physical needs into standardized pick-lists. this includes a way to then vet the harvested data. as part of this we are evaluating the infrastructure needed to containerize tool like this in our environment. . we aim to integrate summmarized data, using some sort of tailored ai and framework or api, into our species workflows in ecosphere as it relates to species information. fill in worflows for reporting and analytics"
Summarization of documents and output to ECOSphere species workflow,Department of the Interior,DOI,FWS,Science & Space,None of the above.,"The U.S. Fish and Wildlife Service has a substantial number of documents that we would like to take advantage of NLP or ML process in order to summarize their content.  Afterwards, we aim to integrate that summmarized data, using some sort of tailored AI and framework or API, into our species workflows in ECOSPhere as it relates to species information.  "," we aim to integrate  summmarized data, using some sort of tailored AI and framework or API, into our species workflows in ECOSPhere as it relates to species information.  

fill in worflows for reporting and analytics",Initiated,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,No,Yes,Unknown,Unknown,Yes,Unknown,Unknown,Unknown,Unknown,Unknown,Yes,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.31746031746031744,"The U.S. Fish and Wildlife Service has a substantial number of documents that we would like to take advantage of NLP or ML process in order to summarize their content.  Afterwards, we aim to integrate that summmarized data, using some sort of tailored AI and framework or API, into our species workflows in ECOSPhere as it relates to species information. . we aim to integrate  summmarized data, using some sort of tailored AI and framework or API, into our species workflows in ECOSPhere as it relates to species information.  

fill in worflows for reporting and analytics","the u.s. fish and wildlife service has a substantial number of documents that we would like to take advantage of nlp or ml process in order to summarize their content. afterwards, we aim to integrate that summmarized data, using some sort of tailored ai and framework or api, into our species workflows in ecosphere as it relates to species information. . we aim to integrate summmarized data, using some sort of tailored ai and framework or api, into our species workflows in ecosphere as it relates to species information. fill in worflows for reporting and analytics"
Prediction of Suitable Habitat for ESA-listed Species,Department of the Interior,DOI,FWS,Science & Space,None of the above.,"The U.S. Fish and Wildlife Service currently uses species distribution models (i.e., machine learning algorithms) to predict potential habitat for ESA-listed threatened and endangered species.  These habitat predictions are vetted by USFWS field biologists and, in some cases, are used as ranges in IPaC to inform users of possible impacts on ESA-listed species.  Species distribution model outputs have also been used to identify suitable habitat on the landscape for potential reintroduction efforts or suitable areas for additional surveys, with the potential to locate new, previously undocumented populations.",fill in worflows for reporting and analytics,Implementation and Assessment,Neither,4/1/2019,4/1/2019,4/1/2019,Developed in-house.,Unknown,No,Yes,No,No,Yes,Species occurrence data.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes, ECOSphere,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The U.S. Fish and Wildlife Service currently uses species distribution models (i.e., machine learning algorithms) to predict potential habitat for ESA-listed threatened and endangered species.  These habitat predictions are vetted by USFWS field biologists and, in some cases, are used as ranges in IPaC to inform users of possible impacts on ESA-listed species.  Species distribution model outputs have also been used to identify suitable habitat on the landscape for potential reintroduction efforts or suitable areas for additional surveys, with the potential to locate new, previously undocumented populations. . fill in worflows for reporting and analytics","the u.s. fish and wildlife service currently uses species distribution models (i.e., machine learning algorithms) to predict potential habitat for esa-listed threatened and endangered species. these habitat predictions are vetted by usfws field biologists and, in some cases, are used as ranges in ipac to inform users of possible impacts on esa-listed species. species distribution model outputs have also been used to identify suitable habitat on the landscape for potential reintroduction efforts or suitable areas for additional surveys, with the potential to locate new, previously undocumented populations. . fill in worflows for reporting and analytics"
Computer Vision Model to Rapidly Identify Habitat Change,Department of the Interior,DOI,FWS,Science & Space,None of the above.,"The U.S. Fish and Wildlife Service is planning to partner with the Chesapeake Conservancy, an NGO, to develop a computer vision model to monitor for early detection of habitat loss across the landscape, a significant threat to biodiversity, including threatened and endangered species.  By using a computer vision model one can rapidly identify and flag areas where habitat loss may be occurring due to natural or human-caused disturbances.  Early detection can facilitate rapid responses, when appropriate, or allow practitioners to accurately calculate habitat loss over time.  More accurate estimates of habitat loss allow for better management decisions and potentially shorter recovery times for threatened and endangered species."," By using a computer vision model one can rapidly identify and flag areas where habitat loss may be occurring due to natural or human-caused disturbances.  Early detection can facilitate rapid responses, when appropriate, or allow practitioners to accurately calculate habitat loss over time.  More accurate estimates of habitat loss allow for better management decisions and potentially shorter recovery times for threatened and endangered species.",Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,No,No,No,Yes,Species occurrence data.,Unknown,Unknown,Unknown,Unknown,Unknown,6-12 months,Yes,Yes,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.36507936507936506,"The U.S. Fish and Wildlife Service is planning to partner with the Chesapeake Conservancy, an NGO, to develop a computer vision model to monitor for early detection of habitat loss across the landscape, a significant threat to biodiversity, including threatened and endangered species.  By using a computer vision model one can rapidly identify and flag areas where habitat loss may be occurring due to natural or human-caused disturbances.  Early detection can facilitate rapid responses, when appropriate, or allow practitioners to accurately calculate habitat loss over time.  More accurate estimates of habitat loss allow for better management decisions and potentially shorter recovery times for threatened and endangered species. . By using a computer vision model one can rapidly identify and flag areas where habitat loss may be occurring due to natural or human-caused disturbances.  Early detection can facilitate rapid responses, when appropriate, or allow practitioners to accurately calculate habitat loss over time.  More accurate estimates of habitat loss allow for better management decisions and potentially shorter recovery times for threatened and endangered species.","the u.s. fish and wildlife service is planning to partner with the chesapeake conservancy, an ngo, to develop a computer vision model to monitor for early detection of habitat loss across the landscape, a significant threat to biodiversity, including threatened and endangered species. by using a computer vision model one can rapidly identify and flag areas where habitat loss may be occurring due to natural or human-caused disturbances. early detection can facilitate rapid responses, when appropriate, or allow practitioners to accurately calculate habitat loss over time. more accurate estimates of habitat loss allow for better management decisions and potentially shorter recovery times for threatened and endangered species. . by using a computer vision model one can rapidly identify and flag areas where habitat loss may be occurring due to natural or human-caused disturbances. early detection can facilitate rapid responses, when appropriate, or allow practitioners to accurately calculate habitat loss over time. more accurate estimates of habitat loss allow for better management decisions and potentially shorter recovery times for threatened and endangered species."
Applying Deep Learning to Detect and Classify Ocean Wildlife,Department of the Interior,DOI,FWS,Mission-Enabling,None of the above.,"Deep learning methods are being advanced to automate data processing and improve the cost-efficiency of remote sensing technologies for surveys covering broad geographic areas and generating very large image datasets. The FWS is partnering with the Bureau of Ocean Energy Management (BOEM), U.S. Geological Survey (USGS), academic institutions, and private contractors to accomplish these objectives. Initial focus has been on marine bird and other wildlife surveys given overlapping agency requirements for these data. A workflow is now being established to advance the imagery data from the sensors, to an AI detection model in the aircraft, to a species classification algorithm that is able to distinguish and count each species, and then a georeferenced location is obtained for each bird. More specifically, a cutting-edge artificial intelligence/deep learning algorithm has been developed to automatically detect seabirds in imagery. This detection algorithm has 80 to 90% accuracy across a wide range of seabird species. In FY25, we will continue to iterate on this model using a human-in-the-loop technique to improve model generalization.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,10/1/2016,10/1/2016,On-going development,Developed with both contracting and in-house resources.,Data not reported by submitter and will be updated once additional information is collected,No,No,No,Unknown,Yes,Remote sensing digitial imagery and labels for bird species,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,Yes – source code is publicly available., , ,Unknown,No,Other,Unknown,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6507936507936508,"Deep learning methods are being advanced to automate data processing and improve the cost-efficiency of remote sensing technologies for surveys covering broad geographic areas and generating very large image datasets. The FWS is partnering with the Bureau of Ocean Energy Management (BOEM), U.S. Geological Survey (USGS), academic institutions, and private contractors to accomplish these objectives. Initial focus has been on marine bird and other wildlife surveys given overlapping agency requirements for these data. A workflow is now being established to advance the imagery data from the sensors, to an AI detection model in the aircraft, to a species classification algorithm that is able to distinguish and count each species, and then a georeferenced location is obtained for each bird. More specifically, a cutting-edge artificial intelligence/deep learning algorithm has been developed to automatically detect seabirds in imagery. This detection algorithm has 80 to 90% accuracy across a wide range of seabird species. In FY25, we will continue to iterate on this model using a human-in-the-loop technique to improve model generalization. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","deep learning methods are being advanced to automate data processing and improve the cost-efficiency of remote sensing technologies for surveys covering broad geographic areas and generating very large image datasets. the fws is partnering with the bureau of ocean energy management (boem), u.s. geological survey (usgs), academic institutions, and private contractors to accomplish these objectives. initial focus has been on marine bird and other wildlife surveys given overlapping agency requirements for these data. a workflow is now being established to advance the imagery data from the sensors, to an ai detection model in the aircraft, to a species classification algorithm that is able to distinguish and count each species, and then a georeferenced location is obtained for each bird. more specifically, a cutting-edge artificial intelligence/deep learning algorithm has been developed to automatically detect seabirds in imagery. this detection algorithm has 80 to 90% accuracy across a wide range of seabird species. in fy25, we will continue to iterate on this model using a human-in-the-loop technique to improve model generalization. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Enhancing Migratory Bird Surveys with Thermal Imagery and Deep Learning,Department of the Interior,DOI,FWS,Mission-Enabling,None of the above.,"The U.S. Fish and Wildlife Service (USFWS) Division of Migratory Bird Management is aiming to improve upon migratory bird surveys by utilizing aerial remote sensing combined with deep learning (a form of artificial intelligence) analyses to automate survey counts. The goal is to provide accurate wildlife counts while simultaneously reducing risk to pilots by allowing aerial surveys to occur at higher altitudes. In partnership with the College of William and Mary, USFWS has previously demonstrated that thermal remote sensing technology, coupled with deep learning, can provide accurate counts of sandhill cranes (Antigone canadensis) at night during their critically important migratory stopover in the Platte River Valley of Nebraska","Migratory bird spatial locations, bird species identification, and bird counts",Acquisition and/or Development,Neither,8/15/2020,8/15/2020,7/1/2024,Developed in-house., ,No,No,No,Unknown,Yes,Remote sensing digitial imagery and labels for bird species,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,Yes – source code is publicly available., , ,Unknown,No,Other,Unknown,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6507936507936508,"The U.S. Fish and Wildlife Service (USFWS) Division of Migratory Bird Management is aiming to improve upon migratory bird surveys by utilizing aerial remote sensing combined with deep learning (a form of artificial intelligence) analyses to automate survey counts. The goal is to provide accurate wildlife counts while simultaneously reducing risk to pilots by allowing aerial surveys to occur at higher altitudes. In partnership with the College of William and Mary, USFWS has previously demonstrated that thermal remote sensing technology, coupled with deep learning, can provide accurate counts of sandhill cranes (Antigone canadensis) at night during their critically important migratory stopover in the Platte River Valley of Nebraska . Migratory bird spatial locations, bird species identification, and bird counts","the u.s. fish and wildlife service (usfws) division of migratory bird management is aiming to improve upon migratory bird surveys by utilizing aerial remote sensing combined with deep learning (a form of artificial intelligence) analyses to automate survey counts. the goal is to provide accurate wildlife counts while simultaneously reducing risk to pilots by allowing aerial surveys to occur at higher altitudes. in partnership with the college of william and mary, usfws has previously demonstrated that thermal remote sensing technology, coupled with deep learning, can provide accurate counts of sandhill cranes (antigone canadensis) at night during their critically important migratory stopover in the platte river valley of nebraska . migratory bird spatial locations, bird species identification, and bird counts"
SOCS documents analysis,Department of the Interior,DOI,BOEM,Mission-Enabling,None of the above.," The BOEM Status of the Outer Continental Shelf (SOCS) environmental information portal contains over 900 BOEM documents including Environmental Impact Statements (EISs), Environmental Assessments (EAs), Categorical Exclusion Reviews (CERs), Biological Assessments (BAs), Biological Opinions (BiOPs), National Historic Preservation Act (NHPA) Section 106 documents, policy documents, environmental guidance, reusable content, and environmental studies. Some potential use cases for integrating LLM/AI into SOCS:
 - Generate resource affected sections for different resources based on existing recent EISs and pull in new information to keep the analysis current. For example, if a wind energy lease is being contemplated in the vicinity of an already analyzed wind energy lease, the information would be pulled from the existing environmental analyses as a starting point, incorporating any new information in the system. The drafted information would include citations and a references cited section in a particular format. 
- Have LLM/ summarize key findings from multiple studies on similar resources. For example, multiple studies addressing noise or sound impacts to different species could be summarized with references to the source publications.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,9/9/2024,Unknown,Unknown,Data not reported by submitter and will be updated once additional information is collected,Data not reported by submitter and will be updated once additional information is collected,No,Unknown,Unknown,Unknown,Yes,Data not reported by submitter and will be updated once additional information is collected,Data not reported by submitter and will be updated once additional information is collected,Yes,"Yes – agency has access to source code, but it is not public.",Unknown,Unknown,Less than 6 months,Yes,Yes,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"The BOEM Status of the Outer Continental Shelf (SOCS) environmental information portal contains over 900 BOEM documents including Environmental Impact Statements (EISs), Environmental Assessments (EAs), Categorical Exclusion Reviews (CERs), Biological Assessments (BAs), Biological Opinions (BiOPs), National Historic Preservation Act (NHPA) Section 106 documents, policy documents, environmental guidance, reusable content, and environmental studies. Some potential use cases for integrating LLM/AI into SOCS:
 - Generate resource affected sections for different resources based on existing recent EISs and pull in new information to keep the analysis current. For example, if a wind energy lease is being contemplated in the vicinity of an already analyzed wind energy lease, the information would be pulled from the existing environmental analyses as a starting point, incorporating any new information in the system. The drafted information would include citations and a references cited section in a particular format. 
- Have LLM/ summarize key findings from multiple studies on similar resources. For example, multiple studies addressing noise or sound impacts to different species could be summarized with references to the source publications. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","the boem status of the outer continental shelf (socs) environmental information portal contains over 900 boem documents including environmental impact statements (eiss), environmental assessments (eas), categorical exclusion reviews (cers), biological assessments (bas), biological opinions (biops), national historic preservation act (nhpa) section 106 documents, policy documents, environmental guidance, reusable content, and environmental studies. some potential use cases for integrating llm/ai into socs: - generate resource affected sections for different resources based on existing recent eiss and pull in new information to keep the analysis current. for example, if a wind energy lease is being contemplated in the vicinity of an already analyzed wind energy lease, the information would be pulled from the existing environmental analyses as a starting point, incorporating any new information in the system. the drafted information would include citations and a references cited section in a particular format. - have llm/ summarize key findings from multiple studies on similar resources. for example, multiple studies addressing noise or sound impacts to different species could be summarized with references to the source publications. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
"Predictive AI Applications for Wildlife Monitoring: SeeOtter, a custom built software solution for a",Department of the Interior,DOI,FWS,Science & Space,None of the above.,"Marine Mammals Management is tasked with Stock Assessments under the Marine Mammal Protection Act, Section 117 and sea otter population monitoring has shifted to image-based surveys, relying on on a YoloV5, AI-assisted process to procure a data set of sea otter observations from photos. Department of Interior agencies, USFWS, USGS, and NPS have all adopted an image-based methodology for sea otter population monitoring, but the model requires continued maintenance and refinement with new surveys, sensor upgrades, and model updates. Unfortunately, there are no AI expert permanent staff tased with this project across Alaska DOI programs and the need has been fulfilled through contractors. SeeOtter was a custom developed software by Collin Power and Evan Wetherington, under contract to USFWS Sea Otter Program, to turn hundreds of thousands of images from across the Gulf of Alaska into datasets usable for statistical analyses by USGS and Stock Assessment Reporting by USFWS and NPS. Glacier Bay NPS assisted in developing an SOP for the AI-assisted processing for sea otter data and Collin Power, under contract with USGS built the system out through a Center for Data Integration Project to make it more accessible to interested users. We would like SeeOtter to be a tool availabel to Partners, across DOI as well as Tribes and researchers, who are interested in applying a predictive AI model for counting objects from imagery.

USGS also have plans for large scale photo surveys coming up in 2025/26 and we will be working collaboratively to figure out the support for image processing following the SeeOtter SOP that has been developed in a partnership among USFWS, USGS, and NPS.","The Yolo V5 algorithm provides georeferenced bounding boxes around each object detected in an image, in our case, sea otters with an associated confidence score.",Operation and Maintenance,Neither,Unknown,Unknown,1/3/2022,Developed with contracting resources.,Data not reported by submitter and will be updated once additional information is collected,No,No,No,Unknown,Yes,Images captured during aerial sea otter surveys across Alaska.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Unknown,Unknown,Unknown,Yes,Yes,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"Marine Mammals Management is tasked with Stock Assessments under the Marine Mammal Protection Act, Section 117 and sea otter population monitoring has shifted to image-based surveys, relying on on a YoloV5, AI-assisted process to procure a data set of sea otter observations from photos. Department of Interior agencies, USFWS, USGS, and NPS have all adopted an image-based methodology for sea otter population monitoring, but the model requires continued maintenance and refinement with new surveys, sensor upgrades, and model updates. Unfortunately, there are no AI expert permanent staff tased with this project across Alaska DOI programs and the need has been fulfilled through contractors. SeeOtter was a custom developed software by Collin Power and Evan Wetherington, under contract to USFWS Sea Otter Program, to turn hundreds of thousands of images from across the Gulf of Alaska into datasets usable for statistical analyses by USGS and Stock Assessment Reporting by USFWS and NPS. Glacier Bay NPS assisted in developing an SOP for the AI-assisted processing for sea otter data and Collin Power, under contract with USGS built the system out through a Center for Data Integration Project to make it more accessible to interested users. We would like SeeOtter to be a tool availabel to Partners, across DOI as well as Tribes and researchers, who are interested in applying a predictive AI model for counting objects from imagery.

USGS also have plans for large scale photo surveys coming up in 2025/26 and we will be working collaboratively to figure out the support for image processing following the SeeOtter SOP that has been developed in a partnership among USFWS, USGS, and NPS. . The Yolo V5 algorithm provides georeferenced bounding boxes around each object detected in an image, in our case, sea otters with an associated confidence score.","marine mammals management is tasked with stock assessments under the marine mammal protection act, section 117 and sea otter population monitoring has shifted to image-based surveys, relying on on a yolov5, ai-assisted process to procure a data set of sea otter observations from photos. department of interior agencies, usfws, usgs, and nps have all adopted an image-based methodology for sea otter population monitoring, but the model requires continued maintenance and refinement with new surveys, sensor upgrades, and model updates. unfortunately, there are no ai expert permanent staff tased with this project across alaska doi programs and the need has been fulfilled through contractors. seeotter was a custom developed software by collin power and evan wetherington, under contract to usfws sea otter program, to turn hundreds of thousands of images from across the gulf of alaska into datasets usable for statistical analyses by usgs and stock assessment reporting by usfws and nps. glacier bay nps assisted in developing an sop for the ai-assisted processing for sea otter data and collin power, under contract with usgs built the system out through a center for data integration project to make it more accessible to interested users. we would like seeotter to be a tool availabel to partners, across doi as well as tribes and researchers, who are interested in applying a predictive ai model for counting objects from imagery. usgs also have plans for large scale photo surveys coming up in 2025/26 and we will be working collaboratively to figure out the support for image processing following the seeotter sop that has been developed in a partnership among usfws, usgs, and nps. . the yolo v5 algorithm provides georeferenced bounding boxes around each object detected in an image, in our case, sea otters with an associated confidence score."
Seasonal/Temporary Wetland/Floodplain Delineation using Remote Sensing and Deep Learning,Department of the Interior,DOI,BOR,Mission-Enabling,None of the above.,"Reclamation was interested in determining if recent advancements in machine learning, specifically convolutional neural network architecture in deep learning, can provide improved seasonal/temporary wetland/floodplain delineation (mapping) when high temporal and spatial resolution remote sensing data is available? If so, then these new mappings could inform the management of protected species and provide critical information to decision-makers during scenario analysis for operations and planning.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,10/1/2018,Unknown,Unknown,Developed in-house.,Unknown,No,Unknown,No,No,Yes,"Non-sensitive, mission data describing resource conditions relevant to the condition assessment/prediction question being addressed via AI-ML.",Unknown,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"Reclamation was interested in determining if recent advancements in machine learning, specifically convolutional neural network architecture in deep learning, can provide improved seasonal/temporary wetland/floodplain delineation (mapping) when high temporal and spatial resolution remote sensing data is available? If so, then these new mappings could inform the management of protected species and provide critical information to decision-makers during scenario analysis for operations and planning. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","reclamation was interested in determining if recent advancements in machine learning, specifically convolutional neural network architecture in deep learning, can provide improved seasonal/temporary wetland/floodplain delineation (mapping) when high temporal and spatial resolution remote sensing data is available? if so, then these new mappings could inform the management of protected species and provide critical information to decision-makers during scenario analysis for operations and planning. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Data Driven Sub-Seasonal Forecasting of Temperature and Precipitation ,Department of the Interior,DOI,BOR,Mission-Enabling,None of the above.,"Reclamation has run 2, year-long prize competitions where particants developed and deployed data driven methods for sub-seasonal (2-6 weeks into future) prediction of temperature and precipitation across the western U.S. Particpants outperformed benchmark forecasts from NOAA. Reclamation is currently working with Scripps Institute of Oceanography to further refine, evaluate, and pilot implement the most promising methods from these two copmetitions. Improving sub-seasonal forecasts has significant potential to enhance water management outcomes.  ",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,12/19/2016,Unknown,Unknown,Developed in-house.,Unknown,No,Unknown,No,No,Yes,"Non-sensitive, mission data describing resource conditions relevant to the condition assessment/prediction question being addressed via AI-ML.",Unknown,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"Reclamation has run 2, year-long prize competitions where particants developed and deployed data driven methods for sub-seasonal (2-6 weeks into future) prediction of temperature and precipitation across the western U.S. Particpants outperformed benchmark forecasts from NOAA. Reclamation is currently working with Scripps Institute of Oceanography to further refine, evaluate, and pilot implement the most promising methods from these two copmetitions. Improving sub-seasonal forecasts has significant potential to enhance water management outcomes. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","reclamation has run 2, year-long prize competitions where particants developed and deployed data driven methods for sub-seasonal (2-6 weeks into future) prediction of temperature and precipitation across the western u.s. particpants outperformed benchmark forecasts from noaa. reclamation is currently working with scripps institute of oceanography to further refine, evaluate, and pilot implement the most promising methods from these two copmetitions. improving sub-seasonal forecasts has significant potential to enhance water management outcomes. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Data Driven Streamflow Forecasting,Department of the Interior,DOI,BOR,Mission-Enabling,None of the above.,"Reclamation, along with partners from the CEATI hydropower industry group (e.g. TVA, DOE-PNNL, and others) ran a year-long  evaluation of existing 10-day streamflow foreasting technologies and a companion prize competition open to the public, also focused on 10-day streamflow forecasts. Forecasts were issued every day for a year and verified against observed flows. Across locations and metrics, the top perfoming foreacst product was a private, AI/ML forecasting company - UpstreamTech. Several competitors from the prize competition also performed strongly; outperforming benchmark forecasts from NOAA. Reclamation is working to further evaluate the UpstreamTech forecast products and also the top performers from the prize competition.  ",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,8/1/2020,8/1/2023,12/1/2024,Developed in-house.,Unknown,No,Unknown,No,No,Yes,"Non-sensitive, mission data describing resource conditions relevant to the condition assessment/prediction question being addressed via AI-ML.",Unknown,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Reclamation, along with partners from the CEATI hydropower industry group (e.g. TVA, DOE-PNNL, and others) ran a year-long  evaluation of existing 10-day streamflow foreasting technologies and a companion prize competition open to the public, also focused on 10-day streamflow forecasts. Forecasts were issued every day for a year and verified against observed flows. Across locations and metrics, the top perfoming foreacst product was a private, AI/ML forecasting company - UpstreamTech. Several competitors from the prize competition also performed strongly; outperforming benchmark forecasts from NOAA. Reclamation is working to further evaluate the UpstreamTech forecast products and also the top performers from the prize competition. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","reclamation, along with partners from the ceati hydropower industry group (e.g. tva, doe-pnnl, and others) ran a year-long evaluation of existing 10-day streamflow foreasting technologies and a companion prize competition open to the public, also focused on 10-day streamflow forecasts. forecasts were issued every day for a year and verified against observed flows. across locations and metrics, the top perfoming foreacst product was a private, ai/ml forecasting company - upstreamtech. several competitors from the prize competition also performed strongly; outperforming benchmark forecasts from noaa. reclamation is working to further evaluate the upstreamtech forecast products and also the top performers from the prize competition. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Snowcast Showdown ,Department of the Interior,DOI,BOR,Mission-Enabling,None of the above.,"Reclamation partnered with Bonneville Power Administration, NASA - Goddard Space Flight Center, U.S. Army Corps of Engineers, USDA - Natural Resources Conservation Service, U.S. Geological Survey, National Center for Atmospheric Research, DrivenData, HeroX, Ensemble, and NASA Tournament Lab to run the Snowcast Showdown Prize Competition. In this competition, particiapnts were asked to develop mehtods to estimate distributed snow information by blending observations from different sources  using machine learning methods that provide flexible and efficient algorithms for data-driven models and real-time prediction/estimation. Winning methods are now being evaluated and folded into a follow-on project with NOAA's River Forecast Centers. ",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,4/14/2023,12/7/2021,Unknown,Developed with contracting resources.,Data not reported by submitter and will be updated once additional information is collected,No,Unknown,No,No,Yes,"Non-sensitive, mission data describing resource conditions relevant to the condition assessment/prediction question being addressed via AI-ML.",Data not reported by submitter and will be updated once additional information is collected,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Reclamation partnered with Bonneville Power Administration, NASA - Goddard Space Flight Center, U.S. Army Corps of Engineers, USDA - Natural Resources Conservation Service, U.S. Geological Survey, National Center for Atmospheric Research, DrivenData, HeroX, Ensemble, and NASA Tournament Lab to run the Snowcast Showdown Prize Competition. In this competition, particiapnts were asked to develop mehtods to estimate distributed snow information by blending observations from different sources  using machine learning methods that provide flexible and efficient algorithms for data-driven models and real-time prediction/estimation. Winning methods are now being evaluated and folded into a follow-on project with NOAA's River Forecast Centers. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","reclamation partnered with bonneville power administration, nasa - goddard space flight center, u.s. army corps of engineers, usda - natural resources conservation service, u.s. geological survey, national center for atmospheric research, drivendata, herox, ensemble, and nasa tournament lab to run the snowcast showdown prize competition. in this competition, particiapnts were asked to develop mehtods to estimate distributed snow information by blending observations from different sources using machine learning methods that provide flexible and efficient algorithms for data-driven models and real-time prediction/estimation. winning methods are now being evaluated and folded into a follow-on project with noaa's river forecast centers. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
PyForecast ,Department of the Interior,DOI,BOR,Mission-Enabling,None of the above.,Pyforecast is a statistical/ML water supply forecasting software developed by Reclamation that uses a range of data-driven methods.  ,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,1/1/2020,Unknown,Unknown,Developed in-house.,Unknown,No,Unknown,No,No,Yes,"Non-sensitive, mission data describing resource conditions relevant to the condition assessment/prediction question being addressed via AI-ML.",Unknown,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,Pyforecast is a statistical/ML water supply forecasting software developed by Reclamation that uses a range of data-driven methods. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,pyforecast is a statistical/ml water supply forecasting software developed by reclamation that uses a range of data-driven methods. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Improved Processing and Analysis of Test and Operating Data from Rotating Machines,Department of the Interior,DOI,BOR,Mission-Enabling,None of the above.,"This project is exploring a better method to analyze DC ramp test data from rotating machines. Previous DC ramp test analysis requires engineering expertise to recognize characteristic curves from DC ramp test plots. DC ramp tests produce a plot of voltage vs current for a ramping voltage applied to a rotating machine. By using machine learning/AI tools, such as linear regression, the ramp test plots can be analyzed by computer software, rather than manual engineering analysis, to recognize characteristic curves. The anticipated result will be faster and more reliable analysis of field-performed DC ramp testing.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Unknown,No,No,Yes,"Non-sensitive, mission data describing resource conditions relevant to the condition assessment/prediction question being addressed via AI-ML.",Unknown,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"This project is exploring a better method to analyze DC ramp test data from rotating machines. Previous DC ramp test analysis requires engineering expertise to recognize characteristic curves from DC ramp test plots. DC ramp tests produce a plot of voltage vs current for a ramping voltage applied to a rotating machine. By using machine learning/AI tools, such as linear regression, the ramp test plots can be analyzed by computer software, rather than manual engineering analysis, to recognize characteristic curves. The anticipated result will be faster and more reliable analysis of field-performed DC ramp testing. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","this project is exploring a better method to analyze dc ramp test data from rotating machines. previous dc ramp test analysis requires engineering expertise to recognize characteristic curves from dc ramp test plots. dc ramp tests produce a plot of voltage vs current for a ramping voltage applied to a rotating machine. by using machine learning/ai tools, such as linear regression, the ramp test plots can be analyzed by computer software, rather than manual engineering analysis, to recognize characteristic curves. the anticipated result will be faster and more reliable analysis of field-performed dc ramp testing. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Improving UAS-derived photogrammetric data and analysis accuracy and confidence for high-resolution data sets using artificial intelligence and machine learning,Department of the Interior,DOI,BOR,Mission-Enabling,None of the above.,"UAS derived photogrammetric products contain a large amount of potential information that can be less accurate than required for analysis and time consuming to analyze manually. By formulating a standard reference protocol and applying machine learning/artificial intelligence, this information will be unlocked to provide detailed analysis of Reclamation's assets for better informed decision making.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Unknown,No,No,Yes,"Non-sensitive, mission data describing resource conditions relevant to the condition assessment/prediction question being addressed via AI-ML.",Unknown,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"UAS derived photogrammetric products contain a large amount of potential information that can be less accurate than required for analysis and time consuming to analyze manually. By formulating a standard reference protocol and applying machine learning/artificial intelligence, this information will be unlocked to provide detailed analysis of Reclamation's assets for better informed decision making. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","uas derived photogrammetric products contain a large amount of potential information that can be less accurate than required for analysis and time consuming to analyze manually. by formulating a standard reference protocol and applying machine learning/artificial intelligence, this information will be unlocked to provide detailed analysis of reclamation's assets for better informed decision making. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Photogrammetric Data Set Crack Mapping Technology Search ,Department of the Interior,DOI,BOR,Mission-Enabling,None of the above.,"This project is exploring a specific application of photogrammetric products to process analysis of crack mapping on Reclamation facilites.  This analysis is time consuming and has typically required rope access or other means to photograph and locate areas that can now be reached with drones or other devices.  By formulating a standard reference protocol and applying machine learning/AI, this information will be used to provide detailed analysis of Reclamation assets for better decision making. ",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Unknown,No,No,Yes,Involves use of high resolution imagery of large infrastructure (CUI). ,Unknown,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"This project is exploring a specific application of photogrammetric products to process analysis of crack mapping on Reclamation facilites.  This analysis is time consuming and has typically required rope access or other means to photograph and locate areas that can now be reached with drones or other devices.  By formulating a standard reference protocol and applying machine learning/AI, this information will be used to provide detailed analysis of Reclamation assets for better decision making. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","this project is exploring a specific application of photogrammetric products to process analysis of crack mapping on reclamation facilites. this analysis is time consuming and has typically required rope access or other means to photograph and locate areas that can now be reached with drones or other devices. by formulating a standard reference protocol and applying machine learning/ai, this information will be used to provide detailed analysis of reclamation assets for better decision making. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Effects of vehicle traffic on space use and road crossings of caribou in the Arctic,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Assessing the effects of industrial development on wildlife is a key objective of managers and conservation practitioners. However, wildlife responses are often only investigated with respect to the footprint of infrastructure, even though human activity can strongly mediate development impacts. In Arctic Alaska, there is substantial interest in expanding energy development, raising concerns about the potential effects on barren-ground caribou (Rangifer tarandus granti). While caribou generally avoid industrial infrastructure, little is known about the role of human activity in moderating their responses, and whether managing activity levels could minimize development effects. To address this uncertainty, we examined the influence of traffic volume on caribou summer space use and road crossings in the Central Arctic Herd within the Kuparuk and Milne Point oil fields on the North Slope of Alaska.","Prediction of liquefaction potential at input site, as it compares to existing case history dataset. ",Implementation and Assessment,Neither,Unknown,Unknown,10/3/2023,Developed in-house.,Unknown,No,Yes,No,No,Yes,"Non-sensitive, mission data describing resource conditions relevant to the condition assessment/prediction question being addressed via AI-ML.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,None of the above,Unknown,Unknown,Neither,0.5714285714285714,"Assessing the effects of industrial development on wildlife is a key objective of managers and conservation practitioners. However, wildlife responses are often only investigated with respect to the footprint of infrastructure, even though human activity can strongly mediate development impacts. In Arctic Alaska, there is substantial interest in expanding energy development, raising concerns about the potential effects on barren-ground caribou (Rangifer tarandus granti). While caribou generally avoid industrial infrastructure, little is known about the role of human activity in moderating their responses, and whether managing activity levels could minimize development effects. To address this uncertainty, we examined the influence of traffic volume on caribou summer space use and road crossings in the Central Arctic Herd within the Kuparuk and Milne Point oil fields on the North Slope of Alaska. . Prediction of liquefaction potential at input site, as it compares to existing case history dataset.","assessing the effects of industrial development on wildlife is a key objective of managers and conservation practitioners. however, wildlife responses are often only investigated with respect to the footprint of infrastructure, even though human activity can strongly mediate development impacts. in arctic alaska, there is substantial interest in expanding energy development, raising concerns about the potential effects on barren-ground caribou (rangifer tarandus granti). while caribou generally avoid industrial infrastructure, little is known about the role of human activity in moderating their responses, and whether managing activity levels could minimize development effects. to address this uncertainty, we examined the influence of traffic volume on caribou summer space use and road crossings in the central arctic herd within the kuparuk and milne point oil fields on the north slope of alaska. . prediction of liquefaction potential at input site, as it compares to existing case history dataset."
Automated Walrus Haulout Monitoring,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"The purpose of this software is to provide a framework for using pre-trained image classification convolutional neural network CNN models to (1) make predictions on unlabled image datasets to provide data for further analysis of walrus (Odobenus rosmarus) coastal haulout occupation and (2) make predictions on image datasets where keywords have been manually (i.e. by human reviewers) added to the image ITPC metadata indicating the content of the images, including the presence and absence of walruses so that the model's performance can be evaluated by comparing its predictions to the human-assigned class labels using confusion matrices and standard classifier evaluation metrics such as accuracy, precision, recall, F1-score (a composite of precision and recall).","screening-level prediction of deformation of embankments, dams, or other facilities due to seismic loading. ",Acquisition and/or Development,Neither,Unknown,Unknown,2/29/2024,Developed in-house.,Unknown,No,Yes,No,No,Yes,"Non-sensitive, mission data describing resource conditions relevant to the condition assessment/prediction question being addressed via AI-ML.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The purpose of this software is to provide a framework for using pre-trained image classification convolutional neural network CNN models to (1) make predictions on unlabled image datasets to provide data for further analysis of walrus (Odobenus rosmarus) coastal haulout occupation and (2) make predictions on image datasets where keywords have been manually (i.e. by human reviewers) added to the image ITPC metadata indicating the content of the images, including the presence and absence of walruses so that the model's performance can be evaluated by comparing its predictions to the human-assigned class labels using confusion matrices and standard classifier evaluation metrics such as accuracy, precision, recall, F1-score (a composite of precision and recall). . screening-level prediction of deformation of embankments, dams, or other facilities due to seismic loading.","the purpose of this software is to provide a framework for using pre-trained image classification convolutional neural network cnn models to (1) make predictions on unlabled image datasets to provide data for further analysis of walrus (odobenus rosmarus) coastal haulout occupation and (2) make predictions on image datasets where keywords have been manually (i.e. by human reviewers) added to the image itpc metadata indicating the content of the images, including the presence and absence of walruses so that the model's performance can be evaluated by comparing its predictions to the human-assigned class labels using confusion matrices and standard classifier evaluation metrics such as accuracy, precision, recall, f1-score (a composite of precision and recall). . screening-level prediction of deformation of embankments, dams, or other facilities due to seismic loading."
Forecasting Earthquake Ground Motion Time Series,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Development of a deep learning models to generate earthquake ground motion time series for potential application to Earthquake Early Warning, Operational Aftershock Forecasting, and the National Seismic Hazard Model.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,"Non-sensitive, mission data describing resource conditions relevant to the condition assessment/prediction question being addressed via AI-ML.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Development of a deep learning models to generate earthquake ground motion time series for potential application to Earthquake Early Warning, Operational Aftershock Forecasting, and the National Seismic Hazard Model. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","development of a deep learning models to generate earthquake ground motion time series for potential application to earthquake early warning, operational aftershock forecasting, and the national seismic hazard model. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
CONUS EcoFlows Planning & Prototype,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"To support development of national-scale ecological-flow response models, a benchmark estimate of typical, long-term unaltered flow conditions is needed as a reference benchmark to evaluate flow departures from ""normal"". This model uses unaltered, near-reference USGS gages from across the US, and associated natural landscape datasets, to predict long-term average monthly flows. These models, trained in near-reference sites, are then used to predict the hypothetical near-reference long-term flow conditions of sites with anthropogenic influences, where we have paired biological datasets. By comparing observed flows to these 'natural' expectations, we calculate flow departures from a 'natural normal' and which is used in later ecological modeling.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,10/3/2022,Developed in-house.,Unknown,No,Yes,No,No,Yes,"Non-sensitive, mission data describing resource conditions relevant to the condition assessment/prediction question being addressed via AI-ML.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"To support development of national-scale ecological-flow response models, a benchmark estimate of typical, long-term unaltered flow conditions is needed as a reference benchmark to evaluate flow departures from ""normal"". This model uses unaltered, near-reference USGS gages from across the US, and associated natural landscape datasets, to predict long-term average monthly flows. These models, trained in near-reference sites, are then used to predict the hypothetical near-reference long-term flow conditions of sites with anthropogenic influences, where we have paired biological datasets. By comparing observed flows to these 'natural' expectations, we calculate flow departures from a 'natural normal' and which is used in later ecological modeling. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","to support development of national-scale ecological-flow response models, a benchmark estimate of typical, long-term unaltered flow conditions is needed as a reference benchmark to evaluate flow departures from ""normal"". this model uses unaltered, near-reference usgs gages from across the us, and associated natural landscape datasets, to predict long-term average monthly flows. these models, trained in near-reference sites, are then used to predict the hypothetical near-reference long-term flow conditions of sites with anthropogenic influences, where we have paired biological datasets. by comparing observed flows to these 'natural' expectations, we calculate flow departures from a 'natural normal' and which is used in later ecological modeling. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Prioritized Constituents: Sediment,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Regional prediction of suspended sediment concentration in unmonitored rivers to characterize sediment transport in the Delaware, Illinois, and Colorado River Basins.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,10/2/2023,Developed in-house.,Unknown,No,Yes,No,No,Yes,"Non-sensitive, mission data describing resource conditions relevant to the condition assessment/prediction question being addressed via AI-ML.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Regional prediction of suspended sediment concentration in unmonitored rivers to characterize sediment transport in the Delaware, Illinois, and Colorado River Basins. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","regional prediction of suspended sediment concentration in unmonitored rivers to characterize sediment transport in the delaware, illinois, and colorado river basins. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Avian population estimates from passive acoustic monitoring,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,Reliable estimates of avian abundance from acoustic recordings,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Avian acoustic recordings,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,Reliable estimates of avian abundance from acoustic recordings . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,reliable estimates of avian abundance from acoustic recordings . outputs were not documented and doi will update the use cases once additional data is collected from submitter
FEMA mixed population flood-frequency analysis,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,Classification of historical floods based on causal mechanisms to support improved estimation of flood reoccurrence intervals,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Classification of historical floods based on causal mechanisms to support improved estimation of flood reoccurrence intervals . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,classification of historical floods based on causal mechanisms to support improved estimation of flood reoccurrence intervals . outputs were not documented and doi will update the use cases once additional data is collected from submitter
"Nutrient, Salinity, and temperature model development",Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Multiple projects designed to simulate nutrients (phosphorus and nitrate), temperature, and salinity in streams across the U.S. using machine learning approaches.",see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,"Climate, earth science, land use","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Multiple projects designed to simulate nutrients (phosphorus and nitrate), temperature, and salinity in streams across the U.S. using machine learning approaches. . see description","multiple projects designed to simulate nutrients (phosphorus and nitrate), temperature, and salinity in streams across the u.s. using machine learning approaches. . see description"
Data-Driven Streamflow Drought,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Prototype streamflow drought forecasts using data-driven, machine learning approaches for USGS gage locations across the continental U.S.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,10/3/2022,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Prototype streamflow drought forecasts using data-driven, machine learning approaches for USGS gage locations across the continental U.S. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","prototype streamflow drought forecasts using data-driven, machine learning approaches for usgs gage locations across the continental u.s. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
AI/ML for aquatic science,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"This project aims to develop novel computational frameworks and AI algorithms for individual fish recognition, by leveraging AI, computer vision and deep learning. The main objectives of this project include:
(1) Develop baseline AI models by exploiting visual features and pre-trained deep learning models.
(2) Improve individual fish recognition performance, as well as handling new individuals and exploring dynamic environments.
(3) Evaluate melanistic markings associated with blotchy bass syndrome to assess the capacity for AI detection of diseased fish.
(4) Evaluate deep learning models for individual recognition and respiration rate (ventilate rate) using video data collected in laboratory settings and natural streams.",see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Earth Science,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"This project aims to develop novel computational frameworks and AI algorithms for individual fish recognition, by leveraging AI, computer vision and deep learning. The main objectives of this project include:
(1) Develop baseline AI models by exploiting visual features and pre-trained deep learning models.
(2) Improve individual fish recognition performance, as well as handling new individuals and exploring dynamic environments.
(3) Evaluate melanistic markings associated with blotchy bass syndrome to assess the capacity for AI detection of diseased fish.
(4) Evaluate deep learning models for individual recognition and respiration rate (ventilate rate) using video data collected in laboratory settings and natural streams. . see description","this project aims to develop novel computational frameworks and ai algorithms for individual fish recognition, by leveraging ai, computer vision and deep learning. the main objectives of this project include: (1) develop baseline ai models by exploiting visual features and pre-trained deep learning models. (2) improve individual fish recognition performance, as well as handling new individuals and exploring dynamic environments. (3) evaluate melanistic markings associated with blotchy bass syndrome to assess the capacity for ai detection of diseased fish. (4) evaluate deep learning models for individual recognition and respiration rate (ventilate rate) using video data collected in laboratory settings and natural streams. . see description"
National-Extent Groundwater Quality Prediction for the National Water Census and Regional Integrated,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"The primary objectives of this project are to (1) provide Nationally consistent predictions of groundwater quality (salinity and nutrients) relevant for human and ecological uses and its influence on surface-water, and (2) develop strategies for integrating these predictions into comprehensive water-availability assessments including the National Water Census and regional Integrated Water Availability Assessments.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,10/1/2021,Developed in-house.,Unknown,No,Yes,No,No,Yes,"Earth Science, Land Use, Climate, Water Quality, Population Density","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The primary objectives of this project are to (1) provide Nationally consistent predictions of groundwater quality (salinity and nutrients) relevant for human and ecological uses and its influence on surface-water, and (2) develop strategies for integrating these predictions into comprehensive water-availability assessments including the National Water Census and regional Integrated Water Availability Assessments. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","the primary objectives of this project are to (1) provide nationally consistent predictions of groundwater quality (salinity and nutrients) relevant for human and ecological uses and its influence on surface-water, and (2) develop strategies for integrating these predictions into comprehensive water-availability assessments including the national water census and regional integrated water availability assessments. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Use of artificial intelligence tools for optimization and documentation for computer codes,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"For the USGS National Seismic Hazard Model and other earthquake hazards research, computer codes are needed that implement earthquake rupture forecasts and ground-motion models. This project uses ChatGPT to suggest optimizations and documentation for computer codes.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,10/2/2023,Developed in-house.,Unknown,No,Yes,No,No,Yes,computer codes,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"For the USGS National Seismic Hazard Model and other earthquake hazards research, computer codes are needed that implement earthquake rupture forecasts and ground-motion models. This project uses ChatGPT to suggest optimizations and documentation for computer codes. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","for the usgs national seismic hazard model and other earthquake hazards research, computer codes are needed that implement earthquake rupture forecasts and ground-motion models. this project uses chatgpt to suggest optimizations and documentation for computer codes. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Improved earthquake detection for research studies,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Deep learning methods are being used to improve detection of earthquakes to provide more complete, high-resolution catalogs that are used in research to better understand earthquake occurrence, rupture processes and seismic hazard.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,seismic waveform data,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Deep learning methods are being used to improve detection of earthquakes to provide more complete, high-resolution catalogs that are used in research to better understand earthquake occurrence, rupture processes and seismic hazard. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","deep learning methods are being used to improve detection of earthquakes to provide more complete, high-resolution catalogs that are used in research to better understand earthquake occurrence, rupture processes and seismic hazard. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
NGWOS External R&D - Using advanced computing techniques for image-based monitoring,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,Multiple projects conducting research on AI/ML techniques for image-based water monitoring,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,11/1/2023,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,Multiple projects conducting research on AI/ML techniques for image-based water monitoring . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,multiple projects conducting research on ai/ml techniques for image-based water monitoring . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Water Use Model Development,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,Goal to develop process-based and machine learning models to estimate multiple categories of water use across the U.S.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,10/1/2021,Developed in-house.,Unknown,No,Yes,No,No,Yes,Earth Science,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,Goal to develop process-based and machine learning models to estimate multiple categories of water use across the U.S. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,goal to develop process-based and machine learning models to estimate multiple categories of water use across the u.s. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
National Temperature Observations,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"The objectives of this project are to reduce the burden on Science Centers for the collection, storage, analysis, and processing of quality assurance data with the expectation this will lead to an increase of deployed sensors in the water temperature network. More specifically the project will (1) modify software to allow for  processing and storage of discrete water temperature data collected during streamflow measurements, (2) implement workflows and QA checks in data collection software that supports new temperature policies and procedures (3) create a pilot program to support Science Centers in accomplishing 5-pt temperature checks.",see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Earth Science,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The objectives of this project are to reduce the burden on Science Centers for the collection, storage, analysis, and processing of quality assurance data with the expectation this will lead to an increase of deployed sensors in the water temperature network. More specifically the project will (1) modify software to allow for  processing and storage of discrete water temperature data collected during streamflow measurements, (2) implement workflows and QA checks in data collection software that supports new temperature policies and procedures (3) create a pilot program to support Science Centers in accomplishing 5-pt temperature checks. . see description","the objectives of this project are to reduce the burden on science centers for the collection, storage, analysis, and processing of quality assurance data with the expectation this will lead to an increase of deployed sensors in the water temperature network. more specifically the project will (1) modify software to allow for processing and storage of discrete water temperature data collected during streamflow measurements, (2) implement workflows and qa checks in data collection software that supports new temperature policies and procedures (3) create a pilot program to support science centers in accomplishing 5-pt temperature checks. . see description"
Downscaling and assimilation of meteorology data for local estimates of irrigation water demand.,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"   We found that in the context of satellite remote sensing for estimates of crop water use, high-frequency and low resolution gridded meteorological reanalysis data contributes about half the total error in our estimates. This is due to well-documented localized effects of terrain and land use that aren't accounted for in meteorological forcing datasets such as NLDAS-2 or ERA-5. Our work suggests that assimilating local meteorological station data alongside land use and terrain information may improve the accuracy of our estimates of local-scale atmospheric demand (i.e., reference evapotranspiration, a function of temperature, wind speed, humidity, and solar radiation). The relationship between the reanalysis products and observed local meteorology appears complex and non-linear. We've found that using deep learning techniques, including Long-Short Term Memory (LSTM) time series modeling along Graph Neural Networks to assimilate local meteorological observations offers promise to improve our local-scale estimates atmospheric demand, and thus irrigated crop water use. We hope to use the information we've gathered to add value to existing meteorological forcing datasets by developing software that reliably downscales low spatial resolution datasets to make more accurate estimates of irrigation water use. Such a product would be especially useful in areas without the highly-developed meteorology and climatology datasets that are exclusive to the United States, such as PRISM and GridMET.",see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"We found that in the context of satellite remote sensing for estimates of crop water use, high-frequency and low resolution gridded meteorological reanalysis data contributes about half the total error in our estimates. This is due to well-documented localized effects of terrain and land use that aren't accounted for in meteorological forcing datasets such as NLDAS-2 or ERA-5. Our work suggests that assimilating local meteorological station data alongside land use and terrain information may improve the accuracy of our estimates of local-scale atmospheric demand (i.e., reference evapotranspiration, a function of temperature, wind speed, humidity, and solar radiation). The relationship between the reanalysis products and observed local meteorology appears complex and non-linear. We've found that using deep learning techniques, including Long-Short Term Memory (LSTM) time series modeling along Graph Neural Networks to assimilate local meteorological observations offers promise to improve our local-scale estimates atmospheric demand, and thus irrigated crop water use. We hope to use the information we've gathered to add value to existing meteorological forcing datasets by developing software that reliably downscales low spatial resolution datasets to make more accurate estimates of irrigation water use. Such a product would be especially useful in areas without the highly-developed meteorology and climatology datasets that are exclusive to the United States, such as PRISM and GridMET. . see description","we found that in the context of satellite remote sensing for estimates of crop water use, high-frequency and low resolution gridded meteorological reanalysis data contributes about half the total error in our estimates. this is due to well-documented localized effects of terrain and land use that aren't accounted for in meteorological forcing datasets such as nldas-2 or era-5. our work suggests that assimilating local meteorological station data alongside land use and terrain information may improve the accuracy of our estimates of local-scale atmospheric demand (i.e., reference evapotranspiration, a function of temperature, wind speed, humidity, and solar radiation). the relationship between the reanalysis products and observed local meteorology appears complex and non-linear. we've found that using deep learning techniques, including long-short term memory (lstm) time series modeling along graph neural networks to assimilate local meteorological observations offers promise to improve our local-scale estimates atmospheric demand, and thus irrigated crop water use. we hope to use the information we've gathered to add value to existing meteorological forcing datasets by developing software that reliably downscales low spatial resolution datasets to make more accurate estimates of irrigation water use. such a product would be especially useful in areas without the highly-developed meteorology and climatology datasets that are exclusive to the united states, such as prism and gridmet. . see description"
National-Extent Groundwater Quality Prediction for the Integrated Water Availability Assessments,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"The primary objectives of this project are to (1) provide Nationally consistent predictions of groundwater quality (salinity and nutrients) relevant for human and ecological uses and its influence on surface-water, and (2) develop strategies for integrating these predictions into comprehensive water-availability assessments including the National Water Census and regional Integrated Water Availability Assessments.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"The primary objectives of this project are to (1) provide Nationally consistent predictions of groundwater quality (salinity and nutrients) relevant for human and ecological uses and its influence on surface-water, and (2) develop strategies for integrating these predictions into comprehensive water-availability assessments including the National Water Census and regional Integrated Water Availability Assessments. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","the primary objectives of this project are to (1) provide nationally consistent predictions of groundwater quality (salinity and nutrients) relevant for human and ecological uses and its influence on surface-water, and (2) develop strategies for integrating these predictions into comprehensive water-availability assessments including the national water census and regional integrated water availability assessments. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Building capacity for assessment and prediction of post-wildfire water availability,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,Model development to forecast water quality impacts of wildfires in the western U.S. focused on suspended sediment and salinity,see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Earth Science,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,Model development to forecast water quality impacts of wildfires in the western U.S. focused on suspended sediment and salinity . see description,model development to forecast water quality impacts of wildfires in the western u.s. focused on suspended sediment and salinity . see description
Mapping sagebrush from drones to satellites,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Vegetation maps are critical tools for that are widely used in applications including habitat modeling, evaluating effectiveness of habitat restoration, and understanding the ecological implications of biological invasions. We are using machine learning and imagery from unmanned aerial vehicles (UAV), aircraft, and satellites to extend presence modeling to map fractional cover of sagebrush in the Dakotas, where accurate maps of sagebrush are needed to identify seasonal habitats of sage-grouse for the Bureau of Land Management.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,6/7/2023,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Vegetation maps are critical tools for that are widely used in applications including habitat modeling, evaluating effectiveness of habitat restoration, and understanding the ecological implications of biological invasions. We are using machine learning and imagery from unmanned aerial vehicles (UAV), aircraft, and satellites to extend presence modeling to map fractional cover of sagebrush in the Dakotas, where accurate maps of sagebrush are needed to identify seasonal habitats of sage-grouse for the Bureau of Land Management. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","vegetation maps are critical tools for that are widely used in applications including habitat modeling, evaluating effectiveness of habitat restoration, and understanding the ecological implications of biological invasions. we are using machine learning and imagery from unmanned aerial vehicles (uav), aircraft, and satellites to extend presence modeling to map fractional cover of sagebrush in the dakotas, where accurate maps of sagebrush are needed to identify seasonal habitats of sage-grouse for the bureau of land management. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Delineating sub-surface drainage using satellite imager,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Knowing subsurface drainage (tile-drain) extent is integral to understanding how landscapes respond to precipitation events and subsequent days of drying, as well as how soil characteristics and land management influence stream response. Consequently, a time series of tile-drain extent would inform one aspect of land management that complicates our ability to explain streamflow and water-quality as a function of climate variability or conservation management.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Operation and Maintenance,Neither,Unknown,Unknown,9/30/2023,Developed in-house.,Unknown,No,Yes,No,No,Yes,"A UNet machine-learning model, a convolutional neural network designed to highlight objects of interest within an image, to delineate tile-drain networks in panchromatic satellite imagery without additional data on soils, topography.

Satellite imagery, soils data","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Knowing subsurface drainage (tile-drain) extent is integral to understanding how landscapes respond to precipitation events and subsequent days of drying, as well as how soil characteristics and land management influence stream response. Consequently, a time series of tile-drain extent would inform one aspect of land management that complicates our ability to explain streamflow and water-quality as a function of climate variability or conservation management. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","knowing subsurface drainage (tile-drain) extent is integral to understanding how landscapes respond to precipitation events and subsequent days of drying, as well as how soil characteristics and land management influence stream response. consequently, a time series of tile-drain extent would inform one aspect of land management that complicates our ability to explain streamflow and water-quality as a function of climate variability or conservation management. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Vegetation mapping on the Hawaiian island of Lanai,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Creating high-resolution vegetation mapping approach that combines satellite imagery, machine learning, and expert knowledge to accurately classify plant species across the Hawaiian island of Lanai, producing detailed maps that can support conservation planning and monitoring of both native and invasive species.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Digital Globe WorldView-2 satellite imagery; airborne imagery collected by EagleView,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Creating high-resolution vegetation mapping approach that combines satellite imagery, machine learning, and expert knowledge to accurately classify plant species across the Hawaiian island of Lanai, producing detailed maps that can support conservation planning and monitoring of both native and invasive species. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","creating high-resolution vegetation mapping approach that combines satellite imagery, machine learning, and expert knowledge to accurately classify plant species across the hawaiian island of lanai, producing detailed maps that can support conservation planning and monitoring of both native and invasive species. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Machine Learning for automatic fracture mapping and rock identification,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,Machine learning algorithms are being used to improve detection and characterization of faulting after major surface rupturing earthquakes and identify and collect imagery of fragile geologic features with application to assessment of earthquake hazards.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Geological field data,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,Machine learning algorithms are being used to improve detection and characterization of faulting after major surface rupturing earthquakes and identify and collect imagery of fragile geologic features with application to assessment of earthquake hazards. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,machine learning algorithms are being used to improve detection and characterization of faulting after major surface rupturing earthquakes and identify and collect imagery of fragile geologic features with application to assessment of earthquake hazards. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Reinforcement Learning for Helmholtz Coil Operation and Simulation,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"The USGS is using AI/ML to optimize performance of its magnetic observatories. For example, reinforcement learning (RL) can significantly aid in the operation of a Helmholtz coil by optimizing its performance in generating uniform magnetic fields.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"The USGS is using AI/ML to optimize performance of its magnetic observatories. For example, reinforcement learning (RL) can significantly aid in the operation of a Helmholtz coil by optimizing its performance in generating uniform magnetic fields. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","the usgs is using ai/ml to optimize performance of its magnetic observatories. for example, reinforcement learning (rl) can significantly aid in the operation of a helmholtz coil by optimizing its performance in generating uniform magnetic fields. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Deep-learning Integration into NEIC Operations,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,The National Earthquake Information Center (NEIC) is improving its earthquake detection and processing systems by leveraging artificial intelligence and machine learning.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,The National Earthquake Information Center (NEIC) is improving its earthquake detection and processing systems by leveraging artificial intelligence and machine learning. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,the national earthquake information center (neic) is improving its earthquake detection and processing systems by leveraging artificial intelligence and machine learning. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Predictions of PFAS Concentrations in Groundwater,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"A model of PFAS occurrence in groundwater at the depths of public and private drinking water supplies across the CONUS has been created and is expected to receive national attention. The model predicts occurrence of PFAS (detect or non-detect) in 1x1 km grid cells, and leverages the best data available in 2023. By updating this model, the goals are to (1) provide concentration predictions, instead of occurrence predictions, and (2) improve model accuracy owing to a larger set of sample data to train the model with. Concentration estimates would allow for better leveraging of resources to areas with predicted significant impacts.",see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,"Well Data, water quality","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"A model of PFAS occurrence in groundwater at the depths of public and private drinking water supplies across the CONUS has been created and is expected to receive national attention. The model predicts occurrence of PFAS (detect or non-detect) in 1x1 km grid cells, and leverages the best data available in 2023. By updating this model, the goals are to (1) provide concentration predictions, instead of occurrence predictions, and (2) improve model accuracy owing to a larger set of sample data to train the model with. Concentration estimates would allow for better leveraging of resources to areas with predicted significant impacts. . see description","a model of pfas occurrence in groundwater at the depths of public and private drinking water supplies across the conus has been created and is expected to receive national attention. the model predicts occurrence of pfas (detect or non-detect) in 1x1 km grid cells, and leverages the best data available in 2023. by updating this model, the goals are to (1) provide concentration predictions, instead of occurrence predictions, and (2) improve model accuracy owing to a larger set of sample data to train the model with. concentration estimates would allow for better leveraging of resources to areas with predicted significant impacts. . see description"
NGWOS External R&D - Using advanced computing techniques for mobile monitoring platforms,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,This project is designing a networking approach for underwater robotic sensor platforms so that they can work together without direct human intervention to adjust their monitoring operations to changing bathymetry and water conditions using artificial intelligence algorithms and underwater communications.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,11/1/2023,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Earth Science,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,This project is designing a networking approach for underwater robotic sensor platforms so that they can work together without direct human intervention to adjust their monitoring operations to changing bathymetry and water conditions using artificial intelligence algorithms and underwater communications. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,this project is designing a networking approach for underwater robotic sensor platforms so that they can work together without direct human intervention to adjust their monitoring operations to changing bathymetry and water conditions using artificial intelligence algorithms and underwater communications. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Population and critical habitat modeling of overwintering monarch butterflies,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Monarch butterflies in the western United States overwinter at very specific locations across coastal California. As monarch population decline it become important to identify the characteristics of what makes an overwintering grove a suitable habitat. Understanding the land cover and climatic factors that influence site selection by monarch can aid land managers in both making decisions to support exisiting critical habitat, and identify previously unknown locations where monarchs overwinter",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,10/1/2023,Developed in-house.,Unknown,No,Yes,No,No,Yes,"High resolution land cover data, population abundance data, regional climate data","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Monarch butterflies in the western United States overwinter at very specific locations across coastal California. As monarch population decline it become important to identify the characteristics of what makes an overwintering grove a suitable habitat. Understanding the land cover and climatic factors that influence site selection by monarch can aid land managers in both making decisions to support exisiting critical habitat, and identify previously unknown locations where monarchs overwinter . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","monarch butterflies in the western united states overwinter at very specific locations across coastal california. as monarch population decline it become important to identify the characteristics of what makes an overwintering grove a suitable habitat. understanding the land cover and climatic factors that influence site selection by monarch can aid land managers in both making decisions to support exisiting critical habitat, and identify previously unknown locations where monarchs overwinter . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Automating the Detection and Classification of Wildlife in Aerial Imagery,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"The US Geological Survey (USGS), Bureau of Ocean Energy Management (BOEM), and US Fish and Wildlife Service (FWS) are partnering on a multi-year effort to develop deep learning algorithms and tools for the detection and classification of seabirds and other marine wildlife in aerial imagery. The tools and workflows developed by this project will be used by BOEM to assess wildlife populations as part of planning and monitoring for offshore energy development.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,10/1/2018,Unknown,10/1/2018,Developed in-house.,Unknown,No,Yes,No,No,Yes,"High-resolution aerial imagery, expert annotations","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The US Geological Survey (USGS), Bureau of Ocean Energy Management (BOEM), and US Fish and Wildlife Service (FWS) are partnering on a multi-year effort to develop deep learning algorithms and tools for the detection and classification of seabirds and other marine wildlife in aerial imagery. The tools and workflows developed by this project will be used by BOEM to assess wildlife populations as part of planning and monitoring for offshore energy development. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","the us geological survey (usgs), bureau of ocean energy management (boem), and us fish and wildlife service (fws) are partnering on a multi-year effort to develop deep learning algorithms and tools for the detection and classification of seabirds and other marine wildlife in aerial imagery. the tools and workflows developed by this project will be used by boem to assess wildlife populations as part of planning and monitoring for offshore energy development. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Machine Learning algorithm for stream velocity prediction,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,The algorithm will be used to develop and incorporate a time-of-travel web-based application that will allow users to estimate travel times in a spill response scenario with greater accuracy.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,streamflow data,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,The algorithm will be used to develop and incorporate a time-of-travel web-based application that will allow users to estimate travel times in a spill response scenario with greater accuracy. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,the algorithm will be used to develop and incorporate a time-of-travel web-based application that will allow users to estimate travel times in a spill response scenario with greater accuracy. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Machine Learning for streamflow forecasting,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"In the Pacific Northwest, where the Willamette River is located, floods are becoming more common and severe. To help prevent flooding, the area has 13 dams built to control floods. This project, a collaboration with Portland State University, aims to build a smart system to predict river flows accurately and quickly across the Willamette River Basin by using state-of-the-art statistical and computational models as well as new hydrologic observations. The project aims to improve our understanding of how to use new river flow observational data that USGS provides to make better predictions.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,streamflow data,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"In the Pacific Northwest, where the Willamette River is located, floods are becoming more common and severe. To help prevent flooding, the area has 13 dams built to control floods. This project, a collaboration with Portland State University, aims to build a smart system to predict river flows accurately and quickly across the Willamette River Basin by using state-of-the-art statistical and computational models as well as new hydrologic observations. The project aims to improve our understanding of how to use new river flow observational data that USGS provides to make better predictions. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","in the pacific northwest, where the willamette river is located, floods are becoming more common and severe. to help prevent flooding, the area has 13 dams built to control floods. this project, a collaboration with portland state university, aims to build a smart system to predict river flows accurately and quickly across the willamette river basin by using state-of-the-art statistical and computational models as well as new hydrologic observations. the project aims to improve our understanding of how to use new river flow observational data that usgs provides to make better predictions. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Automated otolith aging using image processing,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Fisheries managers and researchers often need to know the age of fish for population estimates, stock assessment, and similar projects. Fish otoliths (an ear bone) often accumulated rings annual (similar to tress). This process traditionally is done manually and can vary across individual agers. We are training an image process ML program to automate this process to see if we can reduce variability across individual agers and automate the aging process of counting otolith rings possibly saving time.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Otolith images (pictures) with known ages,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Fisheries managers and researchers often need to know the age of fish for population estimates, stock assessment, and similar projects. Fish otoliths (an ear bone) often accumulated rings annual (similar to tress). This process traditionally is done manually and can vary across individual agers. We are training an image process ML program to automate this process to see if we can reduce variability across individual agers and automate the aging process of counting otolith rings possibly saving time. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","fisheries managers and researchers often need to know the age of fish for population estimates, stock assessment, and similar projects. fish otoliths (an ear bone) often accumulated rings annual (similar to tress). this process traditionally is done manually and can vary across individual agers. we are training an image process ml program to automate this process to see if we can reduce variability across individual agers and automate the aging process of counting otolith rings possibly saving time. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Automating blood smear cell counts using machine learning,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"We exposed largemouth bass to an immunogen (poly I:C) to stimulate an antiviral response and collected blood smears from 38 of these fish. The blood smear slides were scanned at 83x, digitized using an Aperio ScanScope CS, and uploaded for labeling using SageMaker Ground Truth software. We are working with USGS Cloud Hosting Solutions to label training sets of images that include lymphocytes, monocytes, and granulocytes. We are in the process of validating and testing the ability of the model to accurately recognize and distinguish WBC images. To accomplish this, we are comparing manual cell counts among the 3 human readers, manual cell counts to automated cell counts by the model, and automated cell counts of novel tiles taken from the training slides.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,10/1/2023,Developed in-house.,Unknown,No,Yes,No,No,Yes,"blood smear slides were scanned at 83x, digitized using an Aperio ScanScope CS, and uploaded for labeling using SageMaker Ground Truth software","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"We exposed largemouth bass to an immunogen (poly I:C) to stimulate an antiviral response and collected blood smears from 38 of these fish. The blood smear slides were scanned at 83x, digitized using an Aperio ScanScope CS, and uploaded for labeling using SageMaker Ground Truth software. We are working with USGS Cloud Hosting Solutions to label training sets of images that include lymphocytes, monocytes, and granulocytes. We are in the process of validating and testing the ability of the model to accurately recognize and distinguish WBC images. To accomplish this, we are comparing manual cell counts among the 3 human readers, manual cell counts to automated cell counts by the model, and automated cell counts of novel tiles taken from the training slides. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","we exposed largemouth bass to an immunogen (poly i:c) to stimulate an antiviral response and collected blood smears from 38 of these fish. the blood smear slides were scanned at 83x, digitized using an aperio scanscope cs, and uploaded for labeling using sagemaker ground truth software. we are working with usgs cloud hosting solutions to label training sets of images that include lymphocytes, monocytes, and granulocytes. we are in the process of validating and testing the ability of the model to accurately recognize and distinguish wbc images. to accomplish this, we are comparing manual cell counts among the 3 human readers, manual cell counts to automated cell counts by the model, and automated cell counts of novel tiles taken from the training slides. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Machine learning for tsunami source zones,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,State of the art tsunami hazard analysis for coastal communities and infrastructure is computationally demanding. Ml will be used to select the most representative source zones (among thousands of offshore earthquake ruptures),Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,10/1/2024,Developed in-house.,Unknown,No,Yes,No,No,Yes,Offshore fault slip rate data and historical seismicity,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,State of the art tsunami hazard analysis for coastal communities and infrastructure is computationally demanding. Ml will be used to select the most representative source zones (among thousands of offshore earthquake ruptures) . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,state of the art tsunami hazard analysis for coastal communities and infrastructure is computationally demanding. ml will be used to select the most representative source zones (among thousands of offshore earthquake ruptures) . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Shoreline Modeling,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Exploring the prospect of using AI/ML models to predict shoreline change and compare accuracy to traditional models. Examples of AI/ML models include LSTM, CNN, and Transformers",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Exploring the prospect of using AI/ML models to predict shoreline change and compare accuracy to traditional models. Examples of AI/ML models include LSTM, CNN, and Transformers . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","exploring the prospect of using ai/ml models to predict shoreline change and compare accuracy to traditional models. examples of ai/ml models include lstm, cnn, and transformers . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Determining the resource potential of critical minerals in seafloor massive sulfide deposits,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,ML to input data on seafloor massive sulfide geochemstry to predict composition.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Geochemical data,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,ML to input data on seafloor massive sulfide geochemstry to predict composition. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,ml to input data on seafloor massive sulfide geochemstry to predict composition. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
"Oceanographic, coastal, and geomorphic change analysis: data generation, QC/QA, and data management",Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Machine learning to quantify coastal/marine change across broad scales. QC/QA processes in place to assess data robustness. Verified data will be used by USGS projects for forecasting trends (ie, shorelines, role of permafrost) in a variety of coastal/marine settings for US coasts.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,10/1/2024,Developed in-house.,Unknown,No,Yes,No,No,Yes,"Deep Learning based image segmentation, time-series methods, classification & regression methods. Custom scripts/tools using python/pytorch/tensorflow/keras and other ML libraries and APIs","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Machine learning to quantify coastal/marine change across broad scales. QC/QA processes in place to assess data robustness. Verified data will be used by USGS projects for forecasting trends (ie, shorelines, role of permafrost) in a variety of coastal/marine settings for US coasts. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","machine learning to quantify coastal/marine change across broad scales. qc/qa processes in place to assess data robustness. verified data will be used by usgs projects for forecasting trends (ie, shorelines, role of permafrost) in a variety of coastal/marine settings for us coasts. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Synthesizing mapping and monitoring data to inform prairie dog management in National Parks,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Synthesizing Mapping and Monitoring Data to Understand Fluctuations in Prairie Dog Colony Size and Densities in Theodore Roosevelt National Park. There have been efforts to capture variation in the size and extent of prairie dog colonies at Theodore Roosevelt National Park in some form since the 1940s. Prairie dog colonies have been mapped semi-annually since the 1990s, but shifting priorities and a largely static budget have made it difficult for park staff to continue mapping. Furthermore, little research has been conducted with the existing mapping data to assess prairie dog habitat quality and the factors that affect colony size fluctuations. This project is aimed at developing more cost-efficient methods for prairie dog colony mapping (that is, remote sensing techniques) and developing indices and models that can help managers derive population inferences based on colony area. Park staff need modeling and remote analysis tools they can use to assess variability in prairie dog populations within the park. These tools should include a method for remotely assessing prairie dog colony size and a predictive tool that can help park managers understand the relationship between colony size and prairie dog population size.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,10/1/2020,Developed in-house.,Unknown,No,Yes,No,No,Yes,Imagery on prairie dog colony and population size,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Synthesizing Mapping and Monitoring Data to Understand Fluctuations in Prairie Dog Colony Size and Densities in Theodore Roosevelt National Park. There have been efforts to capture variation in the size and extent of prairie dog colonies at Theodore Roosevelt National Park in some form since the 1940s. Prairie dog colonies have been mapped semi-annually since the 1990s, but shifting priorities and a largely static budget have made it difficult for park staff to continue mapping. Furthermore, little research has been conducted with the existing mapping data to assess prairie dog habitat quality and the factors that affect colony size fluctuations. This project is aimed at developing more cost-efficient methods for prairie dog colony mapping (that is, remote sensing techniques) and developing indices and models that can help managers derive population inferences based on colony area. Park staff need modeling and remote analysis tools they can use to assess variability in prairie dog populations within the park. These tools should include a method for remotely assessing prairie dog colony size and a predictive tool that can help park managers understand the relationship between colony size and prairie dog population size. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","synthesizing mapping and monitoring data to understand fluctuations in prairie dog colony size and densities in theodore roosevelt national park. there have been efforts to capture variation in the size and extent of prairie dog colonies at theodore roosevelt national park in some form since the 1940s. prairie dog colonies have been mapped semi-annually since the 1990s, but shifting priorities and a largely static budget have made it difficult for park staff to continue mapping. furthermore, little research has been conducted with the existing mapping data to assess prairie dog habitat quality and the factors that affect colony size fluctuations. this project is aimed at developing more cost-efficient methods for prairie dog colony mapping (that is, remote sensing techniques) and developing indices and models that can help managers derive population inferences based on colony area. park staff need modeling and remote analysis tools they can use to assess variability in prairie dog populations within the park. these tools should include a method for remotely assessing prairie dog colony size and a predictive tool that can help park managers understand the relationship between colony size and prairie dog population size. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
"Quantifying the effects of land-use change and bioenergy crop production on pollinators, wildlife, a",Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"The annual migration of monarch butterflies in North America represents a biological phenomenon unique to our planet,
covering more than 4,000 kilometers and requiring multiple generations of monarchs to complete. The monarch was proposed for listing under the Endangered Species Act in 2014 because of significant population declines and extinction risk. Disappearance of milkweed, the essential host plant for monarch larvae, has been implicated in the decline of the eastern monarch population. The objective of this study is to test the effectiveness of using uncrewed aircraft systems (UAS) and artificial neural networks to quantify the density of common and showy milkweed in working grasslands of Minnesota and North Dakota. First, NPWRC scientists will develop a machine learning algorithm for detecting milkweeds from UAS-collected aerial images. Second, NPWRC will validate the algorithm by comparing plot-level counts of milkweed estimated from UAS images to field count data across a range of milkweed densities. Lastly, NPWRC will take steps towards facilitating the integration of this technology into the Integrated Monarch Monitoring Program by estimating the number of spatially independent UAS images required for achieving accurate and precise estimates of milkweed across entire fields. In fiscal year 2019, preliminary UAS flights were completed; however, this project was put on hold because of a DOI ruling that grounded all UAS flights. No USGS UAS flights will be completed until this ruling is changed.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,10/1/2018,Developed in-house.,Unknown,No,Yes,No,No,Yes,UAS images,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The annual migration of monarch butterflies in North America represents a biological phenomenon unique to our planet,
covering more than 4,000 kilometers and requiring multiple generations of monarchs to complete. The monarch was proposed for listing under the Endangered Species Act in 2014 because of significant population declines and extinction risk. Disappearance of milkweed, the essential host plant for monarch larvae, has been implicated in the decline of the eastern monarch population. The objective of this study is to test the effectiveness of using uncrewed aircraft systems (UAS) and artificial neural networks to quantify the density of common and showy milkweed in working grasslands of Minnesota and North Dakota. First, NPWRC scientists will develop a machine learning algorithm for detecting milkweeds from UAS-collected aerial images. Second, NPWRC will validate the algorithm by comparing plot-level counts of milkweed estimated from UAS images to field count data across a range of milkweed densities. Lastly, NPWRC will take steps towards facilitating the integration of this technology into the Integrated Monarch Monitoring Program by estimating the number of spatially independent UAS images required for achieving accurate and precise estimates of milkweed across entire fields. In fiscal year 2019, preliminary UAS flights were completed; however, this project was put on hold because of a DOI ruling that grounded all UAS flights. No USGS UAS flights will be completed until this ruling is changed. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","the annual migration of monarch butterflies in north america represents a biological phenomenon unique to our planet, covering more than 4,000 kilometers and requiring multiple generations of monarchs to complete. the monarch was proposed for listing under the endangered species act in 2014 because of significant population declines and extinction risk. disappearance of milkweed, the essential host plant for monarch larvae, has been implicated in the decline of the eastern monarch population. the objective of this study is to test the effectiveness of using uncrewed aircraft systems (uas) and artificial neural networks to quantify the density of common and showy milkweed in working grasslands of minnesota and north dakota. first, npwrc scientists will develop a machine learning algorithm for detecting milkweeds from uas-collected aerial images. second, npwrc will validate the algorithm by comparing plot-level counts of milkweed estimated from uas images to field count data across a range of milkweed densities. lastly, npwrc will take steps towards facilitating the integration of this technology into the integrated monarch monitoring program by estimating the number of spatially independent uas images required for achieving accurate and precise estimates of milkweed across entire fields. in fiscal year 2019, preliminary uas flights were completed; however, this project was put on hold because of a doi ruling that grounded all uas flights. no usgs uas flights will be completed until this ruling is changed. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Computationally efficient emulation of spheroidal elastic deformation sources using machine learning,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Elastic continuum mechanical models are widely used to compute deformations due to pressure changes in buried cavities, such as magma reservoirs. In general, analytical models are fast but can be inaccurate as they do not correctly satisfy boundary conditions for many geometries, while numerical models are slow and may require specialized expertise and software. To overcome these limitations, we trained supervised machine learning emulators (model surrogates) based on parallel partial Gaussian processes which predict the output of a finite element numerical model with high fidelity.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,1/2/2023,Developed in-house.,Unknown,No,Yes,No,No,Yes,model output,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Elastic continuum mechanical models are widely used to compute deformations due to pressure changes in buried cavities, such as magma reservoirs. In general, analytical models are fast but can be inaccurate as they do not correctly satisfy boundary conditions for many geometries, while numerical models are slow and may require specialized expertise and software. To overcome these limitations, we trained supervised machine learning emulators (model surrogates) based on parallel partial Gaussian processes which predict the output of a finite element numerical model with high fidelity. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","elastic continuum mechanical models are widely used to compute deformations due to pressure changes in buried cavities, such as magma reservoirs. in general, analytical models are fast but can be inaccurate as they do not correctly satisfy boundary conditions for many geometries, while numerical models are slow and may require specialized expertise and software. to overcome these limitations, we trained supervised machine learning emulators (model surrogates) based on parallel partial gaussian processes which predict the output of a finite element numerical model with high fidelity. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Wildlife species recognition and distance from camera estimation,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,Reliable population estimates of animal density.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Reliable population estimates of animal density. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,reliable population estimates of animal density. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Automating blood smear cell counts using machine learning,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"We exposed largemouth bass to an immunogen (poly I:C) to stimulate an antiviral response and collected blood smears from 38 of these fish. The blood smear slides were scanned at 83x, digitized using an Aperio ScanScope CS, and uploaded for labeling using SageMaker Ground Truth software. We are working with USGS Cloud Hosting Solutions to label training sets of images that include lymphocytes, monocytes, and granulocytes. We are in the process of validating and testing the ability of the model to accurately recognize and distinguish WBC images. To accomplish this, we are comparing manual cell counts among the 3 human readers, manual cell counts to automated cell counts by the model, and automated cell counts of novel tiles taken from the training slides.",see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,"blood smear slides were scanned at 83x, digitized using an Aperio ScanScope CS, and uploaded for labeling using SageMaker Ground Truth software","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"We exposed largemouth bass to an immunogen (poly I:C) to stimulate an antiviral response and collected blood smears from 38 of these fish. The blood smear slides were scanned at 83x, digitized using an Aperio ScanScope CS, and uploaded for labeling using SageMaker Ground Truth software. We are working with USGS Cloud Hosting Solutions to label training sets of images that include lymphocytes, monocytes, and granulocytes. We are in the process of validating and testing the ability of the model to accurately recognize and distinguish WBC images. To accomplish this, we are comparing manual cell counts among the 3 human readers, manual cell counts to automated cell counts by the model, and automated cell counts of novel tiles taken from the training slides. . see description","we exposed largemouth bass to an immunogen (poly i:c) to stimulate an antiviral response and collected blood smears from 38 of these fish. the blood smear slides were scanned at 83x, digitized using an aperio scanscope cs, and uploaded for labeling using sagemaker ground truth software. we are working with usgs cloud hosting solutions to label training sets of images that include lymphocytes, monocytes, and granulocytes. we are in the process of validating and testing the ability of the model to accurately recognize and distinguish wbc images. to accomplish this, we are comparing manual cell counts among the 3 human readers, manual cell counts to automated cell counts by the model, and automated cell counts of novel tiles taken from the training slides. . see description"
Machine Learning to evaluate water quality,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,Examining the effect of physicochemical and meteorological variables on water quality indicators of harmful algal blooms in a shallow hypereutrophic lake using machine learning techniques.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,9/30/2024,Developed in-house.,Unknown,No,Yes,No,No,Yes,water-quality data,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,Examining the effect of physicochemical and meteorological variables on water quality indicators of harmful algal blooms in a shallow hypereutrophic lake using machine learning techniques. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,examining the effect of physicochemical and meteorological variables on water quality indicators of harmful algal blooms in a shallow hypereutrophic lake using machine learning techniques. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Ecological niche models for bat species,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,We are trying to understand what environmental factors determine the presence and absence of bat species across their range.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,1/1/2022,Developed in-house.,Unknown,No,Yes,No,No,Yes,"bat presence locations, environmental raster data","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,We are trying to understand what environmental factors determine the presence and absence of bat species across their range. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,we are trying to understand what environmental factors determine the presence and absence of bat species across their range. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
HotLINK: The volcanic hotspot learning and identification network,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"An increase in volcanic thermal emissions can indicate subsurface and surface processes that precede, or coincide with, volcanic eruptions.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Operation and Maintenance,Neither,Unknown,Unknown,10/1/2023,Developed in-house.,Unknown,No,Yes,No,No,Yes,MODIS and VIIRS infrared satellite imagery from NASA and NOAA,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"An increase in volcanic thermal emissions can indicate subsurface and surface processes that precede, or coincide with, volcanic eruptions. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","an increase in volcanic thermal emissions can indicate subsurface and surface processes that precede, or coincide with, volcanic eruptions. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
"Quantifying Watershed Controls on Fine Sediment Flux to Lake Tahoe, California/Nevada",Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"The variability in precipitation state impacts the fine sediment (FS, < 16 um) flux from upland areas to Lake Tahoe influencing lake clarity. We used supervised random forest regression models to estimate watershed parameters of importance that drive sediment flux.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,10/1/2019,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,"Stage and turbidity from NWIS, water balance variables from Western Land Data Assimilation (NASA) land surface model.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The variability in precipitation state impacts the fine sediment (FS, < 16 um) flux from upland areas to Lake Tahoe influencing lake clarity. We used supervised random forest regression models to estimate watershed parameters of importance that drive sediment flux. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","the variability in precipitation state impacts the fine sediment (fs, < 16 um) flux from upland areas to lake tahoe influencing lake clarity. we used supervised random forest regression models to estimate watershed parameters of importance that drive sediment flux. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Seismology of Magmatic Injection,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,USGS staff are working with a student from Baylor University to use machine learning and network covariance to o understand the nature and dynamics of seismic sources associated with magmatic injection and magmatic transport. This information is necessary to understand volcanic systems. Seismic investigations are also being done for magma plumbing.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,10/1/2023,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,USGS staff are working with a student from Baylor University to use machine learning and network covariance to o understand the nature and dynamics of seismic sources associated with magmatic injection and magmatic transport. This information is necessary to understand volcanic systems. Seismic investigations are also being done for magma plumbing. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,usgs staff are working with a student from baylor university to use machine learning and network covariance to o understand the nature and dynamics of seismic sources associated with magmatic injection and magmatic transport. this information is necessary to understand volcanic systems. seismic investigations are also being done for magma plumbing. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Earthquake Catalog Development,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Using AI/ML to develop more complete and robust earthquake catalogs, including focal mechanisms. This includes volcanic earthquake catalog enhancement using integrated detection, matched-filtering, and relocation tools.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,10/1/2021,Developed in-house.,Unknown,No,Yes,No,No,Yes,Seismic data collected by HVO during a nodal ,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Using AI/ML to develop more complete and robust earthquake catalogs, including focal mechanisms. This includes volcanic earthquake catalog enhancement using integrated detection, matched-filtering, and relocation tools. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","using ai/ml to develop more complete and robust earthquake catalogs, including focal mechanisms. this includes volcanic earthquake catalog enhancement using integrated detection, matched-filtering, and relocation tools. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Seedling Identification and Percent Growth Analysis,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"his project aims to automate the extraction of alphanumeric labels and analyze seedling growth in petri dish images. Labels identifying seed type, treatment, and replication are extracted using Optical Character Recognition (OCR), saving time and reducing human error. Additionally, k-means clustering is applied to segment seedlings from the background, enabling quantification of percent growth over time. The process addresses challenges such as varied image orientations, lighting condidtions, and label placements. By automating label extraction and seedling measurement, the workflow accelerates data analysis, improves accuracy and supports scalable environmental and toxicological research.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,10/1/2023,Developed in-house.,Unknown,No,Yes,No,No,Yes,Numerous images of seedlings taken over a span of 5 days. Around 2000 images in total.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"his project aims to automate the extraction of alphanumeric labels and analyze seedling growth in petri dish images. Labels identifying seed type, treatment, and replication are extracted using Optical Character Recognition (OCR), saving time and reducing human error. Additionally, k-means clustering is applied to segment seedlings from the background, enabling quantification of percent growth over time. The process addresses challenges such as varied image orientations, lighting condidtions, and label placements. By automating label extraction and seedling measurement, the workflow accelerates data analysis, improves accuracy and supports scalable environmental and toxicological research. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","his project aims to automate the extraction of alphanumeric labels and analyze seedling growth in petri dish images. labels identifying seed type, treatment, and replication are extracted using optical character recognition (ocr), saving time and reducing human error. additionally, k-means clustering is applied to segment seedlings from the background, enabling quantification of percent growth over time. the process addresses challenges such as varied image orientations, lighting condidtions, and label placements. by automating label extraction and seedling measurement, the workflow accelerates data analysis, improves accuracy and supports scalable environmental and toxicological research. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Gulf Coast Geologic Energy Machine Learning,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Machine learning has been used to predict expected ultimate recovery of shale oil wells in a previously studied assessment unit. These predictions use multi-layer perceptron based artificial neural networks along with decline-curve based estimated ultimate recoveries and a feature database of geological, reservoir, and well completion parameters. Researchers are developing a ML model using elemental data to predict total organic carbon.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,10/1/2023,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Machine learning has been used to predict expected ultimate recovery of shale oil wells in a previously studied assessment unit. These predictions use multi-layer perceptron based artificial neural networks along with decline-curve based estimated ultimate recoveries and a feature database of geological, reservoir, and well completion parameters. Researchers are developing a ML model using elemental data to predict total organic carbon. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","machine learning has been used to predict expected ultimate recovery of shale oil wells in a previously studied assessment unit. these predictions use multi-layer perceptron based artificial neural networks along with decline-curve based estimated ultimate recoveries and a feature database of geological, reservoir, and well completion parameters. researchers are developing a ml model using elemental data to predict total organic carbon. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Flow Photo Explorer to estimate flow,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"  Full USGS streamgages can be expensive for cooperators, especially on internment streams, so a lower cost, low maintenance method for determining if a stream is flowing and relative flow volumes is needed.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,"Flow Photo Explorer
 field photos","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Full USGS streamgages can be expensive for cooperators, especially on internment streams, so a lower cost, low maintenance method for determining if a stream is flowing and relative flow volumes is needed. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","full usgs streamgages can be expensive for cooperators, especially on internment streams, so a lower cost, low maintenance method for determining if a stream is flowing and relative flow volumes is needed. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Predicting Sparse (Geothermal) Resources Availability by using Machine Learning,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"This research is developing the machine learning (ML) tools (a subset of artificial intelligence) to predict the availability of sparse natural resources (e.g., geothermal, minerals) at regional levels by providing careful consideration to mathematical and geostatistical practices that adhere to geoscience processes. The associated challenges include developing new ML metrics for evaluating model performance that work with sparse natural resources, addressing the extreme mathematical sparsity of these resources at the regional scale, and engineering new evidence layers to inform modeling workflows. The goals of this work include increasing the explainability, reproducibility, and accessibility of the assessment modeling process.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Data on geothermal and mineral resource locations,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"This research is developing the machine learning (ML) tools (a subset of artificial intelligence) to predict the availability of sparse natural resources (e.g., geothermal, minerals) at regional levels by providing careful consideration to mathematical and geostatistical practices that adhere to geoscience processes. The associated challenges include developing new ML metrics for evaluating model performance that work with sparse natural resources, addressing the extreme mathematical sparsity of these resources at the regional scale, and engineering new evidence layers to inform modeling workflows. The goals of this work include increasing the explainability, reproducibility, and accessibility of the assessment modeling process. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","this research is developing the machine learning (ml) tools (a subset of artificial intelligence) to predict the availability of sparse natural resources (e.g., geothermal, minerals) at regional levels by providing careful consideration to mathematical and geostatistical practices that adhere to geoscience processes. the associated challenges include developing new ml metrics for evaluating model performance that work with sparse natural resources, addressing the extreme mathematical sparsity of these resources at the regional scale, and engineering new evidence layers to inform modeling workflows. the goals of this work include increasing the explainability, reproducibility, and accessibility of the assessment modeling process. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
National Wildlife Disease Database,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"The National Wildlife Health Center (NWHC) has contracted the Pacific Northwest National Laboratory (PNNL) to build a national wildlife disease database (NWDD).  Funded through the American Rescue Plan Act of 2021 (ARPA), the NWDD will bring together various wildlife health data streams across informational domains (i.e., laboratory results, environmental observations, news media, etc.) to provide situational awareness and advanced analytics to natural resource authorities around the country.  As part of the NWDD, PNNL plans to integrate their Canvas software.  Canvas is a machine learning, AI, and data science tool that can visualize and contextualize information from one or more sources.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,9/30/2024,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,"Wildlife morbidity & mortality reports, wildlife disease surveillance, environmental observations, media","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The National Wildlife Health Center (NWHC) has contracted the Pacific Northwest National Laboratory (PNNL) to build a national wildlife disease database (NWDD).  Funded through the American Rescue Plan Act of 2021 (ARPA), the NWDD will bring together various wildlife health data streams across informational domains (i.e., laboratory results, environmental observations, news media, etc.) to provide situational awareness and advanced analytics to natural resource authorities around the country.  As part of the NWDD, PNNL plans to integrate their Canvas software.  Canvas is a machine learning, AI, and data science tool that can visualize and contextualize information from one or more sources. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","the national wildlife health center (nwhc) has contracted the pacific northwest national laboratory (pnnl) to build a national wildlife disease database (nwdd). funded through the american rescue plan act of 2021 (arpa), the nwdd will bring together various wildlife health data streams across informational domains (i.e., laboratory results, environmental observations, news media, etc.) to provide situational awareness and advanced analytics to natural resource authorities around the country. as part of the nwdd, pnnl plans to integrate their canvas software. canvas is a machine learning, ai, and data science tool that can visualize and contextualize information from one or more sources. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Landform Mapping GeoAI,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"NASA is helping the USGS is developing a faster, smarter process for creating a national landform dataset using machine learning and active learning techniques. These tools help prioritize which data experts should label, saving time and improving accuracy, especially for rare or complex features. Since the data involves 3D shapes, the project is exploring advanced methods like topological analysis and deep learning to better analyze the data. A basic user interface will also be developed to make labeling easier and lay the foundation for future tools like natural language queries.",see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,"elevation data and derivatives, geographic names and location, topographic maps

Ai Tools:  pytorch, R, tensorflow libraries","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"NASA is helping the USGS is developing a faster, smarter process for creating a national landform dataset using machine learning and active learning techniques. These tools help prioritize which data experts should label, saving time and improving accuracy, especially for rare or complex features. Since the data involves 3D shapes, the project is exploring advanced methods like topological analysis and deep learning to better analyze the data. A basic user interface will also be developed to make labeling easier and lay the foundation for future tools like natural language queries. . see description","nasa is helping the usgs is developing a faster, smarter process for creating a national landform dataset using machine learning and active learning techniques. these tools help prioritize which data experts should label, saving time and improving accuracy, especially for rare or complex features. since the data involves 3d shapes, the project is exploring advanced methods like topological analysis and deep learning to better analyze the data. a basic user interface will also be developed to make labeling easier and lay the foundation for future tools like natural language queries. . see description"
Using Machine Learning Methods for Automatic Discovery and Catalog of North Dakota Stock Ponds and Other Impoundments,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"The purpose of this project is to address the challenges in cataloging stock ponds and other impoundments by developing a machine learning (ML) workflow that processes satellite imagery for the extraction of their location and type. This workflow will be specifically designed to distinguish between multiple types of stock ponds, stock dams and dugouts, and other impoundments. The second goal is to utilize this ML workflow to build a new, comprehensive catalog of stock ponds and other impoundments. This catalog will be unique in its inclusion of smaller stock ponds, which are often omitted in existing databases, thereby offering a more accurate and complete understanding of stock pond and impoundment distribution and types.",see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,"LiDAR data for the state of North Dakota
AI Tools:   UNets","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The purpose of this project is to address the challenges in cataloging stock ponds and other impoundments by developing a machine learning (ML) workflow that processes satellite imagery for the extraction of their location and type. This workflow will be specifically designed to distinguish between multiple types of stock ponds, stock dams and dugouts, and other impoundments. The second goal is to utilize this ML workflow to build a new, comprehensive catalog of stock ponds and other impoundments. This catalog will be unique in its inclusion of smaller stock ponds, which are often omitted in existing databases, thereby offering a more accurate and complete understanding of stock pond and impoundment distribution and types. . see description","the purpose of this project is to address the challenges in cataloging stock ponds and other impoundments by developing a machine learning (ml) workflow that processes satellite imagery for the extraction of their location and type. this workflow will be specifically designed to distinguish between multiple types of stock ponds, stock dams and dugouts, and other impoundments. the second goal is to utilize this ml workflow to build a new, comprehensive catalog of stock ponds and other impoundments. this catalog will be unique in its inclusion of smaller stock ponds, which are often omitted in existing databases, thereby offering a more accurate and complete understanding of stock pond and impoundment distribution and types. . see description"
Improved point cloud classification of 3DEP lidar data using Deep Learning models,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Point clouds are remote sensing data that is represented by points in space that can represent earths surface. The class of the point can identify the surface as belonging to buildings, trees, or roads for example. This study is testing methods for enhancing 3DEP lidar point data classification to include more surface types at a finer resolution. The tests are using deep learning (DL) models to refine and enrich the 3DEP data classification.",see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Point clouds are remote sensing data that is represented by points in space that can represent earths surface. The class of the point can identify the surface as belonging to buildings, trees, or roads for example. This study is testing methods for enhancing 3DEP lidar point data classification to include more surface types at a finer resolution. The tests are using deep learning (DL) models to refine and enrich the 3DEP data classification. . see description","point clouds are remote sensing data that is represented by points in space that can represent earths surface. the class of the point can identify the surface as belonging to buildings, trees, or roads for example. this study is testing methods for enhancing 3dep lidar point data classification to include more surface types at a finer resolution. the tests are using deep learning (dl) models to refine and enrich the 3dep data classification. . see description"
Knowledge Graph development,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,Research and development of knowledge graph designs to make information more interoperable based on logical relationships automatically identified from text. The research work includes using AI for natural language processing.,see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,"Domain knowledge bases or ontologies for each use case (names, elevation, points of interest, etc.)

 AI Tools: ESRI Knowledge","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,Research and development of knowledge graph designs to make information more interoperable based on logical relationships automatically identified from text. The research work includes using AI for natural language processing. . see description,research and development of knowledge graph designs to make information more interoperable based on logical relationships automatically identified from text. the research work includes using ai for natural language processing. . see description
Predicting inundation dynamics of small forested wetlands,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,This project aims to help land managers in the Upper Midwest understand the wetting/drying dynamics of small wetlands relevant to amphibians. The approach will leverage field observations of wetland water levels as training data in a random forest model.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,This project aims to help land managers in the Upper Midwest understand the wetting/drying dynamics of small wetlands relevant to amphibians. The approach will leverage field observations of wetland water levels as training data in a random forest model. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,this project aims to help land managers in the upper midwest understand the wetting/drying dynamics of small wetlands relevant to amphibians. the approach will leverage field observations of wetland water levels as training data in a random forest model. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Hydrograhy feature extraction from remotely sensed data,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Surface water features shown on maps, also known as hydrography, show the available water that can greatly influence the quality of life and the environment. Mapping hydrography is a complex task because the amount and distribution of surface water continuously varies with weather conditions. This research is testing machine learning techniques to develop models that predict the location of surface water from remotely sensed elevation and image data. Machine learning techniques are methods that train a model based on existing hydrography features in training area. After training a model, it can be applied to other areas where hydrography data do not exist, and the model can potentially be applied to new remote sensing data to help update hydrography features over time.",see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,"high resolution elevation data, imagery","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Surface water features shown on maps, also known as hydrography, show the available water that can greatly influence the quality of life and the environment. Mapping hydrography is a complex task because the amount and distribution of surface water continuously varies with weather conditions. This research is testing machine learning techniques to develop models that predict the location of surface water from remotely sensed elevation and image data. Machine learning techniques are methods that train a model based on existing hydrography features in training area. After training a model, it can be applied to other areas where hydrography data do not exist, and the model can potentially be applied to new remote sensing data to help update hydrography features over time. . see description","surface water features shown on maps, also known as hydrography, show the available water that can greatly influence the quality of life and the environment. mapping hydrography is a complex task because the amount and distribution of surface water continuously varies with weather conditions. this research is testing machine learning techniques to develop models that predict the location of surface water from remotely sensed elevation and image data. machine learning techniques are methods that train a model based on existing hydrography features in training area. after training a model, it can be applied to other areas where hydrography data do not exist, and the model can potentially be applied to new remote sensing data to help update hydrography features over time. . see description"
Using machine learning to detect invasive bullfrogs,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,Detecting bullfrogs along their invasion front in order to inform removal efforts,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,5/1/2020,Developed in-house.,Unknown,No,Yes,No,No,Yes,audio recordings,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,Detecting bullfrogs along their invasion front in order to inform removal efforts . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,detecting bullfrogs along their invasion front in order to inform removal efforts . outputs were not documented and doi will update the use cases once additional data is collected from submitter
"Deep Learning application for automated mapping of surficial landforms, surficial geological deposit",Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,Florence Bascom Geoscience Center (FEDMAP-funded): The Bascom Geoscience Center (FEDMAP) is using the deep-learning capabilities implemented within the ESRI ArGIS Pro software platform to automate the process of mapping surficial landforms and related surficial geologic deposits from lidar-derived topography. We are also using this capability to train models that can identify abandoned mine sites from lidar-derived topography.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,10/1/2024,10/1/2024,Developed in-house.,Unknown,No,Yes,No,No,Yes,Published geologic maps and user-created data,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,Florence Bascom Geoscience Center (FEDMAP-funded): The Bascom Geoscience Center (FEDMAP) is using the deep-learning capabilities implemented within the ESRI ArGIS Pro software platform to automate the process of mapping surficial landforms and related surficial geologic deposits from lidar-derived topography. We are also using this capability to train models that can identify abandoned mine sites from lidar-derived topography. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,florence bascom geoscience center (fedmap-funded): the bascom geoscience center (fedmap) is using the deep-learning capabilities implemented within the esri argis pro software platform to automate the process of mapping surficial landforms and related surficial geologic deposits from lidar-derived topography. we are also using this capability to train models that can identify abandoned mine sites from lidar-derived topography. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Zero shot segmentation to expedite Quaternary geologic mapping,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,Geosciences and Environmental Change (GEC) Science Center (FEDMAP-funded): The construction of detailed geologic maps requires a lot of manual GIS data input to outline the extent of interpreted geologic features. We are working to extend and adapt the Segment Anything Model for identification of Quaternary geologic features in order to expedite the process of creating GIS data for geologic maps.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Geosciences and Environmental Change (GEC) Science Center (FEDMAP-funded): The construction of detailed geologic maps requires a lot of manual GIS data input to outline the extent of interpreted geologic features. We are working to extend and adapt the Segment Anything Model for identification of Quaternary geologic features in order to expedite the process of creating GIS data for geologic maps. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,geosciences and environmental change (gec) science center (fedmap-funded): the construction of detailed geologic maps requires a lot of manual gis data input to outline the extent of interpreted geologic features. we are working to extend and adapt the segment anything model for identification of quaternary geologic features in order to expedite the process of creating gis data for geologic maps. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Oil Spil Response for Ice-Covered Rivers,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"This project uses backscatter data from the SAR (synthetic aperture radar instrument) on the Sentinel-1 satellite to determine ice phenology for large rivers and lakes based on machine learning method (i.e., a Random Forest Classifier). The goal of this DOI Inland Oil Spill Preparedness Program (IOSPP) funded work is to provide rapid, near real-time information to oil spill response crews concerning about the safety of ice-covered areas (FY2023-25).",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Operation and Maintenance,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"This project uses backscatter data from the SAR (synthetic aperture radar instrument) on the Sentinel-1 satellite to determine ice phenology for large rivers and lakes based on machine learning method (i.e., a Random Forest Classifier). The goal of this DOI Inland Oil Spill Preparedness Program (IOSPP) funded work is to provide rapid, near real-time information to oil spill response crews concerning about the safety of ice-covered areas (FY2023-25). . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","this project uses backscatter data from the sar (synthetic aperture radar instrument) on the sentinel-1 satellite to determine ice phenology for large rivers and lakes based on machine learning method (i.e., a random forest classifier). the goal of this doi inland oil spill preparedness program (iospp) funded work is to provide rapid, near real-time information to oil spill response crews concerning about the safety of ice-covered areas (fy2023-25). . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Modeling Sediment Abundance in the Eastern Snake River Plain Aquifer Using Supervised Learning,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,This project aims to model sediment abundance in the eastern Snake River Plain aquifer near the Idaho National Laboratory. We are employing supervised learning techniques to predict sediment presence at various depths within boreholes using natural gamma ray readings. The model is trained with independently derived estimates of sediment probability to enhance its accuracy. This initiative will significantly aid scientists in understanding and managing sediment distribution in the aquifer.,see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,This project aims to model sediment abundance in the eastern Snake River Plain aquifer near the Idaho National Laboratory. We are employing supervised learning techniques to predict sediment presence at various depths within boreholes using natural gamma ray readings. The model is trained with independently derived estimates of sediment probability to enhance its accuracy. This initiative will significantly aid scientists in understanding and managing sediment distribution in the aquifer. . see description,this project aims to model sediment abundance in the eastern snake river plain aquifer near the idaho national laboratory. we are employing supervised learning techniques to predict sediment presence at various depths within boreholes using natural gamma ray readings. the model is trained with independently derived estimates of sediment probability to enhance its accuracy. this initiative will significantly aid scientists in understanding and managing sediment distribution in the aquifer. . see description
Pacific Northwest Stream Flow Permanence,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"We are using machine learning approaches, specifically the random forest algorithm, to provide spatially explicit estimates of the presence of year-round surface flow in streams across large (several US states) geographic extents. Empirical random forest models include discrete flow/no flow observations as the response variable and a broad suite of physio climatic covariates. The models are used to inform management decisions that require streamflow classification of perennial versus non-perennial which is the charge of many land steward agencies including the Bureau of Land Management, U.S. Forest Service and State and private forests. Funding for this project is congressional allocated funds (FY2023-25).",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Operation and Maintenance,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"We are using machine learning approaches, specifically the random forest algorithm, to provide spatially explicit estimates of the presence of year-round surface flow in streams across large (several US states) geographic extents. Empirical random forest models include discrete flow/no flow observations as the response variable and a broad suite of physio climatic covariates. The models are used to inform management decisions that require streamflow classification of perennial versus non-perennial which is the charge of many land steward agencies including the Bureau of Land Management, U.S. Forest Service and State and private forests. Funding for this project is congressional allocated funds (FY2023-25). . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","we are using machine learning approaches, specifically the random forest algorithm, to provide spatially explicit estimates of the presence of year-round surface flow in streams across large (several us states) geographic extents. empirical random forest models include discrete flow/no flow observations as the response variable and a broad suite of physio climatic covariates. the models are used to inform management decisions that require streamflow classification of perennial versus non-perennial which is the charge of many land steward agencies including the bureau of land management, u.s. forest service and state and private forests. funding for this project is congressional allocated funds (fy2023-25). . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
SAMPLE Toolbox,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,A toolbox for land managers to develop plans for monitoring vegetation,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,A toolbox for land managers to develop plans for monitoring vegetation . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,a toolbox for land managers to develop plans for monitoring vegetation . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Mapping wildfire fuels in previously burned landscapes,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,The goal is to understand how land management treatments affect the probability of reburning.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,The goal is to understand how land management treatments affect the probability of reburning. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,the goal is to understand how land management treatments affect the probability of reburning. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Inventorying landforms with convolutional neural networks,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Geosciences and Environmental Change (GEC) Science Center (FEDMAP-funded): LiDAR derived topographic data images earth's surface in unprecedented detail and is available across most of the country. This data reveals landforms relevant for understanding many scientific phenomena (e.g., patterned ground) and natural hazards (e.g., karst). We are developing simple pipelines to train and deploy convolutional neural networks on high resolution topographic data in order to efficiently identify and inventory these features.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Geosciences and Environmental Change (GEC) Science Center (FEDMAP-funded): LiDAR derived topographic data images earth's surface in unprecedented detail and is available across most of the country. This data reveals landforms relevant for understanding many scientific phenomena (e.g., patterned ground) and natural hazards (e.g., karst). We are developing simple pipelines to train and deploy convolutional neural networks on high resolution topographic data in order to efficiently identify and inventory these features. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","geosciences and environmental change (gec) science center (fedmap-funded): lidar derived topographic data images earth's surface in unprecedented detail and is available across most of the country. this data reveals landforms relevant for understanding many scientific phenomena (e.g., patterned ground) and natural hazards (e.g., karst). we are developing simple pipelines to train and deploy convolutional neural networks on high resolution topographic data in order to efficiently identify and inventory these features. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Lava lake thermal pattern classification using self organizing maps and relationships to eruption pr,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,We apply a machine learning algorithm called self-organizing maps (SOM) to thermal infrared time-lapse images .,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,1/1/2024,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,We apply a machine learning algorithm called self-organizing maps (SOM) to thermal infrared time-lapse images . . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,we apply a machine learning algorithm called self-organizing maps (som) to thermal infrared time-lapse images . . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Advancing image-based surveys to support sea duck conservation along the Pacific Flyway,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"For most of their annual cycle, North American sea ducks are densely distributed in estuaries and along the coastal nearshore where they are susceptible oil spills, energy development, changing ocean conditions, and other potential threats. Observer-based aerial surveys have been an important tool for evaluating coastal distributions and estimating population abundances to understand sea duck responses to their changing environment. However, safety, expense, observer bias and lack of methodological consistency are rising concerns associated with observer-based surveys, making it imperative to transition to more sustainable methods. Digital aerial surveys (DAS) that automate counts from aerial imagery using convolutional neural network (CNN) models are one way to improve survey safety and count accuracy. We are developing a standardized DAS for the lower Pacific Flyway to help maximize safety, while improving data consistency and model accuracy among important regions within the Flyway.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"For most of their annual cycle, North American sea ducks are densely distributed in estuaries and along the coastal nearshore where they are susceptible oil spills, energy development, changing ocean conditions, and other potential threats. Observer-based aerial surveys have been an important tool for evaluating coastal distributions and estimating population abundances to understand sea duck responses to their changing environment. However, safety, expense, observer bias and lack of methodological consistency are rising concerns associated with observer-based surveys, making it imperative to transition to more sustainable methods. Digital aerial surveys (DAS) that automate counts from aerial imagery using convolutional neural network (CNN) models are one way to improve survey safety and count accuracy. We are developing a standardized DAS for the lower Pacific Flyway to help maximize safety, while improving data consistency and model accuracy among important regions within the Flyway. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","for most of their annual cycle, north american sea ducks are densely distributed in estuaries and along the coastal nearshore where they are susceptible oil spills, energy development, changing ocean conditions, and other potential threats. observer-based aerial surveys have been an important tool for evaluating coastal distributions and estimating population abundances to understand sea duck responses to their changing environment. however, safety, expense, observer bias and lack of methodological consistency are rising concerns associated with observer-based surveys, making it imperative to transition to more sustainable methods. digital aerial surveys (das) that automate counts from aerial imagery using convolutional neural network (cnn) models are one way to improve survey safety and count accuracy. we are developing a standardized das for the lower pacific flyway to help maximize safety, while improving data consistency and model accuracy among important regions within the flyway. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Probabilistic source classification of large tephra producing eruptions using supervised machine lea,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"  USGS has produced a model that accurately and confidently identifies large tephra-producing eruption volcanic sources in the Alaska-Aleutian arc using only in situ geochemistry. The model is a voting ensemble classifier comprised of six conceptually different machine learning algorithms trained on proximal tephra deposits that have had their source positively identified. Eruptive products from Alaska's Aleutian Arc-Alaska Peninsula and Wrangell volcanic field were used as a test environment for 11 supervised classification algorithms, trained on nearly 2000 electron probe microanalysis measurements of glass major oxides, representing 10 volcanic sources.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"USGS has produced a model that accurately and confidently identifies large tephra-producing eruption volcanic sources in the Alaska-Aleutian arc using only in situ geochemistry. The model is a voting ensemble classifier comprised of six conceptually different machine learning algorithms trained on proximal tephra deposits that have had their source positively identified. Eruptive products from Alaska's Aleutian Arc-Alaska Peninsula and Wrangell volcanic field were used as a test environment for 11 supervised classification algorithms, trained on nearly 2000 electron probe microanalysis measurements of glass major oxides, representing 10 volcanic sources. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","usgs has produced a model that accurately and confidently identifies large tephra-producing eruption volcanic sources in the alaska-aleutian arc using only in situ geochemistry. the model is a voting ensemble classifier comprised of six conceptually different machine learning algorithms trained on proximal tephra deposits that have had their source positively identified. eruptive products from alaska's aleutian arc-alaska peninsula and wrangell volcanic field were used as a test environment for 11 supervised classification algorithms, trained on nearly 2000 electron probe microanalysis measurements of glass major oxides, representing 10 volcanic sources. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
InSAR and other geodetic studies at Volcanoes,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,The USGS uses InSAR (Inferometric Synthetic Aperture Radar) to map ground deformation and track volcanic activity globally. Artificial Intelligence approaches are being used to recognize transient signals in combined InSAR and GPS data that may be indications of impending hazardous volcanic activity.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,The USGS uses InSAR (Inferometric Synthetic Aperture Radar) to map ground deformation and track volcanic activity globally. Artificial Intelligence approaches are being used to recognize transient signals in combined InSAR and GPS data that may be indications of impending hazardous volcanic activity. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,the usgs uses insar (inferometric synthetic aperture radar) to map ground deformation and track volcanic activity globally. artificial intelligence approaches are being used to recognize transient signals in combined insar and gps data that may be indications of impending hazardous volcanic activity. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Climate Futures for Lizards and Snakes in Western North America,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,Identifying new management challenges to reptiles based on shifting environmental conditions,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Operation and Maintenance,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Identifying new management challenges to reptiles based on shifting environmental conditions . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,identifying new management challenges to reptiles based on shifting environmental conditions . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Predicting inundation dynamics of small forested wetlands,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,This project aims to help land managers in the Upper Midwest understand the wetting/drying dynamics of small wetlands relevant to amphibians. The approach will leverage field observations of wetland water levels as training data in a random forest model.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,This project aims to help land managers in the Upper Midwest understand the wetting/drying dynamics of small wetlands relevant to amphibians. The approach will leverage field observations of wetland water levels as training data in a random forest model. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,this project aims to help land managers in the upper midwest understand the wetting/drying dynamics of small wetlands relevant to amphibians. the approach will leverage field observations of wetland water levels as training data in a random forest model. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Intelligent National Map project,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"The Intelligent National Map is a vision by the U.S. Geological Survey (USGS) to make mapping smarter by using advanced technology like artificial intelligence (AI). This project aims to create maps that can update themselves automatically, detect changes in the environment, and provide better, faster information for decisions about land, water, and natural resources.",see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"The Intelligent National Map is a vision by the U.S. Geological Survey (USGS) to make mapping smarter by using advanced technology like artificial intelligence (AI). This project aims to create maps that can update themselves automatically, detect changes in the environment, and provide better, faster information for decisions about land, water, and natural resources. . see description","the intelligent national map is a vision by the u.s. geological survey (usgs) to make mapping smarter by using advanced technology like artificial intelligence (ai). this project aims to create maps that can update themselves automatically, detect changes in the environment, and provide better, faster information for decisions about land, water, and natural resources. . see description"
Machine-learning model to delineate sub-surface agricultural drainage from satellite imagery,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"We trained a UNet machine-learning model, a convolutional neural network designed to highlight objects of interest within an image, to delineate tile-drain networks in panchromatic satellite imagery without additional data on soils, topography, or historical tile-drain extent. This was done by training the model to match the accuracy of human experts manually tracing the surface representation of tile drains in satellite imagery. Our approach began with a library of images that were used to train and quantify the accuracy of the model, with model performance tested on imagery from two areas that were not used to train the model. Satellite imagery included acquisition dates from 2008 to 2020. Training imagery was from agricultural areas within the US Great Lakes basin. Validation imagery was from the upper Maumee River, tributary to western Lake Erie, and an Indiana, Ohio-River headwater tributary. Our analysis of the satellite imagery paired with meteorological and soil data found that during spring, a combination of relatively high solar radiation, intermediate soil-water content and bare fields enabled the best model performance. Each area of interest was heavily tile-drained, where better understanding the movement of water, nutrients, and sediment from fields to downstream water bodies is key to managing harmful algal blooms and hypoxia. The trained UNet model successfully identified tile drains visible in the validation imagery. https://doi.org/10.1002/jeq2.20493",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"We trained a UNet machine-learning model, a convolutional neural network designed to highlight objects of interest within an image, to delineate tile-drain networks in panchromatic satellite imagery without additional data on soils, topography, or historical tile-drain extent. This was done by training the model to match the accuracy of human experts manually tracing the surface representation of tile drains in satellite imagery. Our approach began with a library of images that were used to train and quantify the accuracy of the model, with model performance tested on imagery from two areas that were not used to train the model. Satellite imagery included acquisition dates from 2008 to 2020. Training imagery was from agricultural areas within the US Great Lakes basin. Validation imagery was from the upper Maumee River, tributary to western Lake Erie, and an Indiana, Ohio-River headwater tributary. Our analysis of the satellite imagery paired with meteorological and soil data found that during spring, a combination of relatively high solar radiation, intermediate soil-water content and bare fields enabled the best model performance. Each area of interest was heavily tile-drained, where better understanding the movement of water, nutrients, and sediment from fields to downstream water bodies is key to managing harmful algal blooms and hypoxia. The trained UNet model successfully identified tile drains visible in the validation imagery. https://doi.org/10.1002/jeq2.20493 . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","we trained a unet machine-learning model, a convolutional neural network designed to highlight objects of interest within an image, to delineate tile-drain networks in panchromatic satellite imagery without additional data on soils, topography, or historical tile-drain extent. this was done by training the model to match the accuracy of human experts manually tracing the surface representation of tile drains in satellite imagery. our approach began with a library of images that were used to train and quantify the accuracy of the model, with model performance tested on imagery from two areas that were not used to train the model. satellite imagery included acquisition dates from 2008 to 2020. training imagery was from agricultural areas within the us great lakes basin. validation imagery was from the upper maumee river, tributary to western lake erie, and an indiana, ohio-river headwater tributary. our analysis of the satellite imagery paired with meteorological and soil data found that during spring, a combination of relatively high solar radiation, intermediate soil-water content and bare fields enabled the best model performance. each area of interest was heavily tile-drained, where better understanding the movement of water, nutrients, and sediment from fields to downstream water bodies is key to managing harmful algal blooms and hypoxia. the trained unet model successfully identified tile drains visible in the validation imagery. https://doi.org/10.1002/jeq2.20493 . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Environmental streamflows in the United States: historical patterns and predictions,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"  It is important that environmental streamflow assessments by water managers consider changes in climate, land use, and water management; this cannot be done effectively without understanding historical variability and changes in environmental streamflows. Estimates of environmental streamflows also are needed for ungaged streams and machine-learning methods are likely useful for this. We are analyzing historical change and variability at hundreds of streamflow gages across the United States for a suite of environmental streamflows and using machine-learning methods to estimate environmental streamflows for thousands of ungaged stream reaches.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"It is important that environmental streamflow assessments by water managers consider changes in climate, land use, and water management; this cannot be done effectively without understanding historical variability and changes in environmental streamflows. Estimates of environmental streamflows also are needed for ungaged streams and machine-learning methods are likely useful for this. We are analyzing historical change and variability at hundreds of streamflow gages across the United States for a suite of environmental streamflows and using machine-learning methods to estimate environmental streamflows for thousands of ungaged stream reaches. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","it is important that environmental streamflow assessments by water managers consider changes in climate, land use, and water management; this cannot be done effectively without understanding historical variability and changes in environmental streamflows. estimates of environmental streamflows also are needed for ungaged streams and machine-learning methods are likely useful for this. we are analyzing historical change and variability at hundreds of streamflow gages across the united states for a suite of environmental streamflows and using machine-learning methods to estimate environmental streamflows for thousands of ungaged stream reaches. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
PROSPER,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,  Machine learning approach to estimating the annual probability of streamflow permanence at a sub-reach (10m) scale.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Operation and Maintenance,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Machine learning approach to estimating the annual probability of streamflow permanence at a sub-reach (10m) scale. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,machine learning approach to estimating the annual probability of streamflow permanence at a sub-reach (10m) scale. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
[Un]supervised clustering of [non-]earthquake signals commonly recorded on regional seismic networks,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Surficial mass movements (SMMs), such as landslides and rockfalls, have seismic signatures distinct from other routinely recorded seismic sources like earthquakes and explosions. However, overlaps between the characteristics of these signals can still make it difficult to discriminate between source types during operational seismic monitoring. This ambiguity motivates the development of automated techniques for seismic signal classification. Furthermore, seismic differentiation within the broad class of SMMs has additional scientific and rapid response value. Examination of SMM seismic waveforms highlights a particularly strong contrast between the signals produced by primarily vertical processes versus processes that have a non-negligible horizontal component such as landslides and avalanches. A machine learning (ML) classification scheme is being investigated for differentiating between seismic signals generated by falls versus slides. Additionally included are shallow earthquakes and blasts because these are most similar to SMM signals and are commonly recorded on regional seismic networks. These classes, therefore, are the most useful for automated classification. Test datasets derive from waveform picks in the Exotic Seismic Events Catalog, a diverse collection of non-earthquake seismogenic surface events, and the Advanced National Seismic System (ANSS) Comprehensive Earthquake Catalog. In development is a shallow, feature-based approach to classification using statistical metrics extracted from waveforms. Feature importance metrics provide insight into the ML method, which leverage both unsupervised techniques and supervised techniques.",see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Surficial mass movements (SMMs), such as landslides and rockfalls, have seismic signatures distinct from other routinely recorded seismic sources like earthquakes and explosions. However, overlaps between the characteristics of these signals can still make it difficult to discriminate between source types during operational seismic monitoring. This ambiguity motivates the development of automated techniques for seismic signal classification. Furthermore, seismic differentiation within the broad class of SMMs has additional scientific and rapid response value. Examination of SMM seismic waveforms highlights a particularly strong contrast between the signals produced by primarily vertical processes versus processes that have a non-negligible horizontal component such as landslides and avalanches. A machine learning (ML) classification scheme is being investigated for differentiating between seismic signals generated by falls versus slides. Additionally included are shallow earthquakes and blasts because these are most similar to SMM signals and are commonly recorded on regional seismic networks. These classes, therefore, are the most useful for automated classification. Test datasets derive from waveform picks in the Exotic Seismic Events Catalog, a diverse collection of non-earthquake seismogenic surface events, and the Advanced National Seismic System (ANSS) Comprehensive Earthquake Catalog. In development is a shallow, feature-based approach to classification using statistical metrics extracted from waveforms. Feature importance metrics provide insight into the ML method, which leverage both unsupervised techniques and supervised techniques. . see description","surficial mass movements (smms), such as landslides and rockfalls, have seismic signatures distinct from other routinely recorded seismic sources like earthquakes and explosions. however, overlaps between the characteristics of these signals can still make it difficult to discriminate between source types during operational seismic monitoring. this ambiguity motivates the development of automated techniques for seismic signal classification. furthermore, seismic differentiation within the broad class of smms has additional scientific and rapid response value. examination of smm seismic waveforms highlights a particularly strong contrast between the signals produced by primarily vertical processes versus processes that have a non-negligible horizontal component such as landslides and avalanches. a machine learning (ml) classification scheme is being investigated for differentiating between seismic signals generated by falls versus slides. additionally included are shallow earthquakes and blasts because these are most similar to smm signals and are commonly recorded on regional seismic networks. these classes, therefore, are the most useful for automated classification. test datasets derive from waveform picks in the exotic seismic events catalog, a diverse collection of non-earthquake seismogenic surface events, and the advanced national seismic system (anss) comprehensive earthquake catalog. in development is a shallow, feature-based approach to classification using statistical metrics extracted from waveforms. feature importance metrics provide insight into the ml method, which leverage both unsupervised techniques and supervised techniques. . see description"
"Extracting robust, searchable data from narrative geologic descriptions",Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Geosciences and Environmental Change (GEC) Science Center (FEDMAP-funded): Geologic map units are often described in narrative text descriptions. These descriptions sometime contain useful data (e.g., about lithology, unit thickness), but it is hard for users to operationalize that information since is not codified in any standard way. We are using natural language processing and large language models to parse readily queryable information out of thousands of descriptions in a national database of geology.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,"Descriptions of geologic units taken from published reports.

","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Geosciences and Environmental Change (GEC) Science Center (FEDMAP-funded): Geologic map units are often described in narrative text descriptions. These descriptions sometime contain useful data (e.g., about lithology, unit thickness), but it is hard for users to operationalize that information since is not codified in any standard way. We are using natural language processing and large language models to parse readily queryable information out of thousands of descriptions in a national database of geology. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","geosciences and environmental change (gec) science center (fedmap-funded): geologic map units are often described in narrative text descriptions. these descriptions sometime contain useful data (e.g., about lithology, unit thickness), but it is hard for users to operationalize that information since is not codified in any standard way. we are using natural language processing and large language models to parse readily queryable information out of thousands of descriptions in a national database of geology. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Classifying GPS data to understand flight behavior of birds.,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,This project will help understand under what circumstances eagles are more likely to collide with wind turbines.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,This project will help understand under what circumstances eagles are more likely to collide with wind turbines. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,this project will help understand under what circumstances eagles are more likely to collide with wind turbines. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Whole-lake indexing of round goby abundances with photographic catch data,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"USGS is responsible for monitoring abundances of prey fish across the entirety of the Great Lakes. This project utilizes autonomous vehicles and artificial intelligence to quantify abundances of one of the most abundant prey fishes in the Great Lakes, an invasive species called Round Goby. Robotic surveys of Roung Goby are carried out across three of the five Great Lakes each year. The work also characterizes surface geology, which influence round goby abundances.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"USGS is responsible for monitoring abundances of prey fish across the entirety of the Great Lakes. This project utilizes autonomous vehicles and artificial intelligence to quantify abundances of one of the most abundant prey fishes in the Great Lakes, an invasive species called Round Goby. Robotic surveys of Roung Goby are carried out across three of the five Great Lakes each year. The work also characterizes surface geology, which influence round goby abundances. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","usgs is responsible for monitoring abundances of prey fish across the entirety of the great lakes. this project utilizes autonomous vehicles and artificial intelligence to quantify abundances of one of the most abundant prey fishes in the great lakes, an invasive species called round goby. robotic surveys of roung goby are carried out across three of the five great lakes each year. the work also characterizes surface geology, which influence round goby abundances. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Predicting PFAS in shallow soils in northern New England,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"This project leverages statewide shallow soil data and machine learning methods (boosted regression tree models) to predict PFAS in soils across Maine, New Hampshire, and Vermont.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"This project leverages statewide shallow soil data and machine learning methods (boosted regression tree models) to predict PFAS in soils across Maine, New Hampshire, and Vermont. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","this project leverages statewide shallow soil data and machine learning methods (boosted regression tree models) to predict pfas in soils across maine, new hampshire, and vermont. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Improving accuracy and precision of sonar-based estimates of fish abundance,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"USGS uses sonar to monitoring prey fish populations to support fisheries management decision making for the eight US Great Lakes states, Fish and Wildlife Service, and numerous tribes. Sonar-based estimates of fish abundance are prone to inaccuracies that can limit their utility. New technologies are being strategically cultivated to improve the accuracy and precision of USGS's annual prey fish abundance estimates. Artificial intelligence is being used to accelerate data processing.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Sonar transect data collected by conventional vessels and uncrewed surface vehicles,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"USGS uses sonar to monitoring prey fish populations to support fisheries management decision making for the eight US Great Lakes states, Fish and Wildlife Service, and numerous tribes. Sonar-based estimates of fish abundance are prone to inaccuracies that can limit their utility. New technologies are being strategically cultivated to improve the accuracy and precision of USGS's annual prey fish abundance estimates. Artificial intelligence is being used to accelerate data processing. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","usgs uses sonar to monitoring prey fish populations to support fisheries management decision making for the eight us great lakes states, fish and wildlife service, and numerous tribes. sonar-based estimates of fish abundance are prone to inaccuracies that can limit their utility. new technologies are being strategically cultivated to improve the accuracy and precision of usgs's annual prey fish abundance estimates. artificial intelligence is being used to accelerate data processing. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Classifying GPS data to understand flight behavior of birds.,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,This project will help understand under what circumstances eagles are more likely to collide with wind turbines.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,This project will help understand under what circumstances eagles are more likely to collide with wind turbines. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,this project will help understand under what circumstances eagles are more likely to collide with wind turbines. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Machine learning-based landscape feature classification using satellite and airborne imagery,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,We use existing machine learning classifiers on satellite and airborne imagery to enhance the accuracy of habitat and land cover classifications.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,We use existing machine learning classifiers on satellite and airborne imagery to enhance the accuracy of habitat and land cover classifications. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,we use existing machine learning classifiers on satellite and airborne imagery to enhance the accuracy of habitat and land cover classifications. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Predicting PFAS occurrence in groundwater using machine learning,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"This project leverages USGS groundwater data and machine learning methods (boosted regression tree models) to predict PFAS occurrence in groundwater at the depths of drinking water supplies across the conterminous U.S. A paper with the results of this effort was published in early FY25. A second iteration of this project will aim to predict concentration ranges instead of only occurrence, and commenced at the beginning of FY25.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"This project leverages USGS groundwater data and machine learning methods (boosted regression tree models) to predict PFAS occurrence in groundwater at the depths of drinking water supplies across the conterminous U.S. A paper with the results of this effort was published in early FY25. A second iteration of this project will aim to predict concentration ranges instead of only occurrence, and commenced at the beginning of FY25. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","this project leverages usgs groundwater data and machine learning methods (boosted regression tree models) to predict pfas occurrence in groundwater at the depths of drinking water supplies across the conterminous u.s. a paper with the results of this effort was published in early fy25. a second iteration of this project will aim to predict concentration ranges instead of only occurrence, and commenced at the beginning of fy25. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Machine Learning Image Classification,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"The mission of the PLACE (Patterns in the Landscape - Analyses of Cause and Effect) project is to inform land managers, planners and researchers about historical and current changes to human and natural environments. PLACE focuses on floods, droughts, and fires, which are increasing in severity, extent and frequency around the globe. The PLACE project utilizes random forest machine learning to classify wetlands and soil moisture at large scales, and to quantify causal processes behind wildfire.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"The mission of the PLACE (Patterns in the Landscape - Analyses of Cause and Effect) project is to inform land managers, planners and researchers about historical and current changes to human and natural environments. PLACE focuses on floods, droughts, and fires, which are increasing in severity, extent and frequency around the globe. The PLACE project utilizes random forest machine learning to classify wetlands and soil moisture at large scales, and to quantify causal processes behind wildfire. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","the mission of the place (patterns in the landscape - analyses of cause and effect) project is to inform land managers, planners and researchers about historical and current changes to human and natural environments. place focuses on floods, droughts, and fires, which are increasing in severity, extent and frequency around the globe. the place project utilizes random forest machine learning to classify wetlands and soil moisture at large scales, and to quantify causal processes behind wildfire. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Zero shot segmentation to expedite Quaternary geologic mapping,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,The construction of detailed geologic maps requires a lot of manual GIS data input to outline the extent of interpreted geologic features. We are working to extend and adapt the Segment Anything Model for identification of Quaternary geologic features in order to expedite the process of creating GIS data for geologic maps.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,The construction of detailed geologic maps requires a lot of manual GIS data input to outline the extent of interpreted geologic features. We are working to extend and adapt the Segment Anything Model for identification of Quaternary geologic features in order to expedite the process of creating GIS data for geologic maps. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,the construction of detailed geologic maps requires a lot of manual gis data input to outline the extent of interpreted geologic features. we are working to extend and adapt the segment anything model for identification of quaternary geologic features in order to expedite the process of creating gis data for geologic maps. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Inventorying landforms with convolutional neural networks,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"LiDAR derived topographic data images earth's surface in unprecedented detail and is available across most of the country. This data reveals landforms relevant for understanding many scientific phenomena (e.g., patterned ground) and natural hazards (e.g., karst). We are developing simple pipelines to train and deploy convolutional neural networks on high resolution topographic data in order to efficiently identify and inventory these features.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"LiDAR derived topographic data images earth's surface in unprecedented detail and is available across most of the country. This data reveals landforms relevant for understanding many scientific phenomena (e.g., patterned ground) and natural hazards (e.g., karst). We are developing simple pipelines to train and deploy convolutional neural networks on high resolution topographic data in order to efficiently identify and inventory these features. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","lidar derived topographic data images earth's surface in unprecedented detail and is available across most of the country. this data reveals landforms relevant for understanding many scientific phenomena (e.g., patterned ground) and natural hazards (e.g., karst). we are developing simple pipelines to train and deploy convolutional neural networks on high resolution topographic data in order to efficiently identify and inventory these features. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Tracking wetlands and water movement across watersheds,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Accurate prediction of flood and drought impacts requires understanding upstream surface water storage dynamics and storage capacity. We use machine learning algorithms to classify satellite imagery into open and vegetated water extent. We then use machine learning and deep learning algorithms to relate daily river discharge to meteorology and surface water storage dynamics. This work is funded by USEPA, Office of Research and Development.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Accurate prediction of flood and drought impacts requires understanding upstream surface water storage dynamics and storage capacity. We use machine learning algorithms to classify satellite imagery into open and vegetated water extent. We then use machine learning and deep learning algorithms to relate daily river discharge to meteorology and surface water storage dynamics. This work is funded by USEPA, Office of Research and Development. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","accurate prediction of flood and drought impacts requires understanding upstream surface water storage dynamics and storage capacity. we use machine learning algorithms to classify satellite imagery into open and vegetated water extent. we then use machine learning and deep learning algorithms to relate daily river discharge to meteorology and surface water storage dynamics. this work is funded by usepa, office of research and development. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
"Everglades-Flux, Digital Surveys",Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,We are creating a program that can automatically process Normalized Difference Vegetation Index images and come up with a true value of live vegetation. It will also gap fill missing data via AI/ML programs.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,We are creating a program that can automatically process Normalized Difference Vegetation Index images and come up with a true value of live vegetation. It will also gap fill missing data via AI/ML programs. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,we are creating a program that can automatically process normalized difference vegetation index images and come up with a true value of live vegetation. it will also gap fill missing data via ai/ml programs. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Cooperative Agreement Oakridge National Lab-USGS Research:  Using Artificial Intelligence to Improve,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"The research objectives of this project will focus on developing workflow processes and tools that will form the framework for implementation of data management and improvements to data management lifecycles. This includes removing dependencies on personnel to perform manual tasks that enable discovery, access, and formatting of potentially desirable data to users, and instead automate the long-term management of those data to ensure that they are readily usable under various automation scenarios. 

Specific objectives of this project aligned with AI use cases include:
1.  Research and design an algorithm to characterize and visualize data contents in USGS data repositories and DOE ARM holdings by adapting AI and machine learning approaches.
2.  AI Tool development to help scientists generate standardized metadata (ISO/FGDC)
3.  Using AI principles, automate methods for performing FAIR (Findable, Accessible, Interoperable, and Reusable) data assessments using training data developed for USGS's State of
the Data Rubric",see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"The research objectives of this project will focus on developing workflow processes and tools that will form the framework for implementation of data management and improvements to data management lifecycles. This includes removing dependencies on personnel to perform manual tasks that enable discovery, access, and formatting of potentially desirable data to users, and instead automate the long-term management of those data to ensure that they are readily usable under various automation scenarios. 

Specific objectives of this project aligned with AI use cases include:
1.  Research and design an algorithm to characterize and visualize data contents in USGS data repositories and DOE ARM holdings by adapting AI and machine learning approaches.
2.  AI Tool development to help scientists generate standardized metadata (ISO/FGDC)
3.  Using AI principles, automate methods for performing FAIR (Findable, Accessible, Interoperable, and Reusable) data assessments using training data developed for USGS's State of
the Data Rubric . see description","the research objectives of this project will focus on developing workflow processes and tools that will form the framework for implementation of data management and improvements to data management lifecycles. this includes removing dependencies on personnel to perform manual tasks that enable discovery, access, and formatting of potentially desirable data to users, and instead automate the long-term management of those data to ensure that they are readily usable under various automation scenarios. specific objectives of this project aligned with ai use cases include: 1. research and design an algorithm to characterize and visualize data contents in usgs data repositories and doe arm holdings by adapting ai and machine learning approaches. 2. ai tool development to help scientists generate standardized metadata (iso/fgdc) 3. using ai principles, automate methods for performing fair (findable, accessible, interoperable, and reusable) data assessments using training data developed for usgs's state of the data rubric . see description"
Improved earthquake detection for research studies,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Deep learning methods are being used to improve detection of earthquakes to provide more complete, high-resolution catalogs that are used in research to better understand earthquake occurrence, rupture processes and seismic hazard.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Deep learning methods are being used to improve detection of earthquakes to provide more complete, high-resolution catalogs that are used in research to better understand earthquake occurrence, rupture processes and seismic hazard. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","deep learning methods are being used to improve detection of earthquakes to provide more complete, high-resolution catalogs that are used in research to better understand earthquake occurrence, rupture processes and seismic hazard. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Machine learning for tsunami source zones,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"State-of-the=art tsunami hazard analysis for coastal communities and infrastructure is computationally demanding. In order to select the most representative source zone among the thousands of possible offshore earthquake ruptures, unsupervised machine learning is needed.  Using this type of AI will more accurately determine representative earthquake ruptures from offshore tsunami wave heights than previous interpretive methods that are subject to significant uncertainty. The result will yield a transparent and consistent methodology for tsunami source selection.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"State-of-the=art tsunami hazard analysis for coastal communities and infrastructure is computationally demanding. In order to select the most representative source zone among the thousands of possible offshore earthquake ruptures, unsupervised machine learning is needed.  Using this type of AI will more accurately determine representative earthquake ruptures from offshore tsunami wave heights than previous interpretive methods that are subject to significant uncertainty. The result will yield a transparent and consistent methodology for tsunami source selection. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","state-of-the=art tsunami hazard analysis for coastal communities and infrastructure is computationally demanding. in order to select the most representative source zone among the thousands of possible offshore earthquake ruptures, unsupervised machine learning is needed. using this type of ai will more accurately determine representative earthquake ruptures from offshore tsunami wave heights than previous interpretive methods that are subject to significant uncertainty. the result will yield a transparent and consistent methodology for tsunami source selection. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Mendenhall postdoctoral fellow using machine learning,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"The Global Marine Minerals Project, with the Coastal and Marine Hazards and Resources Program, currently employs a Mendenhall postdoctoral fellow who is using a machine learning approach to predict the composition of seafloor massive sulfide deposits. The fellow proposed this work in their submission in 2023, which was accepted for funding. They are currently executing the work and preparing publicaitons.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"The Global Marine Minerals Project, with the Coastal and Marine Hazards and Resources Program, currently employs a Mendenhall postdoctoral fellow who is using a machine learning approach to predict the composition of seafloor massive sulfide deposits. The fellow proposed this work in their submission in 2023, which was accepted for funding. They are currently executing the work and preparing publicaitons. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","the global marine minerals project, with the coastal and marine hazards and resources program, currently employs a mendenhall postdoctoral fellow who is using a machine learning approach to predict the composition of seafloor massive sulfide deposits. the fellow proposed this work in their submission in 2023, which was accepted for funding. they are currently executing the work and preparing publicaitons. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Critical Mineral Assessment with AI Support (CriticalMAAS),Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"An AI-assisted workflow could enable the USGS to accomplish its mission, produce high-quality derivative products from raw input data, and deliver timely assessments that reduce exploration risk and support decisions affecting the management of strategic domestic resources.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"An AI-assisted workflow could enable the USGS to accomplish its mission, produce high-quality derivative products from raw input data, and deliver timely assessments that reduce exploration risk and support decisions affecting the management of strategic domestic resources. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","an ai-assisted workflow could enable the usgs to accomplish its mission, produce high-quality derivative products from raw input data, and deliver timely assessments that reduce exploration risk and support decisions affecting the management of strategic domestic resources. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Shoreline modeling,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"We're trying to evaluate the prospect of using AI/ML methods/models (e.g., LSTM, CNN, Transformers) to predict shoreline evolution and compare their accuracy to traditional physics-based models.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"We're trying to evaluate the prospect of using AI/ML methods/models (e.g., LSTM, CNN, Transformers) to predict shoreline evolution and compare their accuracy to traditional physics-based models. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","we're trying to evaluate the prospect of using ai/ml methods/models (e.g., lstm, cnn, transformers) to predict shoreline evolution and compare their accuracy to traditional physics-based models. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Cell Phone Application for Oil Spill Detection,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,We would like to develop a model that can be used to interpret cell phone images to predict oil in environmental samples. The tool can be rapidly deployed for use in the field by the oil spill responder community.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,We would like to develop a model that can be used to interpret cell phone images to predict oil in environmental samples. The tool can be rapidly deployed for use in the field by the oil spill responder community. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,we would like to develop a model that can be used to interpret cell phone images to predict oil in environmental samples. the tool can be rapidly deployed for use in the field by the oil spill responder community. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Wave runup and total water level observations from time series imagery at several sites with varying,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,Tool is used for separation (segmentation) of land and water in images. The resulting mask is used to calculate water levels. Tool will be used to compare to forecasted water levels and may be displayed on USGS webpages,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Tool is used for separation (segmentation) of land and water in images. The resulting mask is used to calculate water levels. Tool will be used to compare to forecasted water levels and may be displayed on USGS webpages . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,tool is used for separation (segmentation) of land and water in images. the resulting mask is used to calculate water levels. tool will be used to compare to forecasted water levels and may be displayed on usgs webpages . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Development of a Ploidy Distinction Application: A Machine Learning Approach for Discriminating Trip,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"A machine learning approach is being researched as a way for fisheries personnel in the field to determine the ploidy of wild-caught invasive carp. The use of machine learning in biomedical imaging accumulates substantial volumes of data via digital images, applies computational tools for processing these data, and can combine images from various microscopes by enhancing and equalizing image resolution. Thus, a computational model was trained with selected brightfield microscopic images of whole blood smears made from known diploid and triploid Grass Carp. Digital images were generated by using two types of microscopes: (1) a laboratory-dedicated, high caliber scope, and (2) a portable microscope meant for use in the field.",see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"A machine learning approach is being researched as a way for fisheries personnel in the field to determine the ploidy of wild-caught invasive carp. The use of machine learning in biomedical imaging accumulates substantial volumes of data via digital images, applies computational tools for processing these data, and can combine images from various microscopes by enhancing and equalizing image resolution. Thus, a computational model was trained with selected brightfield microscopic images of whole blood smears made from known diploid and triploid Grass Carp. Digital images were generated by using two types of microscopes: (1) a laboratory-dedicated, high caliber scope, and (2) a portable microscope meant for use in the field. . see description","a machine learning approach is being researched as a way for fisheries personnel in the field to determine the ploidy of wild-caught invasive carp. the use of machine learning in biomedical imaging accumulates substantial volumes of data via digital images, applies computational tools for processing these data, and can combine images from various microscopes by enhancing and equalizing image resolution. thus, a computational model was trained with selected brightfield microscopic images of whole blood smears made from known diploid and triploid grass carp. digital images were generated by using two types of microscopes: (1) a laboratory-dedicated, high caliber scope, and (2) a portable microscope meant for use in the field. . see description"
Storm Induced Erosion Response Network,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,Provide the next generation of Total Water Level and Coastal Change Forecast,see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Provide the next generation of Total Water Level and Coastal Change Forecast . see description,provide the next generation of total water level and coastal change forecast . see description
Coastal Ecosystem Prediction System,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"The USGS is partnering with NOAA National Centers for Coastal Ocean Science to co-develop an integrated, national-scale framework for projecting wetland vulnerability to sea level rise. Multi-model ensemble predictions will show a range of possible future conditions and inform the protection of coastal communities, economies and their natural resources.",see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"The USGS is partnering with NOAA National Centers for Coastal Ocean Science to co-develop an integrated, national-scale framework for projecting wetland vulnerability to sea level rise. Multi-model ensemble predictions will show a range of possible future conditions and inform the protection of coastal communities, economies and their natural resources. . see description","the usgs is partnering with noaa national centers for coastal ocean science to co-develop an integrated, national-scale framework for projecting wetland vulnerability to sea level rise. multi-model ensemble predictions will show a range of possible future conditions and inform the protection of coastal communities, economies and their natural resources. . see description"
National Wildlife Disease Database,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"The National Wildlife Health Center (NWHC) has contracted the Pacific Northwest National Laboratory (PNNL) to build a national wildlife disease database (NWDD).  Funded through the American Rescue Plan Act of 2021 (ARPA), the NWDD will bring together various wildlife health data streams across informational domains (i.e., laboratory results, environmental observations, news media, etc.) to provide situational awareness and advanced analytics to natural resource authorities around the country.  As part of the NWDD, PNNL plans to integrate their Canvas software.  Canvas is a machine learning, AI, and data science tool that can visualize and contextualize information from one or more sources.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"The National Wildlife Health Center (NWHC) has contracted the Pacific Northwest National Laboratory (PNNL) to build a national wildlife disease database (NWDD).  Funded through the American Rescue Plan Act of 2021 (ARPA), the NWDD will bring together various wildlife health data streams across informational domains (i.e., laboratory results, environmental observations, news media, etc.) to provide situational awareness and advanced analytics to natural resource authorities around the country.  As part of the NWDD, PNNL plans to integrate their Canvas software.  Canvas is a machine learning, AI, and data science tool that can visualize and contextualize information from one or more sources. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","the national wildlife health center (nwhc) has contracted the pacific northwest national laboratory (pnnl) to build a national wildlife disease database (nwdd). funded through the american rescue plan act of 2021 (arpa), the nwdd will bring together various wildlife health data streams across informational domains (i.e., laboratory results, environmental observations, news media, etc.) to provide situational awareness and advanced analytics to natural resource authorities around the country. as part of the nwdd, pnnl plans to integrate their canvas software. canvas is a machine learning, ai, and data science tool that can visualize and contextualize information from one or more sources. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Sediment Transport in Coastal Environments (funded by San Francisco Bay-Delta PES),Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,Machine learning based time-series imputation of oceanographic time-series,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Machine learning based time-series imputation of oceanographic time-series . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,machine learning based time-series imputation of oceanographic time-series . outputs were not documented and doi will update the use cases once additional data is collected from submitter
"Machine learning based shoreline time-series imputation, classification and forecasting (time-series",Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Supports basic data generation and QC/QA procedures, for large scale and short-term forecasting of shoreline trends",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Supports basic data generation and QC/QA procedures, for large scale and short-term forecasting of shoreline trends . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","supports basic data generation and qc/qa procedures, for large scale and short-term forecasting of shoreline trends . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
National Oceanographic Partnership Program (NOPP),Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,Machine Learning based coastal sediments assessment and prediction,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Machine Learning based coastal sediments assessment and prediction . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,machine learning based coastal sediments assessment and prediction . outputs were not documented and doi will update the use cases once additional data is collected from submitter
RSCC and TCA projects.,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Machine Learning (ML) methods for identifying, assessing, and quantifying coastal features and habitats and coastal change hazards.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Machine Learning (ML) methods for identifying, assessing, and quantifying coastal features and habitats and coastal change hazards. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","machine learning (ml) methods for identifying, assessing, and quantifying coastal features and habitats and coastal change hazards. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Machinine Learning based shoreline detection and sea ice dynamics using coastal cameras,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Supports basic data generation and QC/QA procedures, for large scale and short-term forecasting of shoreline trends and sea-ice dynamics in coastal environments",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Supports basic data generation and QC/QA procedures, for large scale and short-term forecasting of shoreline trends and sea-ice dynamics in coastal environments . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","supports basic data generation and qc/qa procedures, for large scale and short-term forecasting of shoreline trends and sea-ice dynamics in coastal environments . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Using Machine Learning in USGS StreamStats to make suspended sediment and bedload predictions,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,A tool for resource managers who need estimates of suspended sediment and bedload in Minnesota rivers without sampling data.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Operation and Maintenance,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,A tool for resource managers who need estimates of suspended sediment and bedload in Minnesota rivers without sampling data. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,a tool for resource managers who need estimates of suspended sediment and bedload in minnesota rivers without sampling data. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Merbok Supplemental,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Machine Learning based shoreline detection and mapping, automated data suitability analyses from satellite imagery",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Machine Learning based shoreline detection and mapping, automated data suitability analyses from satellite imagery . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","machine learning based shoreline detection and mapping, automated data suitability analyses from satellite imagery . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Annual National Land Cover Database Deep Learning,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,Using geospatial artificial intelligence methods to map annual land cover and other land surface characteristics for the Nation.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Operation and Maintenance,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Using geospatial artificial intelligence methods to map annual land cover and other land surface characteristics for the Nation. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,using geospatial artificial intelligence methods to map annual land cover and other land surface characteristics for the nation. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
"Event and sequence (life-stage, behavior, activity, movement modality) identification, segmentation,",Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Animal movement data reflect various activities from individuals, including the use and availability of habitats and resources necessary for survival, behaviors such as feeding, flying, or nesting, and interactions with neighbors, predators, and prey. These biological patterns and processes are not readily observable from remotely tracked individuals and require identification from complex data streams. Complete representation of desired activites and behaviors may require multiple individual tools to satisfy all objectives.",see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Animal movement data reflect various activities from individuals, including the use and availability of habitats and resources necessary for survival, behaviors such as feeding, flying, or nesting, and interactions with neighbors, predators, and prey. These biological patterns and processes are not readily observable from remotely tracked individuals and require identification from complex data streams. Complete representation of desired activites and behaviors may require multiple individual tools to satisfy all objectives. . see description","animal movement data reflect various activities from individuals, including the use and availability of habitats and resources necessary for survival, behaviors such as feeding, flying, or nesting, and interactions with neighbors, predators, and prey. these biological patterns and processes are not readily observable from remotely tracked individuals and require identification from complex data streams. complete representation of desired activites and behaviors may require multiple individual tools to satisfy all objectives. . see description"
Data-driven approaches to filling missing time-series data within the San Francisco Bay-Delta,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"In measurements of natural systems, incomplete time-series data are the rule, not the exception. Environmental time-series data may suffer from gaps at a variety of time scales, significantly reducing the number of observations to understand phenomena, identify change, calibrate models, and predict future behavior. Data may be missing because of sensor degradation (e.g., biofouling, mechanical issues, power failure), failure to meet basic quality assurance checks, or resampling of paired observations to a common timestamp.

Decades of water-quality time-series data (e.g., turbidity, salinity, temperature) collected throughout the San Francisco Bay-Delta (SFBD) by a variety of agencies including USGS, CA DWR, USBR, USFWS, and USACE are used as a critical indicator of estuary health. These data are no exception to the rule and contain numerous gaps. While bad data points can be relatively straightforward to identify (e.g., as anomalously high or low values), they are challenging to replace and are often flagged and left blank, which can bias long-term observational records.

We will test and develop several (~5) proposed methods to fill gaps in water-quality time-series data and quantify the uncertainty in those estimates. The methods will range in complexity, from linear regression to machine learning and deep learning approaches (Lepot and others, 2017), and will characterize uncertainty in the filled data (Cox and others, 2003).  For this proposal, we will initially restrict our scope to turbidity data, because of the relatively high biofouling rate of the optical sensors typically used to measure turbidity, as well as its ecological significance. In addition, we are already actively implementing one of the methods to fill gaps in turbidity data collected in a south S.F. Bay salt-marsh tidal creek (Figure 1) as part of a project investigating temporal variability in sediment delivery to marshes funded by the S.F. Bay Regional Monitoring Program.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"In measurements of natural systems, incomplete time-series data are the rule, not the exception. Environmental time-series data may suffer from gaps at a variety of time scales, significantly reducing the number of observations to understand phenomena, identify change, calibrate models, and predict future behavior. Data may be missing because of sensor degradation (e.g., biofouling, mechanical issues, power failure), failure to meet basic quality assurance checks, or resampling of paired observations to a common timestamp.

Decades of water-quality time-series data (e.g., turbidity, salinity, temperature) collected throughout the San Francisco Bay-Delta (SFBD) by a variety of agencies including USGS, CA DWR, USBR, USFWS, and USACE are used as a critical indicator of estuary health. These data are no exception to the rule and contain numerous gaps. While bad data points can be relatively straightforward to identify (e.g., as anomalously high or low values), they are challenging to replace and are often flagged and left blank, which can bias long-term observational records.

We will test and develop several (~5) proposed methods to fill gaps in water-quality time-series data and quantify the uncertainty in those estimates. The methods will range in complexity, from linear regression to machine learning and deep learning approaches (Lepot and others, 2017), and will characterize uncertainty in the filled data (Cox and others, 2003).  For this proposal, we will initially restrict our scope to turbidity data, because of the relatively high biofouling rate of the optical sensors typically used to measure turbidity, as well as its ecological significance. In addition, we are already actively implementing one of the methods to fill gaps in turbidity data collected in a south S.F. Bay salt-marsh tidal creek (Figure 1) as part of a project investigating temporal variability in sediment delivery to marshes funded by the S.F. Bay Regional Monitoring Program. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","in measurements of natural systems, incomplete time-series data are the rule, not the exception. environmental time-series data may suffer from gaps at a variety of time scales, significantly reducing the number of observations to understand phenomena, identify change, calibrate models, and predict future behavior. data may be missing because of sensor degradation (e.g., biofouling, mechanical issues, power failure), failure to meet basic quality assurance checks, or resampling of paired observations to a common timestamp. decades of water-quality time-series data (e.g., turbidity, salinity, temperature) collected throughout the san francisco bay-delta (sfbd) by a variety of agencies including usgs, ca dwr, usbr, usfws, and usace are used as a critical indicator of estuary health. these data are no exception to the rule and contain numerous gaps. while bad data points can be relatively straightforward to identify (e.g., as anomalously high or low values), they are challenging to replace and are often flagged and left blank, which can bias long-term observational records. we will test and develop several (~5) proposed methods to fill gaps in water-quality time-series data and quantify the uncertainty in those estimates. the methods will range in complexity, from linear regression to machine learning and deep learning approaches (lepot and others, 2017), and will characterize uncertainty in the filled data (cox and others, 2003). for this proposal, we will initially restrict our scope to turbidity data, because of the relatively high biofouling rate of the optical sensors typically used to measure turbidity, as well as its ecological significance. in addition, we are already actively implementing one of the methods to fill gaps in turbidity data collected in a south s.f. bay salt-marsh tidal creek (figure 1) as part of a project investigating temporal variability in sediment delivery to marshes funded by the s.f. bay regional monitoring program. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Seabird and Marine Mammal Surveys Near Potential Renewable Energy Sites Offshore Central and Souther,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"  Using rapidly developing machine- learning (ML) techniques, the USGS WERC team is developing new methods to automate the detection and counts of seabirds and marine mammals from digital imagery.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Using rapidly developing machine- learning (ML) techniques, the USGS WERC team is developing new methods to automate the detection and counts of seabirds and marine mammals from digital imagery. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","using rapidly developing machine- learning (ml) techniques, the usgs werc team is developing new methods to automate the detection and counts of seabirds and marine mammals from digital imagery. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Data-driven approaches to filling missing time-series data within the San Francisco Bay-Delta,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"In measurements of natural systems, incomplete time-series data are the rule, not the exception. Environmental time-series data may suffer from gaps at a variety of time scales, significantly reducing the number of observations to understand phenomena, identify change, calibrate models, and predict future behavior. Data may be missing because of sensor degradation (e.g., biofouling, mechanical issues, power failure), failure to meet basic quality assurance checks, or resampling of paired observations to a common timestamp.

Decades of water-quality time-series data (e.g., turbidity, salinity, temperature) collected throughout the San Francisco Bay-Delta (SFBD) by a variety of agencies including USGS, CA DWR, USBR, USFWS, and USACE are used as a critical indicator of estuary health. These data are no exception to the rule and contain numerous gaps. While bad data points can be relatively straightforward to identify (e.g., as anomalously high or low values), they are challenging to replace and are often flagged and left blank, which can bias long-term observational records.

We will test and develop several (~5) proposed methods to fill gaps in water-quality time-series data and quantify the uncertainty in those estimates. The methods will range in complexity, from linear regression to machine learning and deep learning approaches (Lepot and others, 2017), and will characterize uncertainty in the filled data (Cox and others, 2003).  For this proposal, we will initially restrict our scope to turbidity data, because of the relatively high biofouling rate of the optical sensors typically used to measure turbidity, as well as its ecological significance. In addition, we are already actively implementing one of the methods to fill gaps in turbidity data collected in a south S.F. Bay salt-marsh tidal creek (Figure 1) as part of a project investigating temporal variability in sediment delivery to marshes funded by the S.F. Bay Regional Monitoring Program.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"In measurements of natural systems, incomplete time-series data are the rule, not the exception. Environmental time-series data may suffer from gaps at a variety of time scales, significantly reducing the number of observations to understand phenomena, identify change, calibrate models, and predict future behavior. Data may be missing because of sensor degradation (e.g., biofouling, mechanical issues, power failure), failure to meet basic quality assurance checks, or resampling of paired observations to a common timestamp.

Decades of water-quality time-series data (e.g., turbidity, salinity, temperature) collected throughout the San Francisco Bay-Delta (SFBD) by a variety of agencies including USGS, CA DWR, USBR, USFWS, and USACE are used as a critical indicator of estuary health. These data are no exception to the rule and contain numerous gaps. While bad data points can be relatively straightforward to identify (e.g., as anomalously high or low values), they are challenging to replace and are often flagged and left blank, which can bias long-term observational records.

We will test and develop several (~5) proposed methods to fill gaps in water-quality time-series data and quantify the uncertainty in those estimates. The methods will range in complexity, from linear regression to machine learning and deep learning approaches (Lepot and others, 2017), and will characterize uncertainty in the filled data (Cox and others, 2003).  For this proposal, we will initially restrict our scope to turbidity data, because of the relatively high biofouling rate of the optical sensors typically used to measure turbidity, as well as its ecological significance. In addition, we are already actively implementing one of the methods to fill gaps in turbidity data collected in a south S.F. Bay salt-marsh tidal creek (Figure 1) as part of a project investigating temporal variability in sediment delivery to marshes funded by the S.F. Bay Regional Monitoring Program. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","in measurements of natural systems, incomplete time-series data are the rule, not the exception. environmental time-series data may suffer from gaps at a variety of time scales, significantly reducing the number of observations to understand phenomena, identify change, calibrate models, and predict future behavior. data may be missing because of sensor degradation (e.g., biofouling, mechanical issues, power failure), failure to meet basic quality assurance checks, or resampling of paired observations to a common timestamp. decades of water-quality time-series data (e.g., turbidity, salinity, temperature) collected throughout the san francisco bay-delta (sfbd) by a variety of agencies including usgs, ca dwr, usbr, usfws, and usace are used as a critical indicator of estuary health. these data are no exception to the rule and contain numerous gaps. while bad data points can be relatively straightforward to identify (e.g., as anomalously high or low values), they are challenging to replace and are often flagged and left blank, which can bias long-term observational records. we will test and develop several (~5) proposed methods to fill gaps in water-quality time-series data and quantify the uncertainty in those estimates. the methods will range in complexity, from linear regression to machine learning and deep learning approaches (lepot and others, 2017), and will characterize uncertainty in the filled data (cox and others, 2003). for this proposal, we will initially restrict our scope to turbidity data, because of the relatively high biofouling rate of the optical sensors typically used to measure turbidity, as well as its ecological significance. in addition, we are already actively implementing one of the methods to fill gaps in turbidity data collected in a south s.f. bay salt-marsh tidal creek (figure 1) as part of a project investigating temporal variability in sediment delivery to marshes funded by the s.f. bay regional monitoring program. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Image classification from images on robotic platforms,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Tool identifies different terrains based on training on labeled images. Application developed for NASA Mars rovers, but applicable elsewhere.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Tool identifies different terrains based on training on labeled images. Application developed for NASA Mars rovers, but applicable elsewhere. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","tool identifies different terrains based on training on labeled images. application developed for nasa mars rovers, but applicable elsewhere. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
ChatGPT to write Python scripts for ArcGIS Pro Maps to be CVD-Friendly,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"   I do not know how to code, but we are attempting to automate the process of changing colors of planetary geologic map units within ArcGIS pro so that they are all color-vision deficiency friendly. This process is very time-consuming when done by hand, so we're using ChatGPT to help write scripts about this.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"I do not know how to code, but we are attempting to automate the process of changing colors of planetary geologic map units within ArcGIS pro so that they are all color-vision deficiency friendly. This process is very time-consuming when done by hand, so we're using ChatGPT to help write scripts about this. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","i do not know how to code, but we are attempting to automate the process of changing colors of planetary geologic map units within arcgis pro so that they are all color-vision deficiency friendly. this process is very time-consuming when done by hand, so we're using chatgpt to help write scripts about this. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Analytical Chemistry & Geologic Resources Identification from Laser Spectroscopy,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Determining composition of geologic, mineral, and resource targets from laser-based spectroscopic platforms",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Determining composition of geologic, mineral, and resource targets from laser-based spectroscopic platforms . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","determining composition of geologic, mineral, and resource targets from laser-based spectroscopic platforms . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Analytical Chemistry & Geologic Resources Identification from X-rays,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,  Determining chemistry and rock composition from X-ray-based analytical chemistry platforms,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Determining chemistry and rock composition from X-ray-based analytical chemistry platforms . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,determining chemistry and rock composition from x-ray-based analytical chemistry platforms . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Retrieval-Augmented Generation Using USGS Documentation,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,Generative AI chatbots continue to make inroads across our lives. This project seeks to test creating and using a RAG model on USGS documentation with the ultimate goal of creating an internal only chat bot that has knowledge of internal USGS documentation.,see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Generative AI chatbots continue to make inroads across our lives. This project seeks to test creating and using a RAG model on USGS documentation with the ultimate goal of creating an internal only chat bot that has knowledge of internal USGS documentation. . see description,generative ai chatbots continue to make inroads across our lives. this project seeks to test creating and using a rag model on usgs documentation with the ultimate goal of creating an internal only chat bot that has knowledge of internal usgs documentation. . see description
Foundation Models to Advance Earth Science,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Advance the understanding of Earth's conditions and processes by developing and deploying generalist AI models (Foundation Models) trained on Earth Observations from field, suborbital, and orbital sensors. These models, along with the resulting insights, will empower scientists and land user managers to achieve more while also advancing the broader field of AI and machine learning science.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Advance the understanding of Earth's conditions and processes by developing and deploying generalist AI models (Foundation Models) trained on Earth Observations from field, suborbital, and orbital sensors. These models, along with the resulting insights, will empower scientists and land user managers to achieve more while also advancing the broader field of AI and machine learning science. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","advance the understanding of earth's conditions and processes by developing and deploying generalist ai models (foundation models) trained on earth observations from field, suborbital, and orbital sensors. these models, along with the resulting insights, will empower scientists and land user managers to achieve more while also advancing the broader field of ai and machine learning science. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Rangeland Condition Monitoring Assessment and Projection (RCMAP),Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Rangelands, comprised of open grasslands and/or shrublands, occupy huge swathes of land in the US, typically where climate and/or soils are too harsh for either forest or agriculture. Rangeland ecosystems provide critical wildlife habitat (e.g., greater sage grouse, pronghorn, black-footed ferret), forage for livestock, carbon sequestration, provision of water resources, and recreational opportunities. At the same time, rangelands are vulnerable to climate change, fire, and anthropogenic disturbances. The arid-semiarid climate in most rangelands fluctuates widely, impacting livestock forage availability, wildlife habitat, and water resources. Many of these changes can be subtle or evolve over long time periods, responding to climate, anthropogenic, and disturbance driving forces. To address the need for long-term tracking of vegetation change, scientists from the USGS and Bureau of Land Management (BLM) developed the Rangeland Condition Monitoring Assessment and Projection (RCMAP) project. RCMAP provides maps of vegetation cover at yearly time-steps, a critical refence to advancing science in the BLM and assessing Landscape Health standards. RCMAP quantifies the percent cover of ten rangeland components (annual herbaceous, bare ground, herbaceous, litter, non-sagebrush shrub, perennial herbaceous, sagebrush, shrub, and tree cover and shrub height) at yearly time-steps across the western U.S. using field training data, Landsat imagery, and machine learning.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Operation and Maintenance,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Rangelands, comprised of open grasslands and/or shrublands, occupy huge swathes of land in the US, typically where climate and/or soils are too harsh for either forest or agriculture. Rangeland ecosystems provide critical wildlife habitat (e.g., greater sage grouse, pronghorn, black-footed ferret), forage for livestock, carbon sequestration, provision of water resources, and recreational opportunities. At the same time, rangelands are vulnerable to climate change, fire, and anthropogenic disturbances. The arid-semiarid climate in most rangelands fluctuates widely, impacting livestock forage availability, wildlife habitat, and water resources. Many of these changes can be subtle or evolve over long time periods, responding to climate, anthropogenic, and disturbance driving forces. To address the need for long-term tracking of vegetation change, scientists from the USGS and Bureau of Land Management (BLM) developed the Rangeland Condition Monitoring Assessment and Projection (RCMAP) project. RCMAP provides maps of vegetation cover at yearly time-steps, a critical refence to advancing science in the BLM and assessing Landscape Health standards. RCMAP quantifies the percent cover of ten rangeland components (annual herbaceous, bare ground, herbaceous, litter, non-sagebrush shrub, perennial herbaceous, sagebrush, shrub, and tree cover and shrub height) at yearly time-steps across the western U.S. using field training data, Landsat imagery, and machine learning. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","rangelands, comprised of open grasslands and/or shrublands, occupy huge swathes of land in the us, typically where climate and/or soils are too harsh for either forest or agriculture. rangeland ecosystems provide critical wildlife habitat (e.g., greater sage grouse, pronghorn, black-footed ferret), forage for livestock, carbon sequestration, provision of water resources, and recreational opportunities. at the same time, rangelands are vulnerable to climate change, fire, and anthropogenic disturbances. the arid-semiarid climate in most rangelands fluctuates widely, impacting livestock forage availability, wildlife habitat, and water resources. many of these changes can be subtle or evolve over long time periods, responding to climate, anthropogenic, and disturbance driving forces. to address the need for long-term tracking of vegetation change, scientists from the usgs and bureau of land management (blm) developed the rangeland condition monitoring assessment and projection (rcmap) project. rcmap provides maps of vegetation cover at yearly time-steps, a critical refence to advancing science in the blm and assessing landscape health standards. rcmap quantifies the percent cover of ten rangeland components (annual herbaceous, bare ground, herbaceous, litter, non-sagebrush shrub, perennial herbaceous, sagebrush, shrub, and tree cover and shrub height) at yearly time-steps across the western u.s. using field training data, landsat imagery, and machine learning. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Global Food Security-Support Analysis Data (GFSAD) Project,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Climate variability and ballooning populations are putting unprecedented pressure on agricultural croplands and their water use, which are vital for ensuring global food and water security in the twenty-first century. In addition, the COVID-19 pandemic, military conflicts, and changing diets have added to looming global food insecurity. Therefore, there is a critical need to produce consistent and accurate global cropland products at fine spatial resolution (e.g., farm-scale, 30m or better), which are generated consistently, accurately, and routinely (e.g., every year).

Therefore, the overarching goal of this continuity NASA MEaSUREs (Making Earth System Data Records for Use in Research Environments) proposal is to develop a comprehensive global food security-support analysis data (GFSAD) project that will produce multiple cropland models, maps, and monitoring tools leading to a wide array of products using machine learning algorithms (MLAs), Artificial Intelligence (AI), and satellite sensor big-data analytics through cloud-computing. The GFSAD project, will produce four distinct Landsat-derived global cropland products. These products are:
1. Global Cropland Extent Product @ 30m (LGCEP30-2020, LGCEP30-2025).
2. Global Rainfed and Irrigated Product @ 30m (LGRIP30-2020, LGRIP30-2025).
3. Global Cropping Intensity Product @ 30m (LGCIP30-2020 & LGRIP30-2025) &
4. Global Crop Type Product @ 30m for USA, Canada, and India (LGCTY30-2020USACAN, LGCTY30-2025USACAN; LGCTY30-2020India, LGCTY30-2025India).

The GFSAD Data is released through NASA's LP DAAC (links below):
https://lpdaac.usgs.gov/news/release-of-gfsad-30-meter-cropland-extent-products/
https://lpdaac.usgs.gov/news/release-of-lgrip30-data-product/
https://lpdaac.usgs.gov/products/gfsad1kcdv001/
https://lpdaac.usgs.gov/products/gfsad1kcmv001/

GFSAD Data can be visualized at:
https://www.usgs.gov/apps/croplands/app/map

GFSAD website for further information:
www.usgs.gov/wgsc/gfsad30",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Climate variability and ballooning populations are putting unprecedented pressure on agricultural croplands and their water use, which are vital for ensuring global food and water security in the twenty-first century. In addition, the COVID-19 pandemic, military conflicts, and changing diets have added to looming global food insecurity. Therefore, there is a critical need to produce consistent and accurate global cropland products at fine spatial resolution (e.g., farm-scale, 30m or better), which are generated consistently, accurately, and routinely (e.g., every year).

Therefore, the overarching goal of this continuity NASA MEaSUREs (Making Earth System Data Records for Use in Research Environments) proposal is to develop a comprehensive global food security-support analysis data (GFSAD) project that will produce multiple cropland models, maps, and monitoring tools leading to a wide array of products using machine learning algorithms (MLAs), Artificial Intelligence (AI), and satellite sensor big-data analytics through cloud-computing. The GFSAD project, will produce four distinct Landsat-derived global cropland products. These products are:
1. Global Cropland Extent Product @ 30m (LGCEP30-2020, LGCEP30-2025).
2. Global Rainfed and Irrigated Product @ 30m (LGRIP30-2020, LGRIP30-2025).
3. Global Cropping Intensity Product @ 30m (LGCIP30-2020 & LGRIP30-2025) &
4. Global Crop Type Product @ 30m for USA, Canada, and India (LGCTY30-2020USACAN, LGCTY30-2025USACAN; LGCTY30-2020India, LGCTY30-2025India).

The GFSAD Data is released through NASA's LP DAAC (links below):
https://lpdaac.usgs.gov/news/release-of-gfsad-30-meter-cropland-extent-products/
https://lpdaac.usgs.gov/news/release-of-lgrip30-data-product/
https://lpdaac.usgs.gov/products/gfsad1kcdv001/
https://lpdaac.usgs.gov/products/gfsad1kcmv001/

GFSAD Data can be visualized at:
https://www.usgs.gov/apps/croplands/app/map

GFSAD website for further information:
www.usgs.gov/wgsc/gfsad30 . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","climate variability and ballooning populations are putting unprecedented pressure on agricultural croplands and their water use, which are vital for ensuring global food and water security in the twenty-first century. in addition, the covid-19 pandemic, military conflicts, and changing diets have added to looming global food insecurity. therefore, there is a critical need to produce consistent and accurate global cropland products at fine spatial resolution (e.g., farm-scale, 30m or better), which are generated consistently, accurately, and routinely (e.g., every year). therefore, the overarching goal of this continuity nasa measures (making earth system data records for use in research environments) proposal is to develop a comprehensive global food security-support analysis data (gfsad) project that will produce multiple cropland models, maps, and monitoring tools leading to a wide array of products using machine learning algorithms (mlas), artificial intelligence (ai), and satellite sensor big-data analytics through cloud-computing. the gfsad project, will produce four distinct landsat-derived global cropland products. these products are: 1. global cropland extent product @ 30m (lgcep30-2020, lgcep30-2025). 2. global rainfed and irrigated product @ 30m (lgrip30-2020, lgrip30-2025). 3. global cropping intensity product @ 30m (lgcip30-2020 & lgrip30-2025) & 4. global crop type product @ 30m for usa, canada, and india (lgcty30-2020usacan, lgcty30-2025usacan; lgcty30-2020india, lgcty30-2025india). the gfsad data is released through nasa's lp daac (links below): https://lpdaac.usgs.gov/news/release-of-gfsad-30-meter-cropland-extent-products/ https://lpdaac.usgs.gov/news/release-of-lgrip30-data-product/ https://lpdaac.usgs.gov/products/gfsad1kcdv001/ https://lpdaac.usgs.gov/products/gfsad1kcmv001/ gfsad data can be visualized at: https://www.usgs.gov/apps/croplands/app/map gfsad website for further information: www.usgs.gov/wgsc/gfsad30 . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Extracting analysis ready information from narrative geologic descriptions,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"I work in the geologic mapping group in the USGS on a new national synthesis of geologic maps.  The different rocks depicted on geologic maps are typically described in narratives of 1-2 paragraphs, but often contain sparse details users would like to be able to search for/operate on. For example, how thick a unit is, analytical measurements associated with that unit, the presence of certain of-interest materials.

From some experimenting with the web interface it seems the doi-hosted chatgpt does a great job extracting this sort of information with relatively simple prompts. We were hoping to use this programmatically to more systematically parse information in our geologic map synthesis database. At present, this is ~10k descriptions that consist of on average ~250 characters of text, and I would seek to run a few prompt versions on each unit (plus some testing) - so perhaps a few 10s of thousands of requests. This would then be evaluated for accuracy against a subset of human-derived extractions.",see description,Initiated,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Science Support System (SSS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"I work in the geologic mapping group in the USGS on a new national synthesis of geologic maps.  The different rocks depicted on geologic maps are typically described in narratives of 1-2 paragraphs, but often contain sparse details users would like to be able to search for/operate on. For example, how thick a unit is, analytical measurements associated with that unit, the presence of certain of-interest materials.

From some experimenting with the web interface it seems the doi-hosted chatgpt does a great job extracting this sort of information with relatively simple prompts. We were hoping to use this programmatically to more systematically parse information in our geologic map synthesis database. At present, this is ~10k descriptions that consist of on average ~250 characters of text, and I would seek to run a few prompt versions on each unit (plus some testing) - so perhaps a few 10s of thousands of requests. This would then be evaluated for accuracy against a subset of human-derived extractions. . see description","i work in the geologic mapping group in the usgs on a new national synthesis of geologic maps. the different rocks depicted on geologic maps are typically described in narratives of 1-2 paragraphs, but often contain sparse details users would like to be able to search for/operate on. for example, how thick a unit is, analytical measurements associated with that unit, the presence of certain of-interest materials. from some experimenting with the web interface it seems the doi-hosted chatgpt does a great job extracting this sort of information with relatively simple prompts. we were hoping to use this programmatically to more systematically parse information in our geologic map synthesis database. at present, this is ~10k descriptions that consist of on average ~250 characters of text, and i would seek to run a few prompt versions on each unit (plus some testing) - so perhaps a few 10s of thousands of requests. this would then be evaluated for accuracy against a subset of human-derived extractions. . see description"
21st Century Prospecting: AI-assisted Surveying of Critical Mineral Potential,Department of the Interior,DOI,USGS,Mission-Enabling,None of the above.,"Based on the Congressional mandate to assess critical minerals distributions in the US, the USGS Mineral Resources Program has partnered with DARPA. The objective of this partnership is to accelerate advances in science for understanding critical minerals, assessing unknown resources, and increase mineral security for the Nation.  As part of an extensive suite of tools, large language models are being used to extract information from text in scanned paper geologic maps as well as mining reports (mostly government documents).

(This is a re-submission as part of the new use case process)",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Data not reported by submitter and will be updated once additional information is collected,Data not reported by submitter and will be updated once additional information is collected,No,Unknown,Unknown,No,Yes,Data not reported by submitter and will be updated once additional information is collected,Data not reported by submitter and will be updated once additional information is collected,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.30158730158730157,"Based on the Congressional mandate to assess critical minerals distributions in the US, the USGS Mineral Resources Program has partnered with DARPA. The objective of this partnership is to accelerate advances in science for understanding critical minerals, assessing unknown resources, and increase mineral security for the Nation.  As part of an extensive suite of tools, large language models are being used to extract information from text in scanned paper geologic maps as well as mining reports (mostly government documents).

(This is a re-submission as part of the new use case process) . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","based on the congressional mandate to assess critical minerals distributions in the us, the usgs mineral resources program has partnered with darpa. the objective of this partnership is to accelerate advances in science for understanding critical minerals, assessing unknown resources, and increase mineral security for the nation. as part of an extensive suite of tools, large language models are being used to extract information from text in scanned paper geologic maps as well as mining reports (mostly government documents). (this is a re-submission as part of the new use case process) . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Machine Learning Applied to Geotechnical Engineering: Statistical Methods Applied to Seismic Analysis 1,Department of the Interior,DOI,BOR,Mission-Enabling,None of the above.,Purpose is to create machine learning models that predict soil liquefaction during a seismic event and compare to the geotechnical case history database to gain confidence in the machine learning estimates. The expected benefits are the existence of another tool for geotechnical staff to predict the probability of liquefaction and reducing the amount of more complex approaches to be conducted.,"Prediction of liquefaction potential at input site, as it compares to existing case history dataset. ",Implementation and Assessment,Neither,10/1/2018,6/1/2021,1/1/2023,Developed in-house.,Unknown,No,Unknown,No,No,Yes,"Non-sensitive, mission data describing resource conditions relevant to the condition assessment/prediction question being addressed via AI-ML.",Unknown,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Purpose is to create machine learning models that predict soil liquefaction during a seismic event and compare to the geotechnical case history database to gain confidence in the machine learning estimates. The expected benefits are the existence of another tool for geotechnical staff to predict the probability of liquefaction and reducing the amount of more complex approaches to be conducted. . Prediction of liquefaction potential at input site, as it compares to existing case history dataset.","purpose is to create machine learning models that predict soil liquefaction during a seismic event and compare to the geotechnical case history database to gain confidence in the machine learning estimates. the expected benefits are the existence of another tool for geotechnical staff to predict the probability of liquefaction and reducing the amount of more complex approaches to be conducted. . prediction of liquefaction potential at input site, as it compares to existing case history dataset."
Machine Learning Applied to Geotechnical Engineering: Statistical Methods Applied to Seismic Analysis 1,Department of the Interior,DOI,BOR,Mission-Enabling,None of the above.,"Purpose is to create machine learning models to predict crest movement of an embankment dam during a seismic event. The expected benefits are the creation of a screening-level tool to be used prior to large investments in field exploration, lab testing, and large scale modeling efforts.","screening-level prediction of deformation of embankments, dams, or other facilities due to seismic loading. ",Acquisition and/or Development,Neither,12/19/2016,Unknown,Unknown,Developed in-house.,Unknown,No,Unknown,No,No,Yes,"Non-sensitive, mission data describing resource conditions relevant to the condition assessment/prediction question being addressed via AI-ML.",Unknown,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"Purpose is to create machine learning models to predict crest movement of an embankment dam during a seismic event. The expected benefits are the creation of a screening-level tool to be used prior to large investments in field exploration, lab testing, and large scale modeling efforts. . screening-level prediction of deformation of embankments, dams, or other facilities due to seismic loading.","purpose is to create machine learning models to predict crest movement of an embankment dam during a seismic event. the expected benefits are the creation of a screening-level tool to be used prior to large investments in field exploration, lab testing, and large scale modeling efforts. . screening-level prediction of deformation of embankments, dams, or other facilities due to seismic loading."
Machine Learning Working Group,Department of the Interior,DOI,BOR,Mission-Enabling,None of the above.,"Description: Perform networking and reach out to Reclamation employees to gauge staff experience and interest in creating machine learning models.  Also document potential applications of machine learning to various problems encountered by staff at Reclamation in their daily work.   
How it helps: This gives us a better idea of the in-house capabilities of staff at Reclamation and of the potential uses that machine learning can play to help solve problems faced by Reclamation.     
Stage of development: Ongoing discussion and information gathering with staff is occurring.  ",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Acquisition and/or Development,Neither,8/1/2020,Unknown,Unknown,Developed in-house.,Unknown,No,Unknown,No,No,Yes,"Non-sensitive, mission data describing resource conditions relevant to the condition assessment/prediction question being addressed via AI-ML.",Unknown,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"Description: Perform networking and reach out to Reclamation employees to gauge staff experience and interest in creating machine learning models.  Also document potential applications of machine learning to various problems encountered by staff at Reclamation in their daily work.   
How it helps: This gives us a better idea of the in-house capabilities of staff at Reclamation and of the potential uses that machine learning can play to help solve problems faced by Reclamation.     
Stage of development: Ongoing discussion and information gathering with staff is occurring. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter",description: perform networking and reach out to reclamation employees to gauge staff experience and interest in creating machine learning models. also document potential applications of machine learning to various problems encountered by staff at reclamation in their daily work. how it helps: this gives us a better idea of the in-house capabilities of staff at reclamation and of the potential uses that machine learning can play to help solve problems faced by reclamation. stage of development: ongoing discussion and information gathering with staff is occurring. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Machine Learning for Chemical Savings at Reverse Osmosis Plants,Department of the Interior,DOI,BOR,Mission-Enabling,None of the above.,Chemical use for cleaning membranes in reverse osmosis (RO) water treatment facilities can  contribute significant costs to the treatment process. Reclamation’s Desalination and Water  Purification Research Program (DWPR) funded GHD to apply machine learning techniques to optimize Clean In Place chemical usage at RO plants. The techniques analyze water chemistry data to inform application of chemicals and result in less chemical per unit of water produced without sacrificing operational uptime or membrane life. The project was implemented at full scale operational facilities of the Water Replenishment District located in Southern California. The project demonstrated that machine learning can be a useful tool to optimize cleaning frequency of RO ystems.,Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,4/14/2023,Unknown,Unknown,Developed with contracting resources.,Data not reported by submitter and will be updated once additional information is collected,No,Unknown,No,No,Yes,"Non-sensitive, mission data describing resource conditions relevant to the condition assessment/prediction question being addressed via AI-ML.",Data not reported by submitter and will be updated once additional information is collected,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,Chemical use for cleaning membranes in reverse osmosis (RO) water treatment facilities can  contribute significant costs to the treatment process. Reclamation’s Desalination and Water  Purification Research Program (DWPR) funded GHD to apply machine learning techniques to optimize Clean In Place chemical usage at RO plants. The techniques analyze water chemistry data to inform application of chemicals and result in less chemical per unit of water produced without sacrificing operational uptime or membrane life. The project was implemented at full scale operational facilities of the Water Replenishment District located in Southern California. The project demonstrated that machine learning can be a useful tool to optimize cleaning frequency of RO ystems. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,chemical use for cleaning membranes in reverse osmosis (ro) water treatment facilities can contribute significant costs to the treatment process. reclamation’s desalination and water purification research program (dwpr) funded ghd to apply machine learning techniques to optimize clean in place chemical usage at ro plants. the techniques analyze water chemistry data to inform application of chemicals and result in less chemical per unit of water produced without sacrificing operational uptime or membrane life. the project was implemented at full scale operational facilities of the water replenishment district located in southern california. the project demonstrated that machine learning can be a useful tool to optimize cleaning frequency of ro ystems. . outputs were not documented and doi will update the use cases once additional data is collected from submitter
Machine Learning Refines Quagga Habitat Suitability,Department of the Interior,DOI,BOR,Mission-Enabling,None of the above.,"Extensive research has been conducted to prevent the spread of quagga mussels, a costly and damaging invasive species. Notably, several detected introductions failed to establish populations in hypothetically suitable waterbodies by pH and calcium levels. To better parameterize quagga habitat suitability, we collected ecological data at 20 stations across four invaded and two uninvaded, connected waterbodies in Arizona during 2021-2023. Data were analyzed by gradient boosted machine, an ensemble machine learning algorithm that aggregates iteratively optimized decision trees. Results identified water conditions and plankton taxa that further differentiated quagga-invaded from uninvaded stations, within a system of pH- and calcium-suitable waterbodies.",Outputs were not documented and DOI will update the use cases once additional data is collected from submitter,Implementation and Assessment,Neither,1/1/2020,Unknown,1/1/2024,Developed in-house.,Unknown,No,Unknown,No,No,Yes,"Non-sensitive, mission data describing resource conditions relevant to the condition assessment/prediction question being addressed via AI-ML.",Unknown,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Extensive research has been conducted to prevent the spread of quagga mussels, a costly and damaging invasive species. Notably, several detected introductions failed to establish populations in hypothetically suitable waterbodies by pH and calcium levels. To better parameterize quagga habitat suitability, we collected ecological data at 20 stations across four invaded and two uninvaded, connected waterbodies in Arizona during 2021-2023. Data were analyzed by gradient boosted machine, an ensemble machine learning algorithm that aggregates iteratively optimized decision trees. Results identified water conditions and plankton taxa that further differentiated quagga-invaded from uninvaded stations, within a system of pH- and calcium-suitable waterbodies. . Outputs were not documented and DOI will update the use cases once additional data is collected from submitter","extensive research has been conducted to prevent the spread of quagga mussels, a costly and damaging invasive species. notably, several detected introductions failed to establish populations in hypothetically suitable waterbodies by ph and calcium levels. to better parameterize quagga habitat suitability, we collected ecological data at 20 stations across four invaded and two uninvaded, connected waterbodies in arizona during 2021-2023. data were analyzed by gradient boosted machine, an ensemble machine learning algorithm that aggregates iteratively optimized decision trees. results identified water conditions and plankton taxa that further differentiated quagga-invaded from uninvaded stations, within a system of ph- and calcium-suitable waterbodies. . outputs were not documented and doi will update the use cases once additional data is collected from submitter"
Autonomous Drone Inspections,Department of the Interior,DOI,BSEE,Science & Space,None of the above.,"BSEE has entered into an Interagency Agreement with the Massachusetts Institute of Technology (MIT) Lincoln Laboratory (LL) to research autonomous unmanned aerial systems (UAS) and sensor technologies, with the goal of extending BSEE’s inspection capabilities to previously inaccessible environments on the Outer Continental Shelf (OSC). This research aims to build upon and leverage the work conducted by NASA's Advanced Supercomputer Division on corrosion models developed for the Level 1 Survey Report's Corrosion Level Classifications.","Multiple outputs for inspections, including decisions on corrosion levels, if a platform is boardable, does it have methane leaks.",Acquisition and/or Development,Neither,5/13/2022,10/1/2024,Unknown,Developed with contracting resources.,IAA E24PG00019,No,No,No,No,Other,The model development stage has not yet begun. Currently in information gathering stage.,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"BSEE has entered into an Interagency Agreement with the Massachusetts Institute of Technology (MIT) Lincoln Laboratory (LL) to research autonomous unmanned aerial systems (UAS) and sensor technologies, with the goal of extending BSEE’s inspection capabilities to previously inaccessible environments on the Outer Continental Shelf (OSC). This research aims to build upon and leverage the work conducted by NASA's Advanced Supercomputer Division on corrosion models developed for the Level 1 Survey Report's Corrosion Level Classifications. . Multiple outputs for inspections, including decisions on corrosion levels, if a platform is boardable, does it have methane leaks.","bsee has entered into an interagency agreement with the massachusetts institute of technology (mit) lincoln laboratory (ll) to research autonomous unmanned aerial systems (uas) and sensor technologies, with the goal of extending bsee’s inspection capabilities to previously inaccessible environments on the outer continental shelf (osc). this research aims to build upon and leverage the work conducted by nasa's advanced supercomputer division on corrosion models developed for the level 1 survey report's corrosion level classifications. . multiple outputs for inspections, including decisions on corrosion levels, if a platform is boardable, does it have methane leaks."
Well Activity Report Classification,Department of the Interior,DOI,BSEE,Science & Space,None of the above.,Researching the use of Masked Language Models and Convolutional deep neural networks to identify classification systems for significant well event using data from Well Activity Reports. ,Decision on what type of siginificant event a well activity report should report.,Acquisition and/or Development,Neither,8/1/2022,8/1/2022,Unknown,Developed in-house.,Unknown,No,No,No,No,Other,Data in oracle tables and unstructured attachment data.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,6-12 months,No,No,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,Researching the use of Masked Language Models and Convolutional deep neural networks to identify classification systems for significant well event using data from Well Activity Reports. . Decision on what type of siginificant event a well activity report should report.,researching the use of masked language models and convolutional deep neural networks to identify classification systems for significant well event using data from well activity reports. . decision on what type of siginificant event a well activity report should report.
Level 1 Survey Report Corrosion Level Classification,Department of the Interior,DOI,BSEE,Science & Space,None of the above.,"Offshore operators conduct Level 1 surveys annually to report on platform structural integrity, as mandated by 30 CFR 250.901(a)(7), and submit these surveys to BSEE. Each survey includes a corrosion assessment of the platform with accompanying photos. Each area is assigned a coating grade and are key indicators of a platforms overall structural health. Currently, BSEE manually reviews each report to determine if a platform requires further audits, a process that is both time and labor intensive. To expedite this, BSEE has been collaborating with NASA's Advanced Supercomputer Division to develop a machine learning model designed to flag potential mislabeling of corrosion within the survey reports. The current version of the model has shown promising accuracy in identifying sections with potential misratings, especially in areas with the most severe corrosion. BSEE and NASA will continue to focus on further improving the model's accuracy while also exploring ways to integrate it into BSEE's IT infrastructure and operational workflows for potential future implementation.",Corrosion Level Decision,Acquisition and/or Development,Neither,10/1/2018,3/24/2021,Unknown,Developed with contracting resources.,IAA E21PG00001,No,No,No,No,Other,Unstructured Level 1 Surveys,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"Offshore operators conduct Level 1 surveys annually to report on platform structural integrity, as mandated by 30 CFR 250.901(a)(7), and submit these surveys to BSEE. Each survey includes a corrosion assessment of the platform with accompanying photos. Each area is assigned a coating grade and are key indicators of a platforms overall structural health. Currently, BSEE manually reviews each report to determine if a platform requires further audits, a process that is both time and labor intensive. To expedite this, BSEE has been collaborating with NASA's Advanced Supercomputer Division to develop a machine learning model designed to flag potential mislabeling of corrosion within the survey reports. The current version of the model has shown promising accuracy in identifying sections with potential misratings, especially in areas with the most severe corrosion. BSEE and NASA will continue to focus on further improving the model's accuracy while also exploring ways to integrate it into BSEE's IT infrastructure and operational workflows for potential future implementation. . Corrosion Level Decision","offshore operators conduct level 1 surveys annually to report on platform structural integrity, as mandated by 30 cfr 250.901(a)(7), and submit these surveys to bsee. each survey includes a corrosion assessment of the platform with accompanying photos. each area is assigned a coating grade and are key indicators of a platforms overall structural health. currently, bsee manually reviews each report to determine if a platform requires further audits, a process that is both time and labor intensive. to expedite this, bsee has been collaborating with nasa's advanced supercomputer division to develop a machine learning model designed to flag potential mislabeling of corrosion within the survey reports. the current version of the model has shown promising accuracy in identifying sections with potential misratings, especially in areas with the most severe corrosion. bsee and nasa will continue to focus on further improving the model's accuracy while also exploring ways to integrate it into bsee's it infrastructure and operational workflows for potential future implementation. . corrosion level decision"
Well Risk Assessment,Department of the Interior,DOI,BSEE,Science & Space,None of the above.,"NASA's Advanced Supercomputer Division will utilize the work performed in the sustained casing pressure research to explore the development of machine learning models to identify various precursors of risk factors for wells. By identifying these risk factors it would help inform BSEE engineers of potential problems with the well during its various stages of development. Currently, BSEE and NASA are focusing on identifying the precursors to sustained casing pressure and evaluating their contribution to the overall risk of a well.",Well risk score,Acquisition and/or Development,Neither,3/21/2024,3/21/2024,Unknown,Developed with both contracting and in-house resources.,IAA E21PG00001,No,No,No,No,Other,Data in oracle tables and unstructured attachment data.,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"NASA's Advanced Supercomputer Division will utilize the work performed in the sustained casing pressure research to explore the development of machine learning models to identify various precursors of risk factors for wells. By identifying these risk factors it would help inform BSEE engineers of potential problems with the well during its various stages of development. Currently, BSEE and NASA are focusing on identifying the precursors to sustained casing pressure and evaluating their contribution to the overall risk of a well. . Well risk score","nasa's advanced supercomputer division will utilize the work performed in the sustained casing pressure research to explore the development of machine learning models to identify various precursors of risk factors for wells. by identifying these risk factors it would help inform bsee engineers of potential problems with the well during its various stages of development. currently, bsee and nasa are focusing on identifying the precursors to sustained casing pressure and evaluating their contribution to the overall risk of a well. . well risk score"
Sustained Casing Pressure Identification,Department of the Interior,DOI,BSEE,Science & Space,None of the above.,"Well casing pressure requests are submitted to BSEE to determine whether a well platform is experiencing a sustained casing pressure (SCP) problem. SCP is usually caused by gas migration from a high-pressured subsurface formation through the leaking cement sheath in one of the wells casing annuli, but SCP can also be caused by defects in tube connections, downhole accessories, or seals. Because SCP can lead to major safety issues, quickly identifying wells with SCP could greatly mitigate accidents on the well platforms. BSEE entered into an Inter-Agency Agreement with NASA's Advanced Supercomputing Division to help research the use of various AI techniques.",Sustained Casing Pressure Prediction,Acquisition and/or Development,Neither,10/1/2018,3/24/2021,Unknown,Developed with contracting resources.,IAA E21PG00001,No,No,No,No,Other,Currently no agency owned data is being used to train and test the model.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"Well casing pressure requests are submitted to BSEE to determine whether a well platform is experiencing a sustained casing pressure (SCP) problem. SCP is usually caused by gas migration from a high-pressured subsurface formation through the leaking cement sheath in one of the wells casing annuli, but SCP can also be caused by defects in tube connections, downhole accessories, or seals. Because SCP can lead to major safety issues, quickly identifying wells with SCP could greatly mitigate accidents on the well platforms. BSEE entered into an Inter-Agency Agreement with NASA's Advanced Supercomputing Division to help research the use of various AI techniques. . Sustained Casing Pressure Prediction","well casing pressure requests are submitted to bsee to determine whether a well platform is experiencing a sustained casing pressure (scp) problem. scp is usually caused by gas migration from a high-pressured subsurface formation through the leaking cement sheath in one of the wells casing annuli, but scp can also be caused by defects in tube connections, downhole accessories, or seals. because scp can lead to major safety issues, quickly identifying wells with scp could greatly mitigate accidents on the well platforms. bsee entered into an inter-agency agreement with nasa's advanced supercomputing division to help research the use of various ai techniques. . sustained casing pressure prediction"
ROV Smart Touch Subsea Pipeline Inspections,Department of the Interior,DOI,BSEE,Science & Space,None of the above.,"The BSEE funded project with the University of Houston is researching the development a state-of-the-art robotic system, ""Smart Touch,"" to enhance subsea pipeline inspections by integrating advanced robotics and machine learning technologies. The system uses Remote Operated Vehicles (ROVs) equipped with stress wave-based smart touch sensors, video cameras, and scanning sonars to detect flange loosening and pipeline leaks autonomously. By leveraging machine learning for active sensing and robotic navigation, the project aims to improve inspection efficiency, reduce reliance on human operators, and mitigate risks associated with subsea operations. Collaborations with industry will support development, testing, and potential commercialization.",Bolt and flange tightness level prediction,Acquisition and/or Development,Neither,7/27/2023,7/27/2023,Unknown,Developed with contracting resources.,Firm Fixed Price 140E0123C0007,No,No,No,No,Other,Data in oracle tables and unstructured attachment data.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"The BSEE funded project with the University of Houston is researching the development a state-of-the-art robotic system, ""Smart Touch,"" to enhance subsea pipeline inspections by integrating advanced robotics and machine learning technologies. The system uses Remote Operated Vehicles (ROVs) equipped with stress wave-based smart touch sensors, video cameras, and scanning sonars to detect flange loosening and pipeline leaks autonomously. By leveraging machine learning for active sensing and robotic navigation, the project aims to improve inspection efficiency, reduce reliance on human operators, and mitigate risks associated with subsea operations. Collaborations with industry will support development, testing, and potential commercialization. . Bolt and flange tightness level prediction","the bsee funded project with the university of houston is researching the development a state-of-the-art robotic system, ""smart touch,"" to enhance subsea pipeline inspections by integrating advanced robotics and machine learning technologies. the system uses remote operated vehicles (rovs) equipped with stress wave-based smart touch sensors, video cameras, and scanning sonars to detect flange loosening and pipeline leaks autonomously. by leveraging machine learning for active sensing and robotic navigation, the project aims to improve inspection efficiency, reduce reliance on human operators, and mitigate risks associated with subsea operations. collaborations with industry will support development, testing, and potential commercialization. . bolt and flange tightness level prediction"
Form Recognition model for Benefits Forms,Department of Labor,DOL,OWCP,Government Services (includes Benefits and Service Delivery),None of the above.,Custom machine learning model to extract data from complex forms to tag data entries to field headers. The input is a document or scanned image of the form and the output is a JSON response with key/value pairs extracted by running the form against the custom trained form recognition model.,The AI system output is a recommendation.,Operation and Maintenance,Neither,10/1/2022,10/1/2022,3/1/2023,Developed with both contracting and in-house resources.,"1605DC19A0004
NNG15SD60B",No,No,Yes,Yes,No,"No specific dataset. The AI engine came ready off the shelf, and the final product has been tested ad hoc by agency users and their NLP free text data. ","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,Yes,Azure,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,Custom machine learning model to extract data from complex forms to tag data entries to field headers. The input is a document or scanned image of the form and the output is a JSON response with key/value pairs extracted by running the form against the custom trained form recognition model. . The AI system output is a recommendation.,custom machine learning model to extract data from complex forms to tag data entries to field headers. the input is a document or scanned image of the form and the output is a json response with key/value pairs extracted by running the form against the custom trained form recognition model. . the ai system output is a recommendation.
Audio Transcription,Department of Labor,DOL,OWCP||VETS||WHD,Mission-Enabling (internal agency support),None of the above.,Transcription of speech to text for records keeping using natural language processing models.,Text transcription,Operation and Maintenance,Neither,3/22/2023,3/22/2023,3/22/2023,Developed with contracting resources., NNG15SD60B,No,No,No,No,Other,Unknown,Documentation is complete,No,No – agency does not have access to source code.,Yes,Azure,Less than 6 months,Yes,Unknown,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,Transcription of speech to text for records keeping using natural language processing models. . Text transcription,transcription of speech to text for records keeping using natural language processing models. . text transcription
Text to Speech Conversion,Department of Labor,DOL,EBSA||OHR,Mission-Enabling (internal agency support),None of the above.,Text to speech (Neural) for more realistic human sounding applications using natural language processing models.,"

Audio files used on Power point presentations,",Operation and Maintenance,Neither,1/22/2023,1/22/2023,1/22/2023,Developed with contracting resources.,NNG15SD60B,No,No,No,NO,Other,Unknown,Documentation is complete,No,No – agency does not have access to source code.,Yes,Azure,Less than 6 months,Yes,Unknown,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Text to speech (Neural) for more realistic human sounding applications using natural language processing models. . Audio files used on Power point presentations,","text to speech (neural) for more realistic human sounding applications using natural language processing models. . audio files used on power point presentations,"
Website Chatbot Assistant,Department of Labor,DOL,OTAA,Mission-Enabling (internal agency support),None of the above.,"The chatbot helps the end user with basic information about the program, information on who to contact, or seeking petition case status.","

More than AI, it is question and answer bot based on the pre determine question and answers, there is no AI which would give any out put",Operation and Maintenance,Neither,2/22/2023,4/23/2023,5/10/2023,Developed with both contracting and in-house resources.," NNG15SD22B
1605DC19A0004",No,Yes,No,NO,NO,Frequently asked questions and their answers pertaining to the TAA program.,Documentation is complete,Yes,Yes,Yes,AWS / DOL.gov,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"The chatbot helps the end user with basic information about the program, information on who to contact, or seeking petition case status. . More than AI, it is question and answer bot based on the pre determine question and answers, there is no AI which would give any out put","the chatbot helps the end user with basic information about the program, information on who to contact, or seeking petition case status. . more than ai, it is question and answer bot based on the pre determine question and answers, there is no ai which would give any out put"
Hololens,Department of Labor,DOL,OSHA,Education & Workforce,None of the above.,AI used to train Inspectors to visually inspect  unsafe areas from a safe location.,"

Pictures of the location",Operation and Maintenance,Neither,8/21/2023,8/21/2023,8/21/2023,Developed with both contracting and in-house resources., 1605TA20C0001,No,No,Unknown,No,No,Purchased,Documentation is missing or not available,No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,AI used to train Inspectors to visually inspect  unsafe areas from a safe location. . Pictures of the location,ai used to train inspectors to visually inspect unsafe areas from a safe location. . pictures of the location
DOL Intranet Website Chatbot Assistant,Department of Labor,DOL,OSPE,Mission-Enabling (internal agency support),None of the above.,"
Conversational AI Assistant & DOL intranet websites to help answer common procurement questions, as well as specific contract questions.",It will answer question based on the question pertaining to the knowledge base.,Operation and Maintenance,Neither,3/1/2023,8/1/2023,10/1/2024,Developed with both contracting and in-house resources.,"1605DC19A0004
NNG15SD60B",No,No,No,No,No,Agency Generated,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,"Yes – agency has access to source code, but it is not public.",No,Azure,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Conversational AI Assistant & DOL intranet websites to help answer common procurement questions, as well as specific contract questions. . It will answer question based on the question pertaining to the knowledge base.","conversational ai assistant & dol intranet websites to help answer common procurement questions, as well as specific contract questions. . it will answer question based on the question pertaining to the knowledge base."
Call Recording Analysis,Department of Labor,DOL,EBSA,Mission-Enabling (internal agency support),None of the above.,"

Automatic analysis of recorded calls made to Benefits Advisors in the DOL Interactive Voice Response (IVR) center. AI is not used for analysis, AI is used only for transcription","

The output is a transcription of calls. The AI is only doing the transcriptions",Operation and Maintenance,Neither,2/23/2023,4/1/2024,5/24/2024,Developed with both contracting and in-house resources.,"1605DC19A0004
NNG15SD60B",No,No,No,NO,Other,Agency owned data to do analysis of call recordings,Documentation is complete,No,Yes,Yes,Azure,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"Automatic analysis of recorded calls made to Benefits Advisors in the DOL Interactive Voice Response (IVR) center. AI is not used for analysis, AI is used only for transcription . The output is a transcription of calls. The AI is only doing the transcriptions","automatic analysis of recorded calls made to benefits advisors in the dol interactive voice response (ivr) center. ai is not used for analysis, ai is used only for transcription . the output is a transcription of calls. the ai is only doing the transcriptions"
Automatic Document Processing,Department of Labor,DOL,OWCP,Government Services (includes Benefits and Service Delivery),None of the above.,Automatic processing of continuation of benefits form to extract pre-defined selection boxes. AI tool will extract data from the forms.,"

AI tool will extract data from the forms for further processing",Operation and Maintenance,Neither,2/23/2023,2/23/2023,5/29/2023,Developed with both contracting and in-house resources.,"1605DC19A0004
 NNG15SD60B",No,No,Yes,Yes,No,Agency Generated,Documentation is complete,No,No – agency does not have access to source code.,Yes,Azure,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,Automatic processing of continuation of benefits form to extract pre-defined selection boxes. AI tool will extract data from the forms. . AI tool will extract data from the forms for further processing,automatic processing of continuation of benefits form to extract pre-defined selection boxes. ai tool will extract data from the forms. . ai tool will extract data from the forms for further processing
Automatic Data Processing Workflow with Form Recognizer ,Department of Labor,DOL,ETA,Mission-Enabling (internal agency support),None of the above.,Automatic processing of current complex workflow to extract required data. ,The output of the AI system is a consolidated data for review. The GPT will extract the data and into a spreadsheet and gets reconciled. ,Acquisition and/or Development,Neither,7/22/2023,11/30/2023,Unknown,Developed with both contracting and in-house resources.,"NNG15SD22B, 1605DC19A0004",No,No,No,No,No,"Historical ETA-9130/ETA-9178 forms, 9178/9130 data from R1 states.",Documentation is widely available,Unknown,Unknown,Yes,Azure,Less than 6 months,Yes,No,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,Automatic processing of current complex workflow to extract required data. . The output of the AI system is a consolidated data for review. The GPT will extract the data and into a spreadsheet and gets reconciled.,automatic processing of current complex workflow to extract required data. . the output of the ai system is a consolidated data for review. the gpt will extract the data and into a spreadsheet and gets reconciled.
OEWS Occupation Autocoder,Department of Labor,DOL,BLS,Mission-Enabling (internal agency support),None of the above.,The input is state submitted response files that include occupation title and sometimes job description of the surveyed units. The autocoder reads the job title and assigns up to two 6-digit Standard Occupational Classification (SOC) codes along with their probabilities as recommendations for human coders. Codes above a certain threshold are appended to the submitted response file and sent back to states to assist them with their SOC code assignment.,6-digit Standard Occupational Classification (SOC) codes,Operation and Maintenance,Neither,1/1/2015,1/1/2015,1/1/2018,Developed in-house.,Unknown,No,No,No,No,Yes,Agency Generated,Documentation is complete,Yes,Yes,Yes,BLS Internal system,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,The input is state submitted response files that include occupation title and sometimes job description of the surveyed units. The autocoder reads the job title and assigns up to two 6-digit Standard Occupational Classification (SOC) codes along with their probabilities as recommendations for human coders. Codes above a certain threshold are appended to the submitted response file and sent back to states to assist them with their SOC code assignment. . 6-digit Standard Occupational Classification (SOC) codes,the input is state submitted response files that include occupation title and sometimes job description of the surveyed units. the autocoder reads the job title and assigns up to two 6-digit standard occupational classification (soc) codes along with their probabilities as recommendations for human coders. codes above a certain threshold are appended to the submitted response file and sent back to states to assist them with their soc code assignment. . 6-digit standard occupational classification (soc) codes
Scanner Data Product Classification,Department of Labor,DOL,BLS,Mission-Enabling (internal agency support),None of the above.,"BLS receives bulk data from some corporations related to the cost of goods they sell and services they provide. Consumer Price Index (CPI) staff have hand-coded a segment of the items in these data into Entry Level Item (ELI) codes. To accept and make use of these bulk data transfers at scale, BLS has begun to use machine learning to label data with ELI codes. The machine learning model takes as input word frequency counts from item descriptions. Logistic regression is then used to estimate the probability of each item being classified in each ELI category based on the word frequency categorizations. The highest probability category is selected for inclusion in the data. Any selected classifications that do not meet a certain probability threshold are flagged for human review. Benefits: real-time turnaround, productivity improvements, cost saves",Entry Level Item codes,Operation and Maintenance,Neither,1/1/2018,1/1/2018,3/1/2019,Developed in-house.,Unknown,No,No,No,Unknown,Yes,Agency Generated; respondent-provided category and product description information,Documentation is complete,Yes,Yes,Yes,BLS Internal system,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"BLS receives bulk data from some corporations related to the cost of goods they sell and services they provide. Consumer Price Index (CPI) staff have hand-coded a segment of the items in these data into Entry Level Item (ELI) codes. To accept and make use of these bulk data transfers at scale, BLS has begun to use machine learning to label data with ELI codes. The machine learning model takes as input word frequency counts from item descriptions. Logistic regression is then used to estimate the probability of each item being classified in each ELI category based on the word frequency categorizations. The highest probability category is selected for inclusion in the data. Any selected classifications that do not meet a certain probability threshold are flagged for human review. Benefits: real-time turnaround, productivity improvements, cost saves . Entry Level Item codes","bls receives bulk data from some corporations related to the cost of goods they sell and services they provide. consumer price index (cpi) staff have hand-coded a segment of the items in these data into entry level item (eli) codes. to accept and make use of these bulk data transfers at scale, bls has begun to use machine learning to label data with eli codes. the machine learning model takes as input word frequency counts from item descriptions. logistic regression is then used to estimate the probability of each item being classified in each eli category based on the word frequency categorizations. the highest probability category is selected for inclusion in the data. any selected classifications that do not meet a certain probability threshold are flagged for human review. benefits: real-time turnaround, productivity improvements, cost saves . entry level item codes"
Expenditure Classification Autocoder,Department of Labor,DOL,BLS,Mission-Enabling (internal agency support),None of the above.,"Purpose: assign a reported expense description from Consumer Expenditure Diary Survey respondents to expense classification categories known as item codes. Benefits: improve productivity, cost savings",BLS-internal item codes (expenditure classification categories),Operation and Maintenance,Neither,6/1/2019,1/1/2022,1/1/2024,Developed with both contracting and in-house resources.,1605TA-21-F-00064,No,No,No,Unknown,Yes,Agency-internal; expenditure descriptions and corresponding item code assignments,Documentation is complete,Yes,Yes,Yes,BLS Internal system,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Purpose: assign a reported expense description from Consumer Expenditure Diary Survey respondents to expense classification categories known as item codes. Benefits: improve productivity, cost savings . BLS-internal item codes (expenditure classification categories)","purpose: assign a reported expense description from consumer expenditure diary survey respondents to expense classification categories known as item codes. benefits: improve productivity, cost savings . bls-internal item codes (expenditure classification categories)"
Generative AI Assistant,Department of Labor,DOL,OCIO,Mission-Enabling (internal agency support),None of the above.,"Private and secure in-house solution to evaluate business use cases that can solve problems using Generative AI models and semantic search. Example use cases include text summarization, text analysis, and document comparisons.","

The AI system provides recommendations.",Operation and Maintenance,Neither,8/15/2023,8/15/2023,8/16/2024,Developed with both contracting and in-house resources.,"1605DC19A0004
NNG15SD60B",No,No,No,Yes,Other,"No specific dataset. The AI engine came ready off the shelf, and the final product has been tested ad hoc by agency users and their NLP free text data. ",Documentation is widely available,No,No – agency does not have access to source code.,Yes,Azure Open AI and AWS,Less than 6 months,Yes,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"Private and secure in-house solution to evaluate business use cases that can solve problems using Generative AI models and semantic search. Example use cases include text summarization, text analysis, and document comparisons. . The AI system provides recommendations.","private and secure in-house solution to evaluate business use cases that can solve problems using generative ai models and semantic search. example use cases include text summarization, text analysis, and document comparisons. . the ai system provides recommendations."
PII Redaction,Department of Labor,DOL,OSHA,Mission-Enabling (internal agency support),None of the above.,Using Amazon Web Services Personal Identifying Information scrubber for ITA text fields and Named Entity Recognition to remove additional names from the text fields.,Input text returned with personal identifying information removed.,Operation and Maintenance,"Rights-Impacting
",5/20/2024,5/20/2024,11/19/2024,Developed with both contracting and in-house resources., NNG15SD22B,No,No,Yes,YES,Yes,Agency Generated,Documentation is available,No,YES,Yes,AWS,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,YES,Risk of PII been published,Yes – by the CAIO,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",Unknown,Review,General solicitations of comments from the public,Unknown,Rights-Impacting,0.746031746031746,Using Amazon Web Services Personal Identifying Information scrubber for ITA text fields and Named Entity Recognition to remove additional names from the text fields. . Input text returned with personal identifying information removed. . Risk of PII been published,using amazon web services personal identifying information scrubber for ita text fields and named entity recognition to remove additional names from the text fields. . input text returned with personal identifying information removed. . risk of pii been published
Website Chatbot Assistant,Department of Labor,DOL,ODEP,Mission-Enabling (internal agency support),None of the above.,The chatbot helps the end user with basic information about the Workforce Recruitment Program and information on who to contact.,Generate content from the WRP program,Operation and Maintenance,Neither,1/18/2024,3/18/2024,8/16/2024,Developed with both contracting and in-house resources.,"NNG15SD22B
 1605DC19A0004",No,Yes,No,NO,NO,Agency provided list of questions and answers.,Documentation is complete,Yes,Yes,Yes,AWS,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,The chatbot helps the end user with basic information about the Workforce Recruitment Program and information on who to contact. . Generate content from the WRP program,the chatbot helps the end user with basic information about the workforce recruitment program and information on who to contact. . generate content from the wrp program
Worker PLUS Microsimulation Program,Department of Labor,DOL,CEO ,Government Services (includes Benefits and Service Delivery),None of the above.,"The Worker PLUS model was developed as an evolutionary iteration of the Paid Family and Medical Leave 
Simulator Model developed by Albelda and Clayton-Matthews (the ACM model)","The model simulates specific leave-taking behavior and outcomes (including number of leaves, leave lengths, benefit levels, and benefit eligibility) with individual workers in a state using data from the five-year American Community Survey (ACS) Public Use Microdata Sample (PUMS)",Operation and Maintenance,Neither,9/27/2017,9/27/2017,3/1/2021,Developed with contracting resources.,DOLQ129633247,No,Yes,No,NO,Other,"CEO/WHD 2018 FMLA Employee Survey, American Community Survey",Documentation is widely available,Yes,Yes,No,Unknown,More than 12 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"The Worker PLUS model was developed as an evolutionary iteration of the Paid Family and Medical Leave 
Simulator Model developed by Albelda and Clayton-Matthews (the ACM model) . The model simulates specific leave-taking behavior and outcomes (including number of leaves, leave lengths, benefit levels, and benefit eligibility) with individual workers in a state using data from the five-year American Community Survey (ACS) Public Use Microdata Sample (PUMS)","the worker plus model was developed as an evolutionary iteration of the paid family and medical leave simulator model developed by albelda and clayton-matthews (the acm model) . the model simulates specific leave-taking behavior and outcomes (including number of leaves, leave lengths, benefit levels, and benefit eligibility) with individual workers in a state using data from the five-year american community survey (acs) public use microdata sample (pums)"
Computer-Assisted Coding: SOII Autocoder,Department of Labor,DOL,BLS,Mission-Enabling (internal agency support),None of the above.,"The Survey of Occupational Injuries and Illnesses (SOII) collects hundreds of thousands of narratives describing cases of work-related injury and illness annually. Using narratives and other relevant information, SOII Autocoder automatically assigns classifications for SOII elements, which include worker occupation, nature of the injury, part of body affected, event that resulted in the injury, source and secondary source (if it exists) that caused the injury. The use of SOII autocoder initially began in 2012 for review purposes, then gradually expanded to automatically assign these classification codes. In reference year (RY) 2022, 92% of all SOII elements were automatically coded, which were then subsequently validated by human staff.

The SOII Autocoder is a transformer-based text classification model using millions of SOII cases as the labeled training data and a publicly available, third-party language model as a pre-trained base model. The Autocoder is trained annually in house using a GPU server that is owned and maintained by OCWC. Once trained, the Autocoder is deployed internally via REST API, autocoding batches of SOII and MSHA cases on a routine basis during the production cycle, all within the BLS network.",SOC and OIICS codes,Operation and Maintenance,Neither,1/12/2012,1/13/2013,1/15/2015,Developed in-house.,Unknown,No,No,Yes,No,Yes,Agency Generated,Documentation is complete,Yes,Yes,Yes,BLS Internal System,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The Survey of Occupational Injuries and Illnesses (SOII) collects hundreds of thousands of narratives describing cases of work-related injury and illness annually. Using narratives and other relevant information, SOII Autocoder automatically assigns classifications for SOII elements, which include worker occupation, nature of the injury, part of body affected, event that resulted in the injury, source and secondary source (if it exists) that caused the injury. The use of SOII autocoder initially began in 2012 for review purposes, then gradually expanded to automatically assign these classification codes. In reference year (RY) 2022, 92% of all SOII elements were automatically coded, which were then subsequently validated by human staff.

The SOII Autocoder is a transformer-based text classification model using millions of SOII cases as the labeled training data and a publicly available, third-party language model as a pre-trained base model. The Autocoder is trained annually in house using a GPU server that is owned and maintained by OCWC. Once trained, the Autocoder is deployed internally via REST API, autocoding batches of SOII and MSHA cases on a routine basis during the production cycle, all within the BLS network. . SOC and OIICS codes","the survey of occupational injuries and illnesses (soii) collects hundreds of thousands of narratives describing cases of work-related injury and illness annually. using narratives and other relevant information, soii autocoder automatically assigns classifications for soii elements, which include worker occupation, nature of the injury, part of body affected, event that resulted in the injury, source and secondary source (if it exists) that caused the injury. the use of soii autocoder initially began in 2012 for review purposes, then gradually expanded to automatically assign these classification codes. in reference year (ry) 2022, 92% of all soii elements were automatically coded, which were then subsequently validated by human staff. the soii autocoder is a transformer-based text classification model using millions of soii cases as the labeled training data and a publicly available, third-party language model as a pre-trained base model. the autocoder is trained annually in house using a gpu server that is owned and maintained by ocwc. once trained, the autocoder is deployed internally via rest api, autocoding batches of soii and msha cases on a routine basis during the production cycle, all within the bls network. . soc and oiics codes"
CFOI Record Matching,Department of Labor,DOL,BLS,Mission-Enabling (internal agency support),None of the above.,"The Census of Fatal Occupational Injuries (CFOI) collects and publishes a complete count of work-related fatal injuries and descriptive data on their circumstances. The CFOI Record Matching programs matches records from various sources, including Occupational Safety and Health Administration (OSHA) Information System (OIS) files, news articles, and Quarterly Census of Employment and Wages (QCEW), to identify missing or inconsistent data in CFOI.

The CFOI Record Matching programs use various aspects of AI in the areas of natural language processing and record matching. The CFOI-OIS matching uses random forest classifier model trained on previously matched CFOI-OIS data. The CFOI-QCEW matching uses TF-IDF vectors and cosine similarity to match establishment names in the CFOI-QCEW data. And the CFOI-CPDMS matching uses a publicly available, third-party question-answering language model to identify relevant CFOI elements from CPDMS data and then uses random forest classifier model to match CPDMS data to CFOI data. These processes are all conducted within the BLS network, and the outputs, in the form of spreadsheets, are provided once or twice a year to the staff in the CFOI program for review. These processes are all run within the BLS network.",Similarity score between records,Operation and Maintenance,Neither,9/18/2024,10/18/2024,9/20/2024,Developed in-house.,Unknown,No,No,Yes,No,Yes,Agency Generated,Documentation is complete,Yes,Yes,Yes,BLS Internal System,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The Census of Fatal Occupational Injuries (CFOI) collects and publishes a complete count of work-related fatal injuries and descriptive data on their circumstances. The CFOI Record Matching programs matches records from various sources, including Occupational Safety and Health Administration (OSHA) Information System (OIS) files, news articles, and Quarterly Census of Employment and Wages (QCEW), to identify missing or inconsistent data in CFOI.

The CFOI Record Matching programs use various aspects of AI in the areas of natural language processing and record matching. The CFOI-OIS matching uses random forest classifier model trained on previously matched CFOI-OIS data. The CFOI-QCEW matching uses TF-IDF vectors and cosine similarity to match establishment names in the CFOI-QCEW data. And the CFOI-CPDMS matching uses a publicly available, third-party question-answering language model to identify relevant CFOI elements from CPDMS data and then uses random forest classifier model to match CPDMS data to CFOI data. These processes are all conducted within the BLS network, and the outputs, in the form of spreadsheets, are provided once or twice a year to the staff in the CFOI program for review. These processes are all run within the BLS network. . Similarity score between records","the census of fatal occupational injuries (cfoi) collects and publishes a complete count of work-related fatal injuries and descriptive data on their circumstances. the cfoi record matching programs matches records from various sources, including occupational safety and health administration (osha) information system (ois) files, news articles, and quarterly census of employment and wages (qcew), to identify missing or inconsistent data in cfoi. the cfoi record matching programs use various aspects of ai in the areas of natural language processing and record matching. the cfoi-ois matching uses random forest classifier model trained on previously matched cfoi-ois data. the cfoi-qcew matching uses tf-idf vectors and cosine similarity to match establishment names in the cfoi-qcew data. and the cfoi-cpdms matching uses a publicly available, third-party question-answering language model to identify relevant cfoi elements from cpdms data and then uses random forest classifier model to match cpdms data to cfoi data. these processes are all conducted within the bls network, and the outputs, in the form of spreadsheets, are provided once or twice a year to the staff in the cfoi program for review. these processes are all run within the bls network. . similarity score between records"
AI Assisted coding -Microsoft GitHub Copilot,Department of Labor,DOL,OCIO,Mission-Enabling (internal agency support),None of the above.,"Automatically generate software code using Microsoft Copilot, this will reduce the time for generating code","Automatically generate software code using Microsoft Copilot, this will reduce the time for generating code",Operation and Maintenance,Neither,9/1/2024,9/1/2024,9/1/2024,Developed in-house.,NNG15SD60B,No,No,No,No,No,Unknown,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",No,"Yes – agency has access to source code, but it is not public.",Yes,Azure,Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Automatically generate software code using Microsoft Copilot, this will reduce the time for generating code . Automatically generate software code using Microsoft Copilot, this will reduce the time for generating code","automatically generate software code using microsoft copilot, this will reduce the time for generating code . automatically generate software code using microsoft copilot, this will reduce the time for generating code"
CPS OTC Prediction,Department of Labor,DOL,BLS,Mission-Enabling (internal agency support),None of the above.,"The BLS productivity office publishes measures of hours worked by major sector and industry. As part of the calculation for this measure, BLS uses Current Employment Statistics (CES) data on hours paid is used to estimate hours for payroll workers. Additional adjustments are made to this measure, such as removing paid time off (PTO), adding off-the-clock (OTC) hours, and adding in hours worked by self-employed and unpaid family workers. BLS uses the Current Population Survey (CPS) dataset to identify the amount of hours that are worked off-the-clock (OTC) by workers. There are some workers in the CPS dataset that do not report whether their time off was paid, or whether they get paid hourly or not. This data needs to be imputed to calculate the ratio of total hours worked to paid hours worked. A random forest model is used to predict the responses for workers that did not report this information by training it on characteristics (such as industry, occupation, education level, age, etc.) of respondents that have reported that information. The benefit of this AI use case is increased accuracy.","

worker's time off was paid, or whether they get paid hourly or not",Operation and Maintenance,Neither,1/1/2022,2/1/2022,11/1/2022,Developed in-house.,Unknown,No,No,No,No,Yes,Agency Generated,Documentation is complete,Yes,Yes,Yes,BLS Internal system,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The BLS productivity office publishes measures of hours worked by major sector and industry. As part of the calculation for this measure, BLS uses Current Employment Statistics (CES) data on hours paid is used to estimate hours for payroll workers. Additional adjustments are made to this measure, such as removing paid time off (PTO), adding off-the-clock (OTC) hours, and adding in hours worked by self-employed and unpaid family workers. BLS uses the Current Population Survey (CPS) dataset to identify the amount of hours that are worked off-the-clock (OTC) by workers. There are some workers in the CPS dataset that do not report whether their time off was paid, or whether they get paid hourly or not. This data needs to be imputed to calculate the ratio of total hours worked to paid hours worked. A random forest model is used to predict the responses for workers that did not report this information by training it on characteristics (such as industry, occupation, education level, age, etc.) of respondents that have reported that information. The benefit of this AI use case is increased accuracy. . worker's time off was paid, or whether they get paid hourly or not","the bls productivity office publishes measures of hours worked by major sector and industry. as part of the calculation for this measure, bls uses current employment statistics (ces) data on hours paid is used to estimate hours for payroll workers. additional adjustments are made to this measure, such as removing paid time off (pto), adding off-the-clock (otc) hours, and adding in hours worked by self-employed and unpaid family workers. bls uses the current population survey (cps) dataset to identify the amount of hours that are worked off-the-clock (otc) by workers. there are some workers in the cps dataset that do not report whether their time off was paid, or whether they get paid hourly or not. this data needs to be imputed to calculate the ratio of total hours worked to paid hours worked. a random forest model is used to predict the responses for workers that did not report this information by training it on characteristics (such as industry, occupation, education level, age, etc.) of respondents that have reported that information. the benefit of this ai use case is increased accuracy. . worker's time off was paid, or whether they get paid hourly or not"
Sample Refinement: Frame API,Department of Labor,DOL,BLS,Mission-Enabling (internal agency support),None of the above.,"BLS establishment-based surveys, such as the Survey of Occupational Injuries and Illnesses (SOII), the National Compensation Survey (NCS), and the Occupational Requirements Survey (ORS), use the Longitudinal Database (LDB) as the frame for their survey samples. For each of these programs, there is a significant time-lag between when the sample is drawn for a program and when data collection begins. A critical and time-consuming part of data collection is sample refinement: checking the latest Quarterly Census of Employment and Wages (QCEW) and LDB data to adjust for any changes to the sampled establishment during that time-lag. The QCEW/LDB Frame API allows fast retrieval of the latest QCEW/LDB data for a nationwide sample so that users can perform sample refinement more efficiently. The SOII program uses the Frame API to perform over 20 sample refinement comparison checks, which outputs a report for the data collectors.

The Frame API uses TF-IDF vectors and cosine similarity to compare company names, mailing addresses, and unit descriptions between the survey sample and the latest QCEW/LDB data to determine what components may have changed. The Frame API also has the flexibility for the users to match records using other combinations of variables. These processes are all run within the BLS network.",Similarity score between records,Operation and Maintenance,Neither,5/20/2024,6/20/2024,9/20/2024,Developed in-house.,Unknown,No,No,No,No,Yes,Agency Generated,Documentation is complete,Yes,Yes,Yes,BLS Internal System,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"BLS establishment-based surveys, such as the Survey of Occupational Injuries and Illnesses (SOII), the National Compensation Survey (NCS), and the Occupational Requirements Survey (ORS), use the Longitudinal Database (LDB) as the frame for their survey samples. For each of these programs, there is a significant time-lag between when the sample is drawn for a program and when data collection begins. A critical and time-consuming part of data collection is sample refinement: checking the latest Quarterly Census of Employment and Wages (QCEW) and LDB data to adjust for any changes to the sampled establishment during that time-lag. The QCEW/LDB Frame API allows fast retrieval of the latest QCEW/LDB data for a nationwide sample so that users can perform sample refinement more efficiently. The SOII program uses the Frame API to perform over 20 sample refinement comparison checks, which outputs a report for the data collectors.

The Frame API uses TF-IDF vectors and cosine similarity to compare company names, mailing addresses, and unit descriptions between the survey sample and the latest QCEW/LDB data to determine what components may have changed. The Frame API also has the flexibility for the users to match records using other combinations of variables. These processes are all run within the BLS network. . Similarity score between records","bls establishment-based surveys, such as the survey of occupational injuries and illnesses (soii), the national compensation survey (ncs), and the occupational requirements survey (ors), use the longitudinal database (ldb) as the frame for their survey samples. for each of these programs, there is a significant time-lag between when the sample is drawn for a program and when data collection begins. a critical and time-consuming part of data collection is sample refinement: checking the latest quarterly census of employment and wages (qcew) and ldb data to adjust for any changes to the sampled establishment during that time-lag. the qcew/ldb frame api allows fast retrieval of the latest qcew/ldb data for a nationwide sample so that users can perform sample refinement more efficiently. the soii program uses the frame api to perform over 20 sample refinement comparison checks, which outputs a report for the data collectors. the frame api uses tf-idf vectors and cosine similarity to compare company names, mailing addresses, and unit descriptions between the survey sample and the latest qcew/ldb data to determine what components may have changed. the frame api also has the flexibility for the users to match records using other combinations of variables. these processes are all run within the bls network. . similarity score between records"
CE Interview Item Code Estimation,Department of Labor,DOL,BLS,Mission-Enabling (internal agency support),None of the above.,"Custom machine learning model to predict an expense classification category (item code) for expenditures with text descriptions from Consumer Expenditure Interview Survey. Item Code Estimation assists with human function by approximating decision-making. It does not assign categories; it only suggests categories that the economist can then apply if correct, to speed up human review and decision-making. Benefits: improved productivity.",Expense Classification Category,Operation and Maintenance,Neither,10/1/2016,10/1/2016,7/19/2024,Developed in-house.,Unknown,No,No,No,Unknown,Yes,Agency-internal; expenditure descriptions and corresponding item code assignments,Documentation is complete,Yes,Yes,Yes,BLS Internal system,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Custom machine learning model to predict an expense classification category (item code) for expenditures with text descriptions from Consumer Expenditure Interview Survey. Item Code Estimation assists with human function by approximating decision-making. It does not assign categories; it only suggests categories that the economist can then apply if correct, to speed up human review and decision-making. Benefits: improved productivity. . Expense Classification Category","custom machine learning model to predict an expense classification category (item code) for expenditures with text descriptions from consumer expenditure interview survey. item code estimation assists with human function by approximating decision-making. it does not assign categories; it only suggests categories that the economist can then apply if correct, to speed up human review and decision-making. benefits: improved productivity. . expense classification category"
CE Interview Imputations,Department of Labor,DOL,BLS,Mission-Enabling (internal agency support),None of the above.,"Custom machine learning statistical models to impute missing expenditure values within the Consumer Expenditure Interview Survey when survey respondents answer ""don't know"" or refuse to provide an answer for an amount. The machine learning models are a more modern statistical method for replacing missing data than previous statistical methods. Benefits: improved statistical quality of imputations.","
Imputed expense amounts",Operation and Maintenance,Neither,3/1/2020,3/1/2020,10/20/2024,Developed in-house.,Unknown,No,No,No,Unknown,Yes,Agency Generated,Documentation is complete,Yes,Yes,Yes,BLS Internal system,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Custom machine learning statistical models to impute missing expenditure values within the Consumer Expenditure Interview Survey when survey respondents answer ""don't know"" or refuse to provide an answer for an amount. The machine learning models are a more modern statistical method for replacing missing data than previous statistical methods. Benefits: improved statistical quality of imputations. . Imputed expense amounts","custom machine learning statistical models to impute missing expenditure values within the consumer expenditure interview survey when survey respondents answer ""don't know"" or refuse to provide an answer for an amount. the machine learning models are a more modern statistical method for replacing missing data than previous statistical methods. benefits: improved statistical quality of imputations. . imputed expense amounts"
GovChat,Environmental Protection Agency,EPA,Office of Mission Support,Government Services (includes Benefits and Service Delivery),None of the above.,The purpose of this application is to provide a generative AI chatbot tool to EPA staff. It is materialized as a web site user interface which users interact with on their web browser when on the EPA network.,"The Use Case produces a variety of output depending on what users use it for internally which can range from content, code, and other uses of ChatGPT. A rules of behavior requires that users do not use it for final decisions or recommendations. Additionally, users will potentially use this environment for a broad set of general AI uses. Individual use cases may be described if the system becomes widely adopted and patterns of use are determined.",Operation and Maintenance,Neither,3/1/2024,3/1/2024,12/10/2024,Developed in-house.,Unknown,No,No,No,Yes,No,"No data is used to train, etc the model.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,Yes – source code is publicly available.,Yes,EPA Azure Cloud Hosting System ,6-12 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"The purpose of this application is to provide a generative AI chatbot tool to EPA staff. It is materialized as a web site user interface which users interact with on their web browser when on the EPA network. . The Use Case produces a variety of output depending on what users use it for internally which can range from content, code, and other uses of ChatGPT. A rules of behavior requires that users do not use it for final decisions or recommendations. Additionally, users will potentially use this environment for a broad set of general AI uses. Individual use cases may be described if the system becomes widely adopted and patterns of use are determined.","the purpose of this application is to provide a generative ai chatbot tool to epa staff. it is materialized as a web site user interface which users interact with on their web browser when on the epa network. . the use case produces a variety of output depending on what users use it for internally which can range from content, code, and other uses of chatgpt. a rules of behavior requires that users do not use it for final decisions or recommendations. additionally, users will potentially use this environment for a broad set of general ai uses. individual use cases may be described if the system becomes widely adopted and patterns of use are determined."
Agency Records Management System (ARMS),Environmental Protection Agency,EPA,Office of Mission Support,Mission-Enabling,None of the above.,"purpose: to improve accuracy of Records schedule prediction
expected benefit: reduce administrative burden on EPA staff ",the machine learning model predicts records schedules ,Operation and Maintenance,Neither,10/1/2020,10/1/2020,11/1/2023,Developed in-house.,Unknown,No,No,Yes,Yes,No,we used a sub-set of agency records,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Agency Records Management System (ARMS Application),More than 12 months,Yes,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"purpose: to improve accuracy of Records schedule prediction
expected benefit: reduce administrative burden on EPA staff . the machine learning model predicts records schedules",purpose: to improve accuracy of records schedule prediction expected benefit: reduce administrative burden on epa staff . the machine learning model predicts records schedules
Intelligent Superfund Search & Chatbot capabilities,Environmental Protection Agency,EPA,Office of Land and Emergency Management,Energy & the Environment,Searching for information using AI.,"The intended purpose of this AI use case is to modernize and enhance the EPA's Superfund Enterprise Management System (SEMS) document search and processing system through intelligent automation. Key expected benefits include:

1. Dramatic improvement in operational efficiency by reducing search time from months/weeks to minutes
2. Enhanced accuracy in document retrieval and information extraction
3. Better user experience through intuitive natural language search capabilities
4. Cost reduction through automated document processing
5. Improved compliance and quality control in environmental site management
6. Increased ability to process and derive insights from millions of technical documents
7. Better decision support for EPA staff managing contaminated sites
","The AI system will generate the following outputs:

1. Intelligent Document Processing Results:
   - Extracted key entities (site locations, contaminants, responsible parties)
   - Document classifications and metadata
   - Automated summaries of technical reports

2. Search and Retrieval Outputs:
   - Relevant document recommendations based on natural language queries
   - Direct answers to specific questions with source citations
   - Cross-document insights and connections

3. Analysis and Decision Support:
   - Site contamination pattern analysis
   - Related case recommendations
   - Risk assessment summaries

4. Quality Assurance Outputs:
   - Confidence scores for extracted information
   - Source attribution for all provided information
",Acquisition and/or Development,Neither,4/1/2024,4/1/2024,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,No,"No data is used for AI training, finetuning. Subset of agency records is used for evaluation.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,Yes,Superfund Enterprise Management System,6-12 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.38095238095238093,"The intended purpose of this AI use case is to modernize and enhance the EPA's Superfund Enterprise Management System (SEMS) document search and processing system through intelligent automation. Key expected benefits include:

1. Dramatic improvement in operational efficiency by reducing search time from months/weeks to minutes
2. Enhanced accuracy in document retrieval and information extraction
3. Better user experience through intuitive natural language search capabilities
4. Cost reduction through automated document processing
5. Improved compliance and quality control in environmental site management
6. Increased ability to process and derive insights from millions of technical documents
7. Better decision support for EPA staff managing contaminated sites . The AI system will generate the following outputs:

1. Intelligent Document Processing Results:
   - Extracted key entities (site locations, contaminants, responsible parties)
   - Document classifications and metadata
   - Automated summaries of technical reports

2. Search and Retrieval Outputs:
   - Relevant document recommendations based on natural language queries
   - Direct answers to specific questions with source citations
   - Cross-document insights and connections

3. Analysis and Decision Support:
   - Site contamination pattern analysis
   - Related case recommendations
   - Risk assessment summaries

4. Quality Assurance Outputs:
   - Confidence scores for extracted information
   - Source attribution for all provided information","the intended purpose of this ai use case is to modernize and enhance the epa's superfund enterprise management system (sems) document search and processing system through intelligent automation. key expected benefits include: 1. dramatic improvement in operational efficiency by reducing search time from months/weeks to minutes 2. enhanced accuracy in document retrieval and information extraction 3. better user experience through intuitive natural language search capabilities 4. cost reduction through automated document processing 5. improved compliance and quality control in environmental site management 6. increased ability to process and derive insights from millions of technical documents 7. better decision support for epa staff managing contaminated sites . the ai system will generate the following outputs: 1. intelligent document processing results: - extracted key entities (site locations, contaminants, responsible parties) - document classifications and metadata - automated summaries of technical reports 2. search and retrieval outputs: - relevant document recommendations based on natural language queries - direct answers to specific questions with source citations - cross-document insights and connections 3. analysis and decision support: - site contamination pattern analysis - related case recommendations - risk assessment summaries 4. quality assurance outputs: - confidence scores for extracted information - source attribution for all provided information"
Resource Conservation and Recovery Act (RCRA) Predictive Analytics,Environmental Protection Agency,EPA,Office of Enforcement and Compliance Assurance,Law & Justice,None of the above.,"Risk scoring of Large Quantity Generators (LQGs) to support RCRA inspections
","Risk scoring (integers 0 to 4, inclusive) for LQGs
",Implementation and Assessment,Safety-Impacting,7/31/2019,7/31/2019,10/12/2022,Unknown,Unknown,Unknown,No,No,No,Yes,"From https://echo.epa.gov/tools/data-downloads : Facility Registry Service, Air Stationary Source Data Downloads, Water National Pollutant Discharge Elimination System (NPDES) Data Downloads, Water Biosolids Data Download, Hazardous Waste Data Download","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Data Management and Analytics Platform (DMAP),Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Safety-Impacting,0.5079365079365079,"Risk scoring of Large Quantity Generators (LQGs) to support RCRA inspections . Risk scoring (integers 0 to 4, inclusive) for LQGs","risk scoring of large quantity generators (lqgs) to support rcra inspections . risk scoring (integers 0 to 4, inclusive) for lqgs"
Streamflow Duration Assessment Modeling,Environmental Protection Agency,EPA,Office of Water,Science & Space,None of the above.,"Streams exhibit a diverse range of hydrologic regimes, and the hydrologic regime strongly influences the physical, chemical, and biological characteristics of active stream channels and their adjacent riparian areas. One important aspect of the hydrologic regime is streamflow duration—the length of time that a stream sustains flowing surface water. Determining streamflow duration class can help support a variety of regulatory and non-regulatory water resource management decisions. This random forest modeling supports prediction of the appropriate streamflow duration classification across a range of geographies. Full information is available at: https://www.epa.gov/streamflow-duration-assessment/random-forest-modeling","Prediction of whether a streamflow is “perennial,” “intermittent,” or “ephemeral”",Operation and Maintenance,Neither,3/6/2009,3/6/2009,11/15/2011,Unknown,Unknown,Unknown,No,No,Yes,No,"Pacific Northwest https://www.hydroshare.org/resource/fb4e7b8758d0478cbfe0d6c786f0f968/
Northeast and Southeast https://catalog.data.gov/dataset/nese-betasdam-final-data-and-code
Arid West https://catalog.data.gov/dataset/aw-betasdam-final-data-and-code
Western Mountains https://catalog.data.gov/dataset/wm-betasdam-final-data-and-code
Great Plains https://catalog.data.gov/dataset/gp-betasdam-final-data-and-code
","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,Yes – source code is publicly available.,Yes,Data Management and Analytics Platform (DMAP) (Posit Connect),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Streams exhibit a diverse range of hydrologic regimes, and the hydrologic regime strongly influences the physical, chemical, and biological characteristics of active stream channels and their adjacent riparian areas. One important aspect of the hydrologic regime is streamflow duration—the length of time that a stream sustains flowing surface water. Determining streamflow duration class can help support a variety of regulatory and non-regulatory water resource management decisions. This random forest modeling supports prediction of the appropriate streamflow duration classification across a range of geographies. Full information is available at: https://www.epa.gov/streamflow-duration-assessment/random-forest-modeling . Prediction of whether a streamflow is “perennial,” “intermittent,” or “ephemeral”","streams exhibit a diverse range of hydrologic regimes, and the hydrologic regime strongly influences the physical, chemical, and biological characteristics of active stream channels and their adjacent riparian areas. one important aspect of the hydrologic regime is streamflow duration—the length of time that a stream sustains flowing surface water. determining streamflow duration class can help support a variety of regulatory and non-regulatory water resource management decisions. this random forest modeling supports prediction of the appropriate streamflow duration classification across a range of geographies. full information is available at: https://www.epa.gov/streamflow-duration-assessment/random-forest-modeling . prediction of whether a streamflow is “perennial,” “intermittent,” or “ephemeral”"
Solicitation Review Tool (SRT),General Services Administration,GSA,OGP,Other,None of the above.,"The SRT intakes SAM.gov data for all ICT solicitations. The system then compiles the data into a database to be used by machine learning algorithms. The first of these is a Natural Language Processing model that determines if a solicitation contains compliance language. If a solicitation does not have compliance language, then it is marked as non-compliant. Each agency is asked to review their data and validate the SRT predictions. GSA also conducts random manual reviews monthly.",Determination whether a solicitation is compliant or not. Text form.,Implementation and Assessment,Neither,5/1/2018,5/1/2019,6/1/2020,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Yes,SAM.gov data sets,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,SRT,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,No,"False negatives, and identified through random QA sessions.",Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Unknown,Post-transaction customer feedback collections,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.7936507936507936,"The SRT intakes SAM.gov data for all ICT solicitations. The system then compiles the data into a database to be used by machine learning algorithms. The first of these is a Natural Language Processing model that determines if a solicitation contains compliance language. If a solicitation does not have compliance language, then it is marked as non-compliant. Each agency is asked to review their data and validate the SRT predictions. GSA also conducts random manual reviews monthly. . Determination whether a solicitation is compliant or not. Text form. . False negatives, and identified through random QA sessions.","the srt intakes sam.gov data for all ict solicitations. the system then compiles the data into a database to be used by machine learning algorithms. the first of these is a natural language processing model that determines if a solicitation contains compliance language. if a solicitation does not have compliance language, then it is marked as non-compliant. each agency is asked to review their data and validate the srt predictions. gsa also conducts random manual reviews monthly. . determination whether a solicitation is compliant or not. text form. . false negatives, and identified through random qa sessions."
Acquisition Analytics,General Services Administration,GSA,FAS,Mission-Enabling,None of the above.,Takes Detailed Data on transactions and classifies each transaction within the Government-wide Category Management Taxonomy,Acquisition Analytics,Implementation and Assessment,Neither,10/1/2018,4/1/2019,4/1/2019,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Yes,Metadata Link,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,EDA,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,No,Misclassification and subsequent misalignment of transaction throughput through GWAS categories,Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.7777777777777778,Takes Detailed Data on transactions and classifies each transaction within the Government-wide Category Management Taxonomy . Acquisition Analytics . Misclassification and subsequent misalignment of transaction throughput through GWAS categories,takes detailed data on transactions and classifies each transaction within the government-wide category management taxonomy . acquisition analytics . misclassification and subsequent misalignment of transaction throughput through gwas categories
City Pairs Program Ticket Forecast and Scenario Analysis Tools,General Services Administration,GSA,FAS,Transportation,None of the above.,"Takes segment-level City Pair Program air travel purchase data and creates near-term forecasts for the current and upcoming fiscal year by month and at various levels of granularity including DOD vs Civilian, Agency, and Region.",City Pairs Program Ticket Forecast and Scenario Analysis Tools,Implementation and Assessment,Neither,10/1/2021,2/1/2022,4/1/2019,Developed in-house.,Unknown,No,No,No,No,Yes,Metadata Link,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,EDA,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,No,Over or underestimation and therefore mistargeting initiatives around KPIs ,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.7777777777777778,"Takes segment-level City Pair Program air travel purchase data and creates near-term forecasts for the current and upcoming fiscal year by month and at various levels of granularity including DOD vs Civilian, Agency, and Region. . City Pairs Program Ticket Forecast and Scenario Analysis Tools . Over or underestimation and therefore mistargeting initiatives around KPIs","takes segment-level city pair program air travel purchase data and creates near-term forecasts for the current and upcoming fiscal year by month and at various levels of granularity including dod vs civilian, agency, and region. . city pairs program ticket forecast and scenario analysis tools . over or underestimation and therefore mistargeting initiatives around kpis"
Category Taxonomy Classifier,General Services Administration,GSA,FAS,Mission-Enabling,None of the above.,Classification of obligation transactions in to GWAS subcategories when existing FPDS coding scheme(PSC/NAICS) is insufficient for positive classification of a transaction. ,Category Taxonomy Classifier,Implementation and Assessment,Neither,6/1/2021,6/1/2021,10/1/2022,Developed in-house.,Unknown,No,No,No,No,Yes,Metadata Link,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,EDA,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,No,Misclassification and subsequent misalignment of transaction throughput through GWAS categories,Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.7777777777777778,Classification of obligation transactions in to GWAS subcategories when existing FPDS coding scheme(PSC/NAICS) is insufficient for positive classification of a transaction. . Category Taxonomy Classifier . Misclassification and subsequent misalignment of transaction throughput through GWAS categories,classification of obligation transactions in to gwas subcategories when existing fpds coding scheme(psc/naics) is insufficient for positive classification of a transaction. . category taxonomy classifier . misclassification and subsequent misalignment of transaction throughput through gwas categories
CPRM Contract Breach Forecast,General Services Administration,GSA,FAS,Mission-Enabling,None of the above.,Applies obligation forecasting to ITC contracts to provide situational awareness of risk of contract ceiling breach. ,CPRM Contract Breach Forecast,Implementation and Assessment,Neither,8/1/2022,Unknown,Unknown,Developed in-house.,Unknown,No,No,No,No,Yes,Metadata Link,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,EDA,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,No,1) Breach predicted too far in the future. 2) Breach predicted to early. ,Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.746031746031746,Applies obligation forecasting to ITC contracts to provide situational awareness of risk of contract ceiling breach. . CPRM Contract Breach Forecast . 1) Breach predicted too far in the future. 2) Breach predicted to early.,applies obligation forecasting to itc contracts to provide situational awareness of risk of contract ceiling breach. . cprm contract breach forecast . 1) breach predicted too far in the future. 2) breach predicted to early.
Category Taxonomy Refinement Using NLP,General Services Administration,GSA,FAS,Mission-Enabling,None of the above.,Uses token extraction from product descriptions more accurately shape intended markets for PSCs.,,Retired,Neither,Unknown,Unknown,Unknown,Developed in-house.,Unknown,No,No,No,No,No,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.6666666666666666,Uses token extraction from product descriptions more accurately shape intended markets for PSCs.,uses token extraction from product descriptions more accurately shape intended markets for pscs.
Key KPI Forecasts for GWCM,General Services Administration,GSA,FAS,Mission-Enabling,None of the above.,"Takes monthly historical data for underlying components used to calculate KPIs and creates near-term forecasts for the upcoming fiscal year. Pilot effort focuses on total agency/category spend (the denominator in multiple KPIs). If the pilot program is successful, the same methodology can be extended to other KPIs.",,Retired,Neither,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,No,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.6507936507936508,"Takes monthly historical data for underlying components used to calculate KPIs and creates near-term forecasts for the upcoming fiscal year. Pilot effort focuses on total agency/category spend (the denominator in multiple KPIs). If the pilot program is successful, the same methodology can be extended to other KPIs.","takes monthly historical data for underlying components used to calculate kpis and creates near-term forecasts for the upcoming fiscal year. pilot effort focuses on total agency/category spend (the denominator in multiple kpis). if the pilot program is successful, the same methodology can be extended to other kpis."
Enterprise Brain,General Services Administration,GSA,GSA IT (IDT),Other,None of the above.,Enterprise Brain is a document repository by Tanjo (tanjo.ai) that leverages AI to improve document discovery.,,Retired,Neither,10/1/2022,Unknown,Unknown,Developed with contracting resources.,47QFCA21F0014,No,No,No,No,No,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.6984126984126984,Enterprise Brain is a document repository by Tanjo (tanjo.ai) that leverages AI to improve document discovery.,enterprise brain is a document repository by tanjo (tanjo.ai) that leverages ai to improve document discovery.
AWS Captcha Solver,General Services Administration,GSA,GSA IT (IDT),Other,None of the above.,"As part of a project to automate actions in AWS console using Selenium, we developed an AI model that can solve AWS's captcha so our automated system won't be blocked for being detected as a bot. The model is used in an AWS Lambda that uses a headless Selenium driver. The model was trained from scratch using Tensorflow. We have discussed making a more general form of this solution that could be applied to other AI/ML problems.",It outputs the solution to a captcha.,Initiated,Neither,1/1/2022,1/1/2022,1/1/2022,Developed in-house.,Unknown,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,No,,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.7619047619047619,"As part of a project to automate actions in AWS console using Selenium, we developed an AI model that can solve AWS's captcha so our automated system won't be blocked for being detected as a bot. The model is used in an AWS Lambda that uses a headless Selenium driver. The model was trained from scratch using Tensorflow. We have discussed making a more general form of this solution that could be applied to other AI/ML problems. . It outputs the solution to a captcha.","as part of a project to automate actions in aws console using selenium, we developed an ai model that can solve aws's captcha so our automated system won't be blocked for being detected as a bot. the model is used in an aws lambda that uses a headless selenium driver. the model was trained from scratch using tensorflow. we have discussed making a more general form of this solution that could be applied to other ai/ml problems. . it outputs the solution to a captcha."
ServiceNow Generic Ticket Classification,General Services Administration,GSA,GSA IT (IDT),Other,None of the above.,"We are building a model to take generic Service Now tickets and classify them so that they can be automatically re-routed to the correct team that handles these types of tickets. The process of re-routing generic tickets is currently done manually, so the model will allow us to automate it. The initial model will target the top 5 most common ticket types.",It outputs the most likely group that a Servicenow ticket should be assigned to.,Initiated,Neither,10/1/2022,10/1/2022,Unknown,Developed with contracting resources.,47QFCA21F0014,No,No,Yes,No,Yes,Service Now Ticket Data,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Yes,EIP,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,No,,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.7936507936507936,"We are building a model to take generic Service Now tickets and classify them so that they can be automatically re-routed to the correct team that handles these types of tickets. The process of re-routing generic tickets is currently done manually, so the model will allow us to automate it. The initial model will target the top 5 most common ticket types. . It outputs the most likely group that a Servicenow ticket should be assigned to.","we are building a model to take generic service now tickets and classify them so that they can be automatically re-routed to the correct team that handles these types of tickets. the process of re-routing generic tickets is currently done manually, so the model will allow us to automate it. the initial model will target the top 5 most common ticket types. . it outputs the most likely group that a servicenow ticket should be assigned to."
ServiceNow Virtual Agent (Curie),General Services Administration,GSA,GSA IT (IDT),Other,None of the above.,"Virtual agent that uses ML to provide predictive results for chat entries. A natural language chatbot (virtual assistant), we named Curie, as part of a multi-model customer service experience for employee's IT service requests leveraging knowledge-based articles.",Text response generated the most likely answer base on the user input. ,Initiated,Neither,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,Yes,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Unknown,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.6984126984126984,"Virtual agent that uses ML to provide predictive results for chat entries. A natural language chatbot (virtual assistant), we named Curie, as part of a multi-model customer service experience for employee's IT service requests leveraging knowledge-based articles. . Text response generated the most likely answer base on the user input.","virtual agent that uses ml to provide predictive results for chat entries. a natural language chatbot (virtual assistant), we named curie, as part of a multi-model customer service experience for employee's it service requests leveraging knowledge-based articles. . text response generated the most likely answer base on the user input."
GREX AI Document Classification,General Services Administration,GSA,PBS,Government Services (includes Benefits and Service Delivery),None of the above.,"Office of Leasing and IT Modernization partnered in a pilot program to test the feasibility of using artificial intelligence (AI) and machine learning (ML) using the AWS cloud service from FAS to classify pdf documents emailed into G-REX. The pilot built the pipeline to train individual document types that are typically uploaded in PDF format. The pilot was successful at illustrating how valuable time could be saved by completing this project in production, improving accuracy of document type assignment.
The pilot utilized AWS FAS service, which would require additional funding. Appian announced they will provide the functionality as an out of the box feature in late 2024, early 2025, to be able to scan pdf document types if G-REX can move to the cloud. The cloud assessment is ongoing as of submission of the need.
This was an approved FY23 Need.",,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,No,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.6349206349206349,"Office of Leasing and IT Modernization partnered in a pilot program to test the feasibility of using artificial intelligence (AI) and machine learning (ML) using the AWS cloud service from FAS to classify pdf documents emailed into G-REX. The pilot built the pipeline to train individual document types that are typically uploaded in PDF format. The pilot was successful at illustrating how valuable time could be saved by completing this project in production, improving accuracy of document type assignment.
The pilot utilized AWS FAS service, which would require additional funding. Appian announced they will provide the functionality as an out of the box feature in late 2024, early 2025, to be able to scan pdf document types if G-REX can move to the cloud. The cloud assessment is ongoing as of submission of the need.
This was an approved FY23 Need.","office of leasing and it modernization partnered in a pilot program to test the feasibility of using artificial intelligence (ai) and machine learning (ml) using the aws cloud service from fas to classify pdf documents emailed into g-rex. the pilot built the pipeline to train individual document types that are typically uploaded in pdf format. the pilot was successful at illustrating how valuable time could be saved by completing this project in production, improving accuracy of document type assignment. the pilot utilized aws fas service, which would require additional funding. appian announced they will provide the functionality as an out of the box feature in late 2024, early 2025, to be able to scan pdf document types if g-rex can move to the cloud. the cloud assessment is ongoing as of submission of the need. this was an approved fy23 need."
Contract Acquisition Lifecycle Intelligence (CALI),General Services Administration,GSA,FAS (QP0A),Mission-Enabling,None of the above.,"CALI tool is an automated machine learning evaluation tool built to streamline the evaluation of vendor proposals against the solicitation requirements to support the Source Selection process. CALI is offered by Octo Consulting as Infrastructure as a Service and is implemented and hosted within Amazon Web Services (AWS) public cloud US East and US West regions. Once the Contracting Officer (CO) has received vendor proposals for a solicitation and is ready to perform the evaluation process, the CO will initiate evaluation by sending solicitation documents along with all associated vendor proposal documents to Bizagi’s Source Selection module, which will pass all documents to CALI. CALI will process the documents, associated metadata and begin analyzing the proposals in four key areas: format compliance, forms validation, reps and certs compliance, and requirements compliance. The designated evaluation members can review the evaluation results in CALI and submit finalized evaluation results back to Source Selection in Bizagi. CALI is currently being trained with sample data from the EULAs under the Multiple Award Schedule program.",,Retired,Neither,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,No,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.6507936507936508,"CALI tool is an automated machine learning evaluation tool built to streamline the evaluation of vendor proposals against the solicitation requirements to support the Source Selection process. CALI is offered by Octo Consulting as Infrastructure as a Service and is implemented and hosted within Amazon Web Services (AWS) public cloud US East and US West regions. Once the Contracting Officer (CO) has received vendor proposals for a solicitation and is ready to perform the evaluation process, the CO will initiate evaluation by sending solicitation documents along with all associated vendor proposal documents to Bizagi’s Source Selection module, which will pass all documents to CALI. CALI will process the documents, associated metadata and begin analyzing the proposals in four key areas: format compliance, forms validation, reps and certs compliance, and requirements compliance. The designated evaluation members can review the evaluation results in CALI and submit finalized evaluation results back to Source Selection in Bizagi. CALI is currently being trained with sample data from the EULAs under the Multiple Award Schedule program.","cali tool is an automated machine learning evaluation tool built to streamline the evaluation of vendor proposals against the solicitation requirements to support the source selection process. cali is offered by octo consulting as infrastructure as a service and is implemented and hosted within amazon web services (aws) public cloud us east and us west regions. once the contracting officer (co) has received vendor proposals for a solicitation and is ready to perform the evaluation process, the co will initiate evaluation by sending solicitation documents along with all associated vendor proposal documents to bizagi’s source selection module, which will pass all documents to cali. cali will process the documents, associated metadata and begin analyzing the proposals in four key areas: format compliance, forms validation, reps and certs compliance, and requirements compliance. the designated evaluation members can review the evaluation results in cali and submit finalized evaluation results back to source selection in bizagi. cali is currently being trained with sample data from the eulas under the multiple award schedule program."
Survey Comment Ham / Spam Tester,General Services Administration,GSA,FAS (TTS),Other,None of the above.,The website USA.gov receives a lot of survey comments. We need a way to determine which comments are worth the time of analysts reading and which are not.,"A determination of whether the data is ""worth reading""",Retired,Neither,Unknown,4/1/2020,7/1/2020,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,No,Compared to thousands of evaluations made by people,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,USA.gov,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.7619047619047619,"The website USA.gov receives a lot of survey comments. We need a way to determine which comments are worth the time of analysts reading and which are not. . A determination of whether the data is ""worth reading""","the website usa.gov receives a lot of survey comments. we need a way to determine which comments are worth the time of analysts reading and which are not. . a determination of whether the data is ""worth reading"""
Classifying Qualitative Data with Medallia,General Services Administration,GSA,FAS (TTS),Other,None of the above.,"USA.gov has a lot of customer-driven qualitative data to be classified into topics: survey comments from multiple surveys, web chat transcripts, contact center agent case notes, and search box queries. It is important to know the topics of the text chunks. This is very challenging because of the massive diversity of USA.gov's content and topics covered. The focus is on Medallia, which offers natural language processing in which users can create rules based on words and their relationships with other words to tag qualitative data with our topics (passports, tax refunds, etc.). Medallia also uses AI to do classifying on its own. However, value has not been derived from this functionality and it is therefore not in use.
",Topic Groups / Classification,Acquisition and/or Development,Neither,4/1/2020,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,No,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,Yes,USA.gov,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.6984126984126984,"USA.gov has a lot of customer-driven qualitative data to be classified into topics: survey comments from multiple surveys, web chat transcripts, contact center agent case notes, and search box queries. It is important to know the topics of the text chunks. This is very challenging because of the massive diversity of USA.gov's content and topics covered. The focus is on Medallia, which offers natural language processing in which users can create rules based on words and their relationships with other words to tag qualitative data with our topics (passports, tax refunds, etc.). Medallia also uses AI to do classifying on its own. However, value has not been derived from this functionality and it is therefore not in use. . Topic Groups / Classification","usa.gov has a lot of customer-driven qualitative data to be classified into topics: survey comments from multiple surveys, web chat transcripts, contact center agent case notes, and search box queries. it is important to know the topics of the text chunks. this is very challenging because of the massive diversity of usa.gov's content and topics covered. the focus is on medallia, which offers natural language processing in which users can create rules based on words and their relationships with other words to tag qualitative data with our topics (passports, tax refunds, etc.). medallia also uses ai to do classifying on its own. however, value has not been derived from this functionality and it is therefore not in use. . topic groups / classification"
Elastic Machine Learning Threat Detection,General Services Administration,GSA,GSA,Other,None of the above.,Use AI to further analyze security logs and alert on anomalous data patterns for a human SOC analyst to review. The goal is for a human/AI partnership that can more effectively and quickly search through the data.,Write anomaly data to Security Information and Event Management (SIEM) tool and create alerts for Security Operations Center (SOC) team.,Acquisition and/or Development,Neither,4/1/2022,Unknown,Unknown,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Yes,"Office of Leasing and IT Modernization partnered in a pilot program to test the feasibility of using artificial intelligence and machine learning using the Amazon Web Services (AWS) cloud service from the GSA Federal Acquisition Service (FAS) to classify pdf documents emailed into G-REX. The pilot built the pipeline to train individual document types that are typically uploaded in PDF format. The pilot was successful at illustrating how valuable time could be saved by completing this project in production, improving accuracy of document type assignment. The pilot utilized AWS FAS service, which would require additional funding. Appian announced they will provide the functionality as an out-of-the-box feature in late 2024, early 2025, to be able to scan pdf document types if G-REX can move to the cloud. The cloud assessment is ongoing as of submission of the need. This was an approved FY23 need.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,I-SecTools,Less than 6 months,Yes,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,No,There are no key risks currently as this system is designed to run beside a traditional human-operated SOC. All outputs are consumed by SOC operators and verified. No automated activity is taken by the AI beyond generating the initial alerts.,Planned or in-progress,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,No demographic data is collected by the SIEM tool. No demographic data fed to AI system for training or inference.,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.746031746031746,Use AI to further analyze security logs and alert on anomalous data patterns for a human SOC analyst to review. The goal is for a human/AI partnership that can more effectively and quickly search through the data. . Write anomaly data to Security Information and Event Management (SIEM) tool and create alerts for Security Operations Center (SOC) team. . There are no key risks currently as this system is designed to run beside a traditional human-operated SOC. All outputs are consumed by SOC operators and verified. No automated activity is taken by the AI beyond generating the initial alerts.,use ai to further analyze security logs and alert on anomalous data patterns for a human soc analyst to review. the goal is for a human/ai partnership that can more effectively and quickly search through the data. . write anomaly data to security information and event management (siem) tool and create alerts for security operations center (soc) team. . there are no key risks currently as this system is designed to run beside a traditional human-operated soc. all outputs are consumed by soc operators and verified. no automated activity is taken by the ai beyond generating the initial alerts.
Elastic Machine Learning Threat Detection - Phase 2,General Services Administration,GSA,GSA,Other,None of the above.,"Leverage an LLM that has been given access to our security logs, to provide a more efficient search and discovery tool for SOC analysts. The benefit would be helping the SOC analyst find records of interest and/or associated logs quicker. It will also analyze logs automatically and suggest records of interest.",Text displayed on the user's screen,Initiated,Neither,Q4FY25,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Yes,System and application logs collected into our SIEM tool,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,"Yes – agency has access to source code, but it is not public.",Yes,I-SecTools,Less than 6 months,Yes,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,No key risks identified. No PII or other demographic data fed into the AI system.,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,No demographic data is collected by the SIEM tool. No demographic data fed to AI system for training or inference.,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.746031746031746,"Leverage an LLM that has been given access to our security logs, to provide a more efficient search and discovery tool for SOC analysts. The benefit would be helping the SOC analyst find records of interest and/or associated logs quicker. It will also analyze logs automatically and suggest records of interest. . Text displayed on the user's screen . No key risks identified. No PII or other demographic data fed into the AI system.","leverage an llm that has been given access to our security logs, to provide a more efficient search and discovery tool for soc analysts. the benefit would be helping the soc analyst find records of interest and/or associated logs quicker. it will also analyze logs automatically and suggest records of interest. . text displayed on the user's screen . no key risks identified. no pii or other demographic data fed into the ai system."
Chatbot for Federal Acquisition Community,General Services Administration,GSA,FAS / GSA IT (IC),Other,None of the above.,"The introduction of a chatbot will enable the GSA FAS NCSC to streamline the customer experience process, and automate providing answers to documented commonly asked questions through public facing knowledge articles. The end goal is this will reduce staffing requirements for NCSC’s live chat programs and allow the NCSC resources to be dedicated to other proactive customer services initiatives. Customers will still have the option to connect to a live agent if they choose by requesting an agent.",Text displayed on the user's screen,Initiated,Neither,Q4FY25,Unknown,Unknown,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,No key risks identified. No PII or other demographic data fed into the AI system.,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.6825396825396826,"The introduction of a chatbot will enable the GSA FAS NCSC to streamline the customer experience process, and automate providing answers to documented commonly asked questions through public facing knowledge articles. The end goal is this will reduce staffing requirements for NCSC’s live chat programs and allow the NCSC resources to be dedicated to other proactive customer services initiatives. Customers will still have the option to connect to a live agent if they choose by requesting an agent. . Text displayed on the user's screen . No key risks identified. No PII or other demographic data fed into the AI system.","the introduction of a chatbot will enable the gsa fas ncsc to streamline the customer experience process, and automate providing answers to documented commonly asked questions through public facing knowledge articles. the end goal is this will reduce staffing requirements for ncsc’s live chat programs and allow the ncsc resources to be dedicated to other proactive customer services initiatives. customers will still have the option to connect to a live agent if they choose by requesting an agent. . text displayed on the user's screen . no key risks identified. no pii or other demographic data fed into the ai system."
Document Workflow / Intelligent Data Capture and Extraction,General Services Administration,GSA,GSA IT (IC),Other,None of the above.,"GSA is driving towards a more accurate and scalable document workflow platform. GSA seeks to intelligently capture, classify, and transfer critical data from unstructured and structured documents, namely PDF files, to the right process, workflow, or decision engine.",,Retired,Neither,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,No,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.6507936507936508,"GSA is driving towards a more accurate and scalable document workflow platform. GSA seeks to intelligently capture, classify, and transfer critical data from unstructured and structured documents, namely PDF files, to the right process, workflow, or decision engine.","gsa is driving towards a more accurate and scalable document workflow platform. gsa seeks to intelligently capture, classify, and transfer critical data from unstructured and structured documents, namely pdf files, to the right process, workflow, or decision engine."
OAS Kudos Chatbot,General Services Administration,GSA,OAS / OCFO,Other,None of the above.,"The Office of Administrative Services would like to utilize a ChatBot to easily capture employee peer to peer recognitions. The OAS Kudos Chatbot leverages Natural Language Processing, inherent to Google Dialogflow, to facilitate a conversation between the AI software and the end user. This allows the chatbot to interpret and apply branching logic to what the user enters into the chat window.",,Retired,Neither,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,No,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.6507936507936508,"The Office of Administrative Services would like to utilize a ChatBot to easily capture employee peer to peer recognitions. The OAS Kudos Chatbot leverages Natural Language Processing, inherent to Google Dialogflow, to facilitate a conversation between the AI software and the end user. This allows the chatbot to interpret and apply branching logic to what the user enters into the chat window.","the office of administrative services would like to utilize a chatbot to easily capture employee peer to peer recognitions. the oas kudos chatbot leverages natural language processing, inherent to google dialogflow, to facilitate a conversation between the ai software and the end user. this allows the chatbot to interpret and apply branching logic to what the user enters into the chat window."
NCMMS AI Chatbot,General Services Administration,GSA,"PBS, GSA IT (IC)",Other,None of the above.,"Provide inline tech support for NCMMS / IBM Maximo via a  chatbot.  

This is an off-the-shelf product that provides natural language driven, contextual help using Maximo.","Chatbot responses to users questions about how to use NCMMS / Maximo, based on NCMMS training and support documents, videos, etc.",Acquisition and/or Development,Neither,Unknown,7/1/2024,TODO,Developed with contracting resources.,47QFWA24F0012,No,No,No,No,Other,We will train the system using our NCMMS program's training and  support documents,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,"OMNI Maximo Assistant.

NCMMS is GSA's program name for IBM Maximo.

OMNI is TRM's product name for their AI-enhanced inline help tool for Maximo",Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,N/A.  It's an inline tech support tool for NCMMS / IBM Maximo,Post-transaction customer feedback collections,Yes,Neither,0.7619047619047619,"Provide inline tech support for NCMMS / IBM Maximo via a  chatbot.  

This is an off-the-shelf product that provides natural language driven, contextual help using Maximo. . Chatbot responses to users questions about how to use NCMMS / Maximo, based on NCMMS training and support documents, videos, etc.","provide inline tech support for ncmms / ibm maximo via a chatbot. this is an off-the-shelf product that provides natural language driven, contextual help using maximo. . chatbot responses to users questions about how to use ncmms / maximo, based on ncmms training and support documents, videos, etc."
Login.gov,General Services Administration,GSA,FAS (TTS),Other,None of the above.,"The purpose of Login.gov's identity verification service is to establish a link between the claimed identity and the subject creating a user account. The facial matching AI component of this service provides enhanced controls to protect users from identity fraud and the government from fraudulent access. 

The benefit to the user is to provide a fast, convenient way to prove their identity remotely and protect the public from identity theft.

The benefit to the government is to that government partners may use this tool to verify the identities of people accessing government websites in a remote, unattended session and protect Government systems from fraudulent access.",The system delivers a match/no-match output.,Implementation and Assessment,"Rights-Impacting
",10/1/2023,4/12/2024,5/1/2024,Developed with contracting resources.,47QPCA24F0002,Yes,No,No,Yes,Yes,"The model is not trained or tuned using agency data.

GSA evaluated several facial-matching models in its Identity Proofing Performance Study using GSA data in a testing environment. More information about that study and the data it collected is available on this website and in GSA's Privacy Impact Assessment for the Study, which is located here under ""GSA Identity Proofing Equity Study"".

In addition to in-depth studies like above, GSA upholds best product monitoring practices, such as evaluating success rates at each identity verification step, including facial matching. ","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,Yes,"Login serves HISPs at VA, USDA, and SSA",Less than 6 months,Yes,Other,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Yes – Agency requested an extension for this use case.,Yes,"AI outputs that are biased in certain ways, including by being discriminatory or have a discriminatory effect. This was identified by agency personnel through literature reviews and initial testing of facial matching algorithms.",Yes – by another appropriate agency office that was not directly involved in the system’s development,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"To assess the potential risk of disparate outcomes, GSA is conducting the GSA Identity Proofing Performance Study (the “Study”) to understand how various vendors’ facial matching services perform across various demographics in uncontrolled environments similar to how this AI use case would operate.
- The preliminary results of the Study have already been helpful in acquisition and design decisions to mitigate the risk of biased outcomes. For example, Login.gov requested that BPA Call Order vendors provide both internal and independent research on their products, including requesting the NIST FRTE 1:1 Demographic Differentials Summary that examines the false negative rate of 1:1 facial matching algorithms for different demographics, which includes testing the algorithms with photographs of people who are African or African American.
- Over FY25, the Identity Proofing Performance Study team and academic partners will continue to evaluate the data and provide Login.gov with additional recommendations, which Login.gov will consider when maintaining and developing its service. For example, data from Login.gov’s operational identity-proofing process will be compared to similar datasets from the Study to develop and validate continuous performance indicators.
",Post-transaction customer feedback collections,Yes,Rights-Impacting,0.7936507936507936,"The purpose of Login.gov's identity verification service is to establish a link between the claimed identity and the subject creating a user account. The facial matching AI component of this service provides enhanced controls to protect users from identity fraud and the government from fraudulent access. 

The benefit to the user is to provide a fast, convenient way to prove their identity remotely and protect the public from identity theft.

The benefit to the government is to that government partners may use this tool to verify the identities of people accessing government websites in a remote, unattended session and protect Government systems from fraudulent access. . The system delivers a match/no-match output. . AI outputs that are biased in certain ways, including by being discriminatory or have a discriminatory effect. This was identified by agency personnel through literature reviews and initial testing of facial matching algorithms.","the purpose of login.gov's identity verification service is to establish a link between the claimed identity and the subject creating a user account. the facial matching ai component of this service provides enhanced controls to protect users from identity fraud and the government from fraudulent access. the benefit to the user is to provide a fast, convenient way to prove their identity remotely and protect the public from identity theft. the benefit to the government is to that government partners may use this tool to verify the identities of people accessing government websites in a remote, unattended session and protect government systems from fraudulent access. . the system delivers a match/no-match output. . ai outputs that are biased in certain ways, including by being discriminatory or have a discriminatory effect. this was identified by agency personnel through literature reviews and initial testing of facial matching algorithms."
Gemini for Workspace,General Services Administration,GSA,GSA IT (IDE),Government Services (includes Benefits and Service Delivery),None of the above.,"The purpose of the Gemini for Workspace pilot study is to evaluate how advanced AI capabilities can enhance productivity, collaboration, and efficiency within the General Services Administration (GSA). Over a 90-day period, 200 users will interact with Gemini to assess its functionality and impact on daily operations. The system offers intelligent assistance for tasks like drafting documents and composing emails, enhances communication through smart scheduling and automated meeting summaries, provides data analysis and insights from large datasets, and automates routine administrative tasks to reduce manual workloads. All features available with Gemini Enterprise feature set will be made available within the test profile instance for pilot users. These features aim to help users accomplish productivity tasks more quickly and accurately, and purport to increase productivity across the agency. Enhanced communication tools are designed to facilitate better teamwork and coordination among staff, while access to advanced analytics could support data-driven decision-making. By automating routine tasks, employees can focus on higher-value activities, contributing further to operational efficiency and innovation. Access to the latest Gemini models can also support code development and debugging practices previously identified in earlier studies. The pilot will help GSA understand user engagement with AI tools and assess the practical benefits of integrating Gemini into the Google workspace. Users will only be permitted to use the features for non-sensitive use cases that involve public information and datasets. ","Feedback from this study will inform future AI investments and strategies, supporting GSA's objective to modernize its technology infrastructure and enhance the agency's effectiveness in fulfilling its mission.",Acquisition and/or Development,Neither,11/1/2024,11/1/2024,11/1/2024,Developed with contracting resources.,47HAA022F0129,No,No,No,Yes,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.7142857142857143,"The purpose of the Gemini for Workspace pilot study is to evaluate how advanced AI capabilities can enhance productivity, collaboration, and efficiency within the General Services Administration (GSA). Over a 90-day period, 200 users will interact with Gemini to assess its functionality and impact on daily operations. The system offers intelligent assistance for tasks like drafting documents and composing emails, enhances communication through smart scheduling and automated meeting summaries, provides data analysis and insights from large datasets, and automates routine administrative tasks to reduce manual workloads. All features available with Gemini Enterprise feature set will be made available within the test profile instance for pilot users. These features aim to help users accomplish productivity tasks more quickly and accurately, and purport to increase productivity across the agency. Enhanced communication tools are designed to facilitate better teamwork and coordination among staff, while access to advanced analytics could support data-driven decision-making. By automating routine tasks, employees can focus on higher-value activities, contributing further to operational efficiency and innovation. Access to the latest Gemini models can also support code development and debugging practices previously identified in earlier studies. The pilot will help GSA understand user engagement with AI tools and assess the practical benefits of integrating Gemini into the Google workspace. Users will only be permitted to use the features for non-sensitive use cases that involve public information and datasets. . Feedback from this study will inform future AI investments and strategies, supporting GSA's objective to modernize its technology infrastructure and enhance the agency's effectiveness in fulfilling its mission.","the purpose of the gemini for workspace pilot study is to evaluate how advanced ai capabilities can enhance productivity, collaboration, and efficiency within the general services administration (gsa). over a 90-day period, 200 users will interact with gemini to assess its functionality and impact on daily operations. the system offers intelligent assistance for tasks like drafting documents and composing emails, enhances communication through smart scheduling and automated meeting summaries, provides data analysis and insights from large datasets, and automates routine administrative tasks to reduce manual workloads. all features available with gemini enterprise feature set will be made available within the test profile instance for pilot users. these features aim to help users accomplish productivity tasks more quickly and accurately, and purport to increase productivity across the agency. enhanced communication tools are designed to facilitate better teamwork and coordination among staff, while access to advanced analytics could support data-driven decision-making. by automating routine tasks, employees can focus on higher-value activities, contributing further to operational efficiency and innovation. access to the latest gemini models can also support code development and debugging practices previously identified in earlier studies. the pilot will help gsa understand user engagement with ai tools and assess the practical benefits of integrating gemini into the google workspace. users will only be permitted to use the features for non-sensitive use cases that involve public information and datasets. . feedback from this study will inform future ai investments and strategies, supporting gsa's objective to modernize its technology infrastructure and enhance the agency's effectiveness in fulfilling its mission."
Test Fit Layouts,General Services Administration,GSA,GSA,Government Services (includes Benefits and Service Delivery),None of the above.,The purpose of the QBIQ pilot is to test the capabilities to rapidly test-fit spaces in consideration to evaluate whether AI can sufficiently be used to speed up vetting of space to fit requirements as well as utilize visualizations as a communication bridging tool with tenant agencies,Floor plan and 3D conceptual layouts,Acquisition and/or Development,Neither,4/1/2024,Unknown,Unknown,Developed with contracting resources.,One-time micro-purchase for 3 test fits,No,No,No,Yes,No,Floor plan and 3D conceptual layouts,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,qbiq.ai,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,Exposure of floorplans,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Neither,0.7301587301587301,The purpose of the QBIQ pilot is to test the capabilities to rapidly test-fit spaces in consideration to evaluate whether AI can sufficiently be used to speed up vetting of space to fit requirements as well as utilize visualizations as a communication bridging tool with tenant agencies . Floor plan and 3D conceptual layouts . Exposure of floorplans,the purpose of the qbiq pilot is to test the capabilities to rapidly test-fit spaces in consideration to evaluate whether ai can sufficiently be used to speed up vetting of space to fit requirements as well as utilize visualizations as a communication bridging tool with tenant agencies . floor plan and 3d conceptual layouts . exposure of floorplans
Design Your Facility,Department of Health and Human Services,HHS,ACF,Mission-Enabling (internal agency support),None of the above.,"The Office of Refugee Resettlement (ORR) manages many facilities for unaccompanied children and must regularly determine bed availability, configuration, and assignments. The current process for gathering this information takes lengthy training and assistance from system administrators.

The Design Your Facility interface leverages LLMs to help users more quickly configure the correct building, room, and bed layout needed to create their facility’s “digital twin” in the UC Bed Network Facility Mapper without lengthy training or deskside assistance from system administrators.","The Design Your Facility tool uses large language models (LLMs) to transform building layout information shared by users in a conversational manner into data about buildings, rooms, and bed layers within the Unaccompanied Children (UC) Bed Network tooling.

The AI model suggests object configurations that should be applied to correctly model a facility’s “digital twin” of the building, room, and bed layout. The workflow relies solely on user descriptions and user confirmations.",Acquisition and/or Development,Neither,2/1/2024,4/1/2024,Unknown,Developed with contracting resources.,75D30122F15723,Unknown,Unknown,Unknown,Unknown,No,No training or fine-tuning; we are using secure commercially available LLMs.,Documentation has been partially completed,Unknown,Unknown,Yes,Horizon (ACF instance of Palantir Foundry),Less than 6 months,No,Yes,No,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"The Office of Refugee Resettlement (ORR) manages many facilities for unaccompanied children and must regularly determine bed availability, configuration, and assignments. The current process for gathering this information takes lengthy training and assistance from system administrators.

The Design Your Facility interface leverages LLMs to help users more quickly configure the correct building, room, and bed layout needed to create their facility’s “digital twin” in the UC Bed Network Facility Mapper without lengthy training or deskside assistance from system administrators. . The Design Your Facility tool uses large language models (LLMs) to transform building layout information shared by users in a conversational manner into data about buildings, rooms, and bed layers within the Unaccompanied Children (UC) Bed Network tooling.

The AI model suggests object configurations that should be applied to correctly model a facility’s “digital twin” of the building, room, and bed layout. The workflow relies solely on user descriptions and user confirmations.","the office of refugee resettlement (orr) manages many facilities for unaccompanied children and must regularly determine bed availability, configuration, and assignments. the current process for gathering this information takes lengthy training and assistance from system administrators. the design your facility interface leverages llms to help users more quickly configure the correct building, room, and bed layout needed to create their facility’s “digital twin” in the uc bed network facility mapper without lengthy training or deskside assistance from system administrators. . the design your facility tool uses large language models (llms) to transform building layout information shared by users in a conversational manner into data about buildings, rooms, and bed layers within the unaccompanied children (uc) bed network tooling. the ai model suggests object configurations that should be applied to correctly model a facility’s “digital twin” of the building, room, and bed layout. the workflow relies solely on user descriptions and user confirmations."
Discover Financial Business Intelligence System (FBIS) Report Analysis (Sub-CAN Line Items),Department of Health and Human Services,HHS,ACF,Mission-Enabling (internal agency support),None of the above.,"This project provides a resource for ACF Discover users in the budget community to identify related line items across disparate reports from FBIS. The process searches the data and aggregates information (e.g. supplier name, document number, total obligations) to create an up-to-date, real-time spend plan that pulls from the open-closed report, requisition purchase order report, and integrates with funds distribution report.

AI returns line items from the open-closed report and requisition purchase order report related to the sub-CAN line item description such as supplier name, document number, document type, and total obligations for that sub-CAN line item. The Discover FBIS Report Analysis (Sub-CAN Line Items) LLM helps find related obligations and line items from various reports and returns information crucial for a budget officer when they are creating a spend plan for their office. The LLM aids ACF Discover track various line items and pulls the information into one place: the Spend Plan module where ACF can track yearly budgets and monitor budget health.","A large-language model helps compile an aggregated report of costs by intaking a specific Congressional Appropriation Number (CAN), category, sub-CAN line item descriptions (which are not standardized), and a projection cost from the user (either by a CSV or manual entry).",Acquisition and/or Development,Neither,8/1/2024,9/1/2024,Unknown,Developed with contracting resources.,75P00122A00010/75P00122F37003/P00004,Unknown,Unknown,Unknown,Unknown,No,No training or fine-tuning. HHS-FBIS reports are used as a data source for retrieval-augmented generation.,Documentation is complete,Unknown,Unknown,Yes,ACF Discover (HHS instance of Palantir Foundry),Less than 6 months,No,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"This project provides a resource for ACF Discover users in the budget community to identify related line items across disparate reports from FBIS. The process searches the data and aggregates information (e.g. supplier name, document number, total obligations) to create an up-to-date, real-time spend plan that pulls from the open-closed report, requisition purchase order report, and integrates with funds distribution report.

AI returns line items from the open-closed report and requisition purchase order report related to the sub-CAN line item description such as supplier name, document number, document type, and total obligations for that sub-CAN line item. The Discover FBIS Report Analysis (Sub-CAN Line Items) LLM helps find related obligations and line items from various reports and returns information crucial for a budget officer when they are creating a spend plan for their office. The LLM aids ACF Discover track various line items and pulls the information into one place: the Spend Plan module where ACF can track yearly budgets and monitor budget health. . A large-language model helps compile an aggregated report of costs by intaking a specific Congressional Appropriation Number (CAN), category, sub-CAN line item descriptions (which are not standardized), and a projection cost from the user (either by a CSV or manual entry).","this project provides a resource for acf discover users in the budget community to identify related line items across disparate reports from fbis. the process searches the data and aggregates information (e.g. supplier name, document number, total obligations) to create an up-to-date, real-time spend plan that pulls from the open-closed report, requisition purchase order report, and integrates with funds distribution report. ai returns line items from the open-closed report and requisition purchase order report related to the sub-can line item description such as supplier name, document number, document type, and total obligations for that sub-can line item. the discover fbis report analysis (sub-can line items) llm helps find related obligations and line items from various reports and returns information crucial for a budget officer when they are creating a spend plan for their office. the llm aids acf discover track various line items and pulls the information into one place: the spend plan module where acf can track yearly budgets and monitor budget health. . a large-language model helps compile an aggregated report of costs by intaking a specific congressional appropriation number (can), category, sub-can line item descriptions (which are not standardized), and a projection cost from the user (either by a csv or manual entry)."
Grant Spend Health Analysis,Department of Health and Human Services,HHS,ACF,Mission-Enabling (internal agency support),None of the above.,"The mission of the Office of Grants Management (OGM)is to provide high-quality fiscal stewardship and management for ACF-funded recipients across the country and in U.S. territories. This includes responsive oversight and technical assistance for discretionary awards, non-discretionary awards, and cooperative agreements. OGM seeks ways to prioritize proactive technical assistance to grant recipients based on likelihood that awarded funds may not be fully utilized. 

The goal is to identify still-active grants that are potentially on the path towards deobligating funding. This information would be visible to grant specialists within ACF. OGM anticipates developing customized training and technical assistance materials that can assist recipients if we predict that their grant is trending towards leaving funds unutilized.","As part of this project, we are conducting exploratory analyses including supervised and unsupervised AI techniques to determine whether the data we have on grant spend over time can power a predictive model that reasonably identifies grants with a higher likelihood of under-utilizing funds. ",Acquisition and/or Development,Neither,3/1/2024,3/1/2024,Unknown,Developed with contracting resources.,75ACF123F80045,Unknown,Unknown,Unknown,Unknown,Yes,Grant financial data from usaspending.gov and the Payment Management System. Recipient data from SAM.gov.,Documentation is widely available,Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"The mission of the Office of Grants Management (OGM)is to provide high-quality fiscal stewardship and management for ACF-funded recipients across the country and in U.S. territories. This includes responsive oversight and technical assistance for discretionary awards, non-discretionary awards, and cooperative agreements. OGM seeks ways to prioritize proactive technical assistance to grant recipients based on likelihood that awarded funds may not be fully utilized. 

The goal is to identify still-active grants that are potentially on the path towards deobligating funding. This information would be visible to grant specialists within ACF. OGM anticipates developing customized training and technical assistance materials that can assist recipients if we predict that their grant is trending towards leaving funds unutilized. . As part of this project, we are conducting exploratory analyses including supervised and unsupervised AI techniques to determine whether the data we have on grant spend over time can power a predictive model that reasonably identifies grants with a higher likelihood of under-utilizing funds.","the mission of the office of grants management (ogm)is to provide high-quality fiscal stewardship and management for acf-funded recipients across the country and in u.s. territories. this includes responsive oversight and technical assistance for discretionary awards, non-discretionary awards, and cooperative agreements. ogm seeks ways to prioritize proactive technical assistance to grant recipients based on likelihood that awarded funds may not be fully utilized. the goal is to identify still-active grants that are potentially on the path towards deobligating funding. this information would be visible to grant specialists within acf. ogm anticipates developing customized training and technical assistance materials that can assist recipients if we predict that their grant is trending towards leaving funds unutilized. . as part of this project, we are conducting exploratory analyses including supervised and unsupervised ai techniques to determine whether the data we have on grant spend over time can power a predictive model that reasonably identifies grants with a higher likelihood of under-utilizing funds."
Structuring and Validating Completeness of Case Data Information,Department of Health and Human Services,HHS,ACF,Mission-Enabling (internal agency support),None of the above.,"In accordance with the Ms L vs ICE settlement, the U.S. Customs and Border Protection (CBP) must provide specific pieces of information about family separations. Beginning November 2024, CPB will release this information. ACF's Office of Refugee Resettlement (ORR) plans to add family separation information to existing child profile information for reference.

The AI output structures free form data into a more easily searchable format. These data will be used to assist in the placement process.","Creates a structured data asset of  the critical data points needed about a separation case. AI will be used to conduct initial parsing of the data provided by CBP and highlight whether or not the required fields from the Ms L vs. ICE settlement are included and can therefore be updated into the child's profile in ORR's data system.  ORR's Intakes Team does final review and in cases where data appears to be missing, the ORR Intakes Team reaches back out to CBP for that information.",Acquisition and/or Development,Safety-Impacting,8/1/2024,11/1/2024,Unknown,Developed with contracting resources.,75D30122F15723,Unknown,Unknown,Unknown,Unknown,No,No training or fine tuning; we are prompting secure commercially-available LLMs.,Documentation has been partially completed,Unknown,Unknown,Yes,Horizon (ACF instance of Palantir Foundry),Less than 6 months,No,Yes,No,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Safety-Impacting,0.4126984126984127,"In accordance with the Ms L vs ICE settlement, the U.S. Customs and Border Protection (CBP) must provide specific pieces of information about family separations. Beginning November 2024, CPB will release this information. ACF's Office of Refugee Resettlement (ORR) plans to add family separation information to existing child profile information for reference.

The AI output structures free form data into a more easily searchable format. These data will be used to assist in the placement process. . Creates a structured data asset of  the critical data points needed about a separation case. AI will be used to conduct initial parsing of the data provided by CBP and highlight whether or not the required fields from the Ms L vs. ICE settlement are included and can therefore be updated into the child's profile in ORR's data system.  ORR's Intakes Team does final review and in cases where data appears to be missing, the ORR Intakes Team reaches back out to CBP for that information.","in accordance with the ms l vs ice settlement, the u.s. customs and border protection (cbp) must provide specific pieces of information about family separations. beginning november 2024, cpb will release this information. acf's office of refugee resettlement (orr) plans to add family separation information to existing child profile information for reference. the ai output structures free form data into a more easily searchable format. these data will be used to assist in the placement process. . creates a structured data asset of the critical data points needed about a separation case. ai will be used to conduct initial parsing of the data provided by cbp and highlight whether or not the required fields from the ms l vs. ice settlement are included and can therefore be updated into the child's profile in orr's data system. orr's intakes team does final review and in cases where data appears to be missing, the orr intakes team reaches back out to cbp for that information."
Triaging Notice of Concern Submissions,Department of Health and Human Services,HHS,ACF,Government Services (includes Benefits and Service Delivery),None of the above.,"The Office of Refugee Resettlement (ORR) has amassed a large backlog of Notice of Concern (NOC) PDF forms that contain critical information regarding safety of children who have left ORR's care. 

The AI-powered triage tool will support Office of Refugee Resettlement staff more effectively and efficiently review NOCs by helping structure and prioritize received NOCs. Through more effective triage, ORR will begin to cut down on the large backlog that has accumulated.","In the NOC triage system, AI will be used for two purposes:

1. We will use AI to parse the NOCs to generate structured fields from the text and validate categorizations.

2. AI will provide prioritization recommendations -- but not make decisions -- on the most critical NOCs for the ORR team to review first. ",Acquisition and/or Development,Safety-Impacting,6/1/2024,6/1/2024,Unknown,Developed with contracting resources.,75D30122F15723,Unknown,Unknown,Unknown,Unknown,No,No training or fine tuning.  Notional NOCs were used to prompt engineer secure commercially-available LLMs and evaluate performance. End-user testing was used to evaluate on real NOCs.,Documentation has been partially completed,Unknown,Unknown,Yes,Horizon (ACF instance of Palantir Foundry),Less than 6 months,No,Yes,No,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Safety-Impacting,0.4126984126984127,"The Office of Refugee Resettlement (ORR) has amassed a large backlog of Notice of Concern (NOC) PDF forms that contain critical information regarding safety of children who have left ORR's care. 

The AI-powered triage tool will support Office of Refugee Resettlement staff more effectively and efficiently review NOCs by helping structure and prioritize received NOCs. Through more effective triage, ORR will begin to cut down on the large backlog that has accumulated. . In the NOC triage system, AI will be used for two purposes:

1. We will use AI to parse the NOCs to generate structured fields from the text and validate categorizations.

2. AI will provide prioritization recommendations -- but not make decisions -- on the most critical NOCs for the ORR team to review first.","the office of refugee resettlement (orr) has amassed a large backlog of notice of concern (noc) pdf forms that contain critical information regarding safety of children who have left orr's care. the ai-powered triage tool will support office of refugee resettlement staff more effectively and efficiently review nocs by helping structure and prioritize received nocs. through more effective triage, orr will begin to cut down on the large backlog that has accumulated. . in the noc triage system, ai will be used for two purposes: 1. we will use ai to parse the nocs to generate structured fields from the text and validate categorizations. 2. ai will provide prioritization recommendations -- but not make decisions -- on the most critical nocs for the orr team to review first."
Unaccompanied Children Program Policy & Procedure Research Tool,Department of Health and Human Services,HHS,ACF,Mission-Enabling (internal agency support),None of the above.,"The Office of Refugee Resettlement (ORR) conducts monitoring visits at least monthly to ensure that care providers meet minimum standards for the care and timely release of unaccompanied children, and that they abide by all Federal and State laws and regulations, licensing and accreditation standards, ORR policies and procedures, and child welfare standards. If ORR monitoring finds a care provider to be out of compliance with requirements, ORR issues corrective action findings and requires the care provider to resolve the issue within a specified time frame. Compliance determination involves research into the various laws, standards, policies, and procedures.

The UC Program Policy & Research Tool assists the UC monitoring team with research that in many cases take a long time. The goal of the  UC Program Policy & Research Tool is to speed up this research as children's health and well-being may be impacted before a corrective action finding is issued and the issue is resolved.","The UC Program Policy & Procedure Research Tool speeds up research of relevant laws, standards, policies, and procedures content curated and approved by ORR's policy team. This research is one part of the process that informs ORR's monitoring team's decisions on whether corrective actions are needed and if so, what corrective actions. AI is not used to suggest corrective actions but rather support determination of whether care providers are in compliance.",Acquisition and/or Development,Neither,10/1/2024,11/1/2024,Unknown,Developed with contracting resources.,75D30122F15723,Unknown,Unknown,Unknown,Unknown,Yes,No training or fine-tuning. ORR-approved policy content are used as a data source for retrieval-augmented generation.,Documentation is complete,Unknown,Unknown,Yes,Horizon (ACF instance of Palantir Foundry),More than 12 months,No,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"The Office of Refugee Resettlement (ORR) conducts monitoring visits at least monthly to ensure that care providers meet minimum standards for the care and timely release of unaccompanied children, and that they abide by all Federal and State laws and regulations, licensing and accreditation standards, ORR policies and procedures, and child welfare standards. If ORR monitoring finds a care provider to be out of compliance with requirements, ORR issues corrective action findings and requires the care provider to resolve the issue within a specified time frame. Compliance determination involves research into the various laws, standards, policies, and procedures.

The UC Program Policy & Research Tool assists the UC monitoring team with research that in many cases take a long time. The goal of the  UC Program Policy & Research Tool is to speed up this research as children's health and well-being may be impacted before a corrective action finding is issued and the issue is resolved. . The UC Program Policy & Procedure Research Tool speeds up research of relevant laws, standards, policies, and procedures content curated and approved by ORR's policy team. This research is one part of the process that informs ORR's monitoring team's decisions on whether corrective actions are needed and if so, what corrective actions. AI is not used to suggest corrective actions but rather support determination of whether care providers are in compliance.","the office of refugee resettlement (orr) conducts monitoring visits at least monthly to ensure that care providers meet minimum standards for the care and timely release of unaccompanied children, and that they abide by all federal and state laws and regulations, licensing and accreditation standards, orr policies and procedures, and child welfare standards. if orr monitoring finds a care provider to be out of compliance with requirements, orr issues corrective action findings and requires the care provider to resolve the issue within a specified time frame. compliance determination involves research into the various laws, standards, policies, and procedures. the uc program policy & research tool assists the uc monitoring team with research that in many cases take a long time. the goal of the uc program policy & research tool is to speed up this research as children's health and well-being may be impacted before a corrective action finding is issued and the issue is resolved. . the uc program policy & procedure research tool speeds up research of relevant laws, standards, policies, and procedures content curated and approved by orr's policy team. this research is one part of the process that informs orr's monitoring team's decisions on whether corrective actions are needed and if so, what corrective actions. ai is not used to suggest corrective actions but rather support determination of whether care providers are in compliance."
Ask HR Policy,Department of Health and Human Services,HHS,ACF,Mission-Enabling (internal agency support),None of the above.,"This project provides a resource for ACF Executive Officers/Administrative Officers and managers to easily navigate Human Resources (HR) Policy documentation and quickly get answers to their questions that are based on the official HR Policy Library that is on the HHS.gov website (https://www.hhs.gov/about/agencies/asa/ohr/hr-library/index.html)

Ask HR Policy provides a secure interface permissioned only to select ACF Executive Officers and Administrative Officers.  Users type in questions about managing employees covered by the HR Policy Library, and Ask HR Policy provides a narrative of its “thought process” then suggests an answer based in the documentation, alongside links that take the user to the relevant section of the official document.",We are using LLMs to analyze the questions inputted by users. The AI model synthesizes the provided documentation sources and then summarizes and answers the users questions with cited sources.,Operation and Maintenance,Neither,5/1/2024,5/1/2024,5/1/2024,Developed with contracting resources.,75P00122A00010/75P00122F37003/P00004,Unknown,No,No,No,No,No training or fine-tuning. HHS HR Policies were used as a data source for retrieval-augmented generation.,Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",Yes,ACF Discover (HHS instance of Palantir Foundry),Less than 6 months,No,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"This project provides a resource for ACF Executive Officers/Administrative Officers and managers to easily navigate Human Resources (HR) Policy documentation and quickly get answers to their questions that are based on the official HR Policy Library that is on the HHS.gov website (https://www.hhs.gov/about/agencies/asa/ohr/hr-library/index.html)

Ask HR Policy provides a secure interface permissioned only to select ACF Executive Officers and Administrative Officers.  Users type in questions about managing employees covered by the HR Policy Library, and Ask HR Policy provides a narrative of its “thought process” then suggests an answer based in the documentation, alongside links that take the user to the relevant section of the official document. . We are using LLMs to analyze the questions inputted by users. The AI model synthesizes the provided documentation sources and then summarizes and answers the users questions with cited sources.","this project provides a resource for acf executive officers/administrative officers and managers to easily navigate human resources (hr) policy documentation and quickly get answers to their questions that are based on the official hr policy library that is on the hhs.gov website (https://www.hhs.gov/about/agencies/asa/ohr/hr-library/index.html) ask hr policy provides a secure interface permissioned only to select acf executive officers and administrative officers. users type in questions about managing employees covered by the hr policy library, and ask hr policy provides a narrative of its “thought process” then suggests an answer based in the documentation, alongside links that take the user to the relevant section of the official document. . we are using llms to analyze the questions inputted by users. the ai model synthesizes the provided documentation sources and then summarizes and answers the users questions with cited sources."
Child Welfare Information Gateway OneReach Application,Department of Health and Human Services,HHS,ACF,Government Services (includes Benefits and Service Delivery),None of the above.,"The Childrens Bureau runs the Children Welfare Information Gateway, a connection to trusted resources on the child welfare continuum. The Information Gateway has a hotline for answering questions or requesting information: https://www.childwelfare.gov/stay-connected/contact/

The Information Gateway hotline automates some of the more routine questions and requests, allowing staff to focus on more complex, nuanced situations.","The Information Gateway hotline connects to a phone interactive voice response (IVR) managed by OneReach AI. OneReach maintains a database of state hotlines for reporting child abuse and neglect that it can connect a caller to based on their inbound phone area code. Additionally, OneReach offers a limited FAQ texting service that utilizes natural language processing to answer user queries. User queries are used for reinforcement training by a human AI trainer and to develop additional FAQs.",Operation and Maintenance,Neither,3/1/2020,4/1/2020,6/1/2020,Developed with contracting resources.,20EMPO0106,Unknown,Yes,Yes,No,No,Unknown,Documentation is widely available,No,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The Childrens Bureau runs the Children Welfare Information Gateway, a connection to trusted resources on the child welfare continuum. The Information Gateway has a hotline for answering questions or requesting information: https://www.childwelfare.gov/stay-connected/contact/

The Information Gateway hotline automates some of the more routine questions and requests, allowing staff to focus on more complex, nuanced situations. . The Information Gateway hotline connects to a phone interactive voice response (IVR) managed by OneReach AI. OneReach maintains a database of state hotlines for reporting child abuse and neglect that it can connect a caller to based on their inbound phone area code. Additionally, OneReach offers a limited FAQ texting service that utilizes natural language processing to answer user queries. User queries are used for reinforcement training by a human AI trainer and to develop additional FAQs.","the childrens bureau runs the children welfare information gateway, a connection to trusted resources on the child welfare continuum. the information gateway has a hotline for answering questions or requesting information: https://www.childwelfare.gov/stay-connected/contact/ the information gateway hotline automates some of the more routine questions and requests, allowing staff to focus on more complex, nuanced situations. . the information gateway hotline connects to a phone interactive voice response (ivr) managed by onereach ai. onereach maintains a database of state hotlines for reporting child abuse and neglect that it can connect a caller to based on their inbound phone area code. additionally, onereach offers a limited faq texting service that utilizes natural language processing to answer user queries. user queries are used for reinforcement training by a human ai trainer and to develop additional faqs."
Collective Bargaining Compass,Department of Health and Human Services,HHS,ACF,Mission-Enabling (internal agency support),None of the above.,"This project provides a resource for ACF managers to easily navigate Collective Bargaining Agreement documentation and quickly get answers to their questions that are based on the official 400-page rulebook.

The Collective Bargaining Compass provides a secure Virtual Assistant interface permissioned only to select ACF managers.  Users type in questions about managing employees covered by the Collective Bargaining Agreement, and the Virtual Assistant provides a narrative of its “thought process” then suggests an answer based in the documentation, alongside links that take the user to the relevant section of the official document.","We are using LLMs to analyze the questions inputted by users, which then the AI model synthesizes the provided documentation sources and then summarizes and answers the users questions with cited sources.",Operation and Maintenance,Neither,2/1/2024,2/1/2024,3/1/2024,Developed with contracting resources.,75P00122A00010/75P00122F37003/P00004,Unknown,No,No,No,No,No training or fine-tuning. HHS-The NTEU 2023 Collective Bargaining Agreement was used as a data source for retrieval-augmented generation.,Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",Yes,ACF Discover (HHS instance of Palantir Foundry),Less than 6 months,No,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"This project provides a resource for ACF managers to easily navigate Collective Bargaining Agreement documentation and quickly get answers to their questions that are based on the official 400-page rulebook.

The Collective Bargaining Compass provides a secure Virtual Assistant interface permissioned only to select ACF managers.  Users type in questions about managing employees covered by the Collective Bargaining Agreement, and the Virtual Assistant provides a narrative of its “thought process” then suggests an answer based in the documentation, alongside links that take the user to the relevant section of the official document. . We are using LLMs to analyze the questions inputted by users, which then the AI model synthesizes the provided documentation sources and then summarizes and answers the users questions with cited sources.","this project provides a resource for acf managers to easily navigate collective bargaining agreement documentation and quickly get answers to their questions that are based on the official 400-page rulebook. the collective bargaining compass provides a secure virtual assistant interface permissioned only to select acf managers. users type in questions about managing employees covered by the collective bargaining agreement, and the virtual assistant provides a narrative of its “thought process” then suggests an answer based in the documentation, alongside links that take the user to the relevant section of the official document. . we are using llms to analyze the questions inputted by users, which then the ai model synthesizes the provided documentation sources and then summarizes and answers the users questions with cited sources."
Discover User Assistant,Department of Health and Human Services,HHS,ACF,Mission-Enabling (internal agency support),None of the above.,"This project provides a resource for ACF Discover Staff Management Module users (i.e. ACF Executive Officers and other ACF administrative and management staff) to easily navigate ACF Discover Staff Management platform and instructional documentation and quickly get answers to their questions about the module’s features.

The User Documentation Assistant provides a secure virtual assistant interface that is only available to ACF Discover Users.  Users are able to ask the assistant specific questions about the capabilities of ACF Discover along with how to leverage tools and applications. The Virtual Assistant is able to provide answers by referencing the User Reference guide.","We are using LLMs to analyze the questions inputted by users, which then the AI model synthesizes the provided documentation sources and then summarizes and answers the users' questions with cited sources.",Operation and Maintenance,Neither,1/1/2024,1/1/2024,1/1/2024,Developed with contracting resources.,75P00122A00010/75P00122F37003/P00004,Unknown,No,No,No,No,No training or fine-tuning. HHS-The Discover Staff Management Module User Reference Guide was used as a data source for retrieval-augmented generation.,Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",Yes,ACF Discover (HHS instance of Palantir Foundry),Less than 6 months,No,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"This project provides a resource for ACF Discover Staff Management Module users (i.e. ACF Executive Officers and other ACF administrative and management staff) to easily navigate ACF Discover Staff Management platform and instructional documentation and quickly get answers to their questions about the module’s features.

The User Documentation Assistant provides a secure virtual assistant interface that is only available to ACF Discover Users.  Users are able to ask the assistant specific questions about the capabilities of ACF Discover along with how to leverage tools and applications. The Virtual Assistant is able to provide answers by referencing the User Reference guide. . We are using LLMs to analyze the questions inputted by users, which then the AI model synthesizes the provided documentation sources and then summarizes and answers the users' questions with cited sources.","this project provides a resource for acf discover staff management module users (i.e. acf executive officers and other acf administrative and management staff) to easily navigate acf discover staff management platform and instructional documentation and quickly get answers to their questions about the module’s features. the user documentation assistant provides a secure virtual assistant interface that is only available to acf discover users. users are able to ask the assistant specific questions about the capabilities of acf discover along with how to leverage tools and applications. the virtual assistant is able to provide answers by referencing the user reference guide. . we are using llms to analyze the questions inputted by users, which then the ai model synthesizes the provided documentation sources and then summarizes and answers the users' questions with cited sources."
Policy Knowledge Base Data Migration,Department of Health and Human Services,HHS,ACF,Mission-Enabling (internal agency support),None of the above.,"The Office of Family Assistance (OFA) provides technical assistance to grantees, including answering questions about policy regarding the Temporary Assistance for Needy Families (TANF) program. Historically, OFA has tracked its accumulated TANF knowledge across disparate locations and file types.?

Given the quantity of knowledge base articles involved in this project (>1,100), the AI draft article topics and categories helped speed up the creation of the knowledge base minimum viable product.","As part of a broader effort to establish a TANF knowledge base on a unified platform, large language models were used to: 1) generate draft knowledge base article topics based on historical tracker information; and 2) categorize articles to help organize the knowledge base.",Operation and Maintenance,Neither,6/1/2024,6/1/2024,9/1/2024,Developed with both contracting and in-house resources.,75ACF123F80045,Unknown,No,No,No,No,Historical trackers of questions and answers to TANF policy questions,Documentation has been partially completed,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The Office of Family Assistance (OFA) provides technical assistance to grantees, including answering questions about policy regarding the Temporary Assistance for Needy Families (TANF) program. Historically, OFA has tracked its accumulated TANF knowledge across disparate locations and file types.?

Given the quantity of knowledge base articles involved in this project (>1,100), the AI draft article topics and categories helped speed up the creation of the knowledge base minimum viable product. . As part of a broader effort to establish a TANF knowledge base on a unified platform, large language models were used to: 1) generate draft knowledge base article topics based on historical tracker information; and 2) categorize articles to help organize the knowledge base.","the office of family assistance (ofa) provides technical assistance to grantees, including answering questions about policy regarding the temporary assistance for needy families (tanf) program. historically, ofa has tracked its accumulated tanf knowledge across disparate locations and file types.? given the quantity of knowledge base articles involved in this project (>1,100), the ai draft article topics and categories helped speed up the creation of the knowledge base minimum viable product. . as part of a broader effort to establish a tanf knowledge base on a unified platform, large language models were used to: 1) generate draft knowledge base article topics based on historical tracker information; and 2) categorize articles to help organize the knowledge base."
Qualitative analysis,Department of Health and Human Services,HHS,ACF,Mission-Enabling (internal agency support),None of the above.,"ACF staff often conduct surveys and interviews, which generate qualitative data that needs to be analyzed for themes and trends.

The AI functionality assists users in identifying relevant passages in their narrative data related to specific topics.",We use commercial off-the-shelf software that have built-in AI functionality to suggest topic tagging of qualitative data.,Operation and Maintenance,Neither,3/1/2023,3/1/2023,9/1/2023,Developed in-house.,Unknown,Unknown,No,Yes,No,No,Unknown,Documentation is complete,No,No – agency does not have access to source code.,No,Unknown,6-12 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"ACF staff often conduct surveys and interviews, which generate qualitative data that needs to be analyzed for themes and trends.

The AI functionality assists users in identifying relevant passages in their narrative data related to specific topics. . We use commercial off-the-shelf software that have built-in AI functionality to suggest topic tagging of qualitative data.","acf staff often conduct surveys and interviews, which generate qualitative data that needs to be analyzed for themes and trends. the ai functionality assists users in identifying relevant passages in their narrative data related to specific topics. . we use commercial off-the-shelf software that have built-in ai functionality to suggest topic tagging of qualitative data."
AHRQ Search,Department of Health and Human Services,HHS,AHRQ,Mission-Enabling (internal agency support),None of the above.,"Organization wide search that includes Relevancy Tailoring, Auto-generation Synonyms, Automated Suggestions,  Suggested Related Content, Auto Tagging, and ""Did you mean?"" to allow visitors to find specific content.

This AI use case enhances our agency's efficiency and user experience by optimizing search results, auto-completing queries, suggesting relevant searches and content tags, as well as proposing spelling corrections. ","This AI use case aims to optimize search results by adjusting their ranking, ensuring that the most pertinent information is displayed at the top. It also enhances search effectiveness by adding synonyms to queries behind the scenes. It further improves user experience by auto-completing queries as they are being typed, and showing related searches that might offer additional valuable insights. The system also proposes content tags automatically, leveraging machine learning to assess existing content tagging patterns. Additionally, it suggests spelling corrections and reformats search queries based on data from Google Analytics.",Operation and Maintenance,Neither,9/1/2019,9/1/2019,10/1/2020,Developed with contracting resources.,75Q80121A00001,Unknown,No,No,No,No,Unknown,Documentation is missing or not available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,AHRQ AWS Enclave,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Organization wide search that includes Relevancy Tailoring, Auto-generation Synonyms, Automated Suggestions,  Suggested Related Content, Auto Tagging, and ""Did you mean?"" to allow visitors to find specific content.

This AI use case enhances our agency's efficiency and user experience by optimizing search results, auto-completing queries, suggesting relevant searches and content tags, as well as proposing spelling corrections. . This AI use case aims to optimize search results by adjusting their ranking, ensuring that the most pertinent information is displayed at the top. It also enhances search effectiveness by adding synonyms to queries behind the scenes. It further improves user experience by auto-completing queries as they are being typed, and showing related searches that might offer additional valuable insights. The system also proposes content tags automatically, leveraging machine learning to assess existing content tagging patterns. Additionally, it suggests spelling corrections and reformats search queries based on data from Google Analytics.","organization wide search that includes relevancy tailoring, auto-generation synonyms, automated suggestions, suggested related content, auto tagging, and ""did you mean?"" to allow visitors to find specific content. this ai use case enhances our agency's efficiency and user experience by optimizing search results, auto-completing queries, suggesting relevant searches and content tags, as well as proposing spelling corrections. . this ai use case aims to optimize search results by adjusting their ranking, ensuring that the most pertinent information is displayed at the top. it also enhances search effectiveness by adding synonyms to queries behind the scenes. it further improves user experience by auto-completing queries as they are being typed, and showing related searches that might offer additional valuable insights. the system also proposes content tags automatically, leveraging machine learning to assess existing content tagging patterns. additionally, it suggests spelling corrections and reformats search queries based on data from google analytics."
Chatbot,Department of Health and Human Services,HHS,AHRQ,Mission-Enabling (internal agency support),None of the above.,"This use case introduces a conversational interface that enables users to ask questions about AHRQ content using natural language. This will serve as a modern, efficient replacement for the traditional public inquiry telephone line.

Improved user experience and reduced resources responding to public inquiries",An interactive interface that can respond to plain language queries in real time using natural language processing,Operation and Maintenance,Neither,10/1/2020,10/1/2020,2/1/2022,Developed with contracting resources.,75Q80121A00001,Unknown,No,No,No,No,Unknown,Documentation is missing or not available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,AHRQ AWS Enclave,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"This use case introduces a conversational interface that enables users to ask questions about AHRQ content using natural language. This will serve as a modern, efficient replacement for the traditional public inquiry telephone line.

Improved user experience and reduced resources responding to public inquiries . An interactive interface that can respond to plain language queries in real time using natural language processing","this use case introduces a conversational interface that enables users to ask questions about ahrq content using natural language. this will serve as a modern, efficient replacement for the traditional public inquiry telephone line. improved user experience and reduced resources responding to public inquiries . an interactive interface that can respond to plain language queries in real time using natural language processing"
Quality and Safety Review System AI-enabled automated abstraction,Department of Health and Human Services,HHS,AHRQ,Mission-Enabling (internal agency support),None of the above.,"The aim of this AI use case is to enhance or eliminate the need for manual abstraction of patient records and manual data entry into the QSRS tool, thereby streamlining the data entry process.

This AI use case has the potential to significantly help by increasing the volume of cases abstracted while maintaining or lowering government expenditure, thereby enabling more work to be done at the same or reduced cost. Additionally, it can maintain the number of cases abstracted with improved efficiency, thus also reducing government costs by achieving the same output at a lower cost. Furthermore, it will improve abstraction accuracy and expand AI/ML capabilities at both the national and local hospital levels while ensuring data quality improvements so there is trusted and reliable data for reporting purposes.","This AI use case is designed to automate the data abstraction process once the OCR system has transformed PDF images into text. It also aims to automate data entry into the Quality and Safety Review System (QSRS), thereby increasing efficiency and accuracy.",Acquisition and/or Development,Neither,6/1/2024,10/1/2024,Unknown,Developed with contracting resources.,75Q80122F80006,Unknown,Unknown,Unknown,Unknown,No,Unknown,Documentation is missing or not available,Unknown,Unknown,Yes,AHRQ AWS Enclave,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"The aim of this AI use case is to enhance or eliminate the need for manual abstraction of patient records and manual data entry into the QSRS tool, thereby streamlining the data entry process.

This AI use case has the potential to significantly help by increasing the volume of cases abstracted while maintaining or lowering government expenditure, thereby enabling more work to be done at the same or reduced cost. Additionally, it can maintain the number of cases abstracted with improved efficiency, thus also reducing government costs by achieving the same output at a lower cost. Furthermore, it will improve abstraction accuracy and expand AI/ML capabilities at both the national and local hospital levels while ensuring data quality improvements so there is trusted and reliable data for reporting purposes. . This AI use case is designed to automate the data abstraction process once the OCR system has transformed PDF images into text. It also aims to automate data entry into the Quality and Safety Review System (QSRS), thereby increasing efficiency and accuracy.","the aim of this ai use case is to enhance or eliminate the need for manual abstraction of patient records and manual data entry into the qsrs tool, thereby streamlining the data entry process. this ai use case has the potential to significantly help by increasing the volume of cases abstracted while maintaining or lowering government expenditure, thereby enabling more work to be done at the same or reduced cost. additionally, it can maintain the number of cases abstracted with improved efficiency, thus also reducing government costs by achieving the same output at a lower cost. furthermore, it will improve abstraction accuracy and expand ai/ml capabilities at both the national and local hospital levels while ensuring data quality improvements so there is trusted and reliable data for reporting purposes. . this ai use case is designed to automate the data abstraction process once the ocr system has transformed pdf images into text. it also aims to automate data entry into the quality and safety review system (qsrs), thereby increasing efficiency and accuracy."
Document Q&A Interface ,Department of Health and Human Services,HHS,ASPR,Law & Justice,None of the above.,"The SCCT consolidates a number of documents for every response that it monitors, and often analysts have questions about response context stored in those documents

Increase an analyst / user on Protect's knowledge of a specific response or MCM by allowing them to ""ask questions"" of associated / uploaded  SCCT media",Large Language Models are used to query existing documents for answers based on a user's query,Acquisition and/or Development,Neither,7/1/2024,8/1/2024,Unknown,Developed with contracting resources.,000HCVL1-2022-66353,Unknown,Unknown,Unknown,Unknown,No,SCCT Distributor Data,Documentation is complete,Yes,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,Yes,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"The SCCT consolidates a number of documents for every response that it monitors, and often analysts have questions about response context stored in those documents

Increase an analyst / user on Protect's knowledge of a specific response or MCM by allowing them to ""ask questions"" of associated / uploaded  SCCT media . Large Language Models are used to query existing documents for answers based on a user's query","the scct consolidates a number of documents for every response that it monitors, and often analysts have questions about response context stored in those documents increase an analyst / user on protect's knowledge of a specific response or mcm by allowing them to ""ask questions"" of associated / uploaded scct media . large language models are used to query existing documents for answers based on a user's query"
Emergency Inventory Redistribution Model,Department of Health and Human Services,HHS,ASPR,Emergency Management,None of the above.,"Optimization models are needed to identify efficient preparedness and responses strategies for inventory management and distribution during an emergency.

This can be used to identify possible interventions.",This linear programming (constrained optimization) model can identify the optimal strategy for meeting demand for medical supplies during an emergency event.,Implementation and Assessment,Neither,8/1/2023,8/1/2023,Unknown,Developed with contracting resources.,75A50122C00052,Unknown,No,No,No,Yes,SCCT Distributor Data,Documentation has been partially completed,Yes,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,Yes,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Optimization models are needed to identify efficient preparedness and responses strategies for inventory management and distribution during an emergency.

This can be used to identify possible interventions. . This linear programming (constrained optimization) model can identify the optimal strategy for meeting demand for medical supplies during an emergency event.",optimization models are needed to identify efficient preparedness and responses strategies for inventory management and distribution during an emergency. this can be used to identify possible interventions. . this linear programming (constrained optimization) model can identify the optimal strategy for meeting demand for medical supplies during an emergency event.
emPOWER AI,Department of Health and Human Services,HHS,ASPR,Health & Medical,None of the above.,"Public health authorities, first responders and others across the emergency management spectrum indicated that they had to be able to more rapidly access emPOWER publicly available data, particularly in unstable internet conditions, during disasters. emPOWER AI, an  Amazon Alexa Skill, was created to allow anyone with a smartphone to be able to request the data and informational resources. 

First responders, public health authorities, emergency managers, health care providers, and other community partners can use emPOWER AI prior to, during, and after an incident, emergency, or disaster to anticipate, plan for, and respond to the access and functional needs of at-risk populations in their communities using publicly available emPOWER data, informational resources and training.","emPOWER AI gives the user publicly available data from the HHS emPOWER Map on the number of electricity-dependent Medicare beneficiaries at the national, state, territory, county, and ZIP Code levels. It also provides information on other HHS emPOWER Program tools and resources that collectively support emergency preparedness, response, recovery, mitigation, and resilience activities. ",Operation and Maintenance,Neither,1/1/2018,9/1/2017,1/1/2019,Developed with contracting resources.,GSA Schedules,Unknown,Yes,No,No,No,Unknown,Documentation is complete,No,"Yes – agency has access to source code, but it is not public.",Yes,Not Reported,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Public health authorities, first responders and others across the emergency management spectrum indicated that they had to be able to more rapidly access emPOWER publicly available data, particularly in unstable internet conditions, during disasters. emPOWER AI, an  Amazon Alexa Skill, was created to allow anyone with a smartphone to be able to request the data and informational resources. 

First responders, public health authorities, emergency managers, health care providers, and other community partners can use emPOWER AI prior to, during, and after an incident, emergency, or disaster to anticipate, plan for, and respond to the access and functional needs of at-risk populations in their communities using publicly available emPOWER data, informational resources and training. . emPOWER AI gives the user publicly available data from the HHS emPOWER Map on the number of electricity-dependent Medicare beneficiaries at the national, state, territory, county, and ZIP Code levels. It also provides information on other HHS emPOWER Program tools and resources that collectively support emergency preparedness, response, recovery, mitigation, and resilience activities.","public health authorities, first responders and others across the emergency management spectrum indicated that they had to be able to more rapidly access empower publicly available data, particularly in unstable internet conditions, during disasters. empower ai, an amazon alexa skill, was created to allow anyone with a smartphone to be able to request the data and informational resources. first responders, public health authorities, emergency managers, health care providers, and other community partners can use empower ai prior to, during, and after an incident, emergency, or disaster to anticipate, plan for, and respond to the access and functional needs of at-risk populations in their communities using publicly available empower data, informational resources and training. . empower ai gives the user publicly available data from the hhs empower map on the number of electricity-dependent medicare beneficiaries at the national, state, territory, county, and zip code levels. it also provides information on other hhs empower program tools and resources that collectively support emergency preparedness, response, recovery, mitigation, and resilience activities."
Entity Resolution Tool,Department of Health and Human Services,HHS,ASPR,Mission-Enabling (internal agency support),None of the above.,"SCCT receives order information that references thousands of manufacturers and customers. These data need to be standardized.

This allows analysts to aggregate data by manufacturer or customer to better characterize order and delivery patterns.","This tool uses embedding models, hierarchical clustering, and large language models to identify groups of duplicate entity names and assign a new standardized name.",Implementation and Assessment,Neither,6/1/2024,7/1/2024,Unknown,Developed with contracting resources.,75A50122C00052,Unknown,No,No,No,Yes,SCCT Distributor Data,Documentation has been partially completed,Yes,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,Yes,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"SCCT receives order information that references thousands of manufacturers and customers. These data need to be standardized.

This allows analysts to aggregate data by manufacturer or customer to better characterize order and delivery patterns. . This tool uses embedding models, hierarchical clustering, and large language models to identify groups of duplicate entity names and assign a new standardized name.","scct receives order information that references thousands of manufacturers and customers. these data need to be standardized. this allows analysts to aggregate data by manufacturer or customer to better characterize order and delivery patterns. . this tool uses embedding models, hierarchical clustering, and large language models to identify groups of duplicate entity names and assign a new standardized name."
Executive Report Generation AI Copilot ,Department of Health and Human Services,HHS,ASPR,Mission-Enabling (internal agency support),None of the above.,"SCCT produces executive summaries and commercial product awareness reports (CPARs) on supply chain disruptions, weather events, shortages, etc. These utilize a large amount of SCCT data that needs to be summarized and explained in detail.

Used to decrease the amount of time needed to produce an executive report, reducing the need for SCCT analysts to create them from scratch every time",Large language models are utilized to summarize existing documents and data in order to produce the base text for an executive report. ,Operation and Maintenance,Neither,2/1/2024,2/1/2024,3/1/2024,Developed with contracting resources.,000HCVL1-2022-66353,Unknown,No,No,No,Yes,SCCT Distributor Data,Documentation has been partially completed,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,Yes,No,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"SCCT produces executive summaries and commercial product awareness reports (CPARs) on supply chain disruptions, weather events, shortages, etc. These utilize a large amount of SCCT data that needs to be summarized and explained in detail.

Used to decrease the amount of time needed to produce an executive report, reducing the need for SCCT analysts to create them from scratch every time . Large language models are utilized to summarize existing documents and data in order to produce the base text for an executive report.","scct produces executive summaries and commercial product awareness reports (cpars) on supply chain disruptions, weather events, shortages, etc. these utilize a large amount of scct data that needs to be summarized and explained in detail. used to decrease the amount of time needed to produce an executive report, reducing the need for scct analysts to create them from scratch every time . large language models are utilized to summarize existing documents and data in order to produce the base text for an executive report."
Flow Model Edge Prioritization Estimation Tool,Department of Health and Human Services,HHS,ASPR,Emergency Management,None of the above.,"This model is needed to investigate potential impacts of supply disruptions.

This is one component in a simulation tool used to quantify the impact of supply chain disruptions on customers.",This model uses linear regression to summarize patterns in preferential order fulfillment and predict future fill rates.,Operation and Maintenance,Neither,5/1/2023,9/1/2023,8/1/2024,Developed with contracting resources.,75A50122C00052,Unknown,No,No,No,Yes,SCCT Distributor Data,Documentation has been partially completed,Yes,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,Yes,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"This model is needed to investigate potential impacts of supply disruptions.

This is one component in a simulation tool used to quantify the impact of supply chain disruptions on customers. . This model uses linear regression to summarize patterns in preferential order fulfillment and predict future fill rates.",this model is needed to investigate potential impacts of supply disruptions. this is one component in a simulation tool used to quantify the impact of supply chain disruptions on customers. . this model uses linear regression to summarize patterns in preferential order fulfillment and predict future fill rates.
Generalized Demand Model Sensitivity Analyzer,Department of Health and Human Services,HHS,ASPR,Mission-Enabling (internal agency support),None of the above.,"The generalized demand model is used by SCCT to create demand projections for critical medical products.

This is a diagnostic tool used to help analysts understand and refine their models.",This constrained regression model is used to identify the primary drivers of uncertainty in demand models and quantify their impact.,Operation and Maintenance,Neither,9/1/2023,12/1/2023,2/1/2024,Developed with contracting resources.,75A50122C00052,Unknown,No,No,No,Yes,Unknown,Documentation has been partially completed,Yes,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,Yes,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"The generalized demand model is used by SCCT to create demand projections for critical medical products.

This is a diagnostic tool used to help analysts understand and refine their models. . This constrained regression model is used to identify the primary drivers of uncertainty in demand models and quantify their impact.",the generalized demand model is used by scct to create demand projections for critical medical products. this is a diagnostic tool used to help analysts understand and refine their models. . this constrained regression model is used to identify the primary drivers of uncertainty in demand models and quantify their impact.
Google Search Model Alerts,Department of Health and Human Services,HHS,ASPR,Mission-Enabling (internal agency support),None of the above.,"Publicly available information can enhance the ability to identify supply chain disruptions.

This provides automated alerts that can help trigger review of the relevant data by analysts.","This model, based on exponentially weighted moving averages, identifies rising trends in timeseries related to google search volume for keywords of interest.",Operation and Maintenance,Neither,1/1/2023,6/1/2023,4/1/2024,Developed with contracting resources.,75A50122C00052,Unknown,No,No,No,Yes,Unknown,Documentation has been partially completed,Yes,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,Yes,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Publicly available information can enhance the ability to identify supply chain disruptions.

This provides automated alerts that can help trigger review of the relevant data by analysts. . This model, based on exponentially weighted moving averages, identifies rising trends in timeseries related to google search volume for keywords of interest.","publicly available information can enhance the ability to identify supply chain disruptions. this provides automated alerts that can help trigger review of the relevant data by analysts. . this model, based on exponentially weighted moving averages, identifies rising trends in timeseries related to google search volume for keywords of interest."
Google Search Model Rising Trend Entity Extraction,Department of Health and Human Services,HHS,ASPR,Mission-Enabling (internal agency support),None of the above.,"Publicly available information can enhance the ability to identify supply chain disruptions.

The list of products produced by this model helps analysts identify products that may be in shortage.",This tool uses language models to identify the object in shortage from a list of shortage related rising search terms identified by Google.,Operation and Maintenance,Neither,1/1/2023,6/1/2023,4/1/2024,Developed with contracting resources.,75A50122C00052,Unknown,No,No,No,Yes,Unknown,Documentation has been partially completed,Yes,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,Yes,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Publicly available information can enhance the ability to identify supply chain disruptions.

The list of products produced by this model helps analysts identify products that may be in shortage. . This tool uses language models to identify the object in shortage from a list of shortage related rising search terms identified by Google.",publicly available information can enhance the ability to identify supply chain disruptions. the list of products produced by this model helps analysts identify products that may be in shortage. . this tool uses language models to identify the object in shortage from a list of shortage related rising search terms identified by google.
Manufacturer Name Standardization,Department of Health and Human Services,HHS,ASPR,Mission-Enabling (internal agency support),None of the above.,"The SCCT receives many manufacturer name variants from its many distributors that need to be consolidated and standardized

This allows analysts to have more standardized manufacturer names for reporting","An advanced grouping algorithm groups similar manufacturer names together, and a Large Language Model recommends a standardized name based on all variants",Operation and Maintenance,Neither,4/1/2024,4/1/2024,8/1/2024,Developed with contracting resources.,75A50122C00052,Unknown,No,No,No,Yes,SCCT Distributor Data,Documentation has been partially completed,Yes,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,Yes,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The SCCT receives many manufacturer name variants from its many distributors that need to be consolidated and standardized

This allows analysts to have more standardized manufacturer names for reporting . An advanced grouping algorithm groups similar manufacturer names together, and a Large Language Model recommends a standardized name based on all variants","the scct receives many manufacturer name variants from its many distributors that need to be consolidated and standardized this allows analysts to have more standardized manufacturer names for reporting . an advanced grouping algorithm groups similar manufacturer names together, and a large language model recommends a standardized name based on all variants"
PPE Product Classification Model,Department of Health and Human Services,HHS,ASPR,Mission-Enabling (internal agency support),None of the above.,"SCCT receives information about thousands of products from distributors.  This is used to predict if the product falls into one of the following 7 categories: 1) Gloves – Surgical & Exam, 2) Needles & Syringes, 3) Gowns – Isolation & Surgical, 4) Masks – N95, 5) Masks – Surgical/Procedural, 6) Eye / Face Shields, 7) Test Kits.

These classifications are used to aggregate data in order to make it possible to analyze trends.",This model uses bayesian anomaly detection and random forests to assign products to a list of predefined product classifications.,Operation and Maintenance,Neither,1/1/2023,1/1/2023,1/1/2023,Developed with contracting resources.,000HCVL1-2022-66353,Unknown,No,No,No,Yes,SCCT Distributor Data,Documentation has been partially completed,Yes,No – agency does not have access to source code.,No,Unknown,6-12 months,No,Yes,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"SCCT receives information about thousands of products from distributors.  This is used to predict if the product falls into one of the following 7 categories: 1) Gloves – Surgical & Exam, 2) Needles & Syringes, 3) Gowns – Isolation & Surgical, 4) Masks – N95, 5) Masks – Surgical/Procedural, 6) Eye / Face Shields, 7) Test Kits.

These classifications are used to aggregate data in order to make it possible to analyze trends. . This model uses bayesian anomaly detection and random forests to assign products to a list of predefined product classifications.","scct receives information about thousands of products from distributors. this is used to predict if the product falls into one of the following 7 categories: 1) gloves – surgical & exam, 2) needles & syringes, 3) gowns – isolation & surgical, 4) masks – n95, 5) masks – surgical/procedural, 6) eye / face shields, 7) test kits. these classifications are used to aggregate data in order to make it possible to analyze trends. . this model uses bayesian anomaly detection and random forests to assign products to a list of predefined product classifications."
Adverse Childhood Experiences (ACEs) Literature Review Dashboard,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"This solution will impact the process of conducting literature reviews for scientific research related to ACEs.

Identifying relevant articles in a literature review is a time intensive process. Utilizing large language models, this system enables quick identification of relevant articles to be included in a ACEs literature database. This database allows ACEs researchers within CDC's National Center for Injury Prevention and Control a quick reference to develop hypotheses and identify gaps in the ACEs literature",This solution automatically determines the relevancy of published journal articles. Relevant articles are included in a master dataset that is linked to a PowerBI,Acquisition and/or Development,Neither,2/1/2024,3/1/2024, ,Developed with contracting resources.,Unknown, , , , ,No,Data used are published journal articles which may or may not be owned by CDC but may be accessed either through freely accessible sources or specific agreements CDC has to research articles.,Documentation is missing or not available, , ,Yes,Enterprise Data Analytics and Visualization (EDAV) Platform,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"This solution will impact the process of conducting literature reviews for scientific research related to ACEs.

Identifying relevant articles in a literature review is a time intensive process. Utilizing large language models, this system enables quick identification of relevant articles to be included in a ACEs literature database. This database allows ACEs researchers within CDC's National Center for Injury Prevention and Control a quick reference to develop hypotheses and identify gaps in the ACEs literature . This solution automatically determines the relevancy of published journal articles. Relevant articles are included in a master dataset that is linked to a PowerBI","this solution will impact the process of conducting literature reviews for scientific research related to aces. identifying relevant articles in a literature review is a time intensive process. utilizing large language models, this system enables quick identification of relevant articles to be included in a aces literature database. this database allows aces researchers within cdc's national center for injury prevention and control a quick reference to develop hypotheses and identify gaps in the aces literature . this solution automatically determines the relevancy of published journal articles. relevant articles are included in a master dataset that is linked to a powerbi"
Automated Analysis of Injury Control Research Center (ICRC) Annual Progress Reports (APRs) using Large Language Models,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"This AI affects the process of reviewing and analyzing the Annual Progress Reports (APRs) submitted by Injury Control Research Centers (ICRCs). Specifically, the AI will focus on identifying and analyzing text sections of the APRs that detail reported challenges, as well as other text-heavy sections in future stages of the project. The AI is designed to streamline the review process, improve efficiency, and support the evaluation of the performance and progress of ICRC-funded activities.

The output of the AI will help to quickly and efficiently identify key challenges and insights from ICRC APRs, enabling more effective decision-making in the review process. By automating the extraction and analysis of critical information, the AI allows the ICRC team to focus on higher-level evaluation and strategic planning. The AI will help reduce the time and resources needed for manual review, improve the consistency and accuracy of assessments, and facilitate faster responses to ICRC needs. Ultimately, this will support ICRCs in overcoming challenges and achieving their research and injury control goals, benefiting the public health system as a whole.","The AI analyzes the textual content of APRs. We have started our analysis by focusing on the sections detailing the challenges faced by ICRCs during the 2019 funding cycle. The AI identifies key themes, trends, and critical information that may require further attention. The AI methodology extracts insights and patterns from the data, which can then be compared with manual qualitative analysis outcomes. In subsequent stages, the AI will be expanded to analyze other sections of the APRs, such as progress toward goals and program impact.",Acquisition and/or Development,Neither,8/1/2023,12/1/2023, ,Developed in-house., , , , , ,No,Injury Control Research Center Annual Progress Reports,Documentation is widely available, , ,Yes,Enterprise Data Analytics and Visualization (EDAV) Platform,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"This AI affects the process of reviewing and analyzing the Annual Progress Reports (APRs) submitted by Injury Control Research Centers (ICRCs). Specifically, the AI will focus on identifying and analyzing text sections of the APRs that detail reported challenges, as well as other text-heavy sections in future stages of the project. The AI is designed to streamline the review process, improve efficiency, and support the evaluation of the performance and progress of ICRC-funded activities.

The output of the AI will help to quickly and efficiently identify key challenges and insights from ICRC APRs, enabling more effective decision-making in the review process. By automating the extraction and analysis of critical information, the AI allows the ICRC team to focus on higher-level evaluation and strategic planning. The AI will help reduce the time and resources needed for manual review, improve the consistency and accuracy of assessments, and facilitate faster responses to ICRC needs. Ultimately, this will support ICRCs in overcoming challenges and achieving their research and injury control goals, benefiting the public health system as a whole. . The AI analyzes the textual content of APRs. We have started our analysis by focusing on the sections detailing the challenges faced by ICRCs during the 2019 funding cycle. The AI identifies key themes, trends, and critical information that may require further attention. The AI methodology extracts insights and patterns from the data, which can then be compared with manual qualitative analysis outcomes. In subsequent stages, the AI will be expanded to analyze other sections of the APRs, such as progress toward goals and program impact.","this ai affects the process of reviewing and analyzing the annual progress reports (aprs) submitted by injury control research centers (icrcs). specifically, the ai will focus on identifying and analyzing text sections of the aprs that detail reported challenges, as well as other text-heavy sections in future stages of the project. the ai is designed to streamline the review process, improve efficiency, and support the evaluation of the performance and progress of icrc-funded activities. the output of the ai will help to quickly and efficiently identify key challenges and insights from icrc aprs, enabling more effective decision-making in the review process. by automating the extraction and analysis of critical information, the ai allows the icrc team to focus on higher-level evaluation and strategic planning. the ai will help reduce the time and resources needed for manual review, improve the consistency and accuracy of assessments, and facilitate faster responses to icrc needs. ultimately, this will support icrcs in overcoming challenges and achieving their research and injury control goals, benefiting the public health system as a whole. . the ai analyzes the textual content of aprs. we have started our analysis by focusing on the sections detailing the challenges faced by icrcs during the 2019 funding cycle. the ai identifies key themes, trends, and critical information that may require further attention. the ai methodology extracts insights and patterns from the data, which can then be compared with manual qualitative analysis outcomes. in subsequent stages, the ai will be expanded to analyze other sections of the aprs, such as progress toward goals and program impact."
Detecting Stimulant and Opioid Misuse and Illicit Use,Department of Health and Human Services,HHS,CDC,Health & Medical,None of the above.,"Federal Health Survey analysis. The machine learning models created in this project will have an impact on the statistical analysis of electronic health record (EHR) data gathered through the (NHCS) National Hospital Care Survey. Furthermore, when the model is made available to the public, it may also influence the analysis of other user datasets that contain EHR clinical notes.

Certain non-therapeutic models do not have corresponding medical codes available in clinical notes and the information is only available in free-text fields. The machine learning models help to discover non-therapeutic drug use as described in EHR clinical notes which are novel insights previously impossible to gleam from EHRs without AI.","Two machine learning models will be created, one for internal use and another for public release. In both instances, these models will be utilized alongside rule-based text analysis to ascertain whether a patient in a hospital setting has used a drug therapeutically (in accordance with medical prescription) or non-therapeutically. This information is used to gather new insights from EHRs for health statistics published through the NHCS.",Acquisition and/or Development,Neither,3/1/2024,9/1/2024, ,Developed in-house., , , , , ,No,National Hospital Care Survey 2020 clinical notes,Documentation has been partially completed, , ,No, ,More than 12 months,Yes,Yes,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"Federal Health Survey analysis. The machine learning models created in this project will have an impact on the statistical analysis of electronic health record (EHR) data gathered through the (NHCS) National Hospital Care Survey. Furthermore, when the model is made available to the public, it may also influence the analysis of other user datasets that contain EHR clinical notes.

Certain non-therapeutic models do not have corresponding medical codes available in clinical notes and the information is only available in free-text fields. The machine learning models help to discover non-therapeutic drug use as described in EHR clinical notes which are novel insights previously impossible to gleam from EHRs without AI. . Two machine learning models will be created, one for internal use and another for public release. In both instances, these models will be utilized alongside rule-based text analysis to ascertain whether a patient in a hospital setting has used a drug therapeutically (in accordance with medical prescription) or non-therapeutically. This information is used to gather new insights from EHRs for health statistics published through the NHCS.","federal health survey analysis. the machine learning models created in this project will have an impact on the statistical analysis of electronic health record (ehr) data gathered through the (nhcs) national hospital care survey. furthermore, when the model is made available to the public, it may also influence the analysis of other user datasets that contain ehr clinical notes. certain non-therapeutic models do not have corresponding medical codes available in clinical notes and the information is only available in free-text fields. the machine learning models help to discover non-therapeutic drug use as described in ehr clinical notes which are novel insights previously impossible to gleam from ehrs without ai. . two machine learning models will be created, one for internal use and another for public release. in both instances, these models will be utilized alongside rule-based text analysis to ascertain whether a patient in a hospital setting has used a drug therapeutically (in accordance with medical prescription) or non-therapeutically. this information is used to gather new insights from ehrs for health statistics published through the nhcs."
DHP Virtual Assistant,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"Literature Review for HIV research. The AI virtual assistant will help HIV researchers with information retrieval on information related to HIV and HIV research. 

Our AI virtual assistant will help increase productivity among our HIV researchers by retrieving information related to HIV and HIV research.",This AI assistant will use retrieval augmented generation (RAG) to return information related to HIV based on a user's query and other documents.,Acquisition and/or Development,Neither,11/1/2024,11/1/2024, ,Developed in-house., , , , , ,Yes,"Internal documentation containing mapped eHARS LOINC codes, IQVIA data dictionaries, APRs",Documentation is complete, , ,Yes,Enterprise Data Analytics and Visualization (EDAV) Platform,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"Literature Review for HIV research. The AI virtual assistant will help HIV researchers with information retrieval on information related to HIV and HIV research. 

Our AI virtual assistant will help increase productivity among our HIV researchers by retrieving information related to HIV and HIV research. . This AI assistant will use retrieval augmented generation (RAG) to return information related to HIV based on a user's query and other documents.",literature review for hiv research. the ai virtual assistant will help hiv researchers with information retrieval on information related to hiv and hiv research. our ai virtual assistant will help increase productivity among our hiv researchers by retrieving information related to hiv and hiv research. . this ai assistant will use retrieval augmented generation (rag) to return information related to hiv based on a user's query and other documents.
Fuzzy matching tool,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"Clearance and review management of CDC publication. CDC's Office of Science (OS) manages the eClearance information system, which is responsible for tracking the submission of CDC-authored journal manuscripts and Morbidity and Mortality Weekly Reports (MMWRs) for CDC's internal peer review process.

This tool will help ensure CDC-authored publications have completed the National Institutes of Health Manuscript Submission (NIHMS) process and are compliant with CDC's public access to publications policy. It will also be used to help centers and divisions identify publications from their organization for science prioritization and impact analyses.","The fuzzy matching tool uses pre-trained, open source LLMs to identify similar records between eClearance submissions and CDC-authored publications. This tool is intended for internal use only.",Acquisition and/or Development,Neither,9/1/2022,3/1/2023, ,Developed in-house., , , , , ,No,eClearance submissions data and Science Clips data,Documentation is complete, , ,No, ,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"Clearance and review management of CDC publication. CDC's Office of Science (OS) manages the eClearance information system, which is responsible for tracking the submission of CDC-authored journal manuscripts and Morbidity and Mortality Weekly Reports (MMWRs) for CDC's internal peer review process.

This tool will help ensure CDC-authored publications have completed the National Institutes of Health Manuscript Submission (NIHMS) process and are compliant with CDC's public access to publications policy. It will also be used to help centers and divisions identify publications from their organization for science prioritization and impact analyses. . The fuzzy matching tool uses pre-trained, open source LLMs to identify similar records between eClearance submissions and CDC-authored publications. This tool is intended for internal use only.","clearance and review management of cdc publication. cdc's office of science (os) manages the eclearance information system, which is responsible for tracking the submission of cdc-authored journal manuscripts and morbidity and mortality weekly reports (mmwrs) for cdc's internal peer review process. this tool will help ensure cdc-authored publications have completed the national institutes of health manuscript submission (nihms) process and are compliant with cdc's public access to publications policy. it will also be used to help centers and divisions identify publications from their organization for science prioritization and impact analyses. . the fuzzy matching tool uses pre-trained, open source llms to identify similar records between eclearance submissions and cdc-authored publications. this tool is intended for internal use only."
Global (GIB) Initiative for Influenza Vaccine Equity (GIIVE): Ensuring Universal Access,Department of Health and Human Services,HHS,CDC,Health & Medical,None of the above.,"Literature Review

The output is a list of abstracts that may have related information to vaccine inequity. These identified abstracts will be reviewed ""by-hand"" and then those associated publications will be reviewed by epidemiologists further.","The Through APIs, large language models are used to review a large number of abstracts to help identify which kinds of groups may be least likely to obtain vaccines in the global space--marginalized groups within country, for example.",Acquisition and/or Development,Neither,10/1/2024,10/1/2024, ,Developed in-house., , , , , ,No,Data used are published journal articles which may or may not be owned by CDC but may be accessed either through freely accessible sources or specific agreements CDC has to research articles.,Documentation has been partially completed, , ,Yes,Enterprise Data Analytics and Visualization (EDAV) Platform,Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"Literature Review

The output is a list of abstracts that may have related information to vaccine inequity. These identified abstracts will be reviewed ""by-hand"" and then those associated publications will be reviewed by epidemiologists further. . The Through APIs, large language models are used to review a large number of abstracts to help identify which kinds of groups may be least likely to obtain vaccines in the global space--marginalized groups within country, for example.","literature review the output is a list of abstracts that may have related information to vaccine inequity. these identified abstracts will be reviewed ""by-hand"" and then those associated publications will be reviewed by epidemiologists further. . the through apis, large language models are used to review a large number of abstracts to help identify which kinds of groups may be least likely to obtain vaccines in the global space--marginalized groups within country, for example."
HIV Data Virtual Assistant,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"The AI will impact the data retrieval and automation process for HIV researchers, specifically in providing information from datasets and potentially generating code for desired variable names and analysis or generating information about a particular dataset.

This solution aims to alleviate the challenges faced by scientists searching for the appropriate HIV-related data to address their inquiries. This will improve productivity and research efforts.","This AI assistant will use retrieval augmented generation (RAG) to return information related to HIV based on a user's query. Our aim is to input all dataset documentation and metadata into an index with the goal of creating a virtual assistant. This assistant offers support to our HIV researchers by providing the information they seek from the data and potentially generating SAS, R, or Python code for desired variable names. For instance, if we require a dataset containing HIV PrEP information, we can request it to generate a list of current datasets that contain data on PrEP medication usage in 2019, along with the associated variable names. Subsequently, we can instruct it to produce lines of code, such as a logistic regression, which can be readily copied and pasted into our analysis software of choice. Another example would be if our researchers would like information returned about a given dataset, this assistant could return specific information based on the researcher's query.",Acquisition and/or Development,Neither,11/1/2024,11/1/2024, ,Developed in-house., , , , , ,Yes,"Internal documentation containing mapped eHARS LOINC codes, IQVIA data dictionaries",Documentation is complete, , ,Yes,Enterprise Data Analytics and Visualization (EDAV) Platform,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"The AI will impact the data retrieval and automation process for HIV researchers, specifically in providing information from datasets and potentially generating code for desired variable names and analysis or generating information about a particular dataset.

This solution aims to alleviate the challenges faced by scientists searching for the appropriate HIV-related data to address their inquiries. This will improve productivity and research efforts. . This AI assistant will use retrieval augmented generation (RAG) to return information related to HIV based on a user's query. Our aim is to input all dataset documentation and metadata into an index with the goal of creating a virtual assistant. This assistant offers support to our HIV researchers by providing the information they seek from the data and potentially generating SAS, R, or Python code for desired variable names. For instance, if we require a dataset containing HIV PrEP information, we can request it to generate a list of current datasets that contain data on PrEP medication usage in 2019, along with the associated variable names. Subsequently, we can instruct it to produce lines of code, such as a logistic regression, which can be readily copied and pasted into our analysis software of choice. Another example would be if our researchers would like information returned about a given dataset, this assistant could return specific information based on the researcher's query.","the ai will impact the data retrieval and automation process for hiv researchers, specifically in providing information from datasets and potentially generating code for desired variable names and analysis or generating information about a particular dataset. this solution aims to alleviate the challenges faced by scientists searching for the appropriate hiv-related data to address their inquiries. this will improve productivity and research efforts. . this ai assistant will use retrieval augmented generation (rag) to return information related to hiv based on a user's query. our aim is to input all dataset documentation and metadata into an index with the goal of creating a virtual assistant. this assistant offers support to our hiv researchers by providing the information they seek from the data and potentially generating sas, r, or python code for desired variable names. for instance, if we require a dataset containing hiv prep information, we can request it to generate a list of current datasets that contain data on prep medication usage in 2019, along with the associated variable names. subsequently, we can instruct it to produce lines of code, such as a logistic regression, which can be readily copied and pasted into our analysis software of choice. another example would be if our researchers would like information returned about a given dataset, this assistant could return specific information based on the researcher's query."
Identify infrastructure supports for physical activity (e.g. sidewalks) in satellite and roadway images,Department of Health and Human Services,HHS,CDC,Health & Medical,None of the above.,"The physical environment, such as sidewalk availability, in areas may be correlated to levels of physical activity. The Physical Activity (PA) and Health Branch is responsible for monitoring and gathering data on physical activity behaviors, as well as assessing the environments that promote physical activity.

The implementation of this technology has the potential to streamline the surveillance of sidewalks, which play a crucial role in supporting physical activity (PA). The existing methods for cataloging sidewalks, such as manual inspection, are labor-intensive and costly. However, this technology has the capability to significantly minimize the effort required for such tasks.","The Division of Nutrition, Physical Activity, and Obesity within CDC's National Center for Chronic Disease Prevention and Health Promotion aims to explore and advance the use of machine learning methods for identifying sidewalks, bicycle lanes, and other relevant infrastructure in various types of images, including satellite and roadway images. The input data would consist of image-based data, while the outputs could take the form of geocoded data tables, maps, GIS layers, or summary reports. The goal is to develop and promote these techniques to enhance understanding and analysis of built environments related to physical activity.",Acquisition and/or Development,Neither,9/1/2022,9/1/2023, ,Developed with both contracting and in-house resources.,Unknown, , , , ,No,none - we used publicly-available images,Documentation is complete, , ,No, ,Less than 6 months,Yes,Yes,No,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"The physical environment, such as sidewalk availability, in areas may be correlated to levels of physical activity. The Physical Activity (PA) and Health Branch is responsible for monitoring and gathering data on physical activity behaviors, as well as assessing the environments that promote physical activity.

The implementation of this technology has the potential to streamline the surveillance of sidewalks, which play a crucial role in supporting physical activity (PA). The existing methods for cataloging sidewalks, such as manual inspection, are labor-intensive and costly. However, this technology has the capability to significantly minimize the effort required for such tasks. . The Division of Nutrition, Physical Activity, and Obesity within CDC's National Center for Chronic Disease Prevention and Health Promotion aims to explore and advance the use of machine learning methods for identifying sidewalks, bicycle lanes, and other relevant infrastructure in various types of images, including satellite and roadway images. The input data would consist of image-based data, while the outputs could take the form of geocoded data tables, maps, GIS layers, or summary reports. The goal is to develop and promote these techniques to enhance understanding and analysis of built environments related to physical activity.","the physical environment, such as sidewalk availability, in areas may be correlated to levels of physical activity. the physical activity (pa) and health branch is responsible for monitoring and gathering data on physical activity behaviors, as well as assessing the environments that promote physical activity. the implementation of this technology has the potential to streamline the surveillance of sidewalks, which play a crucial role in supporting physical activity (pa). the existing methods for cataloging sidewalks, such as manual inspection, are labor-intensive and costly. however, this technology has the capability to significantly minimize the effort required for such tasks. . the division of nutrition, physical activity, and obesity within cdc's national center for chronic disease prevention and health promotion aims to explore and advance the use of machine learning methods for identifying sidewalks, bicycle lanes, and other relevant infrastructure in various types of images, including satellite and roadway images. the input data would consist of image-based data, while the outputs could take the form of geocoded data tables, maps, gis layers, or summary reports. the goal is to develop and promote these techniques to enhance understanding and analysis of built environments related to physical activity."
Immunization Information Systems Guidance Documentation Navigation and Management (IDAB EDAV Azure OpenAI Technology Use),Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"This AI solution (chatbot) helps with retrieving information and updating guidance documentation by increasing interactivity and discoverability of information in the Immunization Information Systems (IIS) domains of HL7 electronic data exchange standards, CDSi (Clinical decision support for immunizations), and IIS operational best practices. It provides a modernized approach to knowledge retrieval and real-time guidance, as well as support the creation of new guidance by leveraging the AI capabilities in natural language processing and content generation to draft, refine, and validate new guidance documents efficiently and accurately.

This AI solution creates a better, faster, and more actionable method for IIS SMEs to obtain IIS guidance from previously published and publicly available IIS documentation. This solution will provide a more modernized approach to knowledge retrieval and real-time guidance, as well as support the creation of new guidance by leveraging the chatbots capabilities in natural language processing and content generation to draft, refine, and validate new guidance documents efficiently and accurately.
This AI solution (chatbot) provides an efficient and user-friendly way to search, interact, and update these IIS guidance documents. It can help experts answer questions faster, new employees find information more easily, and employees better understand best practices.","This AI solution (chatbot) uses Retrieval Augmented Generation (RAG) to retrieve and synthesize information from a limited set of publicly available IIS guidance documents related to the HL7 Electronic Exchange Standard specification, Clinical Decision Support for Immunization (CDSi) and IIS Operational Best Practices. It does this using a Question-and-Answer style interface, allowing team members to retrieve answers in an efficient and user-friendly way.
On 07/26/2024 the CDC Information Technology and Data Governance (ITDG) Executive Committee has approved this investment “to use AI to create a system that can conduct a dialogue with a user and answer questions about the contents of already approved public documents”.",Acquisition and/or Development,Neither,10/1/2022,8/1/2024, ,Developed with both contracting and in-house resources.,"75D301-22-C-13291, 75D30124C20195", , , , ,No,"We did not do any training or fine tuning, but we used the following materials to evaluate the chatbot: HL7 Documentation Data, CDSi Logic Specification 4.4 Data. This information is entirely publicly accessible.",Documentation is widely available, , ,Yes,Enterprise Data Analytics and Visualization (EDAV) Platform,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"This AI solution (chatbot) helps with retrieving information and updating guidance documentation by increasing interactivity and discoverability of information in the Immunization Information Systems (IIS) domains of HL7 electronic data exchange standards, CDSi (Clinical decision support for immunizations), and IIS operational best practices. It provides a modernized approach to knowledge retrieval and real-time guidance, as well as support the creation of new guidance by leveraging the AI capabilities in natural language processing and content generation to draft, refine, and validate new guidance documents efficiently and accurately.

This AI solution creates a better, faster, and more actionable method for IIS SMEs to obtain IIS guidance from previously published and publicly available IIS documentation. This solution will provide a more modernized approach to knowledge retrieval and real-time guidance, as well as support the creation of new guidance by leveraging the chatbots capabilities in natural language processing and content generation to draft, refine, and validate new guidance documents efficiently and accurately.
This AI solution (chatbot) provides an efficient and user-friendly way to search, interact, and update these IIS guidance documents. It can help experts answer questions faster, new employees find information more easily, and employees better understand best practices. . This AI solution (chatbot) uses Retrieval Augmented Generation (RAG) to retrieve and synthesize information from a limited set of publicly available IIS guidance documents related to the HL7 Electronic Exchange Standard specification, Clinical Decision Support for Immunization (CDSi) and IIS Operational Best Practices. It does this using a Question-and-Answer style interface, allowing team members to retrieve answers in an efficient and user-friendly way.
On 07/26/2024 the CDC Information Technology and Data Governance (ITDG) Executive Committee has approved this investment “to use AI to create a system that can conduct a dialogue with a user and answer questions about the contents of already approved public documents”.","this ai solution (chatbot) helps with retrieving information and updating guidance documentation by increasing interactivity and discoverability of information in the immunization information systems (iis) domains of hl7 electronic data exchange standards, cdsi (clinical decision support for immunizations), and iis operational best practices. it provides a modernized approach to knowledge retrieval and real-time guidance, as well as support the creation of new guidance by leveraging the ai capabilities in natural language processing and content generation to draft, refine, and validate new guidance documents efficiently and accurately. this ai solution creates a better, faster, and more actionable method for iis smes to obtain iis guidance from previously published and publicly available iis documentation. this solution will provide a more modernized approach to knowledge retrieval and real-time guidance, as well as support the creation of new guidance by leveraging the chatbots capabilities in natural language processing and content generation to draft, refine, and validate new guidance documents efficiently and accurately. this ai solution (chatbot) provides an efficient and user-friendly way to search, interact, and update these iis guidance documents. it can help experts answer questions faster, new employees find information more easily, and employees better understand best practices. . this ai solution (chatbot) uses retrieval augmented generation (rag) to retrieve and synthesize information from a limited set of publicly available iis guidance documents related to the hl7 electronic exchange standard specification, clinical decision support for immunization (cdsi) and iis operational best practices. it does this using a question-and-answer style interface, allowing team members to retrieve answers in an efficient and user-friendly way. on 07/26/2024 the cdc information technology and data governance (itdg) executive committee has approved this investment “to use ai to create a system that can conduct a dialogue with a user and answer questions about the contents of already approved public documents”."
LaserAI,Department of Health and Human Services,HHS,CDC,Health & Medical,None of the above.,"This project aims to evaluate a commercial off the shelf, software as a service systematic review project management software. This software uses machine learning/ natural language processing to support title and abstract screening, PDF retrieval, full text review, and most importantly data extraction to reduce screening time and improve accuracy of all phases of a systematic review.

The extracted data will be synthesized and graded, and will be used to inform the development of evidence-based infection prevention and control recommendations for healthcare settings.","This AI uses natural language process to (1) identify potentially relevant articles and prioritize them for screening, (2) retrieve PDFs from PubMed, and (3) suggest data in PDFs for extraction.",Acquisition and/or Development,Neither,4/1/2024,4/1/2024, ,Developed with contracting resources.,Unknown, , , , ,No,"None: Any data used to train the AI is publicly available, peer-reviewed data.",Documentation is missing or not available, , ,No, ,Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"This project aims to evaluate a commercial off the shelf, software as a service systematic review project management software. This software uses machine learning/ natural language processing to support title and abstract screening, PDF retrieval, full text review, and most importantly data extraction to reduce screening time and improve accuracy of all phases of a systematic review.

The extracted data will be synthesized and graded, and will be used to inform the development of evidence-based infection prevention and control recommendations for healthcare settings. . This AI uses natural language process to (1) identify potentially relevant articles and prioritize them for screening, (2) retrieve PDFs from PubMed, and (3) suggest data in PDFs for extraction.","this project aims to evaluate a commercial off the shelf, software as a service systematic review project management software. this software uses machine learning/ natural language processing to support title and abstract screening, pdf retrieval, full text review, and most importantly data extraction to reduce screening time and improve accuracy of all phases of a systematic review. the extracted data will be synthesized and graded, and will be used to inform the development of evidence-based infection prevention and control recommendations for healthcare settings. . this ai uses natural language process to (1) identify potentially relevant articles and prioritize them for screening, (2) retrieve pdfs from pubmed, and (3) suggest data in pdfs for extraction."
NewsScape,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"Development of early warning indicators. News articles can form an early warning indicator of public health events and other pieces of information which can be utilized across all major domains of public health. Because of the quantity of news articles, manual efforts are impossible to gather this information.

There are a variety of endpoints that this AI output could help support. Various teams across the CDC have expressed interest in being able to quickly get the right news content, with summaries of those articles to help support outbreak detection, report generation, surveillance and monitoring of pathogen specific news, etc. AI lets us efficiently filter and summarize thousands of news articles a day into a handful of daily ""news events"" that user can glean information from. ","NewsScape is a AI enabled news aggregation and summarization that is hosted within the 1CDP platform. The main motivation to build  NewsScape was to develop a system that uses Large Language Models (LLMs) to surface relevant insight from recent news articles. NewsScape ingests a high volume of news articles, in the order of thousands of news articles everyday, and surfaces the information from them that related to the topic we were interested (for example, pathogen related news articles, or U.S. medical supply chain updates).  The tool can be customized based on specific program office needs, and can be deployed independently of one another so that one program office can have their own custom version of NewsScape installed.",Acquisition and/or Development,Neither,1/1/2023,1/1/2023, ,Developed with both contracting and in-house resources.,Unknown, , , , ,No,Unknown,Documentation has been partially completed, , ,Yes,1 CDC Data Platform (1CDP),6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"Development of early warning indicators. News articles can form an early warning indicator of public health events and other pieces of information which can be utilized across all major domains of public health. Because of the quantity of news articles, manual efforts are impossible to gather this information.

There are a variety of endpoints that this AI output could help support. Various teams across the CDC have expressed interest in being able to quickly get the right news content, with summaries of those articles to help support outbreak detection, report generation, surveillance and monitoring of pathogen specific news, etc. AI lets us efficiently filter and summarize thousands of news articles a day into a handful of daily ""news events"" that user can glean information from. . NewsScape is a AI enabled news aggregation and summarization that is hosted within the 1CDP platform. The main motivation to build  NewsScape was to develop a system that uses Large Language Models (LLMs) to surface relevant insight from recent news articles. NewsScape ingests a high volume of news articles, in the order of thousands of news articles everyday, and surfaces the information from them that related to the topic we were interested (for example, pathogen related news articles, or U.S. medical supply chain updates).  The tool can be customized based on specific program office needs, and can be deployed independently of one another so that one program office can have their own custom version of NewsScape installed.","development of early warning indicators. news articles can form an early warning indicator of public health events and other pieces of information which can be utilized across all major domains of public health. because of the quantity of news articles, manual efforts are impossible to gather this information. there are a variety of endpoints that this ai output could help support. various teams across the cdc have expressed interest in being able to quickly get the right news content, with summaries of those articles to help support outbreak detection, report generation, surveillance and monitoring of pathogen specific news, etc. ai lets us efficiently filter and summarize thousands of news articles a day into a handful of daily ""news events"" that user can glean information from. . newsscape is a ai enabled news aggregation and summarization that is hosted within the 1cdp platform. the main motivation to build newsscape was to develop a system that uses large language models (llms) to surface relevant insight from recent news articles. newsscape ingests a high volume of news articles, in the order of thousands of news articles everyday, and surfaces the information from them that related to the topic we were interested (for example, pathogen related news articles, or u.s. medical supply chain updates). the tool can be customized based on specific program office needs, and can be deployed independently of one another so that one program office can have their own custom version of newsscape installed."
RAPID Analysis of Policy and Program Documents (RAPID),Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"Policy and documentation awareness and evaluation

The web application will: 1) save staff time by streamlining laborious document reviews; 2) expand capacity by reducing the need for specialized staff training and skills; 3) provide complete, plain language answers to policy surveillance and evaluation questions along with binary codes or scores; 4) answer questions consistently (i.e., reduce intra-rater variability for redundant coding); and 5) enable easy validation of AI-provided answers.",An internal web application  will enable CDC users to: 1) import and securely store policy or program documents; 2) search for relevant documents; 3) ask questions of relevant text segments; 4) validate answers; and 5) collaborate on policy surveillance and evaluation projects.,Acquisition and/or Development,Neither,11/1/2021,9/1/2022, ,Developed with contracting resources.,Unknown, , , , ,No,"RAPID analysis of DNPAO policy and program data using GPT will results in project specific databases. To ensure accuracy and reliability of these AI-generated data, they will be compared to data of CDC manual reviews of the same documents by SMEs.  ",Documentation is missing or not available, , ,Yes,Enterprise Data Analytics and Visualization (EDAV) Platform,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"Policy and documentation awareness and evaluation

The web application will: 1) save staff time by streamlining laborious document reviews; 2) expand capacity by reducing the need for specialized staff training and skills; 3) provide complete, plain language answers to policy surveillance and evaluation questions along with binary codes or scores; 4) answer questions consistently (i.e., reduce intra-rater variability for redundant coding); and 5) enable easy validation of AI-provided answers. . An internal web application  will enable CDC users to: 1) import and securely store policy or program documents; 2) search for relevant documents; 3) ask questions of relevant text segments; 4) validate answers; and 5) collaborate on policy surveillance and evaluation projects.","policy and documentation awareness and evaluation the web application will: 1) save staff time by streamlining laborious document reviews; 2) expand capacity by reducing the need for specialized staff training and skills; 3) provide complete, plain language answers to policy surveillance and evaluation questions along with binary codes or scores; 4) answer questions consistently (i.e., reduce intra-rater variability for redundant coding); and 5) enable easy validation of ai-provided answers. . an internal web application will enable cdc users to: 1) import and securely store policy or program documents; 2) search for relevant documents; 3) ask questions of relevant text segments; 4) validate answers; and 5) collaborate on policy surveillance and evaluation projects."
SewerScout: Automated on-site sewage facility detection from aerial imagery to identify failed systems,Department of Health and Human Services,HHS,CDC,Health & Medical,None of the above.,"This business process focuses on the automation of the accurate identification of onsite wastewater systems in state, tribal, local, and territorial (STLT) health jurisdictions. This application scans aerial imagery and uses object detection and image classification models to detect onsite sewage facilities. Initial work is focusing on the ability to identify known systems.

 The intent of this project is to identify failed systems, which can be sources of soil and drinking water contamination as well as vectors that carry or transmit disease in the community. This project will allow STLT partners to readily characterize the location and functional status of septic systems without the time and resource consuming effort of visiting each household. The resources in many local health districts (LHDs) are barely adequate to undertake assessments of suitability  for new systems (if not entirely contracted out) and most LHDs do not have any program to routinely inspect current systems. If successful, this project will allow state, tribal, local, and territorial public health departments to more easily identify failing septic systems and work to address them.  If successful, subsequent work will work on differentiating functional from failed systems. Identification of septic systems during routine operations would help bolster and expedite disaster response efforts after major disasters  by having a ready catalog of systems and using pre- / post-disaster satellite images to direct resources in a much more efficient way than on-the-ground door-to-door surveys in rural / remote locations. ","The intent of this project is to identify failed wastewater systems, which can be sources of soil and drinking water contamination as well as vectors that carry or transmit disease in the community.",Acquisition and/or Development,Neither,1/1/2023,1/1/2023, ,Developed in-house., , , , , ,No,The project has not selected the final sources of data but no agency-owned data is anticipated to be used to train the model.,Documentation is widely available, , ,Yes,Enterprise Data Analytics and Visualization (EDAV) Platform,More than 12 months,Yes,Yes,No,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"This business process focuses on the automation of the accurate identification of onsite wastewater systems in state, tribal, local, and territorial (STLT) health jurisdictions. This application scans aerial imagery and uses object detection and image classification models to detect onsite sewage facilities. Initial work is focusing on the ability to identify known systems.

 The intent of this project is to identify failed systems, which can be sources of soil and drinking water contamination as well as vectors that carry or transmit disease in the community. This project will allow STLT partners to readily characterize the location and functional status of septic systems without the time and resource consuming effort of visiting each household. The resources in many local health districts (LHDs) are barely adequate to undertake assessments of suitability  for new systems (if not entirely contracted out) and most LHDs do not have any program to routinely inspect current systems. If successful, this project will allow state, tribal, local, and territorial public health departments to more easily identify failing septic systems and work to address them.  If successful, subsequent work will work on differentiating functional from failed systems. Identification of septic systems during routine operations would help bolster and expedite disaster response efforts after major disasters  by having a ready catalog of systems and using pre- / post-disaster satellite images to direct resources in a much more efficient way than on-the-ground door-to-door surveys in rural / remote locations. . The intent of this project is to identify failed wastewater systems, which can be sources of soil and drinking water contamination as well as vectors that carry or transmit disease in the community.","this business process focuses on the automation of the accurate identification of onsite wastewater systems in state, tribal, local, and territorial (stlt) health jurisdictions. this application scans aerial imagery and uses object detection and image classification models to detect onsite sewage facilities. initial work is focusing on the ability to identify known systems. the intent of this project is to identify failed systems, which can be sources of soil and drinking water contamination as well as vectors that carry or transmit disease in the community. this project will allow stlt partners to readily characterize the location and functional status of septic systems without the time and resource consuming effort of visiting each household. the resources in many local health districts (lhds) are barely adequate to undertake assessments of suitability for new systems (if not entirely contracted out) and most lhds do not have any program to routinely inspect current systems. if successful, this project will allow state, tribal, local, and territorial public health departments to more easily identify failing septic systems and work to address them. if successful, subsequent work will work on differentiating functional from failed systems. identification of septic systems during routine operations would help bolster and expedite disaster response efforts after major disasters by having a ready catalog of systems and using pre- / post-disaster satellite images to direct resources in a much more efficient way than on-the-ground door-to-door surveys in rural / remote locations. . the intent of this project is to identify failed wastewater systems, which can be sources of soil and drinking water contamination as well as vectors that carry or transmit disease in the community."
Sidekick Comms bot Offering User-friendly Tips (SCOUT),Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"The AI will impact the content creation processes for CDC.GOV and reduce burden in creating new webpages. CDC sometimes uses GenAI (generative artificial intelligence) to help create content for the public. This may include content for the web, social media, fact sheets, and graphics. In every case, our experts review our communication materials in an effort to ensure accuracy and quality before use. CDC public health research, data, and recommendations are based on the best science currently available and go through a separate agency review process prior to release.

The solution aims to reduce the workload of health communicators by simplifying the process of creating new web pages and social media posts. By leveraging Generative AI technology, we can produce new content much more quickly than if communications staff were to do it on their own. The goal is to make the published content more accessible and understandable for the general public, ensuring that CDC information reaches a wider audience. It's important to note that all content generated by the AI is carefully reviewed and edited by a human before it is used and the follow disclaimer is stated with public content that this tool has been used with: ""CDC sometimes uses GenAI (generative artificial intelligence) to help create content for the public. This may include content for the web, social media, fact sheets, and graphics. In every case, our experts review our communication materials in an effort to ensure accuracy and quality before use. CDC public health research, data, and recommendations are based on the best science currently available and go through a separate agency review process prior to release.""","CDC's National Center for Chronic Disease Prevention and Health Promotion use of Generative AI in this use case is to help convert existing web content into plain and clear language. This solution uses Gen AI to create new versions of publicly available content that are easier for end-users to understand. By incorporating prompts, the tool makes the content more suitable for a general lay audience. In addition to web articles, the Generative AI tool also allows for the creation of social media posts. The goal of this project is to make information more accessible by providing plain language content on various platforms. CDC sometimes uses GenAI (generative artificial intelligence) to help create content for the public. This may include content for the web, social media, fact sheets, and graphics. In every case, our experts review our communication materials in an effort to ensure accuracy and quality before use. CDC public health research, data, and recommendations are based on the best science currently available and go through a separate agency review process prior to release.",Acquisition and/or Development,Neither,12/1/2024,1/1/2024, ,Developed with contracting resources.,NCCDPHP OD IAA, , , , ,Yes,Unknown,Documentation is complete, , ,Yes,Enterprise Data Analytics and Visualization (EDAV) Platform,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"The AI will impact the content creation processes for CDC.GOV and reduce burden in creating new webpages. CDC sometimes uses GenAI (generative artificial intelligence) to help create content for the public. This may include content for the web, social media, fact sheets, and graphics. In every case, our experts review our communication materials in an effort to ensure accuracy and quality before use. CDC public health research, data, and recommendations are based on the best science currently available and go through a separate agency review process prior to release.

The solution aims to reduce the workload of health communicators by simplifying the process of creating new web pages and social media posts. By leveraging Generative AI technology, we can produce new content much more quickly than if communications staff were to do it on their own. The goal is to make the published content more accessible and understandable for the general public, ensuring that CDC information reaches a wider audience. It's important to note that all content generated by the AI is carefully reviewed and edited by a human before it is used and the follow disclaimer is stated with public content that this tool has been used with: ""CDC sometimes uses GenAI (generative artificial intelligence) to help create content for the public. This may include content for the web, social media, fact sheets, and graphics. In every case, our experts review our communication materials in an effort to ensure accuracy and quality before use. CDC public health research, data, and recommendations are based on the best science currently available and go through a separate agency review process prior to release."" . CDC's National Center for Chronic Disease Prevention and Health Promotion use of Generative AI in this use case is to help convert existing web content into plain and clear language. This solution uses Gen AI to create new versions of publicly available content that are easier for end-users to understand. By incorporating prompts, the tool makes the content more suitable for a general lay audience. In addition to web articles, the Generative AI tool also allows for the creation of social media posts. The goal of this project is to make information more accessible by providing plain language content on various platforms. CDC sometimes uses GenAI (generative artificial intelligence) to help create content for the public. This may include content for the web, social media, fact sheets, and graphics. In every case, our experts review our communication materials in an effort to ensure accuracy and quality before use. CDC public health research, data, and recommendations are based on the best science currently available and go through a separate agency review process prior to release.","the ai will impact the content creation processes for cdc.gov and reduce burden in creating new webpages. cdc sometimes uses genai (generative artificial intelligence) to help create content for the public. this may include content for the web, social media, fact sheets, and graphics. in every case, our experts review our communication materials in an effort to ensure accuracy and quality before use. cdc public health research, data, and recommendations are based on the best science currently available and go through a separate agency review process prior to release. the solution aims to reduce the workload of health communicators by simplifying the process of creating new web pages and social media posts. by leveraging generative ai technology, we can produce new content much more quickly than if communications staff were to do it on their own. the goal is to make the published content more accessible and understandable for the general public, ensuring that cdc information reaches a wider audience. it's important to note that all content generated by the ai is carefully reviewed and edited by a human before it is used and the follow disclaimer is stated with public content that this tool has been used with: ""cdc sometimes uses genai (generative artificial intelligence) to help create content for the public. this may include content for the web, social media, fact sheets, and graphics. in every case, our experts review our communication materials in an effort to ensure accuracy and quality before use. cdc public health research, data, and recommendations are based on the best science currently available and go through a separate agency review process prior to release."" . cdc's national center for chronic disease prevention and health promotion use of generative ai in this use case is to help convert existing web content into plain and clear language. this solution uses gen ai to create new versions of publicly available content that are easier for end-users to understand. by incorporating prompts, the tool makes the content more suitable for a general lay audience. in addition to web articles, the generative ai tool also allows for the creation of social media posts. the goal of this project is to make information more accessible by providing plain language content on various platforms. cdc sometimes uses genai (generative artificial intelligence) to help create content for the public. this may include content for the web, social media, fact sheets, and graphics. in every case, our experts review our communication materials in an effort to ensure accuracy and quality before use. cdc public health research, data, and recommendations are based on the best science currently available and go through a separate agency review process prior to release."
Transcribing Cognitive Interviews with Whisper,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"Federal Health Survey Research. Specifically, Cognitive interviews are a form of qualitative research where interviewers ask probing questions to understand how the interviewee understands a concept which is being asked about in a federal survey. These interviews are a key part of survey design to identify any potential misconceptions and prevent lengthy and hard to answer questions which could create potentially wrong health statistics. These interviews are time consuming to perform and report as they are inherently individual and require extensive hours to transcribe. By using AI to transcribe the interviews, researchers can potentially cut down on the hours or qualitative review, increasing the speed of research publication and quality of survey questions used on federal surveys.

Without transcripts, these interviews have to be listened to repeatedly taking at least the same amount of time as the full length of the interview to find important pieces of information for their work. These transcripts can greatly reduce the burden on researchers by enabling immediate comparison of concepts and answers along with timestamps created for researchers to reference in review.",This AI generates transcripts from recorded interviews for staff to use when performing qualitative research to assist with Federal Health Survey research.,Acquisition and/or Development,Neither,10/1/2022,7/1/2024, ,Developed in-house., , , , , ,No,"None, publicly available data was used to evaluate performance",Documentation has been partially completed, , ,No, ,More than 12 months,Yes,Yes,No,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"Federal Health Survey Research. Specifically, Cognitive interviews are a form of qualitative research where interviewers ask probing questions to understand how the interviewee understands a concept which is being asked about in a federal survey. These interviews are a key part of survey design to identify any potential misconceptions and prevent lengthy and hard to answer questions which could create potentially wrong health statistics. These interviews are time consuming to perform and report as they are inherently individual and require extensive hours to transcribe. By using AI to transcribe the interviews, researchers can potentially cut down on the hours or qualitative review, increasing the speed of research publication and quality of survey questions used on federal surveys.

Without transcripts, these interviews have to be listened to repeatedly taking at least the same amount of time as the full length of the interview to find important pieces of information for their work. These transcripts can greatly reduce the burden on researchers by enabling immediate comparison of concepts and answers along with timestamps created for researchers to reference in review. . This AI generates transcripts from recorded interviews for staff to use when performing qualitative research to assist with Federal Health Survey research.","federal health survey research. specifically, cognitive interviews are a form of qualitative research where interviewers ask probing questions to understand how the interviewee understands a concept which is being asked about in a federal survey. these interviews are a key part of survey design to identify any potential misconceptions and prevent lengthy and hard to answer questions which could create potentially wrong health statistics. these interviews are time consuming to perform and report as they are inherently individual and require extensive hours to transcribe. by using ai to transcribe the interviews, researchers can potentially cut down on the hours or qualitative review, increasing the speed of research publication and quality of survey questions used on federal surveys. without transcripts, these interviews have to be listened to repeatedly taking at least the same amount of time as the full length of the interview to find important pieces of information for their work. these transcripts can greatly reduce the burden on researchers by enabling immediate comparison of concepts and answers along with timestamps created for researchers to reference in review. . this ai generates transcripts from recorded interviews for staff to use when performing qualitative research to assist with federal health survey research."
Use of Natural Language Processing for Topic Modeling to Automate Review of Public Comments to Notice of Proposed Rulemaking,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"This use case is aimed to assist in the review of public comments for Notices of Proposed Rulemaking. All responses are manually reviewed following this tool usage.

By law, Public comments are required to be manually review for any notice of public rule making. These comments are highly labor intensive due to both the quantity and wide range of potential opinions shared by the public. These comments can range in both length of response and quantity serving as a challenge for properly ensuring the insights from the public are accurately gathered in the review process. By using an LLM based Topic Modeling approach, manual reviewers can organize responses based upon theme to enhance both the insights gained, the speed of manual review, and ensure all topics mentioned can be accurately reported in all review processes. This process can use open source python libraries and open-source LLMs to complete different actions. This AI methodology enables manual review processes to be more effective in gaining insights from the public, better organized in themes and categories present within text, and reduce the burden of manual review of comments from notices of proposed rulemaking required by law.",Generates clusters of similar public comments to a 'notice of proposed rulemaking' to aid in manual review.,Acquisition and/or Development,Neither,2/1/2023,4/1/2023, ,Developed in-house., , , , , ,No,Unknown,Documentation has been partially completed, , ,Yes,"Enterprise Data, Analytics, and Visualization Platform",Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"This use case is aimed to assist in the review of public comments for Notices of Proposed Rulemaking. All responses are manually reviewed following this tool usage.

By law, Public comments are required to be manually review for any notice of public rule making. These comments are highly labor intensive due to both the quantity and wide range of potential opinions shared by the public. These comments can range in both length of response and quantity serving as a challenge for properly ensuring the insights from the public are accurately gathered in the review process. By using an LLM based Topic Modeling approach, manual reviewers can organize responses based upon theme to enhance both the insights gained, the speed of manual review, and ensure all topics mentioned can be accurately reported in all review processes. This process can use open source python libraries and open-source LLMs to complete different actions. This AI methodology enables manual review processes to be more effective in gaining insights from the public, better organized in themes and categories present within text, and reduce the burden of manual review of comments from notices of proposed rulemaking required by law. . Generates clusters of similar public comments to a 'notice of proposed rulemaking' to aid in manual review.","this use case is aimed to assist in the review of public comments for notices of proposed rulemaking. all responses are manually reviewed following this tool usage. by law, public comments are required to be manually review for any notice of public rule making. these comments are highly labor intensive due to both the quantity and wide range of potential opinions shared by the public. these comments can range in both length of response and quantity serving as a challenge for properly ensuring the insights from the public are accurately gathered in the review process. by using an llm based topic modeling approach, manual reviewers can organize responses based upon theme to enhance both the insights gained, the speed of manual review, and ensure all topics mentioned can be accurately reported in all review processes. this process can use open source python libraries and open-source llms to complete different actions. this ai methodology enables manual review processes to be more effective in gaining insights from the public, better organized in themes and categories present within text, and reduce the burden of manual review of comments from notices of proposed rulemaking required by law. . generates clusters of similar public comments to a 'notice of proposed rulemaking' to aid in manual review."
Center for Forecasting and Outbreak Analytics Disease Modeling,Department of Health and Human Services,HHS,CDC,Health & Medical,None of the above.,"Federal and State, Tribal, Local, and Territorial health department responses to disease outbreaks depends on being able to predict the future behavior of a given outbreak as well as the potential impact of different responses to the outbreak. This project aims to build models of disease spread that allow for (1) determining if and where infection/impact rates are growing or increasing (2) forecasting future disease progression (3) modeling specific scenarios such as the impact of vaccination.

We use model output to provide information to Federal and SLTT decision makers to improve disease response. This allows those decision makers to determine where and how to direct resources in a disease out break as well as the impact of potential response actions. This kind of insight is critically dependent on mathematical modeling.","This project  models disease spread and impact based upon a set of models, a variety of data sources, and allows for the inclusion of the impact of potential public health actions such as vaccinations. These insights can be used by public health decision makers to inform current and future responses to public health emergencies",Implementation and Assessment,Neither,4/1/2022,4/1/2022, ,Developed with both contracting and in-house resources.,75D30123F16628, ,No,No,No,Yes,"A wide array of public health data including hospital admissions data from NHSN, emergency department visit data from NSSP, case reports provided to CDC from local jurisdictions, variant prevalence data sourced from public health laboratory samples, wastewater data from NWSS, and other related sources.",Documentation is complete,Yes,Yes – source code is publicly available.,Yes,"Enterprise Data Analytics and Visualization (EDAV) Platform, EDAV-EXT, 1 CDC Data Platform (1CDP)",Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"Federal and State, Tribal, Local, and Territorial health department responses to disease outbreaks depends on being able to predict the future behavior of a given outbreak as well as the potential impact of different responses to the outbreak. This project aims to build models of disease spread that allow for (1) determining if and where infection/impact rates are growing or increasing (2) forecasting future disease progression (3) modeling specific scenarios such as the impact of vaccination.

We use model output to provide information to Federal and SLTT decision makers to improve disease response. This allows those decision makers to determine where and how to direct resources in a disease out break as well as the impact of potential response actions. This kind of insight is critically dependent on mathematical modeling. . This project  models disease spread and impact based upon a set of models, a variety of data sources, and allows for the inclusion of the impact of potential public health actions such as vaccinations. These insights can be used by public health decision makers to inform current and future responses to public health emergencies","federal and state, tribal, local, and territorial health department responses to disease outbreaks depends on being able to predict the future behavior of a given outbreak as well as the potential impact of different responses to the outbreak. this project aims to build models of disease spread that allow for (1) determining if and where infection/impact rates are growing or increasing (2) forecasting future disease progression (3) modeling specific scenarios such as the impact of vaccination. we use model output to provide information to federal and sltt decision makers to improve disease response. this allows those decision makers to determine where and how to direct resources in a disease out break as well as the impact of potential response actions. this kind of insight is critically dependent on mathematical modeling. . this project models disease spread and impact based upon a set of models, a variety of data sources, and allows for the inclusion of the impact of potential public health actions such as vaccinations. these insights can be used by public health decision makers to inform current and future responses to public health emergencies"
ChatCDC- CDC Enterprise GenAI Chatbot: Bring your own Enterprise Data (Data Analysis),Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"This AI system is an extension of the GenAI chatbot but is only available to a small group of pilot testers as of November 5, 2024. It operates through an ATO'ed instance of Microsoft Azure OpenAI through CDC's Enterprise Data Analytics and Visualization Platform. Using a method called retrieval augmented generation (RAG), this enhancement enables staff to augment the existing GenAI chatbot with access and “knowledge” of internal or enterprise data such as documents, spreadsheets, and PDFs. Additionally, each chatbot response has citations referencing to the sections of source text that informed the response, which facilitates human-in-the-loop validation for accuracy and relevancy of AI-generated content.

One way users are using this tool is for data extraction, which accelerates analysis of large document sets. By extracting key points from large volumes of text in various file formats, the enterprise-data-enhanced chatbot streamlines access to critical insights from reports, internal documentation, publications, and other documents. For example, one team is using the chatbot to support literature review by extracting information related to housing status and HIV in the US from a set of pre-screened publications. Examples of this information include a binary yes/no on whether the research paper focuses on the topic of interest, the paper’s population of focus, and the type of paper.

The enterprise data chatbot’s ability to quickly and accurately extract relevant information reduces manual search time and errors associated with human data retrieval. The output will be leveraged for more accurate data analysis, timely insights, and better-informed decision-making processes.","When the user enters a prompt or query related to data extraction into the chatbot, the tool uses a GPT-4 model and Azure AI Search within a RAG architecture to retrieve relevant sections of text from the provided enterprise data sources and extract key information as instructed by the user’s prompt.",Implementation and Assessment,Neither,6/1/2024,6/1/2024, ,Developed with both contracting and in-house resources.,Unknown, ,No,No,No,No,Unknown,Documentation is missing or not available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Enterprise Data Analytics and Visualization (EDAV) Platform,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"This AI system is an extension of the GenAI chatbot but is only available to a small group of pilot testers as of November 5, 2024. It operates through an ATO'ed instance of Microsoft Azure OpenAI through CDC's Enterprise Data Analytics and Visualization Platform. Using a method called retrieval augmented generation (RAG), this enhancement enables staff to augment the existing GenAI chatbot with access and “knowledge” of internal or enterprise data such as documents, spreadsheets, and PDFs. Additionally, each chatbot response has citations referencing to the sections of source text that informed the response, which facilitates human-in-the-loop validation for accuracy and relevancy of AI-generated content.

One way users are using this tool is for data extraction, which accelerates analysis of large document sets. By extracting key points from large volumes of text in various file formats, the enterprise-data-enhanced chatbot streamlines access to critical insights from reports, internal documentation, publications, and other documents. For example, one team is using the chatbot to support literature review by extracting information related to housing status and HIV in the US from a set of pre-screened publications. Examples of this information include a binary yes/no on whether the research paper focuses on the topic of interest, the paper’s population of focus, and the type of paper.

The enterprise data chatbot’s ability to quickly and accurately extract relevant information reduces manual search time and errors associated with human data retrieval. The output will be leveraged for more accurate data analysis, timely insights, and better-informed decision-making processes. . When the user enters a prompt or query related to data extraction into the chatbot, the tool uses a GPT-4 model and Azure AI Search within a RAG architecture to retrieve relevant sections of text from the provided enterprise data sources and extract key information as instructed by the user’s prompt.","this ai system is an extension of the genai chatbot but is only available to a small group of pilot testers as of november 5, 2024. it operates through an ato'ed instance of microsoft azure openai through cdc's enterprise data analytics and visualization platform. using a method called retrieval augmented generation (rag), this enhancement enables staff to augment the existing genai chatbot with access and “knowledge” of internal or enterprise data such as documents, spreadsheets, and pdfs. additionally, each chatbot response has citations referencing to the sections of source text that informed the response, which facilitates human-in-the-loop validation for accuracy and relevancy of ai-generated content. one way users are using this tool is for data extraction, which accelerates analysis of large document sets. by extracting key points from large volumes of text in various file formats, the enterprise-data-enhanced chatbot streamlines access to critical insights from reports, internal documentation, publications, and other documents. for example, one team is using the chatbot to support literature review by extracting information related to housing status and hiv in the us from a set of pre-screened publications. examples of this information include a binary yes/no on whether the research paper focuses on the topic of interest, the paper’s population of focus, and the type of paper. the enterprise data chatbot’s ability to quickly and accurately extract relevant information reduces manual search time and errors associated with human data retrieval. the output will be leveraged for more accurate data analysis, timely insights, and better-informed decision-making processes. . when the user enters a prompt or query related to data extraction into the chatbot, the tool uses a gpt-4 model and azure ai search within a rag architecture to retrieve relevant sections of text from the provided enterprise data sources and extract key information as instructed by the user’s prompt."
ChatCDC- CDC Enterprise GenAI Chatbot: Bring your own Enterprise Data (Summarization),Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"This AI system is an extension of the GenAI chatbot but is only available to a small group of pilot testers as of November 5, 2024. It operates through an ATO'ed instance of Microsoft Azure OpenAI through CDC's Enterprise Data Analytics and Visualization Platform. Using a method called retrieval augmented generation (RAG), this enhancement enables staff to augment the existing GenAI chatbot with access and “knowledge” of internal or enterprise data such as documents, spreadsheets, and PDFs. Additionally, each chatbot response has citations referencing to the sections of source text that informed the response, which facilitates human-in-the-loop validation for accuracy and relevancy of AI-generated content.

The enterprise data chatbot’s summaries will enhance operational, informational, and decision-making processes that would otherwise require extensive research and reading time. For an example of a more informational use case, one team provided the chatbot with documentation and resources about a division’s organizational structure, projects, administrative processes, and tools to support the orientation of new staff and (re)orientation of existing staff. The chatbot enables the division’s staff to efficiently find resources for their roles; better understand the division’s initiatives; and track the activities involved in the onboarding process for new staff.","When the user enters a prompt or query related to summarization into the chatbot, the tool uses a GPT-4 model and Azure AI Search within a RAG architecture to retrieve relevant sections of text from the provided enterprise data sources and condense that information into concise summaries in response to the user’s prompt. ",Implementation and Assessment,Neither,6/1/2024,6/1/2024, ,Developed with both contracting and in-house resources.,Unknown, ,No,No,No,No,Unknown,Documentation is missing or not available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Enterprise Data Analytics and Visualization (EDAV) Platform,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"This AI system is an extension of the GenAI chatbot but is only available to a small group of pilot testers as of November 5, 2024. It operates through an ATO'ed instance of Microsoft Azure OpenAI through CDC's Enterprise Data Analytics and Visualization Platform. Using a method called retrieval augmented generation (RAG), this enhancement enables staff to augment the existing GenAI chatbot with access and “knowledge” of internal or enterprise data such as documents, spreadsheets, and PDFs. Additionally, each chatbot response has citations referencing to the sections of source text that informed the response, which facilitates human-in-the-loop validation for accuracy and relevancy of AI-generated content.

The enterprise data chatbot’s summaries will enhance operational, informational, and decision-making processes that would otherwise require extensive research and reading time. For an example of a more informational use case, one team provided the chatbot with documentation and resources about a division’s organizational structure, projects, administrative processes, and tools to support the orientation of new staff and (re)orientation of existing staff. The chatbot enables the division’s staff to efficiently find resources for their roles; better understand the division’s initiatives; and track the activities involved in the onboarding process for new staff. . When the user enters a prompt or query related to summarization into the chatbot, the tool uses a GPT-4 model and Azure AI Search within a RAG architecture to retrieve relevant sections of text from the provided enterprise data sources and condense that information into concise summaries in response to the user’s prompt.","this ai system is an extension of the genai chatbot but is only available to a small group of pilot testers as of november 5, 2024. it operates through an ato'ed instance of microsoft azure openai through cdc's enterprise data analytics and visualization platform. using a method called retrieval augmented generation (rag), this enhancement enables staff to augment the existing genai chatbot with access and “knowledge” of internal or enterprise data such as documents, spreadsheets, and pdfs. additionally, each chatbot response has citations referencing to the sections of source text that informed the response, which facilitates human-in-the-loop validation for accuracy and relevancy of ai-generated content. the enterprise data chatbot’s summaries will enhance operational, informational, and decision-making processes that would otherwise require extensive research and reading time. for an example of a more informational use case, one team provided the chatbot with documentation and resources about a division’s organizational structure, projects, administrative processes, and tools to support the orientation of new staff and (re)orientation of existing staff. the chatbot enables the division’s staff to efficiently find resources for their roles; better understand the division’s initiatives; and track the activities involved in the onboarding process for new staff. . when the user enters a prompt or query related to summarization into the chatbot, the tool uses a gpt-4 model and azure ai search within a rag architecture to retrieve relevant sections of text from the provided enterprise data sources and condense that information into concise summaries in response to the user’s prompt."
Leveraging AI for Metadata Tagging for Enterprise Data Catalog of CDC,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"CDC’s Enterprise Data Catalog provides an internal single centralized inventory of available datasets within CDC staff. It provides data management and search tools to help users find what they need and aims to provide descriptions to aid in relevance determinations for users. This is from the appropriate metadata, e.g. project title, description, or other important information describing the dataset. Because of the previous manual metadata tagging process, many datasets catalogued do not have the appropriate tags to help staff determine the relevance of any particular dataset available within CDC for particular tasks.

Because of the lack of standardization and gaps in metadata fields, this AI's output will greatly increase the speed for applying the appropriate metadata fields for each dataset within the catalogue to increase the usability of the product. This reduces the time and effort required for manual tagging and improves the consistency and completeness of metadata. ",AI is used in this case to generate suggested metadata fields based upon existing metadata information provided by a dataset. These tags would then be used for staff accessing the enterprise data catalogue to discover datasets and find connections to support existing on-going work.,Implementation and Assessment,Neither,6/1/2024,6/1/2024, ,Developed in-house., , ,No,No,No,Yes,Enterprise Data Catalog Metadata fields,Documentation has been partially completed,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Enterprise Data Analytics and Visualization (EDAV) Platform,Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"CDC’s Enterprise Data Catalog provides an internal single centralized inventory of available datasets within CDC staff. It provides data management and search tools to help users find what they need and aims to provide descriptions to aid in relevance determinations for users. This is from the appropriate metadata, e.g. project title, description, or other important information describing the dataset. Because of the previous manual metadata tagging process, many datasets catalogued do not have the appropriate tags to help staff determine the relevance of any particular dataset available within CDC for particular tasks.

Because of the lack of standardization and gaps in metadata fields, this AI's output will greatly increase the speed for applying the appropriate metadata fields for each dataset within the catalogue to increase the usability of the product. This reduces the time and effort required for manual tagging and improves the consistency and completeness of metadata. . AI is used in this case to generate suggested metadata fields based upon existing metadata information provided by a dataset. These tags would then be used for staff accessing the enterprise data catalogue to discover datasets and find connections to support existing on-going work.","cdc’s enterprise data catalog provides an internal single centralized inventory of available datasets within cdc staff. it provides data management and search tools to help users find what they need and aims to provide descriptions to aid in relevance determinations for users. this is from the appropriate metadata, e.g. project title, description, or other important information describing the dataset. because of the previous manual metadata tagging process, many datasets catalogued do not have the appropriate tags to help staff determine the relevance of any particular dataset available within cdc for particular tasks. because of the lack of standardization and gaps in metadata fields, this ai's output will greatly increase the speed for applying the appropriate metadata fields for each dataset within the catalogue to increase the usability of the product. this reduces the time and effort required for manual tagging and improves the consistency and completeness of metadata. . ai is used in this case to generate suggested metadata fields based upon existing metadata information provided by a dataset. these tags would then be used for staff accessing the enterprise data catalogue to discover datasets and find connections to support existing on-going work."
Malaria parasites DNA barcode geography classification,Department of Health and Human Services,HHS,CDC,Health & Medical,None of the above.,"Epidemiological investigations of Malaria

To complement epidemiologic investigations of domestic malaria cases, including cases that were domestically acquired. This information can help assess the origin of the malaria strain in question, which can help us understand how the strain entered the US. For more information, please see the following manuscript: https://journals.asm.org/doi/full/10.1128/aac.01203-24","Algorithm examines a sequence barcode/genotype (our use case is for malaria, but it can be applied to other contexts) and it assigns the malaria parasite genotype to a geographic origin, which may be a continent or some other geographic regional category. While this use case has the barcode as a set of genetic loci from malaria parasites, and the categories are geographic (i.e., continents, subregions), the algorithm could be trained to assign genetic barcodes from other pathogens to any number of different categories assuming the appropriate training dataset exists.",Implementation and Assessment,Neither,7/1/2023,7/1/2023, ,Developed in-house., , ,No,No,No,No,"Data used are a mixture of data generated at CDC and other data available publicly. All CDC data are accessible via the following links: https://www.ncbi.nlm.nih.gov/bioproject/PRJNA428490/, https://www.ncbi.nlm.nih.gov/bioproject/PRJNA1092573/, https://www.ncbi.nlm.nih.gov/bioproject/?term=PRJNA1110244. Travel histories from case patients were used to assess performance of the model to compare the generated output of the model and are available in the following manuscript: https://journals.asm.org/doi/full/10.1128/aac.01203-24. Non-CDC owned data are available through this link: https://apps.malariagen.net/apps/pf7/",Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Advanced Molecular Detection Scientific Computing Platform,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"Epidemiological investigations of Malaria

To complement epidemiologic investigations of domestic malaria cases, including cases that were domestically acquired. This information can help assess the origin of the malaria strain in question, which can help us understand how the strain entered the US. For more information, please see the following manuscript: https://journals.asm.org/doi/full/10.1128/aac.01203-24 . Algorithm examines a sequence barcode/genotype (our use case is for malaria, but it can be applied to other contexts) and it assigns the malaria parasite genotype to a geographic origin, which may be a continent or some other geographic regional category. While this use case has the barcode as a set of genetic loci from malaria parasites, and the categories are geographic (i.e., continents, subregions), the algorithm could be trained to assign genetic barcodes from other pathogens to any number of different categories assuming the appropriate training dataset exists.","epidemiological investigations of malaria to complement epidemiologic investigations of domestic malaria cases, including cases that were domestically acquired. this information can help assess the origin of the malaria strain in question, which can help us understand how the strain entered the us. for more information, please see the following manuscript: https://journals.asm.org/doi/full/10.1128/aac.01203-24 . algorithm examines a sequence barcode/genotype (our use case is for malaria, but it can be applied to other contexts) and it assigns the malaria parasite genotype to a geographic origin, which may be a continent or some other geographic regional category. while this use case has the barcode as a set of genetic loci from malaria parasites, and the categories are geographic (i.e., continents, subregions), the algorithm could be trained to assign genetic barcodes from other pathogens to any number of different categories assuming the appropriate training dataset exists."
The School Closure Awareness System,Department of Health and Human Services,HHS,CDC,Emergency Management,None of the above.,"Identifying unplanned school closures. CDC is responsible for supporting schools before, during, and after and emergency. During COVID 19 this included monitoring school closure or virtual status and reporting this to federal partners, the White House, and the public. Due to cost the process used during COVID was no longer feasible and ceased in DEC 2022. We explored multiple ways to capture this data and our current system met the need best. 

This AI has allowed us to save nearly 2-million-dollars from contracting fees and decrease human work hours by 200 hours. It has made the capture of some of this data possible after cessation of the process used during COVID 19 which we could no longer support. The AI supported system is faster and captures more information about individual closures than the original human-powered process.","This system uses publicly available Facebook posts from approximately 40,000 school or district Facebook accounts. Then uses a large language model to categorize posts as unplanned school closure for several types of events (weather, health, facility issues, safety). It also will denote the type of status change (full closure, virtual, hybrid, or early / late dismissal). The data is checked and recoded (if needed) by a staff person every 24 hours.",Implementation and Assessment,Neither,7/1/2022,11/1/2022, ,Developed in-house., , ,No,No,No,No,Unknown,Documentation has been partially completed,Yes,"Yes – agency has access to source code, but it is not public.",Yes,"Enterprise Data, Analytics, and Visualization Platform",More than 12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"Identifying unplanned school closures. CDC is responsible for supporting schools before, during, and after and emergency. During COVID 19 this included monitoring school closure or virtual status and reporting this to federal partners, the White House, and the public. Due to cost the process used during COVID was no longer feasible and ceased in DEC 2022. We explored multiple ways to capture this data and our current system met the need best. 

This AI has allowed us to save nearly 2-million-dollars from contracting fees and decrease human work hours by 200 hours. It has made the capture of some of this data possible after cessation of the process used during COVID 19 which we could no longer support. The AI supported system is faster and captures more information about individual closures than the original human-powered process. . This system uses publicly available Facebook posts from approximately 40,000 school or district Facebook accounts. Then uses a large language model to categorize posts as unplanned school closure for several types of events (weather, health, facility issues, safety). It also will denote the type of status change (full closure, virtual, hybrid, or early / late dismissal). The data is checked and recoded (if needed) by a staff person every 24 hours.","identifying unplanned school closures. cdc is responsible for supporting schools before, during, and after and emergency. during covid 19 this included monitoring school closure or virtual status and reporting this to federal partners, the white house, and the public. due to cost the process used during covid was no longer feasible and ceased in dec 2022. we explored multiple ways to capture this data and our current system met the need best. this ai has allowed us to save nearly 2-million-dollars from contracting fees and decrease human work hours by 200 hours. it has made the capture of some of this data possible after cessation of the process used during covid 19 which we could no longer support. the ai supported system is faster and captures more information about individual closures than the original human-powered process. . this system uses publicly available facebook posts from approximately 40,000 school or district facebook accounts. then uses a large language model to categorize posts as unplanned school closure for several types of events (weather, health, facility issues, safety). it also will denote the type of status change (full closure, virtual, hybrid, or early / late dismissal). the data is checked and recoded (if needed) by a staff person every 24 hours."
Using Generative AI for Stance Analysis of Public Comments on CDC’s Proposed Rules,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"By law, Public comments are required to be manually review for any notice of public rule making. These comments are highly labor intensive due to both the quantity and wide range of potential opinions shared by the public. These comments can range in both length of response and quantity serving as a challenge for properly ensuring the insights from the public are accurately gathered in the review process. By using an LLM based Topic Modeling approach, manual reviewers can organize responses based upon theme to enhance both the insights gained, the speed of manual review, and ensure all topics mentioned can be accurately reported in all review processes. This process can use open source python libraries and open-source LLMs to complete different actions.

 As public feedback on various rules gets collected across the CDC’s divisions, this initiative along with other NLP approaches can potentially save time for CDC’s public policy experts during public comment review. ","Reviewing the public comments currently involves a manual review process that necessitates a high level of time and effort on the part of regulatory analysts. To improve the process of reviewing public comments as part of regulatory analysis, our team implemented topic modeling (Grootendorst et al., 2022) and sentiment analysis (Hartman et al., 2023) of comments published to the new amendment to foreign quarantine regulation published by CDC on July 10, 2023. This solution expands these efforts by using generative AI models  to try for capture stance expressed in each comment. While there is also an option to manually label and train models for stance detection, custom-trained text classification models may not be transferable to future rules due to rule-specific discussions and arguments. In contrast, using generative AI models for stance detection can be transferable to future rules irrespective of the discussion contexts and arguments.",Implementation and Assessment,Neither,7/1/2023,7/1/2023, ,Developed in-house., , ,No,No,No,No,Unknown,Documentation is missing or not available,Yes,"Yes – agency has access to source code, but it is not public.",No, ,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"By law, Public comments are required to be manually review for any notice of public rule making. These comments are highly labor intensive due to both the quantity and wide range of potential opinions shared by the public. These comments can range in both length of response and quantity serving as a challenge for properly ensuring the insights from the public are accurately gathered in the review process. By using an LLM based Topic Modeling approach, manual reviewers can organize responses based upon theme to enhance both the insights gained, the speed of manual review, and ensure all topics mentioned can be accurately reported in all review processes. This process can use open source python libraries and open-source LLMs to complete different actions.

 As public feedback on various rules gets collected across the CDC’s divisions, this initiative along with other NLP approaches can potentially save time for CDC’s public policy experts during public comment review. . Reviewing the public comments currently involves a manual review process that necessitates a high level of time and effort on the part of regulatory analysts. To improve the process of reviewing public comments as part of regulatory analysis, our team implemented topic modeling (Grootendorst et al., 2022) and sentiment analysis (Hartman et al., 2023) of comments published to the new amendment to foreign quarantine regulation published by CDC on July 10, 2023. This solution expands these efforts by using generative AI models  to try for capture stance expressed in each comment. While there is also an option to manually label and train models for stance detection, custom-trained text classification models may not be transferable to future rules due to rule-specific discussions and arguments. In contrast, using generative AI models for stance detection can be transferable to future rules irrespective of the discussion contexts and arguments.","by law, public comments are required to be manually review for any notice of public rule making. these comments are highly labor intensive due to both the quantity and wide range of potential opinions shared by the public. these comments can range in both length of response and quantity serving as a challenge for properly ensuring the insights from the public are accurately gathered in the review process. by using an llm based topic modeling approach, manual reviewers can organize responses based upon theme to enhance both the insights gained, the speed of manual review, and ensure all topics mentioned can be accurately reported in all review processes. this process can use open source python libraries and open-source llms to complete different actions. as public feedback on various rules gets collected across the cdc’s divisions, this initiative along with other nlp approaches can potentially save time for cdc’s public policy experts during public comment review. . reviewing the public comments currently involves a manual review process that necessitates a high level of time and effort on the part of regulatory analysts. to improve the process of reviewing public comments as part of regulatory analysis, our team implemented topic modeling (grootendorst et al., 2022) and sentiment analysis (hartman et al., 2023) of comments published to the new amendment to foreign quarantine regulation published by cdc on july 10, 2023. this solution expands these efforts by using generative ai models to try for capture stance expressed in each comment. while there is also an option to manually label and train models for stance detection, custom-trained text classification models may not be transferable to future rules due to rule-specific discussions and arguments. in contrast, using generative ai models for stance detection can be transferable to future rules irrespective of the discussion contexts and arguments."
A reusable NLP pipeline for clinical narratives preprocessing and characterization,Department of Health and Human Services,HHS,CDC,Health & Medical,None of the above.,"Data cleaning of Clinical narratives from health records. At scale, clinical narratives are a dataset available to CDC currently underutilized as there is not a structured pipeline to identify structured information output. We are currently working on setting up a process to create a pipeline that cleans data markup (html, rtf, xml) and identifies signs and symptoms using pre-trained Named Entity Recognition (NER) tasks from relevant models published in peer review journals. 





The use of these pretrained models significantly reduces  the amount of time a human to spend tagging data and provides additional features and increased consistency from unstructured data for identification of more novel insights. ","The objective of this project is to take unstructured data, clean it, and use pretrained Named Entity Recognition (NER) tools to extract signs/symptoms in deidentified electronic health records that can be used to characterize a cohort or make further analysis.",Initiated,Neither,7/1/2024,Unknown, , , , , , , , , , , , , , ,Unknown, , ,Unknown, , ,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"Data cleaning of Clinical narratives from health records. At scale, clinical narratives are a dataset available to CDC currently underutilized as there is not a structured pipeline to identify structured information output. We are currently working on setting up a process to create a pipeline that cleans data markup (html, rtf, xml) and identifies signs and symptoms using pre-trained Named Entity Recognition (NER) tasks from relevant models published in peer review journals. 





The use of these pretrained models significantly reduces  the amount of time a human to spend tagging data and provides additional features and increased consistency from unstructured data for identification of more novel insights. . The objective of this project is to take unstructured data, clean it, and use pretrained Named Entity Recognition (NER) tools to extract signs/symptoms in deidentified electronic health records that can be used to characterize a cohort or make further analysis.","data cleaning of clinical narratives from health records. at scale, clinical narratives are a dataset available to cdc currently underutilized as there is not a structured pipeline to identify structured information output. we are currently working on setting up a process to create a pipeline that cleans data markup (html, rtf, xml) and identifies signs and symptoms using pre-trained named entity recognition (ner) tasks from relevant models published in peer review journals. the use of these pretrained models significantly reduces the amount of time a human to spend tagging data and provides additional features and increased consistency from unstructured data for identification of more novel insights. . the objective of this project is to take unstructured data, clean it, and use pretrained named entity recognition (ner) tools to extract signs/symptoms in deidentified electronic health records that can be used to characterize a cohort or make further analysis."
Autocoding to Support Adverse Drug Event Surveillance,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"The model will help surveillance epidemiologists working at CDC to determine whether reported potential adverse drug events in the National Electronic Injury Surveillance System Cooperative Adverse Drug event Surveillance meet the agency's surveillance case definition for the same.

It will speed up coding of the drug event reports, helping epidemiologists produce prevalence estimates for the surveillance system.","The model will take a de-identified free-text description of the patient's emergency department visit encounter, along with a handful of other pre-coded variables, and output the probability that the encounter meets the case definition.",Initiated,Neither,5/1/2024,Unknown, , , , , , , , , , , , , , ,Unknown, , ,Unknown, , ,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"The model will help surveillance epidemiologists working at CDC to determine whether reported potential adverse drug events in the National Electronic Injury Surveillance System Cooperative Adverse Drug event Surveillance meet the agency's surveillance case definition for the same.

It will speed up coding of the drug event reports, helping epidemiologists produce prevalence estimates for the surveillance system. . The model will take a de-identified free-text description of the patient's emergency department visit encounter, along with a handful of other pre-coded variables, and output the probability that the encounter meets the case definition.","the model will help surveillance epidemiologists working at cdc to determine whether reported potential adverse drug events in the national electronic injury surveillance system cooperative adverse drug event surveillance meet the agency's surveillance case definition for the same. it will speed up coding of the drug event reports, helping epidemiologists produce prevalence estimates for the surveillance system. . the model will take a de-identified free-text description of the patient's emergency department visit encounter, along with a handful of other pre-coded variables, and output the probability that the encounter meets the case definition."
Automating LIMS Bioinformatics Workflow Configuration and Enhancing Lab Quality Management with AI,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"The tool proposed will impact the laboratory bioinformatics workflow configuration within the Core Laboratory's Genomics Sequencing Lab (GSL). Specifically, it will improve the efficacy with Clarity LIMS for tracking and processing Next Generation Sequencing (NGS) samples. This includes the creation and customization of bioinformatics workflows in the Laboratory Information Management System, which is currently a time-consuming and expertise-intensive task for bioinformaticians and laboratory scientists.

The RAG system can potentially reduce the time required to configure Clarity LIMS workflows, enabling rapid deployment which is particularly crucial during pandemics or outbreak responses. By simplifying the workflow customization process through natural language inputs, it lowers the barrier to entry for laboratory scientists and bioinformaticians who may not be familiar with XML scripting. The system also enhances team learning and training by providing easy access to relevant documents on quality management and regulations, ensuring that all team members are well-informed about best practices.","We propose to use a Retrieval-Augmented Generation (RAG) framework for two primary functions: It converts natural language lab protocols into precise XML workflows that are compatible with Clarity LIMS. This conversion process involves understanding technical documentations and generating corresponding script code. In addition, it serves as an interactive knowledge base for documentation of Clarity LIMS and Laboratory Quality Management and Regulations. ",Initiated,Neither,1/1/2024,Unknown, , , , , , , , , , , , , , ,Unknown, , ,Unknown, , ,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"The tool proposed will impact the laboratory bioinformatics workflow configuration within the Core Laboratory's Genomics Sequencing Lab (GSL). Specifically, it will improve the efficacy with Clarity LIMS for tracking and processing Next Generation Sequencing (NGS) samples. This includes the creation and customization of bioinformatics workflows in the Laboratory Information Management System, which is currently a time-consuming and expertise-intensive task for bioinformaticians and laboratory scientists.

The RAG system can potentially reduce the time required to configure Clarity LIMS workflows, enabling rapid deployment which is particularly crucial during pandemics or outbreak responses. By simplifying the workflow customization process through natural language inputs, it lowers the barrier to entry for laboratory scientists and bioinformaticians who may not be familiar with XML scripting. The system also enhances team learning and training by providing easy access to relevant documents on quality management and regulations, ensuring that all team members are well-informed about best practices. . We propose to use a Retrieval-Augmented Generation (RAG) framework for two primary functions: It converts natural language lab protocols into precise XML workflows that are compatible with Clarity LIMS. This conversion process involves understanding technical documentations and generating corresponding script code. In addition, it serves as an interactive knowledge base for documentation of Clarity LIMS and Laboratory Quality Management and Regulations.","the tool proposed will impact the laboratory bioinformatics workflow configuration within the core laboratory's genomics sequencing lab (gsl). specifically, it will improve the efficacy with clarity lims for tracking and processing next generation sequencing (ngs) samples. this includes the creation and customization of bioinformatics workflows in the laboratory information management system, which is currently a time-consuming and expertise-intensive task for bioinformaticians and laboratory scientists. the rag system can potentially reduce the time required to configure clarity lims workflows, enabling rapid deployment which is particularly crucial during pandemics or outbreak responses. by simplifying the workflow customization process through natural language inputs, it lowers the barrier to entry for laboratory scientists and bioinformaticians who may not be familiar with xml scripting. the system also enhances team learning and training by providing easy access to relevant documents on quality management and regulations, ensuring that all team members are well-informed about best practices. . we propose to use a retrieval-augmented generation (rag) framework for two primary functions: it converts natural language lab protocols into precise xml workflows that are compatible with clarity lims. this conversion process involves understanding technical documentations and generating corresponding script code. in addition, it serves as an interactive knowledge base for documentation of clarity lims and laboratory quality management and regulations."
DGMH AI Chatbot,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"The DGMH AI chatbot would assist with developing an initial draft response to  inquiries where the response could have been found on our website. This would allow DGMH staff to have a starting point on a response to edit and clear. Additionally, the response could be adjusted for plain language to assist in ensuring the inquirer understands the response being sent back to them. The AI chatbot would allow inquiries to be responded to quicker and would allow staff to focus on other high priority items as well.   

The project will rely on using Retrieval-Augmentation Generation (RAG) to identify relevant information from CDC’s relevant website pages to generate relevant responses using AI. Evaluation of the project will assess how accurate the response was, how complete it was, and how much revision was needed. Evaluation would also explore the answer and if the answer that was generated was consistent throughout time if multiple people asked similar questions.   The goal is consistency of content rather than tone/plain language. The chatbot will help to reduce the turnaround time for responding to inquiries and free up time for staff to address other priorities.","CDC's Division of Global Migration Health receives inquires for information which is available publicly in some form but may be hard to find and requires staff responding to these inquiries to have a broad knowledge and can be time intensive to respond to. The AI Chatbot will draft an initial response to questions using content available on CDC’s public-facing webpages. Each response goes through the existing CDC clearance process regardless of if AI is used, but the Chatbot helps provide an initial draft based on previously published content. The goal is to help with responding to different inquiries the division receives, including but not limited to CDC-INFO and media. ",Initiated,Neither,10/1/2024,Unknown, , , , , , , , , , , , , , ,Unknown, , ,Unknown, , ,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"The DGMH AI chatbot would assist with developing an initial draft response to  inquiries where the response could have been found on our website. This would allow DGMH staff to have a starting point on a response to edit and clear. Additionally, the response could be adjusted for plain language to assist in ensuring the inquirer understands the response being sent back to them. The AI chatbot would allow inquiries to be responded to quicker and would allow staff to focus on other high priority items as well.   

The project will rely on using Retrieval-Augmentation Generation (RAG) to identify relevant information from CDC’s relevant website pages to generate relevant responses using AI. Evaluation of the project will assess how accurate the response was, how complete it was, and how much revision was needed. Evaluation would also explore the answer and if the answer that was generated was consistent throughout time if multiple people asked similar questions.   The goal is consistency of content rather than tone/plain language. The chatbot will help to reduce the turnaround time for responding to inquiries and free up time for staff to address other priorities. . CDC's Division of Global Migration Health receives inquires for information which is available publicly in some form but may be hard to find and requires staff responding to these inquiries to have a broad knowledge and can be time intensive to respond to. The AI Chatbot will draft an initial response to questions using content available on CDC’s public-facing webpages. Each response goes through the existing CDC clearance process regardless of if AI is used, but the Chatbot helps provide an initial draft based on previously published content. The goal is to help with responding to different inquiries the division receives, including but not limited to CDC-INFO and media.","the dgmh ai chatbot would assist with developing an initial draft response to inquiries where the response could have been found on our website. this would allow dgmh staff to have a starting point on a response to edit and clear. additionally, the response could be adjusted for plain language to assist in ensuring the inquirer understands the response being sent back to them. the ai chatbot would allow inquiries to be responded to quicker and would allow staff to focus on other high priority items as well. the project will rely on using retrieval-augmentation generation (rag) to identify relevant information from cdc’s relevant website pages to generate relevant responses using ai. evaluation of the project will assess how accurate the response was, how complete it was, and how much revision was needed. evaluation would also explore the answer and if the answer that was generated was consistent throughout time if multiple people asked similar questions. the goal is consistency of content rather than tone/plain language. the chatbot will help to reduce the turnaround time for responding to inquiries and free up time for staff to address other priorities. . cdc's division of global migration health receives inquires for information which is available publicly in some form but may be hard to find and requires staff responding to these inquiries to have a broad knowledge and can be time intensive to respond to. the ai chatbot will draft an initial response to questions using content available on cdc’s public-facing webpages. each response goes through the existing cdc clearance process regardless of if ai is used, but the chatbot helps provide an initial draft based on previously published content. the goal is to help with responding to different inquiries the division receives, including but not limited to cdc-info and media."
Distiller SR: AI to screen research articles for Community Guide reviews,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"This tool will be used to screen research articles that will be used to inform Community Preventive Services Task Force recommendations (a body that produces guidelines independent of CDC). 

Use of this technology may increase the speed with which systematic reviews can be conducted. This may expedite the time it takes to evaluate the effectiveness of public health programs for a CPSTF recommendation (not a CDC guideline). ",Machine learning will be used to more efficiently screen research articles relevant to evaluating the effectiveness of an intervention. ,Initiated,Neither,5/1/2023,Unknown, , , , , , , , , , , , , , ,Unknown, , ,Unknown, , ,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"This tool will be used to screen research articles that will be used to inform Community Preventive Services Task Force recommendations (a body that produces guidelines independent of CDC). 

Use of this technology may increase the speed with which systematic reviews can be conducted. This may expedite the time it takes to evaluate the effectiveness of public health programs for a CPSTF recommendation (not a CDC guideline). . Machine learning will be used to more efficiently screen research articles relevant to evaluating the effectiveness of an intervention.",this tool will be used to screen research articles that will be used to inform community preventive services task force recommendations (a body that produces guidelines independent of cdc). use of this technology may increase the speed with which systematic reviews can be conducted. this may expedite the time it takes to evaluate the effectiveness of public health programs for a cpstf recommendation (not a cdc guideline). . machine learning will be used to more efficiently screen research articles relevant to evaluating the effectiveness of an intervention.
Evaluating Generative AI for polio containment.,Department of Health and Human Services,HHS,CDC,Health & Medical,None of the above.,"The AI will impact the data retrieval and automation process for HIV researchers, specifically in providing information from datasets and potentially generating code for desired variable names and analysis or generating information about a particular dataset.

As polio eradication progresses, containment of poliovirus becomes critical. This project aims to identify and evaluate Generative AI tools to extract information from publications in an effort to identify poliovirus in places previously thought to be polio-free.  Identification of such a tool may diversify Polio and Picornavirus Branch's surveillance portfolio and increase Branch's capability of processing large volume of data for containment purposes. This project may support CDC PPLB’s polio surveillance activity and containment efforts.","This AI assistant will use retrieval augmented generation (RAG) to return information related to HIV based on a user's query. Our aim is to input all dataset documentation and metadata into an index with the goal of creating a virtual assistant. This assistant offers support to our HIV researchers by providing the information they seek from the data and potentially generating SAS, R, or Python code for desired variable names. For instance, if we require a dataset containing HIV PrEP information, we can request it to generate a list of current datasets that contain data on PrEP medication usage in 2019, along with the associated variable names. Subsequently, we can instruct it to produce lines of code, such as a logistic regression, which can be readily copied and pasted into our analysis software of choice. Another example would be if our researchers would like information returned about a given dataset, this assistant could return specific information based on the researcher's query.",Initiated,Neither,11/1/2023,Unknown, , , , , , , , , , , , , , ,Unknown, , ,Unknown, , ,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"The AI will impact the data retrieval and automation process for HIV researchers, specifically in providing information from datasets and potentially generating code for desired variable names and analysis or generating information about a particular dataset.

As polio eradication progresses, containment of poliovirus becomes critical. This project aims to identify and evaluate Generative AI tools to extract information from publications in an effort to identify poliovirus in places previously thought to be polio-free.  Identification of such a tool may diversify Polio and Picornavirus Branch's surveillance portfolio and increase Branch's capability of processing large volume of data for containment purposes. This project may support CDC PPLB’s polio surveillance activity and containment efforts. . This AI assistant will use retrieval augmented generation (RAG) to return information related to HIV based on a user's query. Our aim is to input all dataset documentation and metadata into an index with the goal of creating a virtual assistant. This assistant offers support to our HIV researchers by providing the information they seek from the data and potentially generating SAS, R, or Python code for desired variable names. For instance, if we require a dataset containing HIV PrEP information, we can request it to generate a list of current datasets that contain data on PrEP medication usage in 2019, along with the associated variable names. Subsequently, we can instruct it to produce lines of code, such as a logistic regression, which can be readily copied and pasted into our analysis software of choice. Another example would be if our researchers would like information returned about a given dataset, this assistant could return specific information based on the researcher's query.","the ai will impact the data retrieval and automation process for hiv researchers, specifically in providing information from datasets and potentially generating code for desired variable names and analysis or generating information about a particular dataset. as polio eradication progresses, containment of poliovirus becomes critical. this project aims to identify and evaluate generative ai tools to extract information from publications in an effort to identify poliovirus in places previously thought to be polio-free. identification of such a tool may diversify polio and picornavirus branch's surveillance portfolio and increase branch's capability of processing large volume of data for containment purposes. this project may support cdc pplb’s polio surveillance activity and containment efforts. . this ai assistant will use retrieval augmented generation (rag) to return information related to hiv based on a user's query. our aim is to input all dataset documentation and metadata into an index with the goal of creating a virtual assistant. this assistant offers support to our hiv researchers by providing the information they seek from the data and potentially generating sas, r, or python code for desired variable names. for instance, if we require a dataset containing hiv prep information, we can request it to generate a list of current datasets that contain data on prep medication usage in 2019, along with the associated variable names. subsequently, we can instruct it to produce lines of code, such as a logistic regression, which can be readily copied and pasted into our analysis software of choice. another example would be if our researchers would like information returned about a given dataset, this assistant could return specific information based on the researcher's query."
Respiratory Virus Response (RVR) Data Analysis Concept,Department of Health and Human Services,HHS,CDC,Health & Medical,None of the above.,"This solution will impact the process of summarizing subject matter expert (SME) interpretations/knowledge during a response. 

Response efforts require prompt turnaround of essential information to the public. This project will impact public health by improving the efficiency of dissemination of information through production of key takeaways which can then be quickly reviewed by SMEs and cleared. This project will also allow for improved understanding of limitations and uncertainties in use of AI methods that could contribute to improved implementation of such methods in public health.","The goal of this solution is to evaluate how AI can be implemented in the process of summarizing subject matter expert (SME) interpretations/knowledge to improve efficiency in knowledge dissemination during a response.  As part of the proof-of-concept, we  seek to evaluate the limitations and uncertainties of AI methods in this use-case, such as bias and hallucinations as such issues will require mitigation before this method can be broadly adopted. Our proposed approach would make use of the ChatGPT interface through EDAV (or alternative accepted platform) as our prompts will include uncleared data from the Respiratory Virus Response (RVR). In this response,  SMEs from different groups provide bulleted information that then needs to be further summarized to address key takeaways.   Bullets from prior weeks will prompt AI to generate summaries. The initial approach will focus on engineering prompts to get key takeaways while future iterations may require fine-tuning to allow for better contextualization of responses as well as improvements in tone and style. ",Initiated,Neither,6/1/2024,Unknown, , , , , , , , , , , , , , ,Unknown, , ,Unknown, , ,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"This solution will impact the process of summarizing subject matter expert (SME) interpretations/knowledge during a response. 

Response efforts require prompt turnaround of essential information to the public. This project will impact public health by improving the efficiency of dissemination of information through production of key takeaways which can then be quickly reviewed by SMEs and cleared. This project will also allow for improved understanding of limitations and uncertainties in use of AI methods that could contribute to improved implementation of such methods in public health. . The goal of this solution is to evaluate how AI can be implemented in the process of summarizing subject matter expert (SME) interpretations/knowledge to improve efficiency in knowledge dissemination during a response.  As part of the proof-of-concept, we  seek to evaluate the limitations and uncertainties of AI methods in this use-case, such as bias and hallucinations as such issues will require mitigation before this method can be broadly adopted. Our proposed approach would make use of the ChatGPT interface through EDAV (or alternative accepted platform) as our prompts will include uncleared data from the Respiratory Virus Response (RVR). In this response,  SMEs from different groups provide bulleted information that then needs to be further summarized to address key takeaways.   Bullets from prior weeks will prompt AI to generate summaries. The initial approach will focus on engineering prompts to get key takeaways while future iterations may require fine-tuning to allow for better contextualization of responses as well as improvements in tone and style.","this solution will impact the process of summarizing subject matter expert (sme) interpretations/knowledge during a response. response efforts require prompt turnaround of essential information to the public. this project will impact public health by improving the efficiency of dissemination of information through production of key takeaways which can then be quickly reviewed by smes and cleared. this project will also allow for improved understanding of limitations and uncertainties in use of ai methods that could contribute to improved implementation of such methods in public health. . the goal of this solution is to evaluate how ai can be implemented in the process of summarizing subject matter expert (sme) interpretations/knowledge to improve efficiency in knowledge dissemination during a response. as part of the proof-of-concept, we seek to evaluate the limitations and uncertainties of ai methods in this use-case, such as bias and hallucinations as such issues will require mitigation before this method can be broadly adopted. our proposed approach would make use of the chatgpt interface through edav (or alternative accepted platform) as our prompts will include uncleared data from the respiratory virus response (rvr). in this response, smes from different groups provide bulleted information that then needs to be further summarized to address key takeaways. bullets from prior weeks will prompt ai to generate summaries. the initial approach will focus on engineering prompts to get key takeaways while future iterations may require fine-tuning to allow for better contextualization of responses as well as improvements in tone and style."
School LLM initial abstract review process ,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"Reviewing research related to school readiness science. CDC/DRRS' School Preparedness unit is responsible for developing research around readiness science. As part of this a large language model was used to abstract themes and code publications that were identified as related to school closure to aid us in supporting our research. If funding were to become available, we would like to develop this static, one-time product into a modifiable tool that could be used for ongoing article abstraction across school and other settings and population.

The AI allows us to efficiently categorize thousands of abstracts in a much shorter time-frame with much less human time required into a user-friendly dashboard. ",The AI used a LLM to extract data from an abstract review and categorize relevant themes and topics into a user-friendly dashboard. This dashboard can be used by a health scientist to pull resources from 2012-2022   for specific school closure outcomes or themes.,Initiated,Neither,8/1/2023,Unknown, , , , , , , , , , , , , , ,Unknown, , ,Unknown, , ,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"Reviewing research related to school readiness science. CDC/DRRS' School Preparedness unit is responsible for developing research around readiness science. As part of this a large language model was used to abstract themes and code publications that were identified as related to school closure to aid us in supporting our research. If funding were to become available, we would like to develop this static, one-time product into a modifiable tool that could be used for ongoing article abstraction across school and other settings and population.

The AI allows us to efficiently categorize thousands of abstracts in a much shorter time-frame with much less human time required into a user-friendly dashboard. . The AI used a LLM to extract data from an abstract review and categorize relevant themes and topics into a user-friendly dashboard. This dashboard can be used by a health scientist to pull resources from 2012-2022   for specific school closure outcomes or themes.","reviewing research related to school readiness science. cdc/drrs' school preparedness unit is responsible for developing research around readiness science. as part of this a large language model was used to abstract themes and code publications that were identified as related to school closure to aid us in supporting our research. if funding were to become available, we would like to develop this static, one-time product into a modifiable tool that could be used for ongoing article abstraction across school and other settings and population. the ai allows us to efficiently categorize thousands of abstracts in a much shorter time-frame with much less human time required into a user-friendly dashboard. . the ai used a llm to extract data from an abstract review and categorize relevant themes and topics into a user-friendly dashboard. this dashboard can be used by a health scientist to pull resources from 2012-2022 for specific school closure outcomes or themes."
ChatCDC - CDC Enterprise Generative AI Chatbot (Content Editing),Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"CDC has an internal productivity tool available to all CDC staff which can be used in a variety of ways. This use case covers staff who will be using this general purpose to assist with editing text previously written, or content editing. This may refer to using the chatbot to help refine emails or messages to other CDC staff, reviewing internal documents, reviewing scheduling or other bureaucratic information and other such information. CDC sometimes uses GenAI (generative artificial intelligence) to help create content for the public. This may include content for the web, social media, fact sheets, and graphics. In every case, our experts review our communication materials in an effort to ensure accuracy and quality before use. CDC public health research, data, and recommendations are based on the best science currently available and go through a separate agency review process prior to release. Currently, CDC has internal Generative AI guidance determining how this tool can be used, and individual Centers, institutes, or Offices may implement their own policy to ensure safe and responsible AI. This includes internal guidance on using AI for External Communications.

This tool enables CDC staff to have the productivity benefits of Generative AI powered chatbots without any of the security risks of data uploads. This tool can be used by all CDC staff to support general business operations and enables a quick review for finding any spelling or grammatical errors, fixing formatting, or other copy-editing type corrections and saves staff time will increasing the quality of communications. CDC sometimes uses GenAI (generative artificial intelligence) to help create content for the public. This may include content for the web, social media, fact sheets, and graphics. In every case, our experts review our communication materials in an effort to ensure accuracy and quality before use. CDC public health research, data, and recommendations are based on the best science currently available and go through a separate agency review process prior to release.","This chatbot, powered by Azure OpenAI Large Language Models, will generate responses to questions asked by the user. In this case, responses will be recommendations on edits for content provided by the user such as a draft email. The chatbot's output can then be copied and pasted or manually entered based upon the users review. CDC sometimes uses GenAI (generative artificial intelligence) to help create content for the public. This may include content for the web, social media, fact sheets, and graphics. In every case, our experts review our communication materials in an effort to ensure accuracy and quality before use. CDC public health research, data, and recommendations are based on the best science currently available and go through a separate agency review process prior to release.",Operation and Maintenance,Neither,10/1/2023,11/1/2023,2/1/2024,Developed with both contracting and in-house resources.,Unknown, ,No,No,No,No,Yes. A series of pilot projects was conducted to evaluate the performance of this chatbot and these pilot projects are included within this inventory.,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Enterprise Data Analytics and Visualization (EDAV) Platform,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"CDC has an internal productivity tool available to all CDC staff which can be used in a variety of ways. This use case covers staff who will be using this general purpose to assist with editing text previously written, or content editing. This may refer to using the chatbot to help refine emails or messages to other CDC staff, reviewing internal documents, reviewing scheduling or other bureaucratic information and other such information. CDC sometimes uses GenAI (generative artificial intelligence) to help create content for the public. This may include content for the web, social media, fact sheets, and graphics. In every case, our experts review our communication materials in an effort to ensure accuracy and quality before use. CDC public health research, data, and recommendations are based on the best science currently available and go through a separate agency review process prior to release. Currently, CDC has internal Generative AI guidance determining how this tool can be used, and individual Centers, institutes, or Offices may implement their own policy to ensure safe and responsible AI. This includes internal guidance on using AI for External Communications.

This tool enables CDC staff to have the productivity benefits of Generative AI powered chatbots without any of the security risks of data uploads. This tool can be used by all CDC staff to support general business operations and enables a quick review for finding any spelling or grammatical errors, fixing formatting, or other copy-editing type corrections and saves staff time will increasing the quality of communications. CDC sometimes uses GenAI (generative artificial intelligence) to help create content for the public. This may include content for the web, social media, fact sheets, and graphics. In every case, our experts review our communication materials in an effort to ensure accuracy and quality before use. CDC public health research, data, and recommendations are based on the best science currently available and go through a separate agency review process prior to release. . This chatbot, powered by Azure OpenAI Large Language Models, will generate responses to questions asked by the user. In this case, responses will be recommendations on edits for content provided by the user such as a draft email. The chatbot's output can then be copied and pasted or manually entered based upon the users review. CDC sometimes uses GenAI (generative artificial intelligence) to help create content for the public. This may include content for the web, social media, fact sheets, and graphics. In every case, our experts review our communication materials in an effort to ensure accuracy and quality before use. CDC public health research, data, and recommendations are based on the best science currently available and go through a separate agency review process prior to release.","cdc has an internal productivity tool available to all cdc staff which can be used in a variety of ways. this use case covers staff who will be using this general purpose to assist with editing text previously written, or content editing. this may refer to using the chatbot to help refine emails or messages to other cdc staff, reviewing internal documents, reviewing scheduling or other bureaucratic information and other such information. cdc sometimes uses genai (generative artificial intelligence) to help create content for the public. this may include content for the web, social media, fact sheets, and graphics. in every case, our experts review our communication materials in an effort to ensure accuracy and quality before use. cdc public health research, data, and recommendations are based on the best science currently available and go through a separate agency review process prior to release. currently, cdc has internal generative ai guidance determining how this tool can be used, and individual centers, institutes, or offices may implement their own policy to ensure safe and responsible ai. this includes internal guidance on using ai for external communications. this tool enables cdc staff to have the productivity benefits of generative ai powered chatbots without any of the security risks of data uploads. this tool can be used by all cdc staff to support general business operations and enables a quick review for finding any spelling or grammatical errors, fixing formatting, or other copy-editing type corrections and saves staff time will increasing the quality of communications. cdc sometimes uses genai (generative artificial intelligence) to help create content for the public. this may include content for the web, social media, fact sheets, and graphics. in every case, our experts review our communication materials in an effort to ensure accuracy and quality before use. cdc public health research, data, and recommendations are based on the best science currently available and go through a separate agency review process prior to release. . this chatbot, powered by azure openai large language models, will generate responses to questions asked by the user. in this case, responses will be recommendations on edits for content provided by the user such as a draft email. the chatbot's output can then be copied and pasted or manually entered based upon the users review. cdc sometimes uses genai (generative artificial intelligence) to help create content for the public. this may include content for the web, social media, fact sheets, and graphics. in every case, our experts review our communication materials in an effort to ensure accuracy and quality before use. cdc public health research, data, and recommendations are based on the best science currently available and go through a separate agency review process prior to release."
ChatCDC - CDC Enterprise Generative AI Chatbot (Data Extraction),Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"CDC has an internal productivity tool available to all CDC staff which can be used in a variety of ways. This use case covers staff who will be using this general purpose tool to assist with extracting information from larger pieces of text. Currently, CDC has internal Generative AI guidance determining how this tool can be used, and individual Centers, institutes, or Offices may implement their own policy to ensure safe and responsible AI. 

From larger documents which may be time consuming to find specific pieces of information, this AI system enables staff to extract key pieces of information to be used in later tasks. This can include action items from meeting transcripts, specific discussion focuses from larger meetings, specific mentions, or other such information. this makes the finding of information from larger document far faster and reduces the manual burden of reading larger documents for one or two specific pieces of information.","This chatbot, powered by Azure OpenAI Large Language Models, will generate responses to questions asked by the user, in this case about some larger piece of text and any specific pieces of information to extract. The chatbot's output can then be copied and pasted or manually entered into the appropriate testing process by the staff to ensure any code used from the chatbot's output meets the criteria of the current task.",Operation and Maintenance,Neither,10/1/2023,11/1/2023,2/1/2024,Developed with both contracting and in-house resources.,Unknown, ,No,No,No,No,Yes. A series of pilot projects was conducted to evaluate the performance of this chatbot and these pilot projects are included within this inventory.,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Enterprise Data Analytics and Visualization (EDAV) Platform,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"CDC has an internal productivity tool available to all CDC staff which can be used in a variety of ways. This use case covers staff who will be using this general purpose tool to assist with extracting information from larger pieces of text. Currently, CDC has internal Generative AI guidance determining how this tool can be used, and individual Centers, institutes, or Offices may implement their own policy to ensure safe and responsible AI. 

From larger documents which may be time consuming to find specific pieces of information, this AI system enables staff to extract key pieces of information to be used in later tasks. This can include action items from meeting transcripts, specific discussion focuses from larger meetings, specific mentions, or other such information. this makes the finding of information from larger document far faster and reduces the manual burden of reading larger documents for one or two specific pieces of information. . This chatbot, powered by Azure OpenAI Large Language Models, will generate responses to questions asked by the user, in this case about some larger piece of text and any specific pieces of information to extract. The chatbot's output can then be copied and pasted or manually entered into the appropriate testing process by the staff to ensure any code used from the chatbot's output meets the criteria of the current task.","cdc has an internal productivity tool available to all cdc staff which can be used in a variety of ways. this use case covers staff who will be using this general purpose tool to assist with extracting information from larger pieces of text. currently, cdc has internal generative ai guidance determining how this tool can be used, and individual centers, institutes, or offices may implement their own policy to ensure safe and responsible ai. from larger documents which may be time consuming to find specific pieces of information, this ai system enables staff to extract key pieces of information to be used in later tasks. this can include action items from meeting transcripts, specific discussion focuses from larger meetings, specific mentions, or other such information. this makes the finding of information from larger document far faster and reduces the manual burden of reading larger documents for one or two specific pieces of information. . this chatbot, powered by azure openai large language models, will generate responses to questions asked by the user, in this case about some larger piece of text and any specific pieces of information to extract. the chatbot's output can then be copied and pasted or manually entered into the appropriate testing process by the staff to ensure any code used from the chatbot's output meets the criteria of the current task."
ChatCDC - CDC Enterprise Generative AI Chatbot (Ideation),Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"CDC has an internal productivity tool available to all CDC staff which can be used in a variety of ways. This use case covers staff who will be using this general purpose to assist with the development of ideas or to overcome writers block in different tasks. When doing a task, staff can ask this tool to ask questions to help evolve any ideas being developed. Currently, CDC has internal Generative AI guidance determining how this tool can be used, and individual Centers, institutes, or Offices may implement their own policy to ensure safe and responsible AI. 

This tool enables CDC staff to have the productivity benefits of Generative AI powered chatbots without any of the security risks of 3rd party AI powered general purpose chatbots. This ensures staff can provide information regarding a current task along with general information with current tasks to help organize thinking, actions to take, and developing ideas for staff to use or reject as a part of their process. This reduces the time spent by staff dealing with situations such as writers block.","This chatbot, powered by Azure OpenAI Large Language Models, will generate responses to questions asked by the user. The chatbot's output can then be copied and pasted or manually entered into the appropriate testing process by the staff to ensure any code used from the chatbot's output meets the criteria of the current task. The output must be copied and pasted or manually entered somewhere for the output to be used by staff.",Operation and Maintenance,Neither,10/1/2023,11/1/2023,2/1/2024,Developed with both contracting and in-house resources.,Unknown, ,No,No,No,No,Yes. A series of pilot projects was conducted to evaluate the performance of this chatbot and these pilot projects are included within this inventory.,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Enterprise Data Analytics and Visualization (EDAV) Platform,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"CDC has an internal productivity tool available to all CDC staff which can be used in a variety of ways. This use case covers staff who will be using this general purpose to assist with the development of ideas or to overcome writers block in different tasks. When doing a task, staff can ask this tool to ask questions to help evolve any ideas being developed. Currently, CDC has internal Generative AI guidance determining how this tool can be used, and individual Centers, institutes, or Offices may implement their own policy to ensure safe and responsible AI. 

This tool enables CDC staff to have the productivity benefits of Generative AI powered chatbots without any of the security risks of 3rd party AI powered general purpose chatbots. This ensures staff can provide information regarding a current task along with general information with current tasks to help organize thinking, actions to take, and developing ideas for staff to use or reject as a part of their process. This reduces the time spent by staff dealing with situations such as writers block. . This chatbot, powered by Azure OpenAI Large Language Models, will generate responses to questions asked by the user. The chatbot's output can then be copied and pasted or manually entered into the appropriate testing process by the staff to ensure any code used from the chatbot's output meets the criteria of the current task. The output must be copied and pasted or manually entered somewhere for the output to be used by staff.","cdc has an internal productivity tool available to all cdc staff which can be used in a variety of ways. this use case covers staff who will be using this general purpose to assist with the development of ideas or to overcome writers block in different tasks. when doing a task, staff can ask this tool to ask questions to help evolve any ideas being developed. currently, cdc has internal generative ai guidance determining how this tool can be used, and individual centers, institutes, or offices may implement their own policy to ensure safe and responsible ai. this tool enables cdc staff to have the productivity benefits of generative ai powered chatbots without any of the security risks of 3rd party ai powered general purpose chatbots. this ensures staff can provide information regarding a current task along with general information with current tasks to help organize thinking, actions to take, and developing ideas for staff to use or reject as a part of their process. this reduces the time spent by staff dealing with situations such as writers block. . this chatbot, powered by azure openai large language models, will generate responses to questions asked by the user. the chatbot's output can then be copied and pasted or manually entered into the appropriate testing process by the staff to ensure any code used from the chatbot's output meets the criteria of the current task. the output must be copied and pasted or manually entered somewhere for the output to be used by staff."
ChatCDC - CDC Enterprise Generative AI Chatbot (Software Development),Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"CDC has an internal productivity tool available to all CDC staff which can be used in a variety of ways. This use case covers staff who will be using this general purpose to assist with the development of software code which can be used for research, data analysis, software development, or other such systems. Currently, CDC has internal Generative AI guidance determining how this tool can be used, and individual Centers, institutes, or Offices may implement their own policy to ensure safe and responsible AI. 

When the AI chatbot is being used for coding assistance or software development, it affects several business processes related to agency operations and core task structures. Specifically, it can be utilized by various personnel within an organization such as the CDC, including software programmers using AI-specific code, those involved in encoding for software development, data analysts, scientific researchers conducting data analysis, data scientists implementing and testing new models, and software engineers developing or updating applications.
The chatbot enhances productivity and efficiency across these roles by providing coding assistance and acting as a learning tool. For instance, data engineers may use the chatbot to generate PySpark code for their data transformation scripts within a larger pipeline. Additionally, individuals new to programming languages like Python can leverage the chatbot as a tutor to help them with tasks such as cleaning up datasets for evaluation purposes.
In summary, the AI chatbot impacts business processes by aiding staff members in performing complex analytical tasks more efficiently and supporting their development work through guided coding assistance. This ultimately contributes to enhancing overall operations within the agency by streamlining research activities and internal process workflows.","This chatbot, powered by Azure OpenAI Large Language Models, will generate responses to questions asked by the user, in this case about either some software code, software code documentation, code concepts, or to help generate software code. The chatbot's output can then be copied and pasted or manually entered into the appropriate testing process by the staff to ensure any code used from the chatbot's output meets the criteria of the current task.",Operation and Maintenance,Neither,10/1/2023,11/1/2023,2/1/2024,Developed with both contracting and in-house resources.,Unknown, ,No,No,No,No,Yes. A series of pilot projects was conducted to evaluate the performance of this chatbot and these pilot projects are included within this inventory.,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Enterprise Data Analytics and Visualization (EDAV) Platform,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"CDC has an internal productivity tool available to all CDC staff which can be used in a variety of ways. This use case covers staff who will be using this general purpose to assist with the development of software code which can be used for research, data analysis, software development, or other such systems. Currently, CDC has internal Generative AI guidance determining how this tool can be used, and individual Centers, institutes, or Offices may implement their own policy to ensure safe and responsible AI. 

When the AI chatbot is being used for coding assistance or software development, it affects several business processes related to agency operations and core task structures. Specifically, it can be utilized by various personnel within an organization such as the CDC, including software programmers using AI-specific code, those involved in encoding for software development, data analysts, scientific researchers conducting data analysis, data scientists implementing and testing new models, and software engineers developing or updating applications.
The chatbot enhances productivity and efficiency across these roles by providing coding assistance and acting as a learning tool. For instance, data engineers may use the chatbot to generate PySpark code for their data transformation scripts within a larger pipeline. Additionally, individuals new to programming languages like Python can leverage the chatbot as a tutor to help them with tasks such as cleaning up datasets for evaluation purposes.
In summary, the AI chatbot impacts business processes by aiding staff members in performing complex analytical tasks more efficiently and supporting their development work through guided coding assistance. This ultimately contributes to enhancing overall operations within the agency by streamlining research activities and internal process workflows. . This chatbot, powered by Azure OpenAI Large Language Models, will generate responses to questions asked by the user, in this case about either some software code, software code documentation, code concepts, or to help generate software code. The chatbot's output can then be copied and pasted or manually entered into the appropriate testing process by the staff to ensure any code used from the chatbot's output meets the criteria of the current task.","cdc has an internal productivity tool available to all cdc staff which can be used in a variety of ways. this use case covers staff who will be using this general purpose to assist with the development of software code which can be used for research, data analysis, software development, or other such systems. currently, cdc has internal generative ai guidance determining how this tool can be used, and individual centers, institutes, or offices may implement their own policy to ensure safe and responsible ai. when the ai chatbot is being used for coding assistance or software development, it affects several business processes related to agency operations and core task structures. specifically, it can be utilized by various personnel within an organization such as the cdc, including software programmers using ai-specific code, those involved in encoding for software development, data analysts, scientific researchers conducting data analysis, data scientists implementing and testing new models, and software engineers developing or updating applications. the chatbot enhances productivity and efficiency across these roles by providing coding assistance and acting as a learning tool. for instance, data engineers may use the chatbot to generate pyspark code for their data transformation scripts within a larger pipeline. additionally, individuals new to programming languages like python can leverage the chatbot as a tutor to help them with tasks such as cleaning up datasets for evaluation purposes. in summary, the ai chatbot impacts business processes by aiding staff members in performing complex analytical tasks more efficiently and supporting their development work through guided coding assistance. this ultimately contributes to enhancing overall operations within the agency by streamlining research activities and internal process workflows. . this chatbot, powered by azure openai large language models, will generate responses to questions asked by the user, in this case about either some software code, software code documentation, code concepts, or to help generate software code. the chatbot's output can then be copied and pasted or manually entered into the appropriate testing process by the staff to ensure any code used from the chatbot's output meets the criteria of the current task."
ChatCDC - CDC Enterprise Generative AI Chatbot (Summarization),Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"CDC has an internal productivity tool available to all CDC staff which can be used in a variety of ways. This use case covers staff who will be using this general purpose tool to summarize and synthesize information from larger documents such as meeting transcripts, research articles, or other documents. Currently, CDC has internal Generative AI guidance determining how this tool can be used, and individual Centers, institutes, or Offices may implement their own policy to ensure safe and responsible AI. 

This tool enables CDC staff to have the productivity benefits of Generative AI powered chatbots without any of the security risks of data uploads to third party tools. Using ChatCDC, Staff can have smaller, more easily digestible summaries to save staff time from reading larger documents if the time to read the full document is not available.","This chatbot, powered by Azure OpenAI Large Language Models, will generate responses to questions asked by the user, in this case a summary of a larger document. The chatbot's output can then be copied and pasted or manually entered into the appropriate testing process by the staff to ensure any code used from the chatbot's output meets the criteria of the current task.",Operation and Maintenance,Neither,10/1/2023,11/1/2023,2/1/2024,Developed with both contracting and in-house resources.,Unknown, ,No,No,No,No,Yes. A series of pilot projects was conducted to evaluate the performance of this chatbot and these pilot projects are included within this inventory.,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Enterprise Data Analytics and Visualization (EDAV) Platform,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"CDC has an internal productivity tool available to all CDC staff which can be used in a variety of ways. This use case covers staff who will be using this general purpose tool to summarize and synthesize information from larger documents such as meeting transcripts, research articles, or other documents. Currently, CDC has internal Generative AI guidance determining how this tool can be used, and individual Centers, institutes, or Offices may implement their own policy to ensure safe and responsible AI. 

This tool enables CDC staff to have the productivity benefits of Generative AI powered chatbots without any of the security risks of data uploads to third party tools. Using ChatCDC, Staff can have smaller, more easily digestible summaries to save staff time from reading larger documents if the time to read the full document is not available. . This chatbot, powered by Azure OpenAI Large Language Models, will generate responses to questions asked by the user, in this case a summary of a larger document. The chatbot's output can then be copied and pasted or manually entered into the appropriate testing process by the staff to ensure any code used from the chatbot's output meets the criteria of the current task.","cdc has an internal productivity tool available to all cdc staff which can be used in a variety of ways. this use case covers staff who will be using this general purpose tool to summarize and synthesize information from larger documents such as meeting transcripts, research articles, or other documents. currently, cdc has internal generative ai guidance determining how this tool can be used, and individual centers, institutes, or offices may implement their own policy to ensure safe and responsible ai. this tool enables cdc staff to have the productivity benefits of generative ai powered chatbots without any of the security risks of data uploads to third party tools. using chatcdc, staff can have smaller, more easily digestible summaries to save staff time from reading larger documents if the time to read the full document is not available. . this chatbot, powered by azure openai large language models, will generate responses to questions asked by the user, in this case a summary of a larger document. the chatbot's output can then be copied and pasted or manually entered into the appropriate testing process by the staff to ensure any code used from the chatbot's output meets the criteria of the current task."
DCIPHER (SEDRIC) AIP for Advanced Foodborne Outbreak Investigation (AI Summarization),Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"This AI solution impacts the process of investigating foodborne disease outbreaks. These foodborne outbreaks require cooperative efforts from CDC staff, FDA, USDA, and local agencies and the AI system is used through a centralized data platform System for Enteric Disease Response, Investigation, and Coordination (also known as SEDRIC). For more information on SEDRIC, please go to our website: https://www.cdc.gov/foodborne-outbreaks/php/foodsafety/tools/index.html

SEDRIC's AIP use case provides CDC epidemiologists the ability to accelerate their investigations of multi-state foodborne disease outbreaks by more effectively leveraging data available in a rich data source, such as receipts from grocery stores, that otherwise requires extensive time and human effort to parse through. In addition, this workflow would free up epidemiologists' time and, potentially, increase the frequency with which both CDC and STLT partners could utilize shopper card, receipts, and free text responses to support investigations. Using the summarization capability, the extensive process of mapping common names of different food items is done automatically, greatly reducing the human labor time to generate dashboards of information regarding current foodborne investigations to serve as a decision point to aid in outbreak response.","The Artificial Intelligence Platform (AIP) available within SEDRIC provides CDC epidemiologists the power to accelerate their investigations of multi-state foodborne disease outbreaks. Using the information previously extracted from receipts and other records, AIP can summarize these results to provide insights from information pulled from shopper receipts. Given ingredients can be found in multiple food products, and some ingredients such as herbs like coriander/cilantro may go by multiple names or be reported in multiple languages, this summarization tool provides a faster way to gather summary information from receipts on different food items which may be part of a foodborne investigation.",Operation and Maintenance,Neither,10/1/2023,10/1/2023,2/1/2024,Developed with contracting resources.,Unknown, ,No,No,No,No,"Data are used in outbreak/response scenarios, such as foodborne illness outbreak response. Data used is dependent on the situation and outbreak, and may be owned by CDC, FDA, USDA, State Health Departments, Tribal Health Departments, Local Health Departments, Territorial Health Departments, or other entities.",Documentation has been partially completed,Yes,"Yes – agency has access to source code, but it is not public.",Yes,1 CDC Data Platform (1CDP),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"This AI solution impacts the process of investigating foodborne disease outbreaks. These foodborne outbreaks require cooperative efforts from CDC staff, FDA, USDA, and local agencies and the AI system is used through a centralized data platform System for Enteric Disease Response, Investigation, and Coordination (also known as SEDRIC). For more information on SEDRIC, please go to our website: https://www.cdc.gov/foodborne-outbreaks/php/foodsafety/tools/index.html

SEDRIC's AIP use case provides CDC epidemiologists the ability to accelerate their investigations of multi-state foodborne disease outbreaks by more effectively leveraging data available in a rich data source, such as receipts from grocery stores, that otherwise requires extensive time and human effort to parse through. In addition, this workflow would free up epidemiologists' time and, potentially, increase the frequency with which both CDC and STLT partners could utilize shopper card, receipts, and free text responses to support investigations. Using the summarization capability, the extensive process of mapping common names of different food items is done automatically, greatly reducing the human labor time to generate dashboards of information regarding current foodborne investigations to serve as a decision point to aid in outbreak response. . The Artificial Intelligence Platform (AIP) available within SEDRIC provides CDC epidemiologists the power to accelerate their investigations of multi-state foodborne disease outbreaks. Using the information previously extracted from receipts and other records, AIP can summarize these results to provide insights from information pulled from shopper receipts. Given ingredients can be found in multiple food products, and some ingredients such as herbs like coriander/cilantro may go by multiple names or be reported in multiple languages, this summarization tool provides a faster way to gather summary information from receipts on different food items which may be part of a foodborne investigation.","this ai solution impacts the process of investigating foodborne disease outbreaks. these foodborne outbreaks require cooperative efforts from cdc staff, fda, usda, and local agencies and the ai system is used through a centralized data platform system for enteric disease response, investigation, and coordination (also known as sedric). for more information on sedric, please go to our website: https://www.cdc.gov/foodborne-outbreaks/php/foodsafety/tools/index.html sedric's aip use case provides cdc epidemiologists the ability to accelerate their investigations of multi-state foodborne disease outbreaks by more effectively leveraging data available in a rich data source, such as receipts from grocery stores, that otherwise requires extensive time and human effort to parse through. in addition, this workflow would free up epidemiologists' time and, potentially, increase the frequency with which both cdc and stlt partners could utilize shopper card, receipts, and free text responses to support investigations. using the summarization capability, the extensive process of mapping common names of different food items is done automatically, greatly reducing the human labor time to generate dashboards of information regarding current foodborne investigations to serve as a decision point to aid in outbreak response. . the artificial intelligence platform (aip) available within sedric provides cdc epidemiologists the power to accelerate their investigations of multi-state foodborne disease outbreaks. using the information previously extracted from receipts and other records, aip can summarize these results to provide insights from information pulled from shopper receipts. given ingredients can be found in multiple food products, and some ingredients such as herbs like coriander/cilantro may go by multiple names or be reported in multiple languages, this summarization tool provides a faster way to gather summary information from receipts on different food items which may be part of a foodborne investigation."
DCIPHER (SEDRIC) AIP for Advanced Foodborne Outbreak Investigation (Receipt Reading),Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"This AI solution impacts the process of investigating foodborne disease outbreaks. These foodborne outbreaks require cooperative efforts from CDC staff, FDA, USDA, and local agencies and the AI system is used through a centralized data platform System for Enteric Disease Response, Investigation, and Coordination (also known as SEDRIC). For more information on SEDRIC, please go to our website: https://www.cdc.gov/foodborne-outbreaks/php/foodsafety/tools/index.html

SEDRIC's AIP use case provides CDC epidemiologists the ability to accelerate their investigations of multi-state foodborne disease outbreaks by more effectively leveraging data available in a rich data source, such as receipts from grocery stores, that otherwise requires extensive time and human effort to parse through. In addition, this workflow would free up epidemiologists' time and, potentially, increase the frequency with which both CDC and STLT partners could utilize shopper card, receipts, and free text responses to support investigations. Manually entering receipt information, shopper card information, or other such free text field is traditionally error prone and time intensive for a variety of information. This AI system provides a human-in-the-loop opportunity to review and update data entry points while reducing time spent by staff to gain these insights. Having a set structured output as well increased standardization of this information and eases reporting in situations requiring cooperation from multiple organizations.","Palantir’s Artificial Intelligence Platform (AIP) available within SEDRIC provides CDC epidemiologists the power to accelerate their investigations of multi-state foodborne disease outbreaks. It can extract structured data from grocery receipts, shopper card records, and free-text responses in order to catalog the food items purchased by affected patients. It can also map those items to SEDRIC-defined vehicles which categorize the items and highlight commonalities across patients, helping to pinpoint potential outbreak vehicles.",Operation and Maintenance,Neither,10/1/2023,10/1/2023,2/1/2024,Developed with contracting resources.,Unknown, ,No,No,No,No,"Data are used in outbreak/response scenarios, such as foodborne illness outbreak response. Data used is dependent on the situation and outbreak, and may be owned by CDC, FDA, USDA, State Health Departments, Tribal Health Departments, Local Health Departments, Territorial Health Departments, or other entities.",Documentation has been partially completed,Yes,"Yes – agency has access to source code, but it is not public.",Yes,1 CDC Data Platform (1CDP),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"This AI solution impacts the process of investigating foodborne disease outbreaks. These foodborne outbreaks require cooperative efforts from CDC staff, FDA, USDA, and local agencies and the AI system is used through a centralized data platform System for Enteric Disease Response, Investigation, and Coordination (also known as SEDRIC). For more information on SEDRIC, please go to our website: https://www.cdc.gov/foodborne-outbreaks/php/foodsafety/tools/index.html

SEDRIC's AIP use case provides CDC epidemiologists the ability to accelerate their investigations of multi-state foodborne disease outbreaks by more effectively leveraging data available in a rich data source, such as receipts from grocery stores, that otherwise requires extensive time and human effort to parse through. In addition, this workflow would free up epidemiologists' time and, potentially, increase the frequency with which both CDC and STLT partners could utilize shopper card, receipts, and free text responses to support investigations. Manually entering receipt information, shopper card information, or other such free text field is traditionally error prone and time intensive for a variety of information. This AI system provides a human-in-the-loop opportunity to review and update data entry points while reducing time spent by staff to gain these insights. Having a set structured output as well increased standardization of this information and eases reporting in situations requiring cooperation from multiple organizations. . Palantir’s Artificial Intelligence Platform (AIP) available within SEDRIC provides CDC epidemiologists the power to accelerate their investigations of multi-state foodborne disease outbreaks. It can extract structured data from grocery receipts, shopper card records, and free-text responses in order to catalog the food items purchased by affected patients. It can also map those items to SEDRIC-defined vehicles which categorize the items and highlight commonalities across patients, helping to pinpoint potential outbreak vehicles.","this ai solution impacts the process of investigating foodborne disease outbreaks. these foodborne outbreaks require cooperative efforts from cdc staff, fda, usda, and local agencies and the ai system is used through a centralized data platform system for enteric disease response, investigation, and coordination (also known as sedric). for more information on sedric, please go to our website: https://www.cdc.gov/foodborne-outbreaks/php/foodsafety/tools/index.html sedric's aip use case provides cdc epidemiologists the ability to accelerate their investigations of multi-state foodborne disease outbreaks by more effectively leveraging data available in a rich data source, such as receipts from grocery stores, that otherwise requires extensive time and human effort to parse through. in addition, this workflow would free up epidemiologists' time and, potentially, increase the frequency with which both cdc and stlt partners could utilize shopper card, receipts, and free text responses to support investigations. manually entering receipt information, shopper card information, or other such free text field is traditionally error prone and time intensive for a variety of information. this ai system provides a human-in-the-loop opportunity to review and update data entry points while reducing time spent by staff to gain these insights. having a set structured output as well increased standardization of this information and eases reporting in situations requiring cooperation from multiple organizations. . palantir’s artificial intelligence platform (aip) available within sedric provides cdc epidemiologists the power to accelerate their investigations of multi-state foodborne disease outbreaks. it can extract structured data from grocery receipts, shopper card records, and free-text responses in order to catalog the food items purchased by affected patients. it can also map those items to sedric-defined vehicles which categorize the items and highlight commonalities across patients, helping to pinpoint potential outbreak vehicles."
DFWED Food Vehicle Investigation Module,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"Foodborne outbreak investigation

Outbreak investigation by providing estimates of output of potential food vehicles can increase speed of tracking foodborne outbreaks in terms of identification of hypothesis generation. This reduces operational burdens in foodborne response efforts.","NLP and Food Vehicle Investigation Module (FVIM) uses language learning models and Text Classification Tools to identify potential vehicles. NLP tool uses text classification to provide probabilities that a food is associated with the outbreak based on reports in the SEDRIC line list. FVIM identifies past outbreak with similar key demographic and temporal characteristics to the selected outbreak to guide hypothesis generation. FVIM analyses reported food exposures uploaded to the SEDRIC line list and categorizes responses into food commodity types that are then compared to FoodNet Population survey results weighted by age, sex and ethnicity and generates a statistical comparison",Operation and Maintenance,Neither,7/1/2022,7/1/2022,5/1/2023,Developed in-house., , ,No,Yes,No,No,"Data are used in outbreak/response scenarios, such as foodborne illness outbreak response. Data used is dependent on the situation and outbreak, and may be owned by CDC, FDA, USDA, State Health Departments, Tribal Health Departments, Local Health Departments, Territorial Health Departments, or other entities.",Documentation is missing or not available,Yes,"Yes – agency has access to source code, but it is not public.",Yes, 1 CDC Data Platform (1CDP),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"Foodborne outbreak investigation

Outbreak investigation by providing estimates of output of potential food vehicles can increase speed of tracking foodborne outbreaks in terms of identification of hypothesis generation. This reduces operational burdens in foodborne response efforts. . NLP and Food Vehicle Investigation Module (FVIM) uses language learning models and Text Classification Tools to identify potential vehicles. NLP tool uses text classification to provide probabilities that a food is associated with the outbreak based on reports in the SEDRIC line list. FVIM identifies past outbreak with similar key demographic and temporal characteristics to the selected outbreak to guide hypothesis generation. FVIM analyses reported food exposures uploaded to the SEDRIC line list and categorizes responses into food commodity types that are then compared to FoodNet Population survey results weighted by age, sex and ethnicity and generates a statistical comparison","foodborne outbreak investigation outbreak investigation by providing estimates of output of potential food vehicles can increase speed of tracking foodborne outbreaks in terms of identification of hypothesis generation. this reduces operational burdens in foodborne response efforts. . nlp and food vehicle investigation module (fvim) uses language learning models and text classification tools to identify potential vehicles. nlp tool uses text classification to provide probabilities that a food is associated with the outbreak based on reports in the sedric line list. fvim identifies past outbreak with similar key demographic and temporal characteristics to the selected outbreak to guide hypothesis generation. fvim analyses reported food exposures uploaded to the sedric line list and categorizes responses into food commodity types that are then compared to foodnet population survey results weighted by age, sex and ethnicity and generates a statistical comparison"
Genetic distance computation method for comparing complex multi-locus parasite (Cyclospora) genotypes,Department of Health and Human Services,HHS,CDC,Health & Medical,None of the above.,"Investigating the similarity of infections used during epidemiologic investigations of cyclosporiasis outbreaks.

It helps produce a clustering output for massive datasets comprising genotypes that are too complex to be analyzed using traditional genetic distance computation methods and aid in identifying similarities of infections when tracking cyclosporiasis outbreaks and other potential parasites.","The AI algorithm compares sets of haplotypes sequenced from a set of clinical samples and computes a set of genetic distances. It performs these comparisons across thousands of genotypes to identify infections that are closely related or unrelated. The resultant genetic distances are then clustered to define groups of closely related infections. This information is used routinely to complement epidemiologic investigations of cyclosporiasis (Cyclospora) outbreaks, and traceback investigations performed by state public health agencies and FDA. While this AI has mostly been applied to Cyclospora, we have also applied it to other parasites, including malaria, and certain parasitic worms.",Operation and Maintenance,Neither,1/1/2018,1/1/2018,9/1/2019,Developed in-house., , ,No,No,No,No,"used Cyclospora sequence data that was generated by CDC, State Public Health Labs, and the Public Health Agency of Canada following a protocol for amplifying 8 genotyping markers. This protocol was developed by CDC. All sequence data generated by CDC and State Public Health Labs are publicly available through NCBI (https://www.ncbi.nlm.nih.gov/bioproject/PRJNA578931/). The sequence data from the Public Health Agency of Canada can typically be found on NCBI as well (https://www.ncbi.nlm.nih.gov/bioproject/PRJNA796535/).",Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Advanced Molecular Detection Scientific Computing Platform,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"Investigating the similarity of infections used during epidemiologic investigations of cyclosporiasis outbreaks.

It helps produce a clustering output for massive datasets comprising genotypes that are too complex to be analyzed using traditional genetic distance computation methods and aid in identifying similarities of infections when tracking cyclosporiasis outbreaks and other potential parasites. . The AI algorithm compares sets of haplotypes sequenced from a set of clinical samples and computes a set of genetic distances. It performs these comparisons across thousands of genotypes to identify infections that are closely related or unrelated. The resultant genetic distances are then clustered to define groups of closely related infections. This information is used routinely to complement epidemiologic investigations of cyclosporiasis (Cyclospora) outbreaks, and traceback investigations performed by state public health agencies and FDA. While this AI has mostly been applied to Cyclospora, we have also applied it to other parasites, including malaria, and certain parasitic worms.","investigating the similarity of infections used during epidemiologic investigations of cyclosporiasis outbreaks. it helps produce a clustering output for massive datasets comprising genotypes that are too complex to be analyzed using traditional genetic distance computation methods and aid in identifying similarities of infections when tracking cyclosporiasis outbreaks and other potential parasites. . the ai algorithm compares sets of haplotypes sequenced from a set of clinical samples and computes a set of genetic distances. it performs these comparisons across thousands of genotypes to identify infections that are closely related or unrelated. the resultant genetic distances are then clustered to define groups of closely related infections. this information is used routinely to complement epidemiologic investigations of cyclosporiasis (cyclospora) outbreaks, and traceback investigations performed by state public health agencies and fda. while this ai has mostly been applied to cyclospora, we have also applied it to other parasites, including malaria, and certain parasitic worms."
MedCoder - Coding literal text cause of death information reported on death certificates to ICD-10,Department of Health and Human Services,HHS,CDC,Health & Medical,None of the above.,"MedCoder is a subsystem of CDC's National Center for Health Statistics (NCHS) National Vital Statistics System for coding causes of death indicated on the death certificates to the International Classification of Diseases 10th Revision (ICD-10) codes.

With the implementation of MedCoder, the percentage of deaths that can be automatically and accurately coded has risen from 70-75% using the previous coding system to over 85%. This improvement translates into substantial cost savings, amounting to hundreds of thousands of dollars that would have been spent on manual coding. Additionally, it significantly enhances the timeliness of data related to urgent public health concerns such as COVID and drug overdose deaths, enabling near real-time surveillance.","MedCoder assists with the identifying cause of death codes from death certificates using ICD-10 Underlying cause codes and automatically identifies and rejects complex causes of death as well as frequently miscoded ones, requiring manual coding or review. Additionally, in production, MedCoder utilizes natural language processing (NLP) to cleanse and standardize literal text cause of death statements before performing coding.",Operation and Maintenance,Neither,10/1/2017,2/1/2018,6/1/2022,Developed in-house., , ,No,No,No,Yes,Unknown,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,National Vital Statistics System (NVSS),More than 12 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"MedCoder is a subsystem of CDC's National Center for Health Statistics (NCHS) National Vital Statistics System for coding causes of death indicated on the death certificates to the International Classification of Diseases 10th Revision (ICD-10) codes.

With the implementation of MedCoder, the percentage of deaths that can be automatically and accurately coded has risen from 70-75% using the previous coding system to over 85%. This improvement translates into substantial cost savings, amounting to hundreds of thousands of dollars that would have been spent on manual coding. Additionally, it significantly enhances the timeliness of data related to urgent public health concerns such as COVID and drug overdose deaths, enabling near real-time surveillance. . MedCoder assists with the identifying cause of death codes from death certificates using ICD-10 Underlying cause codes and automatically identifies and rejects complex causes of death as well as frequently miscoded ones, requiring manual coding or review. Additionally, in production, MedCoder utilizes natural language processing (NLP) to cleanse and standardize literal text cause of death statements before performing coding.","medcoder is a subsystem of cdc's national center for health statistics (nchs) national vital statistics system for coding causes of death indicated on the death certificates to the international classification of diseases 10th revision (icd-10) codes. with the implementation of medcoder, the percentage of deaths that can be automatically and accurately coded has risen from 70-75% using the previous coding system to over 85%. this improvement translates into substantial cost savings, amounting to hundreds of thousands of dollars that would have been spent on manual coding. additionally, it significantly enhances the timeliness of data related to urgent public health concerns such as covid and drug overdose deaths, enabling near real-time surveillance. . medcoder assists with the identifying cause of death codes from death certificates using icd-10 underlying cause codes and automatically identifies and rejects complex causes of death as well as frequently miscoded ones, requiring manual coding or review. additionally, in production, medcoder utilizes natural language processing (nlp) to cleanse and standardize literal text cause of death statements before performing coding."
NCIRD SmartFind ChatBots - Public and Internal ,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"Internal partner mailbox email management including managing knowledge base. Previously, this included public facing chatbots but the public ChatBots are no longer active. This uses previously cleared FAQs and other reviewed information accessible publicly as the knowledge base.

The internal Knowledge-Bot's SharePoint component is helping program staff manage emails from partners more efficiently and effectively including creation and maintenance of a knowledge base that can be utilized across staff managing the mailbox. ","Develop conversational ChatBots (Public Flu, Public COVID-19 Vaccination, Internal Knowledge-Bot) that analyze free text questions entered by the public, healthcare providers, partners, and internal staff, and provide agency-cleared answers which best match the question. Developed in collaboration with Microsoft staff during COVID-19 pandemic using their Cognitive Services, Search, QnA Maker, Azure Healthcare Bot, Power Automate, SharePoint, and webapps. ",Operation and Maintenance,Neither,10/1/2020,3/1/2024,12/1/2024,Developed with both contracting and in-house resources.,Unknown, ,No,No,No,No,"For the public-facing chatbots, we use public-facing FAQs.",Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CDC OD ATO,Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"Internal partner mailbox email management including managing knowledge base. Previously, this included public facing chatbots but the public ChatBots are no longer active. This uses previously cleared FAQs and other reviewed information accessible publicly as the knowledge base.

The internal Knowledge-Bot's SharePoint component is helping program staff manage emails from partners more efficiently and effectively including creation and maintenance of a knowledge base that can be utilized across staff managing the mailbox. . Develop conversational ChatBots (Public Flu, Public COVID-19 Vaccination, Internal Knowledge-Bot) that analyze free text questions entered by the public, healthcare providers, partners, and internal staff, and provide agency-cleared answers which best match the question. Developed in collaboration with Microsoft staff during COVID-19 pandemic using their Cognitive Services, Search, QnA Maker, Azure Healthcare Bot, Power Automate, SharePoint, and webapps.","internal partner mailbox email management including managing knowledge base. previously, this included public facing chatbots but the public chatbots are no longer active. this uses previously cleared faqs and other reviewed information accessible publicly as the knowledge base. the internal knowledge-bot's sharepoint component is helping program staff manage emails from partners more efficiently and effectively including creation and maintenance of a knowledge base that can be utilized across staff managing the mailbox. . develop conversational chatbots (public flu, public covid-19 vaccination, internal knowledge-bot) that analyze free text questions entered by the public, healthcare providers, partners, and internal staff, and provide agency-cleared answers which best match the question. developed in collaboration with microsoft staff during covid-19 pandemic using their cognitive services, search, qna maker, azure healthcare bot, power automate, sharepoint, and webapps."
NIOSH Industry and Occupation Computerized Coding System (NIOCCS),Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"Replaces manual coding of industry and occupation text to standardized codes so that they can be used for research and analysis.

Reduces the high cost of manually coding industry and occupation data while promoting increased coding speed, accuracy, and consistency.",Uses machine learning models to translate industry and occupation text into standardized codes that can be used for research and analysis.,Operation and Maintenance,Neither,10/1/2024,10/1/2024,1/1/2024,Developed in-house., , ,Yes,No,No,Yes,STLT's death record data (received via NCHS) and BRFSS survey data,Documentation has been partially completed,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Modernization Platform NIOSH (MPN),Less than 6 months,Yes,No,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"Replaces manual coding of industry and occupation text to standardized codes so that they can be used for research and analysis.

Reduces the high cost of manually coding industry and occupation data while promoting increased coding speed, accuracy, and consistency. . Uses machine learning models to translate industry and occupation text into standardized codes that can be used for research and analysis.","replaces manual coding of industry and occupation text to standardized codes so that they can be used for research and analysis. reduces the high cost of manually coding industry and occupation data while promoting increased coding speed, accuracy, and consistency. . uses machine learning models to translate industry and occupation text into standardized codes that can be used for research and analysis."
Nowcasting Injury Trends ,Department of Health and Human Services,HHS,CDC,Health & Medical,None of the above.,"Predicting current injury rates and real time estimates of injuries and deaths. The internal dashboard that will be created as a result of this effort will enhance the understanding of injury trends and expedite surveillance and research activities. This initiative will have a direct impact on the speed at which emerging trends in injuries resulting in deaths are identified and investigated.

Gold standard data take time to process and be made available. Real-time insight of injury trends when gold standard data are not available can be a resource resulting in timelier situational awareness to inform injury surveillance efforts. ","An internal-facing, interactive dashboard incorporating multiple traditional and non-traditional datasets and a multi-stage machine learning pipeline to 'nowcast' injury death trends nationally on a week-to-week basis. ",Operation and Maintenance,Neither,1/1/2021,1/1/2021,1/1/2022,Developed in-house., , ,No,No,No,No,Emergendy Department data from the National Syndromic Surveillance Program,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Enterprise Data Analytics and Visualization (EDAV) Platform,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"Predicting current injury rates and real time estimates of injuries and deaths. The internal dashboard that will be created as a result of this effort will enhance the understanding of injury trends and expedite surveillance and research activities. This initiative will have a direct impact on the speed at which emerging trends in injuries resulting in deaths are identified and investigated.

Gold standard data take time to process and be made available. Real-time insight of injury trends when gold standard data are not available can be a resource resulting in timelier situational awareness to inform injury surveillance efforts. . An internal-facing, interactive dashboard incorporating multiple traditional and non-traditional datasets and a multi-stage machine learning pipeline to 'nowcast' injury death trends nationally on a week-to-week basis.","predicting current injury rates and real time estimates of injuries and deaths. the internal dashboard that will be created as a result of this effort will enhance the understanding of injury trends and expedite surveillance and research activities. this initiative will have a direct impact on the speed at which emerging trends in injuries resulting in deaths are identified and investigated. gold standard data take time to process and be made available. real-time insight of injury trends when gold standard data are not available can be a resource resulting in timelier situational awareness to inform injury surveillance efforts. . an internal-facing, interactive dashboard incorporating multiple traditional and non-traditional datasets and a multi-stage machine learning pipeline to 'nowcast' injury death trends nationally on a week-to-week basis."
Risk Assessment Module (RAM) for the National Diabetes Prevention Program (National DPP) Operations Center.,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"Tracking program progress. This AI assists in determining if an organization that is currently operating within the National DPP is at risk of improperly starting, at risk of going inactive, or at risk of not achieving the goals necessary to continue successfully in CDC’s Diabetes Prevention Recognition Program.

The RAM module will help program managers synthesize large amounts of organization-level data in making decisions on how best to assist organizations with their programs, ultimately leading to increased program participation and improved health outcomes in program participants. ","The RAM is a reporting tool within the National DPP Operations Center which already has an ATO. The RAM module ingests large amounts of organization-level data including National DPP participant enrollment records, demographics, and specific risk factors to perform supervised learning and model generation. This machine learning model is used to predict and quantitatively rank those organizations that are at highest risk of failing to meet program objectives. This information, both input and RAM output, is currently fully sequestered to associates of the National DPP of the CDC. In the future, with continued development and validation, the plan is to allow external-to-CDC State Quality Specialist (SQS) users access to the outputs of RAM for additional assistance with improving organization success.",Operation and Maintenance,Neither,8/1/2023,9/1/2023,8/1/2024,Developed with contracting resources.,Unknown, ,No,No,No,No,DDT DPRP Portal - historical data from organization's 6-mth submissions of participant attendance of Lifestyle change classes.,Documentation is missing or not available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,DDT DPRP Portal,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"Tracking program progress. This AI assists in determining if an organization that is currently operating within the National DPP is at risk of improperly starting, at risk of going inactive, or at risk of not achieving the goals necessary to continue successfully in CDC’s Diabetes Prevention Recognition Program.

The RAM module will help program managers synthesize large amounts of organization-level data in making decisions on how best to assist organizations with their programs, ultimately leading to increased program participation and improved health outcomes in program participants. . The RAM is a reporting tool within the National DPP Operations Center which already has an ATO. The RAM module ingests large amounts of organization-level data including National DPP participant enrollment records, demographics, and specific risk factors to perform supervised learning and model generation. This machine learning model is used to predict and quantitatively rank those organizations that are at highest risk of failing to meet program objectives. This information, both input and RAM output, is currently fully sequestered to associates of the National DPP of the CDC. In the future, with continued development and validation, the plan is to allow external-to-CDC State Quality Specialist (SQS) users access to the outputs of RAM for additional assistance with improving organization success.","tracking program progress. this ai assists in determining if an organization that is currently operating within the national dpp is at risk of improperly starting, at risk of going inactive, or at risk of not achieving the goals necessary to continue successfully in cdc’s diabetes prevention recognition program. the ram module will help program managers synthesize large amounts of organization-level data in making decisions on how best to assist organizations with their programs, ultimately leading to increased program participation and improved health outcomes in program participants. . the ram is a reporting tool within the national dpp operations center which already has an ato. the ram module ingests large amounts of organization-level data including national dpp participant enrollment records, demographics, and specific risk factors to perform supervised learning and model generation. this machine learning model is used to predict and quantitatively rank those organizations that are at highest risk of failing to meet program objectives. this information, both input and ram output, is currently fully sequestered to associates of the national dpp of the cdc. in the future, with continued development and validation, the plan is to allow external-to-cdc state quality specialist (sqs) users access to the outputs of ram for additional assistance with improving organization success."
Semi-Automated Nonresponse Detection for Surveys (SANDS),Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"SANDS is a fine-tuned large language model (LLM) designed to assist with human-in-the-loop procedures specifically for open-ended survey responses. Open-ended survey responses vary in quality and are challenging to use effectively because they are labor-intensive to review and therefore cost-prohibitive to use at scale. SANDS is intended to reduce the burden by filtering out clear and obvious non-response such as  ""This is so dumb, why I am I wasting my time on such a stupid question"" or ""Because It is true"" which provide no value to researchers. This LLM is available on HuggingFace and is not routinely updated, but it can be used like any open-source LLM through secure instances of Python.

Manual curation of open-ended survey responses is time-consuming, often requiring extensive hours to identify themes and review outputs. SANDS significantly reduces this manual burden by providing scores for responses, enabling researchers to quickly compile an initial high-quality dataset for qualitative research. SANDS also flags responses needing further examination, streamlining the review process.",CDC's National Center for Health Statistics (NCHS) has developed and released a model to detect nonresponses in open-text survey responses. This helps improve survey data quality and question and questionnaire design. The system is a natural language processing (NLP) model that has been fine-tuned on a custom dataset of survey responses. ,Operation and Maintenance,Neither,6/1/2021,9/1/2021,9/1/2022,Developed in-house., , ,No,Yes,No,No,"3,000 labeled open-ended responses to web probes on questions relating to the COVID-19 pandemic gathered from the Research and Development Survey or RANDS conducted by the Division of Research and Methodology at the National Center for Health Statistics",Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",No, ,Less than 6 months,Yes,Yes,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"SANDS is a fine-tuned large language model (LLM) designed to assist with human-in-the-loop procedures specifically for open-ended survey responses. Open-ended survey responses vary in quality and are challenging to use effectively because they are labor-intensive to review and therefore cost-prohibitive to use at scale. SANDS is intended to reduce the burden by filtering out clear and obvious non-response such as  ""This is so dumb, why I am I wasting my time on such a stupid question"" or ""Because It is true"" which provide no value to researchers. This LLM is available on HuggingFace and is not routinely updated, but it can be used like any open-source LLM through secure instances of Python.

Manual curation of open-ended survey responses is time-consuming, often requiring extensive hours to identify themes and review outputs. SANDS significantly reduces this manual burden by providing scores for responses, enabling researchers to quickly compile an initial high-quality dataset for qualitative research. SANDS also flags responses needing further examination, streamlining the review process. . CDC's National Center for Health Statistics (NCHS) has developed and released a model to detect nonresponses in open-text survey responses. This helps improve survey data quality and question and questionnaire design. The system is a natural language processing (NLP) model that has been fine-tuned on a custom dataset of survey responses.","sands is a fine-tuned large language model (llm) designed to assist with human-in-the-loop procedures specifically for open-ended survey responses. open-ended survey responses vary in quality and are challenging to use effectively because they are labor-intensive to review and therefore cost-prohibitive to use at scale. sands is intended to reduce the burden by filtering out clear and obvious non-response such as ""this is so dumb, why i am i wasting my time on such a stupid question"" or ""because it is true"" which provide no value to researchers. this llm is available on huggingface and is not routinely updated, but it can be used like any open-source llm through secure instances of python. manual curation of open-ended survey responses is time-consuming, often requiring extensive hours to identify themes and review outputs. sands significantly reduces this manual burden by providing scores for responses, enabling researchers to quickly compile an initial high-quality dataset for qualitative research. sands also flags responses needing further examination, streamlining the review process. . cdc's national center for health statistics (nchs) has developed and released a model to detect nonresponses in open-text survey responses. this helps improve survey data quality and question and questionnaire design. the system is a natural language processing (nlp) model that has been fine-tuned on a custom dataset of survey responses."
Sequential Coverage Algorithm (SCA) and partial Expectation-Maximization (EM) estimation in Record Linkage,Department of Health and Human Services,HHS,CDC,Health & Medical,None of the above.,"CDC's National Center for Health Statistics (NCHS) Data Linkage Program has developed a record linkage program designed to maximize the scientific value of the Center’s population-based surveys. Linked data files enable researchers to examine the factors that influence disability, chronic disease, health care utilization, morbidity, and mortality. The linkage algorithms initially used by the program did not implement machine learning in either deterministic or probabilistic linkages and required several manual steps for these processes. 

Using machine learning in the CDC's National Center for Health Statistics (NCHS) Data Linkage Program offers several benefits. Machine learning algorithms help improve the accuracy and efficiency of data linkage by developing joining methods or blocking groups.  Working with very large datasets can be time-consuming and resource-intensive. By implementing supervised and unsupervised machine learning techniques, the Data Linkage Program can automate the process of developing and selecting blocking groups, reducing manual effort and increasing efficiency. This additionally improves the scalability of data linkages. Machine learning algorithms can continuously learn and adapt based on new data inputs and feedback. This allows the Data Linkage Program to refine their linkage algorithms over time.","CDC's National Center for Health Statistics (NCHS) Data Linkage Program has implemented both supervised and unsupervised machine learning (ML) techniques in their linkage algorithms. The Sequential Coverage Algorithm (SCA), a supervised ML algorithm, is used to develop joining methods (or blocking groups) when working with very large datasets. The unsupervised partial Expectation-Maximization (EM) estimation is used to estimate the proportion of pairs that are matches within each block. Both methods improve linkage accuracy and efficiency.",Operation and Maintenance,Neither,2/1/2019,8/1/2019,8/1/2020,Developed with contracting resources.,Unknown, ,No,Yes,No,No,"Data used to evaluate the models include data from the National Hospital Care Survey, the National Health and Nutrition Examination Survey, and the National Health Interview Survey as well as linked administrative data.",Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Consolidated Statistical Platform,6-12 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"CDC's National Center for Health Statistics (NCHS) Data Linkage Program has developed a record linkage program designed to maximize the scientific value of the Center’s population-based surveys. Linked data files enable researchers to examine the factors that influence disability, chronic disease, health care utilization, morbidity, and mortality. The linkage algorithms initially used by the program did not implement machine learning in either deterministic or probabilistic linkages and required several manual steps for these processes. 

Using machine learning in the CDC's National Center for Health Statistics (NCHS) Data Linkage Program offers several benefits. Machine learning algorithms help improve the accuracy and efficiency of data linkage by developing joining methods or blocking groups.  Working with very large datasets can be time-consuming and resource-intensive. By implementing supervised and unsupervised machine learning techniques, the Data Linkage Program can automate the process of developing and selecting blocking groups, reducing manual effort and increasing efficiency. This additionally improves the scalability of data linkages. Machine learning algorithms can continuously learn and adapt based on new data inputs and feedback. This allows the Data Linkage Program to refine their linkage algorithms over time. . CDC's National Center for Health Statistics (NCHS) Data Linkage Program has implemented both supervised and unsupervised machine learning (ML) techniques in their linkage algorithms. The Sequential Coverage Algorithm (SCA), a supervised ML algorithm, is used to develop joining methods (or blocking groups) when working with very large datasets. The unsupervised partial Expectation-Maximization (EM) estimation is used to estimate the proportion of pairs that are matches within each block. Both methods improve linkage accuracy and efficiency.","cdc's national center for health statistics (nchs) data linkage program has developed a record linkage program designed to maximize the scientific value of the center’s population-based surveys. linked data files enable researchers to examine the factors that influence disability, chronic disease, health care utilization, morbidity, and mortality. the linkage algorithms initially used by the program did not implement machine learning in either deterministic or probabilistic linkages and required several manual steps for these processes. using machine learning in the cdc's national center for health statistics (nchs) data linkage program offers several benefits. machine learning algorithms help improve the accuracy and efficiency of data linkage by developing joining methods or blocking groups. working with very large datasets can be time-consuming and resource-intensive. by implementing supervised and unsupervised machine learning techniques, the data linkage program can automate the process of developing and selecting blocking groups, reducing manual effort and increasing efficiency. this additionally improves the scalability of data linkages. machine learning algorithms can continuously learn and adapt based on new data inputs and feedback. this allows the data linkage program to refine their linkage algorithms over time. . cdc's national center for health statistics (nchs) data linkage program has implemented both supervised and unsupervised machine learning (ml) techniques in their linkage algorithms. the sequential coverage algorithm (sca), a supervised ml algorithm, is used to develop joining methods (or blocking groups) when working with very large datasets. the unsupervised partial expectation-maximization (em) estimation is used to estimate the proportion of pairs that are matches within each block. both methods improve linkage accuracy and efficiency."
TowerScout: Automated cooling tower detection from aerial imagery for Legionnaires' Disease outbreak investigation,Department of Health and Human Services,HHS,CDC,Health & Medical,None of the above.,"During Legionella outbreaks, the National Center for Immunization and Respiratory Diseases (NCIRD)/Division of Bacterial Diseases (DBD) needs to identify cooling towers that could potentially be spreading Legionella bacteria. 

Using AI object detection, TowerScout detects cooling towers approximately 600 times faster than manual searches, allowing researchers to more efficiently respond during outbreaks. ",TowerScout uses object detection and image classification models to detect cooling towers within aerial imagery.  ,Operation and Maintenance,Neither,1/1/2021,1/1/2021,5/1/2021,Developed with contracting resources.,Unknown, ,No,No,No,No,Unknown,Documentation is widely available,Yes,Yes – source code is publicly available.,Yes,Enterprise Data Analytics and Visualization (EDAV) Platform,Less than 6 months,Yes,Yes,No,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"During Legionella outbreaks, the National Center for Immunization and Respiratory Diseases (NCIRD)/Division of Bacterial Diseases (DBD) needs to identify cooling towers that could potentially be spreading Legionella bacteria. 

Using AI object detection, TowerScout detects cooling towers approximately 600 times faster than manual searches, allowing researchers to more efficiently respond during outbreaks. . TowerScout uses object detection and image classification models to detect cooling towers within aerial imagery.","during legionella outbreaks, the national center for immunization and respiratory diseases (ncird)/division of bacterial diseases (dbd) needs to identify cooling towers that could potentially be spreading legionella bacteria. using ai object detection, towerscout detects cooling towers approximately 600 times faster than manual searches, allowing researchers to more efficiently respond during outbreaks. . towerscout uses object detection and image classification models to detect cooling towers within aerial imagery."
Creation of synthetic Survey-like Insurance Names for use coding NHIS private insurance responses,Department of Health and Human Services,HHS,CDC,Health & Medical,None of the above.,"This solution will impact the process of manual coding and reviewing text errors in the National Health Interview Survey 

Previous efforts to augment the manual coding have proven ineffective. Given the complex nature of insurance programs, this results in over 200 FTE hours a year being spent on just the initial coding of open text insurance fields. Additional hours are spent reviewing the manual coding for errors. By creating survey-like insurance responses, this could theoretically save over 100 hours of staff labor a year, greatly increasing efficiency and timeliness.","The National Health Interview Survey calculates insurance coverage rates among the US non-institutionalized resident population, including statistics of the percentage of people with private insurance coverage. The collection of information on insurance coverage is accomplished in two parts. First, survey respondents self-identify the type of insurance they have. Secondly, to aid in the verification of insurance type, survey respondents also provide the name of the health plan or program they have in an open text field. The information collected in the open text field may include misspellings, acronyms, and rarely match exactly the true complete insurance plan name. 

This leads to a time intensive process as staff may need to decipher the plan name in the open text field mentioned by the respondent and/or abbreviated by the interviewer and if that plan name confirms the correct type of insurance initially indicated during the interview process. By creating AI generated known acronyms as data points for projects, as an example Blue Cross Blue Shield of Alabama to BCBS of AL and BC/BS of AL, this will create a more representative set of information for staff to use when coding potential saving up to hundreds of FTE hours reviewing responses. This also works in combination with existing efforts to augment manual coding with AI and Machine Learning to increase efficiency.
",Retired,Neither, , , , , , , , , , , , , , , , ,Unknown, , ,Unknown, , ,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"This solution will impact the process of manual coding and reviewing text errors in the National Health Interview Survey 

Previous efforts to augment the manual coding have proven ineffective. Given the complex nature of insurance programs, this results in over 200 FTE hours a year being spent on just the initial coding of open text insurance fields. Additional hours are spent reviewing the manual coding for errors. By creating survey-like insurance responses, this could theoretically save over 100 hours of staff labor a year, greatly increasing efficiency and timeliness. . The National Health Interview Survey calculates insurance coverage rates among the US non-institutionalized resident population, including statistics of the percentage of people with private insurance coverage. The collection of information on insurance coverage is accomplished in two parts. First, survey respondents self-identify the type of insurance they have. Secondly, to aid in the verification of insurance type, survey respondents also provide the name of the health plan or program they have in an open text field. The information collected in the open text field may include misspellings, acronyms, and rarely match exactly the true complete insurance plan name. 

This leads to a time intensive process as staff may need to decipher the plan name in the open text field mentioned by the respondent and/or abbreviated by the interviewer and if that plan name confirms the correct type of insurance initially indicated during the interview process. By creating AI generated known acronyms as data points for projects, as an example Blue Cross Blue Shield of Alabama to BCBS of AL and BC/BS of AL, this will create a more representative set of information for staff to use when coding potential saving up to hundreds of FTE hours reviewing responses. This also works in combination with existing efforts to augment manual coding with AI and Machine Learning to increase efficiency.","this solution will impact the process of manual coding and reviewing text errors in the national health interview survey previous efforts to augment the manual coding have proven ineffective. given the complex nature of insurance programs, this results in over 200 fte hours a year being spent on just the initial coding of open text insurance fields. additional hours are spent reviewing the manual coding for errors. by creating survey-like insurance responses, this could theoretically save over 100 hours of staff labor a year, greatly increasing efficiency and timeliness. . the national health interview survey calculates insurance coverage rates among the us non-institutionalized resident population, including statistics of the percentage of people with private insurance coverage. the collection of information on insurance coverage is accomplished in two parts. first, survey respondents self-identify the type of insurance they have. secondly, to aid in the verification of insurance type, survey respondents also provide the name of the health plan or program they have in an open text field. the information collected in the open text field may include misspellings, acronyms, and rarely match exactly the true complete insurance plan name. this leads to a time intensive process as staff may need to decipher the plan name in the open text field mentioned by the respondent and/or abbreviated by the interviewer and if that plan name confirms the correct type of insurance initially indicated during the interview process. by creating ai generated known acronyms as data points for projects, as an example blue cross blue shield of alabama to bcbs of al and bc/bs of al, this will create a more representative set of information for staff to use when coding potential saving up to hundreds of fte hours reviewing responses. this also works in combination with existing efforts to augment manual coding with ai and machine learning to increase efficiency."
Development of in-silico genomic and patient datasets using generative ML algorithms,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"This solution will impact the process of finding reference genomes

Reduce the development time of custom tools, finding reference genomes, and additional administrative burdens. The datasets developed would be beneficial in testing out bioinformatic pipelines. ","This solution uses NLP generative AI toolsets (e.g. ChatGPT, co-pilot, etc.) to generate code for the development of bacterial and viral in-silico genomic sequences and patient data. This will be compared to already established pipelines to determine factors such as code viability, data accuracy, code licensing, and any drawbacks and/or advantages these services provide. For example, we would ask GPT-4 a prompt, ""generate python code to create Sars COV2 in-silico fastq sequencing data."". We will look at where did it reference the code base to generate the code and is it open source or have licensing requirements.  ",Retired,Neither, , , , , , , , , , , , , , , , ,Unknown, , ,Unknown, , ,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"This solution will impact the process of finding reference genomes

Reduce the development time of custom tools, finding reference genomes, and additional administrative burdens. The datasets developed would be beneficial in testing out bioinformatic pipelines. . This solution uses NLP generative AI toolsets (e.g. ChatGPT, co-pilot, etc.) to generate code for the development of bacterial and viral in-silico genomic sequences and patient data. This will be compared to already established pipelines to determine factors such as code viability, data accuracy, code licensing, and any drawbacks and/or advantages these services provide. For example, we would ask GPT-4 a prompt, ""generate python code to create Sars COV2 in-silico fastq sequencing data."". We will look at where did it reference the code base to generate the code and is it open source or have licensing requirements.","this solution will impact the process of finding reference genomes reduce the development time of custom tools, finding reference genomes, and additional administrative burdens. the datasets developed would be beneficial in testing out bioinformatic pipelines. . this solution uses nlp generative ai toolsets (e.g. chatgpt, co-pilot, etc.) to generate code for the development of bacterial and viral in-silico genomic sequences and patient data. this will be compared to already established pipelines to determine factors such as code viability, data accuracy, code licensing, and any drawbacks and/or advantages these services provide. for example, we would ask gpt-4 a prompt, ""generate python code to create sars cov2 in-silico fastq sequencing data."". we will look at where did it reference the code base to generate the code and is it open source or have licensing requirements."
Mastering Metadata: AI-Powered Governance for Data Insights,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"The implementation of this solution will have a significant influence on the maintenance of data quality and data assets on the data.cdc.gov platform.

Accurate, reliable, and timely data available in data.cdc.gov is critical in making meaningful public health decisions. This publicly available data repository is used worldwide by researchers, policy makers, and general public to access, manage, analyze, and visualize public health data. Through this project, we will be able to scale up the magnitude and quality of datasets from NCIRD that’s made available via data.cdc.gov and thus maximizing public health impact for a diverse audience at multiple levels.","data.cdc.gov serves as a publicly accessible repository for CDC data, utilized by researchers, policymakers, and the general public to monitor and comprehend public health trends. Ensuring the accuracy and reliability of the data on data.cdc.gov is crucial for informed decision-making in public health. To achieve this, the proposed solution involves leveraging AI to maintain CDC's National Center for Immunization and Respiratory Diseases data assets on data.cdc.gov to automate various tasks such as checking metadata completeness, validating metadata against a schema, comparing metadata with other sources, and monitoring metadata changes over time.",Retired,Neither, , , , , , , , , , , , , , , , ,Unknown, , ,Unknown, , ,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"The implementation of this solution will have a significant influence on the maintenance of data quality and data assets on the data.cdc.gov platform.

Accurate, reliable, and timely data available in data.cdc.gov is critical in making meaningful public health decisions. This publicly available data repository is used worldwide by researchers, policy makers, and general public to access, manage, analyze, and visualize public health data. Through this project, we will be able to scale up the magnitude and quality of datasets from NCIRD that’s made available via data.cdc.gov and thus maximizing public health impact for a diverse audience at multiple levels. . data.cdc.gov serves as a publicly accessible repository for CDC data, utilized by researchers, policymakers, and the general public to monitor and comprehend public health trends. Ensuring the accuracy and reliability of the data on data.cdc.gov is crucial for informed decision-making in public health. To achieve this, the proposed solution involves leveraging AI to maintain CDC's National Center for Immunization and Respiratory Diseases data assets on data.cdc.gov to automate various tasks such as checking metadata completeness, validating metadata against a schema, comparing metadata with other sources, and monitoring metadata changes over time.","the implementation of this solution will have a significant influence on the maintenance of data quality and data assets on the data.cdc.gov platform. accurate, reliable, and timely data available in data.cdc.gov is critical in making meaningful public health decisions. this publicly available data repository is used worldwide by researchers, policy makers, and general public to access, manage, analyze, and visualize public health data. through this project, we will be able to scale up the magnitude and quality of datasets from ncird that’s made available via data.cdc.gov and thus maximizing public health impact for a diverse audience at multiple levels. . data.cdc.gov serves as a publicly accessible repository for cdc data, utilized by researchers, policymakers, and the general public to monitor and comprehend public health trends. ensuring the accuracy and reliability of the data on data.cdc.gov is crucial for informed decision-making in public health. to achieve this, the proposed solution involves leveraging ai to maintain cdc's national center for immunization and respiratory diseases data assets on data.cdc.gov to automate various tasks such as checking metadata completeness, validating metadata against a schema, comparing metadata with other sources, and monitoring metadata changes over time."
PII detection using Private AI ,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"Numerous data sources contain personally identifiable information (PII) and protected health information (PHI), which cannot be publicly published or shared among federal partners due to confidentiality concerns. This includes sensitive records like death certificates, survey responses, and electronic health records. To enable broader utilization of this data and enhance insights, a PII detection system was explored in this project. Specifically, the availability of an off-the-shelf tool called Private AI was investigated for identifying PII. However, it is important to note that the license for this tool expired after one year and there are no current plans to renew the contract.

At present, the presence of PII within data and the labor intensive process to ensure PII redaction and removal has prevented broader sharing of data available within CDC. However, implementing an automated system which performs as well as humans and more quickly could significantly reduce the time required for human review, enabling faster dissemination of crucial public health information and data sources.","CDC's National Center for Health Statistics (NCHS) has been assessing the NLP solution provided by Private AI, which is specifically designed to detect, mask, and substitute personally identifiable information (PII) within textual data. This collection of models aims to securely identify and eliminate PII from unstructured text datasets across various platforms within the CDC network.",Retired,Neither, , , , , , , , , , , , , , , , ,Unknown, , ,Unknown, , ,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"Numerous data sources contain personally identifiable information (PII) and protected health information (PHI), which cannot be publicly published or shared among federal partners due to confidentiality concerns. This includes sensitive records like death certificates, survey responses, and electronic health records. To enable broader utilization of this data and enhance insights, a PII detection system was explored in this project. Specifically, the availability of an off-the-shelf tool called Private AI was investigated for identifying PII. However, it is important to note that the license for this tool expired after one year and there are no current plans to renew the contract.

At present, the presence of PII within data and the labor intensive process to ensure PII redaction and removal has prevented broader sharing of data available within CDC. However, implementing an automated system which performs as well as humans and more quickly could significantly reduce the time required for human review, enabling faster dissemination of crucial public health information and data sources. . CDC's National Center for Health Statistics (NCHS) has been assessing the NLP solution provided by Private AI, which is specifically designed to detect, mask, and substitute personally identifiable information (PII) within textual data. This collection of models aims to securely identify and eliminate PII from unstructured text datasets across various platforms within the CDC network.","numerous data sources contain personally identifiable information (pii) and protected health information (phi), which cannot be publicly published or shared among federal partners due to confidentiality concerns. this includes sensitive records like death certificates, survey responses, and electronic health records. to enable broader utilization of this data and enhance insights, a pii detection system was explored in this project. specifically, the availability of an off-the-shelf tool called private ai was investigated for identifying pii. however, it is important to note that the license for this tool expired after one year and there are no current plans to renew the contract. at present, the presence of pii within data and the labor intensive process to ensure pii redaction and removal has prevented broader sharing of data available within cdc. however, implementing an automated system which performs as well as humans and more quickly could significantly reduce the time required for human review, enabling faster dissemination of crucial public health information and data sources. . cdc's national center for health statistics (nchs) has been assessing the nlp solution provided by private ai, which is specifically designed to detect, mask, and substitute personally identifiable information (pii) within textual data. this collection of models aims to securely identify and eliminate pii from unstructured text datasets across various platforms within the cdc network."
Reddit Post Analysis for Sexual Health Using Large Language Models,Department of Health and Human Services,HHS,CDC,Health & Medical,None of the above.,"The AI will impact the data extraction and analysis processes for understanding how users seek sexual health advice online using Reddit.

In additional to traditional public health impacts this can include reducing administrative burdens and other ancillary benefits. This is an important criteria when selecting the initial round of projects.
Understand how and why people seek sexual health advice online. This may lead to finding gaps in online health resources, identifying underserved populations who resort to using online resources, and discovering changes in sexual health behavior over time.","Reddit is a social media website where users can submit and respond to posts around a wide variety of topics. Reddit is organized into subcommunities called subreddits, and many people turn to sexual health focused subreddits, such as the “r/STD” subreddit, for advice. The Reddit posts can be a valuable source of text and image data since these posts reveal immediate sexual health concerns and questions that may be overlooked. However, the volume of posts makes large scale manual analysis impractical, and extracting insights using conventional methods is difficult. This solution uses large language models (LLMs) to assist in and automate data extraction and analysis to better understand how users seek sexual health advice online.",Retired,Neither, , , , , , , , , , , , , , , , ,Unknown, , ,Unknown, , ,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"The AI will impact the data extraction and analysis processes for understanding how users seek sexual health advice online using Reddit.

In additional to traditional public health impacts this can include reducing administrative burdens and other ancillary benefits. This is an important criteria when selecting the initial round of projects.
Understand how and why people seek sexual health advice online. This may lead to finding gaps in online health resources, identifying underserved populations who resort to using online resources, and discovering changes in sexual health behavior over time. . Reddit is a social media website where users can submit and respond to posts around a wide variety of topics. Reddit is organized into subcommunities called subreddits, and many people turn to sexual health focused subreddits, such as the “r/STD” subreddit, for advice. The Reddit posts can be a valuable source of text and image data since these posts reveal immediate sexual health concerns and questions that may be overlooked. However, the volume of posts makes large scale manual analysis impractical, and extracting insights using conventional methods is difficult. This solution uses large language models (LLMs) to assist in and automate data extraction and analysis to better understand how users seek sexual health advice online.","the ai will impact the data extraction and analysis processes for understanding how users seek sexual health advice online using reddit. in additional to traditional public health impacts this can include reducing administrative burdens and other ancillary benefits. this is an important criteria when selecting the initial round of projects. understand how and why people seek sexual health advice online. this may lead to finding gaps in online health resources, identifying underserved populations who resort to using online resources, and discovering changes in sexual health behavior over time. . reddit is a social media website where users can submit and respond to posts around a wide variety of topics. reddit is organized into subcommunities called subreddits, and many people turn to sexual health focused subreddits, such as the “r/std” subreddit, for advice. the reddit posts can be a valuable source of text and image data since these posts reveal immediate sexual health concerns and questions that may be overlooked. however, the volume of posts makes large scale manual analysis impractical, and extracting insights using conventional methods is difficult. this solution uses large language models (llms) to assist in and automate data extraction and analysis to better understand how users seek sexual health advice online."
Retrieval Augmented Generation (RAG) with Q-Bank,Department of Health and Human Services,HHS,CDC,Mission-Enabling (internal agency support),None of the above.,"This solution will impact the process of searching CCQDER’s Q-Bank research publications on question evaluation for surveys. 

Q-Bank reports provide context needed to better understand and interpret data from survey estimates, as well as information about question performance and fit-for-use in various data collection contexts.
This project aims to improve access and visibility of CCQDER’s Q-Bank research publications on question evaluation for surveys. Additionally, indexing Q-Bank as part of this project should help to relieve some of the administrative burden of tagging and organizing reports.","This solution uses Retrieval Augmented Generation to develop an LLM-based search tool based on the Collaborating Center for Questionnaire Design and Evaluation Research’s (CCQDER) Q-Bank reports. The main motivation of the solution is to make it easier to navigate and identify reports relevant to a user’s research interests. Because the reports are already publicly available and they represent a relatively small corpus, they provide an excellent opportunity to develop prototype AI tools.",Retired,Neither, , , , , , , , , , , , , , , , ,Unknown, , ,Unknown, , ,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"This solution will impact the process of searching CCQDER’s Q-Bank research publications on question evaluation for surveys. 

Q-Bank reports provide context needed to better understand and interpret data from survey estimates, as well as information about question performance and fit-for-use in various data collection contexts.
This project aims to improve access and visibility of CCQDER’s Q-Bank research publications on question evaluation for surveys. Additionally, indexing Q-Bank as part of this project should help to relieve some of the administrative burden of tagging and organizing reports. . This solution uses Retrieval Augmented Generation to develop an LLM-based search tool based on the Collaborating Center for Questionnaire Design and Evaluation Research’s (CCQDER) Q-Bank reports. The main motivation of the solution is to make it easier to navigate and identify reports relevant to a user’s research interests. Because the reports are already publicly available and they represent a relatively small corpus, they provide an excellent opportunity to develop prototype AI tools.","this solution will impact the process of searching ccqder’s q-bank research publications on question evaluation for surveys. q-bank reports provide context needed to better understand and interpret data from survey estimates, as well as information about question performance and fit-for-use in various data collection contexts. this project aims to improve access and visibility of ccqder’s q-bank research publications on question evaluation for surveys. additionally, indexing q-bank as part of this project should help to relieve some of the administrative burden of tagging and organizing reports. . this solution uses retrieval augmented generation to develop an llm-based search tool based on the collaborating center for questionnaire design and evaluation research’s (ccqder) q-bank reports. the main motivation of the solution is to make it easier to navigate and identify reports relevant to a user’s research interests. because the reports are already publicly available and they represent a relatively small corpus, they provide an excellent opportunity to develop prototype ai tools."
Using generative AI to gain insight of older adult falls,Department of Health and Human Services,HHS,CDC,Health & Medical,None of the above.,"This solution will impact the process of extracting information from medical narratives 

Traditional methods of data and information extraction from medical narratives involve manual scrutiny and the use of rule-based models. These methods are time-consuming and can overlook crucial details. By utilizing generative AI, a more comprehensive understanding of the circumstances surrounding injuries from falls can be achieved. Understanding the specific causes, circumstances, and outcomes of falls can lead to more targeted public health interventions.",This solution extracts insights regarding older adult falls injuries from Emergency Department medical narratives and testing out methods submitted. The data source to be used is the National Electronic Injury Surveillance System (NEISS) data.,Retired,Neither, , , , , , , , , , , , , , , , ,Unknown, , ,Unknown, , ,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"This solution will impact the process of extracting information from medical narratives 

Traditional methods of data and information extraction from medical narratives involve manual scrutiny and the use of rule-based models. These methods are time-consuming and can overlook crucial details. By utilizing generative AI, a more comprehensive understanding of the circumstances surrounding injuries from falls can be achieved. Understanding the specific causes, circumstances, and outcomes of falls can lead to more targeted public health interventions. . This solution extracts insights regarding older adult falls injuries from Emergency Department medical narratives and testing out methods submitted. The data source to be used is the National Electronic Injury Surveillance System (NEISS) data.","this solution will impact the process of extracting information from medical narratives traditional methods of data and information extraction from medical narratives involve manual scrutiny and the use of rule-based models. these methods are time-consuming and can overlook crucial details. by utilizing generative ai, a more comprehensive understanding of the circumstances surrounding injuries from falls can be achieved. understanding the specific causes, circumstances, and outcomes of falls can lead to more targeted public health interventions. . this solution extracts insights regarding older adult falls injuries from emergency department medical narratives and testing out methods submitted. the data source to be used is the national electronic injury surveillance system (neiss) data."
AI-assisted comment triaging tool,Department of Health and Human Services,HHS,CMS,Mission-Enabling (internal agency support),None of the above.,"Annual rulemaking-- the Physician Fee Schedule receives thousands of public comments and require assistance with reviewing and assigning the comments to the particular topics they address. 

Assists with workload for the PFS team","We use AI to assist in comment review, such as identifying form letters. ",Operation and Maintenance,Neither,6/1/2022,12/1/2021,6/1/2022,Developed with contracting resources.,Unknown,No,No,No,No,Yes,Unknown,Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Annual rulemaking-- the Physician Fee Schedule receives thousands of public comments and require assistance with reviewing and assigning the comments to the particular topics they address. 

Assists with workload for the PFS team . We use AI to assist in comment review, such as identifying form letters.","annual rulemaking-- the physician fee schedule receives thousands of public comments and require assistance with reviewing and assigning the comments to the particular topics they address. assists with workload for the pfs team . we use ai to assist in comment review, such as identifying form letters."
Bankruptcy BOT,Department of Health and Human Services,HHS,CMS,Mission-Enabling (internal agency support),None of the above.,"The Bankruptcy/Overpayment Division of IFM is responsible for carrying out the Agency's oversight responsibilities for our contractors' debt management functions, and for protecting Medicare's interest in debt recoupment activities complicated by bankruptcy.

A timely identification of filed provider bankruptcies allows Medicare to secure recovery rights before timeframes to file a proof of claim expire.",The Bankruptcy BOT proactively identifies bankruptcy case with Medicare interest.,Operation and Maintenance,Neither,1/1/2020,1/1/2020,9/1/2021,Developed with contracting resources.,47QRAA18D001P,No,No,Yes,No,Yes,PECOS and BCRS,Documentation is missing or not available,No,"Yes – agency has access to source code, but it is not public.",Yes,PACER,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"The Bankruptcy/Overpayment Division of IFM is responsible for carrying out the Agency's oversight responsibilities for our contractors' debt management functions, and for protecting Medicare's interest in debt recoupment activities complicated by bankruptcy.

A timely identification of filed provider bankruptcies allows Medicare to secure recovery rights before timeframes to file a proof of claim expire. . The Bankruptcy BOT proactively identifies bankruptcy case with Medicare interest.","the bankruptcy/overpayment division of ifm is responsible for carrying out the agency's oversight responsibilities for our contractors' debt management functions, and for protecting medicare's interest in debt recoupment activities complicated by bankruptcy. a timely identification of filed provider bankruptcies allows medicare to secure recovery rights before timeframes to file a proof of claim expire. . the bankruptcy bot proactively identifies bankruptcy case with medicare interest."
Central Data Abstraction Tool-Modernized (Modernized-CDAT)- Intake Process Automation (PA) Tool,Department of Health and Human Services,HHS,CMS,Law & Justice,None of the above.,"Intake PA uses advanced capabilities (NLP, OCR, AI, ML) to automate, modernize, and reduce manual efforts related to medical record review functions within MA RADV audits

Eliminates manual review of Medical Record (MR) submissions, expedites the Intake process and realizes cost savings in both time and resources.",The PA-BOT tool will replace the RADV audit Intake manual functions for Steps 2 & 3 of the RADV audit process.  The RADV Intake process conducts a review of the MAO Medical Record (MR) submissions and validates that the MR submissions from MAOs have met RADV audit intake requirements and can proceed to MR abstraction/coding.,Operation and Maintenance,Neither,9/1/2017,11/1/2017,4/1/2019,Developed with contracting resources.,Unknown,No,No,Yes,No,No,"Yes, prior RADV results were used to train the models.",Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Central Data Abstraction Tool - Modernized (CDAT-M),Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Intake PA uses advanced capabilities (NLP, OCR, AI, ML) to automate, modernize, and reduce manual efforts related to medical record review functions within MA RADV audits

Eliminates manual review of Medical Record (MR) submissions, expedites the Intake process and realizes cost savings in both time and resources. . The PA-BOT tool will replace the RADV audit Intake manual functions for Steps 2 & 3 of the RADV audit process.  The RADV Intake process conducts a review of the MAO Medical Record (MR) submissions and validates that the MR submissions from MAOs have met RADV audit intake requirements and can proceed to MR abstraction/coding.","intake pa uses advanced capabilities (nlp, ocr, ai, ml) to automate, modernize, and reduce manual efforts related to medical record review functions within ma radv audits eliminates manual review of medical record (mr) submissions, expedites the intake process and realizes cost savings in both time and resources. . the pa-bot tool will replace the radv audit intake manual functions for steps 2 & 3 of the radv audit process. the radv intake process conducts a review of the mao medical record (mr) submissions and validates that the mr submissions from maos have met radv audit intake requirements and can proceed to mr abstraction/coding."
CMS Enterprise Portal Services (CMS Enterprise Portal-Chatbot),Department of Health and Human Services,HHS,CMS,Mission-Enabling (internal agency support),None of the above.,"CMS Enterprise Portal AI for Process Efficiency Improvement| Knowledge Management

 - Empowers users to find information or troubleshoot issues more efficiently
 - Reduces load on system help desks
 - Provides better insight into customer inquiries and issues informing process and design improvements
"," - Automated text-based bot providing support to Portal users
 - Uses machine learning via Amazon Lex to answer a user’s question
 - Takes user input and finds the best match to the question or problem statement
 - Automates tasks such as: customer support; processing of user questions; and solving user problems/issues",Operation and Maintenance,Neither,8/1/2020,12/1/2020,4/1/2021,Developed with contracting resources.,Unknown,No,No,No,No,No,CMS Enterprise Portal and CMS Identity Management (IDM) user guides,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CMS Enterprise Portal,Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"CMS Enterprise Portal AI for Process Efficiency Improvement| Knowledge Management

 - Empowers users to find information or troubleshoot issues more efficiently
 - Reduces load on system help desks
 - Provides better insight into customer inquiries and issues informing process and design improvements . - Automated text-based bot providing support to Portal users
 - Uses machine learning via Amazon Lex to answer a user’s question
 - Takes user input and finds the best match to the question or problem statement
 - Automates tasks such as: customer support; processing of user questions; and solving user problems/issues",cms enterprise portal ai for process efficiency improvement| knowledge management - empowers users to find information or troubleshoot issues more efficiently - reduces load on system help desks - provides better insight into customer inquiries and issues informing process and design improvements . - automated text-based bot providing support to portal users - uses machine learning via amazon lex to answer a user’s question - takes user input and finds the best match to the question or problem statement - automates tasks such as: customer support; processing of user questions; and solving user problems/issues
Enhanced Direct Enrollment Outlier Detection,Department of Health and Human Services,HHS,CMS,Mission-Enabling (internal agency support),None of the above.,"Enhanced Direct Enrollment (EDE) allows consumers to apply for and enroll in an exchange plan directly through an approved partner’s UI, without being redirected through the Healthcare.gov application. These partner systems directly interface with the APIs developed by the FFE. As EDE Partners gain more control over their application process, the FFE must ensure program integrity.

Ensure FFE EDE program integrity","This use-case will implement Machine Learning to identify anomalies/quality issues with partner-submitted person, application, and policy data?.",Operation and Maintenance,Neither,7/1/2019,1/1/2020,1/1/2020,Developed with contracting resources.,Unknown,No,No,No,No,Yes,CMS,Documentation is widely available,Yes,No – agency does not have access to source code.,Yes,FFM,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Enhanced Direct Enrollment (EDE) allows consumers to apply for and enroll in an exchange plan directly through an approved partner’s UI, without being redirected through the Healthcare.gov application. These partner systems directly interface with the APIs developed by the FFE. As EDE Partners gain more control over their application process, the FFE must ensure program integrity.

Ensure FFE EDE program integrity . This use-case will implement Machine Learning to identify anomalies/quality issues with partner-submitted person, application, and policy data?.","enhanced direct enrollment (ede) allows consumers to apply for and enroll in an exchange plan directly through an approved partner’s ui, without being redirected through the healthcare.gov application. these partner systems directly interface with the apis developed by the ffe. as ede partners gain more control over their application process, the ffe must ensure program integrity. ensure ffe ede program integrity . this use-case will implement machine learning to identify anomalies/quality issues with partner-submitted person, application, and policy data?."
Exchange Complaints Review Categorization  ML/AI ,Department of Health and Human Services,HHS,CMS,Law & Justice,None of the above.,"The Exchange Complaints Review Contractor (ECRC) receives hundreds of consumer complaints from the Federally-facilitated Exchange (FFE) call center daily, who then must manually read each complaint and categorize each complaint into one or more categories according to the Complaint Review Standard Operating Procedure (SOP). The categorization assists CPI in determining the nature of the complaint, the action necessary, and the component within CMS who the complaint should be routed to. In general, each complaint can take up to 10 minutes to review and categorize, including a quality review component.   

The output (list of all complaints categorized) allows CMS to expediate routing the complaint to the appropriate CMS caseworker or Issuer to take the appropriate action necessary to make the consumer whole and ensure they can receive the necessary medical treatment. The output when done by AI is done in a matter of minutes, versus weeks when completed by human reviewers. ","CPI utilizes machine learning/artificial intelligence (AI) and continuously feeds the complaints review model thousands of complaints in order to teach the model how to accurately categorize the complaints, based on keywords. This is done in a matter of minutes instead of weeks by a human complaint reviewer.",Operation and Maintenance,Neither,1/1/2022,1/1/2022,10/1/2024,Developed with both contracting and in-house resources.,Unknown,No,No,Yes,No,Yes,"HICS, Marketplace",Documentation has been partially completed,Yes,"Yes – agency has access to source code, but it is not public.",Yes,MIDAS,6-12 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The Exchange Complaints Review Contractor (ECRC) receives hundreds of consumer complaints from the Federally-facilitated Exchange (FFE) call center daily, who then must manually read each complaint and categorize each complaint into one or more categories according to the Complaint Review Standard Operating Procedure (SOP). The categorization assists CPI in determining the nature of the complaint, the action necessary, and the component within CMS who the complaint should be routed to. In general, each complaint can take up to 10 minutes to review and categorize, including a quality review component.   

The output (list of all complaints categorized) allows CMS to expediate routing the complaint to the appropriate CMS caseworker or Issuer to take the appropriate action necessary to make the consumer whole and ensure they can receive the necessary medical treatment. The output when done by AI is done in a matter of minutes, versus weeks when completed by human reviewers. . CPI utilizes machine learning/artificial intelligence (AI) and continuously feeds the complaints review model thousands of complaints in order to teach the model how to accurately categorize the complaints, based on keywords. This is done in a matter of minutes instead of weeks by a human complaint reviewer.","the exchange complaints review contractor (ecrc) receives hundreds of consumer complaints from the federally-facilitated exchange (ffe) call center daily, who then must manually read each complaint and categorize each complaint into one or more categories according to the complaint review standard operating procedure (sop). the categorization assists cpi in determining the nature of the complaint, the action necessary, and the component within cms who the complaint should be routed to. in general, each complaint can take up to 10 minutes to review and categorize, including a quality review component. the output (list of all complaints categorized) allows cms to expediate routing the complaint to the appropriate cms caseworker or issuer to take the appropriate action necessary to make the consumer whole and ensure they can receive the necessary medical treatment. the output when done by ai is done in a matter of minutes, versus weeks when completed by human reviewers. . cpi utilizes machine learning/artificial intelligence (ai) and continuously feeds the complaints review model thousands of complaints in order to teach the model how to accurately categorize the complaints, based on keywords. this is done in a matter of minutes instead of weeks by a human complaint reviewer."
Feedback Analysis Solution (FAS),Department of Health and Human Services,HHS,CMS,Mission-Enabling (internal agency support),None of the above.,"The Feedback Analysis Solution is a system that uses CMS or other publicly available data (such as Regulations.Gov) to review public comments and/or analyze other information from internal and external stakeholders. The FAS uses Natural Language Processing (NLP) tools to aggregate, sort and identify duplicates to create efficiencies in the comment review process. FAS also uses machine learning (ML) tools to identify topics, themes and sentiment outputs for the targeted dataset.  

FAS help categorize stakeholder comments or feedback (collected in multiple venues), thereby enabling analysts to use the system to quickly identify comments that may impact program/policy decisions.","Feedback Analysis Solution (FAS) uses AI/ML/NLP tools to identify topics, themes, and sentiments outputs for the target datasets. Groups and offices within CCSQ use the solution in slightly different ways to ingest unstructured data, interpret and structure it for use in ML models after de-duplication of comments.",Operation and Maintenance,Neither,1/1/2021,9/1/2021,9/1/2021,Developed with both contracting and in-house resources.,75FCMC22A0018/75FCMC23F0001,No,No,No,No,Yes,CMS,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CDRAP,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The Feedback Analysis Solution is a system that uses CMS or other publicly available data (such as Regulations.Gov) to review public comments and/or analyze other information from internal and external stakeholders. The FAS uses Natural Language Processing (NLP) tools to aggregate, sort and identify duplicates to create efficiencies in the comment review process. FAS also uses machine learning (ML) tools to identify topics, themes and sentiment outputs for the targeted dataset.  

FAS help categorize stakeholder comments or feedback (collected in multiple venues), thereby enabling analysts to use the system to quickly identify comments that may impact program/policy decisions. . Feedback Analysis Solution (FAS) uses AI/ML/NLP tools to identify topics, themes, and sentiments outputs for the target datasets. Groups and offices within CCSQ use the solution in slightly different ways to ingest unstructured data, interpret and structure it for use in ML models after de-duplication of comments.","the feedback analysis solution is a system that uses cms or other publicly available data (such as regulations.gov) to review public comments and/or analyze other information from internal and external stakeholders. the fas uses natural language processing (nlp) tools to aggregate, sort and identify duplicates to create efficiencies in the comment review process. fas also uses machine learning (ml) tools to identify topics, themes and sentiment outputs for the targeted dataset. fas help categorize stakeholder comments or feedback (collected in multiple venues), thereby enabling analysts to use the system to quickly identify comments that may impact program/policy decisions. . feedback analysis solution (fas) uses ai/ml/nlp tools to identify topics, themes, and sentiments outputs for the target datasets. groups and offices within ccsq use the solution in slightly different ways to ingest unstructured data, interpret and structure it for use in ml models after de-duplication of comments."
IT System Utilization Optimization,Department of Health and Human Services,HHS,CMS,Mission-Enabling (internal agency support),None of the above.,"The FFE application usage patterns vary and are dependent on differing environment usage periods. CMS resources are currently manually scaled, not allowing immediate actions in correlation with usage changes.

Automation of application scaling",This use-case will implement Machine Learning to determine optimized infrastructure/application scaling to support system volume.,Operation and Maintenance,Neither,7/1/2019,1/1/2020,1/1/2020,Developed with contracting resources.,Unknown,No,No,No,No,Yes,CMS,Documentation is widely available,Yes,No – agency does not have access to source code.,Yes,FFM,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The FFE application usage patterns vary and are dependent on differing environment usage periods. CMS resources are currently manually scaled, not allowing immediate actions in correlation with usage changes.

Automation of application scaling . This use-case will implement Machine Learning to determine optimized infrastructure/application scaling to support system volume.","the ffe application usage patterns vary and are dependent on differing environment usage periods. cms resources are currently manually scaled, not allowing immediate actions in correlation with usage changes. automation of application scaling . this use-case will implement machine learning to determine optimized infrastructure/application scaling to support system volume."
Knowledge Management Platform,Department of Health and Human Services,HHS,CMS,Mission-Enabling (internal agency support),None of the above.,"The business process it affects involves accessing and using structured and unstructured data sources to enable data-driven decision-making. 

It helps by creating efficiencies, reducing burden, and generating an increased return on investment in the achievement of CMS's mission and business objectives.","It leverages natural language processing, knowledge graphs, and large language models to make the knowledge within structured and unstructured data sources accessible and meaningful to users.",Operation and Maintenance,Neither,9/1/2021,10/1/2021,11/1/2021,Developed with contracting resources.,KMP / SEAS IT,No,No,No,No,No,CMS,Documentation has been partially completed,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CEDAR,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The business process it affects involves accessing and using structured and unstructured data sources to enable data-driven decision-making. 

It helps by creating efficiencies, reducing burden, and generating an increased return on investment in the achievement of CMS's mission and business objectives. . It leverages natural language processing, knowledge graphs, and large language models to make the knowledge within structured and unstructured data sources accessible and meaningful to users.","the business process it affects involves accessing and using structured and unstructured data sources to enable data-driven decision-making. it helps by creating efficiencies, reducing burden, and generating an increased return on investment in the achievement of cms's mission and business objectives. . it leverages natural language processing, knowledge graphs, and large language models to make the knowledge within structured and unstructured data sources accessible and meaningful to users."
Risk Adjustment Outlier Analysis,Department of Health and Human Services,HHS,CMS,Mission-Enabling (internal agency support),None of the above.,"The RA program spreads the financial risk borne by Issuers due to offering a variety of plans meeting the need of the diverse population. RA payments are distributed based on population risk levels. The FFE uses a distributed data solution to calculate plan average actuarial risk and associated payments and must avoid potential impacts to annual calculations.

Maintain RA Program risk integrity",This use-case will implement Machine Learning to identify outliers in issuer data / behavior that may unduly influence risk adjustment payments.,Operation and Maintenance,Neither,7/1/2024,6/1/2024,1/1/2020,Developed with contracting resources.,Unknown,No,No,No,No,No,CMS,Documentation is widely available,Yes,No – agency does not have access to source code.,Yes,FFM,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The RA program spreads the financial risk borne by Issuers due to offering a variety of plans meeting the need of the diverse population. RA payments are distributed based on population risk levels. The FFE uses a distributed data solution to calculate plan average actuarial risk and associated payments and must avoid potential impacts to annual calculations.

Maintain RA Program risk integrity . This use-case will implement Machine Learning to identify outliers in issuer data / behavior that may unduly influence risk adjustment payments.",the ra program spreads the financial risk borne by issuers due to offering a variety of plans meeting the need of the diverse population. ra payments are distributed based on population risk levels. the ffe uses a distributed data solution to calculate plan average actuarial risk and associated payments and must avoid potential impacts to annual calculations. maintain ra program risk integrity . this use-case will implement machine learning to identify outliers in issuer data / behavior that may unduly influence risk adjustment payments.
Agent/Broker Fraud Analysis,Department of Health and Human Services,HHS,CMS,Mission-Enabling (internal agency support),None of the above.,"Agent/Brokers (A/Bs) support the consumer enrollment and eligibility process. Because of this, they have learned the intricate details of the FFE for accessing applications, submitting eligibility determinations, and adding enrollments to their line of business, opening up the possibility of fraud. 

Improves the oversight and integrity of Marketplace Applications and Enrollments assisted by Agents and Brokers.  Helps CCIIO identify suspicious behaviors to refer to CPI.",Utilizes Machine Learning to identify suspicious behaviors of Agents and Brokers in the Marketplace,Operation and Maintenance,Neither,7/1/2024,1/1/2024,1/1/2024,Developed with contracting resources.,Unknown,No,No,Yes,No,No,CMS,Documentation is widely available,Yes,No – agency does not have access to source code.,Yes,FFM,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Agent/Brokers (A/Bs) support the consumer enrollment and eligibility process. Because of this, they have learned the intricate details of the FFE for accessing applications, submitting eligibility determinations, and adding enrollments to their line of business, opening up the possibility of fraud. 

Improves the oversight and integrity of Marketplace Applications and Enrollments assisted by Agents and Brokers.  Helps CCIIO identify suspicious behaviors to refer to CPI. . Utilizes Machine Learning to identify suspicious behaviors of Agents and Brokers in the Marketplace","agent/brokers (a/bs) support the consumer enrollment and eligibility process. because of this, they have learned the intricate details of the ffe for accessing applications, submitting eligibility determinations, and adding enrollments to their line of business, opening up the possibility of fraud. improves the oversight and integrity of marketplace applications and enrollments assisted by agents and brokers. helps cciio identify suspicious behaviors to refer to cpi. . utilizes machine learning to identify suspicious behaviors of agents and brokers in the marketplace"
CCSQ ServiceNow AI Search,Department of Health and Human Services,HHS,CMS,Government Services (includes Benefits and Service Delivery),None of the above.,"CCSQ ServiceNow AI Search is a product inside ServiceNow (SaaS).  It replaces traditional Zing search tool (exact search), and enables users with more flexible searches to get relevant and actionable answers quickly.

* Improve search relevance
* Promote self-service by empowering users to find information independently, and potential reduce number of cases","AI Search will
* Display most relevant results first
* Support synonyms, auto-corrections, stop words, and auto-completion
AI Search analytics will provide insights into search usage, performance, trends, metrics, and how to improve search experiences.
Plan to do AI Search tune-up next",Operation and Maintenance,Neither,11/1/2023,2/1/2024,4/1/2024,Developed with contracting resources.,Unknown,No,Yes,No,No,No,"ServiceNow knowledge articles, ideas, catalog items data",Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,QSC,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"CCSQ ServiceNow AI Search is a product inside ServiceNow (SaaS).  It replaces traditional Zing search tool (exact search), and enables users with more flexible searches to get relevant and actionable answers quickly.

* Improve search relevance
* Promote self-service by empowering users to find information independently, and potential reduce number of cases . AI Search will
* Display most relevant results first
* Support synonyms, auto-corrections, stop words, and auto-completion
AI Search analytics will provide insights into search usage, performance, trends, metrics, and how to improve search experiences.
Plan to do AI Search tune-up next","ccsq servicenow ai search is a product inside servicenow (saas). it replaces traditional zing search tool (exact search), and enables users with more flexible searches to get relevant and actionable answers quickly. * improve search relevance * promote self-service by empowering users to find information independently, and potential reduce number of cases . ai search will * display most relevant results first * support synonyms, auto-corrections, stop words, and auto-completion ai search analytics will provide insights into search usage, performance, trends, metrics, and how to improve search experiences. plan to do ai search tune-up next"
MSP Assignment Bot,Department of Health and Human Services,HHS,CMS,Mission-Enabling (internal agency support),None of the above.,"This impacts automatically retrieving records from a contractor's system, and importing them into an internal OPOLE Access database.  Once populated in the database the record is assigned to staff to review and adjudicate.

Pulls more cases out of BCRC and assigns much faster than an analyst. By allowing BOT to pull and assign cases instead of an analyst, this approach provides maximum time for staff to process cases which decreases the response time.",The Medicare Secondary Payer Operations Group (MSPOG) uses a BOT to pull compromise cases out of BCRC and assign to staff in our internal Management Information System (MIS),Implementation and Assessment,Neither,3/1/2022,4/1/2022,8/1/2022,Developed in-house.,Unknown,No,No,Yes,No,Yes,case turn around times,Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"This impacts automatically retrieving records from a contractor's system, and importing them into an internal OPOLE Access database.  Once populated in the database the record is assigned to staff to review and adjudicate.

Pulls more cases out of BCRC and assigns much faster than an analyst. By allowing BOT to pull and assign cases instead of an analyst, this approach provides maximum time for staff to process cases which decreases the response time. . The Medicare Secondary Payer Operations Group (MSPOG) uses a BOT to pull compromise cases out of BCRC and assign to staff in our internal Management Information System (MIS)","this impacts automatically retrieving records from a contractor's system, and importing them into an internal opole access database. once populated in the database the record is assigned to staff to review and adjudicate. pulls more cases out of bcrc and assigns much faster than an analyst. by allowing bot to pull and assign cases instead of an analyst, this approach provides maximum time for staff to process cases which decreases the response time. . the medicare secondary payer operations group (mspog) uses a bot to pull compromise cases out of bcrc and assign to staff in our internal management information system (mis)"
ICPG Governance Tool,Department of Health and Human Services,HHS,CMS,Mission-Enabling (internal agency support),None of the above.,"The business process it will affect involves the processing and analysis of acquisition plans (APs).

It will help by streamlining the manual steps involved in validating and classifying acquisition plans, reducing the time and effort required. ",It will automate the processing and analysis of acquisition plans and classify them as IT-related or non-IT.,Implementation and Assessment,Neither,9/1/2021,9/1/2023,Unknown,Developed with contracting resources.,KMP,No,No,No,No,No,CMS,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CEDAR,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The business process it will affect involves the processing and analysis of acquisition plans (APs).

It will help by streamlining the manual steps involved in validating and classifying acquisition plans, reducing the time and effort required. . It will automate the processing and analysis of acquisition plans and classify them as IT-related or non-IT.","the business process it will affect involves the processing and analysis of acquisition plans (aps). it will help by streamlining the manual steps involved in validating and classifying acquisition plans, reducing the time and effort required. . it will automate the processing and analysis of acquisition plans and classify them as it-related or non-it."
Innovation Center Investment Proposal (ICIP) Model Analysis POC,Department of Health and Human Services,HHS,CMS,Mission-Enabling (internal agency support),None of the above.,"It will affect the process of document analysis, specifically the analysis of ICIP documents.

It will help in  ICIP document analysis, with the goal of extracting of insights and recommendations from these documents.","It will extract relevant insights and recommendations from the ICIPs, making use of a RAG-based architecture, and present them through a natural language interface. This interface will allow users to interact with the documents and retrieve the information they need easily.",Implementation and Assessment,Neither,8/1/2024,8/1/2024,Unknown,Developed with contracting resources.,PPMS,No,No,No,No,No,ICIPs,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CEDAR,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"It will affect the process of document analysis, specifically the analysis of ICIP documents.

It will help in  ICIP document analysis, with the goal of extracting of insights and recommendations from these documents. . It will extract relevant insights and recommendations from the ICIPs, making use of a RAG-based architecture, and present them through a natural language interface. This interface will allow users to interact with the documents and retrieve the information they need easily.","it will affect the process of document analysis, specifically the analysis of icip documents. it will help in icip document analysis, with the goal of extracting of insights and recommendations from these documents. . it will extract relevant insights and recommendations from the icips, making use of a rag-based architecture, and present them through a natural language interface. this interface will allow users to interact with the documents and retrieve the information they need easily."
OAGM Rate Card,Department of Health and Human Services,HHS,CMS,Mission-Enabling (internal agency support),None of the above.,"The business process it will affect involves the analysis of historical price history/trends for labor categories in business proposals for the Office of Acquisition and Grants Management (OAGM).

It will help OAGM make better decisions on future contracts by providing them with a comprehensive and standardized view of historical price data for labor categories.","It will assist in normalizing labor categories, enabling the OAGM team to easily identify the most relevant labor category despite the variations in naming conventions found in business proposals.",Implementation and Assessment,Neither,9/1/2021,11/1/2022,Unknown,Developed with contracting resources.,SEAS IT,No,No,No,No,No,CMS,Documentation has been partially completed,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CEDAR,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The business process it will affect involves the analysis of historical price history/trends for labor categories in business proposals for the Office of Acquisition and Grants Management (OAGM).

It will help OAGM make better decisions on future contracts by providing them with a comprehensive and standardized view of historical price data for labor categories. . It will assist in normalizing labor categories, enabling the OAGM team to easily identify the most relevant labor category despite the variations in naming conventions found in business proposals.","the business process it will affect involves the analysis of historical price history/trends for labor categories in business proposals for the office of acquisition and grants management (oagm). it will help oagm make better decisions on future contracts by providing them with a comprehensive and standardized view of historical price data for labor categories. . it will assist in normalizing labor categories, enabling the oagm team to easily identify the most relevant labor category despite the variations in naming conventions found in business proposals."
CCSQ Now Assist for CSM ,Department of Health and Human Services,HHS,CMS,Government Services (includes Benefits and Service Delivery),None of the above.,"CCSQ Now Assist for CSM (Customer Service Management) is a product inside ServiceNow (SaaS).  It integrates Gen AI with CSM, and uses Now LLM (ServiceNow native Large Language Model) to generate contents based on machine learning (ML).  Now Assist for CSM helps agents to improve productivity and efficiency and deliver better services.  

Improve agents' responsiveness and productivity
* Quickly get familiar with a case/chat by getting case/chat summarization
* Quickly resolve the case by using auto-generated resolution notes.","Agents have quick access to 
- Case summarization
- Chat and agent hand-off summarization
- Resolution Notes Generation",Implementation and Assessment,Neither,2/1/2024,5/1/2024,Unknown,Developed with contracting resources.,Not Reported,No,No,No,No,Yes,Historical ServiceNow case data,Documentation is missing or not available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,QSC,Less than 6 months,Yes,Yes,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"CCSQ Now Assist for CSM (Customer Service Management) is a product inside ServiceNow (SaaS).  It integrates Gen AI with CSM, and uses Now LLM (ServiceNow native Large Language Model) to generate contents based on machine learning (ML).  Now Assist for CSM helps agents to improve productivity and efficiency and deliver better services.  

Improve agents' responsiveness and productivity
* Quickly get familiar with a case/chat by getting case/chat summarization
* Quickly resolve the case by using auto-generated resolution notes. . Agents have quick access to 
- Case summarization
- Chat and agent hand-off summarization
- Resolution Notes Generation","ccsq now assist for csm (customer service management) is a product inside servicenow (saas). it integrates gen ai with csm, and uses now llm (servicenow native large language model) to generate contents based on machine learning (ml). now assist for csm helps agents to improve productivity and efficiency and deliver better services. improve agents' responsiveness and productivity * quickly get familiar with a case/chat by getting case/chat summarization * quickly resolve the case by using auto-generated resolution notes. . agents have quick access to - case summarization - chat and agent hand-off summarization - resolution notes generation"
AI Workspace,Department of Health and Human Services,HHS,CMS,Mission-Enabling (internal agency support),None of the above.,"The business process this affects involves the ability to easily run AI experiments in a safe and secure environment. 

It helps by reducing barriers to AI experimentation, supporting strategic AI infrastructure enablement, and providing hands-on experience for teams to learn and understand the potential risks and benefits of AI.","It utilizes data science tooling, large language models, and other approved AWS services such as Sage Maker and Bedrock to provide a secure and supportive environment for AI experimentation and prototype development.",Acquisition and/or Development,Neither,9/1/2024,3/1/2024,Unknown,Developed with contracting resources.,SEAS IT,No,Unknown,Unknown,Unknown,No,Unknown,Documentation is missing or not available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CEDAR,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"The business process this affects involves the ability to easily run AI experiments in a safe and secure environment. 

It helps by reducing barriers to AI experimentation, supporting strategic AI infrastructure enablement, and providing hands-on experience for teams to learn and understand the potential risks and benefits of AI. . It utilizes data science tooling, large language models, and other approved AWS services such as Sage Maker and Bedrock to provide a secure and supportive environment for AI experimentation and prototype development.","the business process this affects involves the ability to easily run ai experiments in a safe and secure environment. it helps by reducing barriers to ai experimentation, supporting strategic ai infrastructure enablement, and providing hands-on experience for teams to learn and understand the potential risks and benefits of ai. . it utilizes data science tooling, large language models, and other approved aws services such as sage maker and bedrock to provide a secure and supportive environment for ai experimentation and prototype development."
Citation Analysis and Survey Assistant (CASA - Nursing Home Survey CMS 2567),Department of Health and Human Services,HHS,CMS,Law & Justice,None of the above.,"2567 (CASA) enhances the efficiency and effectiveness of monitoring and reviewing nursing home surveys across the US. It enhances how the Quality, Safety, and Oversight Group (QSOG) and Survey Operations Group (SOG) assess how State Survey Agencies (SSAs) are citing nursing home deficiencies, reported on CMS Form 2567, by employing advanced natural language processing and machine learning technologies.

 The system would support the Nursing Home Survey CMS Form 2567 survey work allowing QSOG and SOG staff to follow up on cases of under and over citations at the facilities that are flagged by the system and sent to the State Agencies","It will allow for real-time interaction with survey data, facilitating a deeper understanding of trends and individual case specifics. By flagging discrepancies in citations and providing a robust platform for detailed analysis. CASA supports CMS’s mission to ensure high standards of care and accountability in nursing homes",Acquisition and/or Development,Neither,9/1/2023,9/1/2024,Unknown,Developed with both contracting and in-house resources.,Unknown,No,Unknown,Unknown,Unknown,Yes,CMS,Documentation has been partially completed,Unknown,Unknown,Yes,CDRAP,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"2567 (CASA) enhances the efficiency and effectiveness of monitoring and reviewing nursing home surveys across the US. It enhances how the Quality, Safety, and Oversight Group (QSOG) and Survey Operations Group (SOG) assess how State Survey Agencies (SSAs) are citing nursing home deficiencies, reported on CMS Form 2567, by employing advanced natural language processing and machine learning technologies.

 The system would support the Nursing Home Survey CMS Form 2567 survey work allowing QSOG and SOG staff to follow up on cases of under and over citations at the facilities that are flagged by the system and sent to the State Agencies . It will allow for real-time interaction with survey data, facilitating a deeper understanding of trends and individual case specifics. By flagging discrepancies in citations and providing a robust platform for detailed analysis. CASA supports CMS’s mission to ensure high standards of care and accountability in nursing homes","2567 (casa) enhances the efficiency and effectiveness of monitoring and reviewing nursing home surveys across the us. it enhances how the quality, safety, and oversight group (qsog) and survey operations group (sog) assess how state survey agencies (ssas) are citing nursing home deficiencies, reported on cms form 2567, by employing advanced natural language processing and machine learning technologies. the system would support the nursing home survey cms form 2567 survey work allowing qsog and sog staff to follow up on cases of under and over citations at the facilities that are flagged by the system and sent to the state agencies . it will allow for real-time interaction with survey data, facilitating a deeper understanding of trends and individual case specifics. by flagging discrepancies in citations and providing a robust platform for detailed analysis. casa supports cms’s mission to ensure high standards of care and accountability in nursing homes"
Hazards & Threats Awareness Data Processing Automation,Department of Health and Human Services,HHS,CMS,Mission-Enabling (internal agency support),None of the above.,"EPRO is responsible for developing agency-wide situation reports from various open web and email materials. 

More effective prioritization and processing of materials for situational awareness, faster delivery of usable work product to decision-makers agency-wide. The manual process takes 2-3 hours per day, and the AI system will reduce the work to a human quality assurance/inspection step estimated to take approximately 30 minutes.","The AI will scrape known websites and monitor relevant resource mailboxes, use a categorization algorithm, and summarize and format the significant events each morning.",Acquisition and/or Development,Neither,11/1/2024,11/1/2024,Unknown,Developed with both contracting and in-house resources.,SEAS IT,No,Unknown,Unknown,Unknown,Yes,EPRO,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Hazard and Threat Awareness,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"EPRO is responsible for developing agency-wide situation reports from various open web and email materials. 

More effective prioritization and processing of materials for situational awareness, faster delivery of usable work product to decision-makers agency-wide. The manual process takes 2-3 hours per day, and the AI system will reduce the work to a human quality assurance/inspection step estimated to take approximately 30 minutes. . The AI will scrape known websites and monitor relevant resource mailboxes, use a categorization algorithm, and summarize and format the significant events each morning.","epro is responsible for developing agency-wide situation reports from various open web and email materials. more effective prioritization and processing of materials for situational awareness, faster delivery of usable work product to decision-makers agency-wide. the manual process takes 2-3 hours per day, and the ai system will reduce the work to a human quality assurance/inspection step estimated to take approximately 30 minutes. . the ai will scrape known websites and monitor relevant resource mailboxes, use a categorization algorithm, and summarize and format the significant events each morning."
Independent Dispute Resolution (IDR) Eligibility Rules Engine,Department of Health and Human Services,HHS,CMS,Mission-Enabling (internal agency support),None of the above.,"The current Independent Dispute Resolution (IDR) Technical Assistance (TA) process is very manual and time intensive which limits throughput.  The rules engine should significantly expedite processing and expand capacity. Automating more of the process may also increase consistency across recommendations and result in a more predictable timeframe for the workflow by better positioning disputes for analyst review.

The AI tool will help automate the eligibility review process, reducing time-intensive manual steps and increasing consistency of results.  ","Use of artificial intelligence (AI) models to identify the presence or absence of necessary data points within documentation.  AI tool searches documentation to identify and store necessary data points such as document title, file type, payment date, service code, claim number, and date of service. ",Acquisition and/or Development,Neither,8/1/2024,9/1/2024,Unknown,Developed with contracting resources.,Unknown,No,Unknown,Unknown,Unknown,Yes,Federal Independent Dispute Resolution (IDR) Dispute Data,Documentation is complete,Unknown,Unknown,Yes,AWS,Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"The current Independent Dispute Resolution (IDR) Technical Assistance (TA) process is very manual and time intensive which limits throughput.  The rules engine should significantly expedite processing and expand capacity. Automating more of the process may also increase consistency across recommendations and result in a more predictable timeframe for the workflow by better positioning disputes for analyst review.

The AI tool will help automate the eligibility review process, reducing time-intensive manual steps and increasing consistency of results. . Use of artificial intelligence (AI) models to identify the presence or absence of necessary data points within documentation.  AI tool searches documentation to identify and store necessary data points such as document title, file type, payment date, service code, claim number, and date of service.","the current independent dispute resolution (idr) technical assistance (ta) process is very manual and time intensive which limits throughput. the rules engine should significantly expedite processing and expand capacity. automating more of the process may also increase consistency across recommendations and result in a more predictable timeframe for the workflow by better positioning disputes for analyst review. the ai tool will help automate the eligibility review process, reducing time-intensive manual steps and increasing consistency of results. . use of artificial intelligence (ai) models to identify the presence or absence of necessary data points within documentation. ai tool searches documentation to identify and store necessary data points such as document title, file type, payment date, service code, claim number, and date of service."
Medicare Fee for Service Requirements Modernization (MFRM),Department of Health and Human Services,HHS,CMS,Government Services (includes Benefits and Service Delivery),None of the above.,"EIG is developing a business process to parse, curate, and simplify Medicare coverage and coding requirements by medical condition using AI LLMs. 

After rigorous testing for accuracy, AI output will be uploaded to the MFRM application to be used to enhance CMS and healthcare operational efficiencies, including organization of guidance documents, and identification of source complexities to be triaged with owners.
","Utilizing an LLM (Anthropic’s Claude 3.5) to curate current, accurate, and reliable Medicare requirements information sourced from FFS policy and guidance documents.",Acquisition and/or Development,Neither,1/1/2021,1/1/2024,Unknown,Developed with contracting resources.,Unknown,No,Unknown,Unknown,Unknown,No,Medicare Fee-for-Service coverage and coding information,Documentation has been partially completed,Unknown,Unknown,No,Unknown,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"EIG is developing a business process to parse, curate, and simplify Medicare coverage and coding requirements by medical condition using AI LLMs. 

After rigorous testing for accuracy, AI output will be uploaded to the MFRM application to be used to enhance CMS and healthcare operational efficiencies, including organization of guidance documents, and identification of source complexities to be triaged with owners. . Utilizing an LLM (Anthropic’s Claude 3.5) to curate current, accurate, and reliable Medicare requirements information sourced from FFS policy and guidance documents.","eig is developing a business process to parse, curate, and simplify medicare coverage and coding requirements by medical condition using ai llms. after rigorous testing for accuracy, ai output will be uploaded to the mfrm application to be used to enhance cms and healthcare operational efficiencies, including organization of guidance documents, and identification of source complexities to be triaged with owners. . utilizing an llm (anthropic’s claude 3.5) to curate current, accurate, and reliable medicare requirements information sourced from ffs policy and guidance documents."
Review Regulatory Comments,Department of Health and Human Services,HHS,CMS,Mission-Enabling (internal agency support),None of the above.,"The Division of Issuer Management and Operations is considering having our contractor, AIR, use an AI tool for comment analysis in the Letter to Issuers (LTI) and Notice of Benefits and Payment Parameters (NBPP). This would streamline review of stakeholder comments. 

This tool would reduce the amount of time and resources CCIIO and contracting staff would have to review the LTI and NBPP by 25%. ",The AI tool would identify trends in responses from stakeholders in policy and operational issues proposed by CMS.  ,Initiated,Neither,12/1/2023,Unknown,Unknown,Developed with contracting resources.,Not Reported,No,Unknown,Unknown,Unknown,Unknown,CMS,Unknown,Unknown,Unknown,Yes,Not Reported,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.2857142857142857,"The Division of Issuer Management and Operations is considering having our contractor, AIR, use an AI tool for comment analysis in the Letter to Issuers (LTI) and Notice of Benefits and Payment Parameters (NBPP). This would streamline review of stakeholder comments. 

This tool would reduce the amount of time and resources CCIIO and contracting staff would have to review the LTI and NBPP by 25%. . The AI tool would identify trends in responses from stakeholders in policy and operational issues proposed by CMS.","the division of issuer management and operations is considering having our contractor, air, use an ai tool for comment analysis in the letter to issuers (lti) and notice of benefits and payment parameters (nbpp). this would streamline review of stakeholder comments. this tool would reduce the amount of time and resources cciio and contracting staff would have to review the lti and nbpp by 25%. . the ai tool would identify trends in responses from stakeholders in policy and operational issues proposed by cms."
Improved Data Quality Checks,Department of Health and Human Services,HHS,CMS,Mission-Enabling (internal agency support),None of the above.,"Improve consumer experience by providing an asynchronous solution to detect and provide near real time feedback via outreach to the consumer shortening the overall return cycle time without requiring UI changes. 

Improvement of user experience", develop a POC classifier model to identify incorrect document upload types / low-quality images through use of OCR.,Acquisition and/or Development,Neither,7/1/2024,1/1/2024,Unknown,Developed with contracting resources.,Unknown,No,Unknown,Unknown,Unknown,No,CMS,Documentation is widely available,Unknown,Unknown,Yes,FFM,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"Improve consumer experience by providing an asynchronous solution to detect and provide near real time feedback via outreach to the consumer shortening the overall return cycle time without requiring UI changes. 

Improvement of user experience . develop a POC classifier model to identify incorrect document upload types / low-quality images through use of OCR.",improve consumer experience by providing an asynchronous solution to detect and provide near real time feedback via outreach to the consumer shortening the overall return cycle time without requiring ui changes. improvement of user experience . develop a poc classifier model to identify incorrect document upload types / low-quality images through use of ocr.
Plan Certification Recommendation Model,Department of Health and Human Services,HHS,CMS,Mission-Enabling (internal agency support),None of the above.,"Identify trends/patterns within plan certification justification templates, containing free-form text data through use of NLP and classification techniques. 

Improve efficiency of the plan certification review process", build a supervised machine learning model using historical justification data and associated plan certification outcomes that can be recommended to CMS to build towards a more efficient review process.,Acquisition and/or Development,Neither,7/1/2024,1/1/2024,Unknown,Developed with contracting resources.,Not Reported,No,Unknown,Unknown,Unknown,No,CMS,Documentation is widely available,Unknown,Unknown,Yes,FFM,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"Identify trends/patterns within plan certification justification templates, containing free-form text data through use of NLP and classification techniques. 

Improve efficiency of the plan certification review process . build a supervised machine learning model using historical justification data and associated plan certification outcomes that can be recommended to CMS to build towards a more efficient review process.","identify trends/patterns within plan certification justification templates, containing free-form text data through use of nlp and classification techniques. improve efficiency of the plan certification review process . build a supervised machine learning model using historical justification data and associated plan certification outcomes that can be recommended to cms to build towards a more efficient review process."
AI to Improve Public Access to the Administrative Appeals Process,Department of Health and Human Services,HHS,DAB,Mission-Enabling (internal agency support),None of the above.,"The DAB adjudicates  disputes under more than 60 statutory authorities.  The workload is divided between the DAB's four adjudicatory divisions (e.g., the Board, the Civil Remedies Division, the Medicare Operations Division and the Alternative Disputes Resolution Division. Helping appellants decide which entity within the DAB is responsible for adjudicating their appeal will increase efficiency in adjudication and customer experience. 

The DAB's appellant community (e.g., e-file users) will be provided immediate assistance and accurate information; deaf and hard-of-hearing populations and individuals with limited English proficiency are helped without the use of intermediaries, such as interpreters, translators, and relay services. After determining the appropriate division within the DAB responsible for adjudicating the appeal, the Chatbot can provide a link to the e-file website directly connecting the user to the correct e-file system.  Other benefits include: Improve public access to the appeals process by providing instant, accurate and complete filing information; Increase the DAB's productivity by refocusing the workforce to case resolution instead of responding the filing inquires and processing and appropriately routing misfiled appeals; and Improve resource allocation based on each Division's actual workload.  ","A chatbot on the DAB's website can direct filers to the correct Division and e-file system with which the appeal should be filed by using natural language processing (e.g., questions and responses) with the ability to further refine and increase accuracy based on user response. ",Acquisition and/or Development,Neither,4/1/2023,7/1/2023,Unknown,Developed in-house.,Unknown,Unknown,Unknown,Unknown,Unknown,Yes,Unknown,Documentation is complete,Unknown,Unknown,Yes,DAB E-file,More than 12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.38095238095238093,"The DAB adjudicates  disputes under more than 60 statutory authorities.  The workload is divided between the DAB's four adjudicatory divisions (e.g., the Board, the Civil Remedies Division, the Medicare Operations Division and the Alternative Disputes Resolution Division. Helping appellants decide which entity within the DAB is responsible for adjudicating their appeal will increase efficiency in adjudication and customer experience. 

The DAB's appellant community (e.g., e-file users) will be provided immediate assistance and accurate information; deaf and hard-of-hearing populations and individuals with limited English proficiency are helped without the use of intermediaries, such as interpreters, translators, and relay services. After determining the appropriate division within the DAB responsible for adjudicating the appeal, the Chatbot can provide a link to the e-file website directly connecting the user to the correct e-file system.  Other benefits include: Improve public access to the appeals process by providing instant, accurate and complete filing information; Increase the DAB's productivity by refocusing the workforce to case resolution instead of responding the filing inquires and processing and appropriately routing misfiled appeals; and Improve resource allocation based on each Division's actual workload. . A chatbot on the DAB's website can direct filers to the correct Division and e-file system with which the appeal should be filed by using natural language processing (e.g., questions and responses) with the ability to further refine and increase accuracy based on user response.","the dab adjudicates disputes under more than 60 statutory authorities. the workload is divided between the dab's four adjudicatory divisions (e.g., the board, the civil remedies division, the medicare operations division and the alternative disputes resolution division. helping appellants decide which entity within the dab is responsible for adjudicating their appeal will increase efficiency in adjudication and customer experience. the dab's appellant community (e.g., e-file users) will be provided immediate assistance and accurate information; deaf and hard-of-hearing populations and individuals with limited english proficiency are helped without the use of intermediaries, such as interpreters, translators, and relay services. after determining the appropriate division within the dab responsible for adjudicating the appeal, the chatbot can provide a link to the e-file website directly connecting the user to the correct e-file system. other benefits include: improve public access to the appeals process by providing instant, accurate and complete filing information; increase the dab's productivity by refocusing the workforce to case resolution instead of responding the filing inquires and processing and appropriately routing misfiled appeals; and improve resource allocation based on each division's actual workload. . a chatbot on the dab's website can direct filers to the correct division and e-file system with which the appeal should be filed by using natural language processing (e.g., questions and responses) with the ability to further refine and increase accuracy based on user response."
AI Use Policy Tool,Department of Health and Human Services,HHS,DAB,Mission-Enabling (internal agency support),None of the above.,"The DAB is responsible for issuing fair, impartial, legally correct and defensible determinations which serve as the final decision of the DHHS Secretary.  The AI Use Policy is designed to promote public confidence in the integrity of DAB decisions.  

More effective quality review tool and faster identification of data trends that require additional analysis.  ","AI Use Policy Tool will scan DAB decisions to ensure compliance with quality review standards (e.g., protect PHI, PII and FTI) and identify work product that potentially violates the prohibition of the use of generative AI in DAB decisions.  ",Acquisition and/or Development,Neither,4/1/2023,9/1/2023,Unknown,Developed in-house.,Unknown,Unknown,Unknown,Unknown,Unknown,Yes,TBD,Documentation is complete,Unknown,Unknown,Yes,DABACTS,More than 12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"The DAB is responsible for issuing fair, impartial, legally correct and defensible determinations which serve as the final decision of the DHHS Secretary.  The AI Use Policy is designed to promote public confidence in the integrity of DAB decisions.  

More effective quality review tool and faster identification of data trends that require additional analysis. . AI Use Policy Tool will scan DAB decisions to ensure compliance with quality review standards (e.g., protect PHI, PII and FTI) and identify work product that potentially violates the prohibition of the use of generative AI in DAB decisions.","the dab is responsible for issuing fair, impartial, legally correct and defensible determinations which serve as the final decision of the dhhs secretary. the ai use policy is designed to promote public confidence in the integrity of dab decisions. more effective quality review tool and faster identification of data trends that require additional analysis. . ai use policy tool will scan dab decisions to ensure compliance with quality review standards (e.g., protect phi, pii and fti) and identify work product that potentially violates the prohibition of the use of generative ai in dab decisions."
Resources to Assist the Advisory Board In Identifying AI Tools for Use In An Adjudication Environment,Department of Health and Human Services,HHS,DAB,Mission-Enabling (internal agency support),None of the above.,"The adjudication process from docket analysis, case processing, issuing a decision and quality review (after adjudication) relies on the analysis of large quantities of data. 

AI or machine learning tools will help the Advisory Board determine which technology will best serve the Agency's needs in an adjudication environment. ",The use of AI to analyze the voluminous data received from appellants and interested parties to identify trends and increase efficiency in the adjudication process.  ,Acquisition and/or Development,Neither,4/1/2023,9/1/2023,Unknown,Developed in-house.,Unknown,Unknown,Unknown,Unknown,Unknown,No,Unknown,Documentation is complete,Unknown,Unknown,No,Unknown,More than 12 months,No,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.36507936507936506,"The adjudication process from docket analysis, case processing, issuing a decision and quality review (after adjudication) relies on the analysis of large quantities of data. 

AI or machine learning tools will help the Advisory Board determine which technology will best serve the Agency's needs in an adjudication environment. . The use of AI to analyze the voluminous data received from appellants and interested parties to identify trends and increase efficiency in the adjudication process.","the adjudication process from docket analysis, case processing, issuing a decision and quality review (after adjudication) relies on the analysis of large quantities of data. ai or machine learning tools will help the advisory board determine which technology will best serve the agency's needs in an adjudication environment. . the use of ai to analyze the voluminous data received from appellants and interested parties to identify trends and increase efficiency in the adjudication process."
356H ML Facility Supply Chain Role Classification,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"Applicants submit unstructured text to indicate a facility’s supply chain role, which range from manufacturing, testing, storing, packaging, etc. This use case extracts the supply chain role based on the text and presents it to data stewards who perform data operations

Improve the efficiency of evaluating 356H forms to assess a facility's supply chain role, thereby decreasing the time required for the processing of submissions.",Extracts a facility's supply chain role from industry submissions and displays identified roles on a data-stewards screen for human verification and final determination.,Operation and Maintenance,Neither,6/1/2022,6/1/2022,5/1/2023,Developed with both contracting and in-house resources.,Not Reported,Unknown,No,No,No,No,Applicant submissions,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CDEROne Analytics: FISMA-high environment,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Applicants submit unstructured text to indicate a facility’s supply chain role, which range from manufacturing, testing, storing, packaging, etc. This use case extracts the supply chain role based on the text and presents it to data stewards who perform data operations

Improve the efficiency of evaluating 356H forms to assess a facility's supply chain role, thereby decreasing the time required for the processing of submissions. . Extracts a facility's supply chain role from industry submissions and displays identified roles on a data-stewards screen for human verification and final determination.","applicants submit unstructured text to indicate a facility’s supply chain role, which range from manufacturing, testing, storing, packaging, etc. this use case extracts the supply chain role based on the text and presents it to data stewards who perform data operations improve the efficiency of evaluating 356h forms to assess a facility's supply chain role, thereby decreasing the time required for the processing of submissions. . extracts a facility's supply chain role from industry submissions and displays identified roles on a data-stewards screen for human verification and final determination."
AI Tool for Risk-based FAR Review & Decision Support,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"Assist Field Alert Report (FAR) reviewers by providing risk-based intelligence and insights for review and decision support with a Human-in-the-Loop.

Assist with streamlining an efficient FAR review process by summarizing risk profiles for human exploration that is evaluated with additional data to assist human decisioning.","AI-based machine learning classification of FAR risk into low, medium, and high; provides insights on problem clusters, rare-events, and source variables.",Operation and Maintenance,Neither,6/1/2020,10/1/2020,10/1/2024,Developed in-house.,Unknown,Unknown,No,No,No,Yes,Internal Field Alert Reports data from LSMV,Documentation has been partially completed,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CDEROne Analytics: FISMA-high environment,6-12 months,Yes,Yes,No,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Assist Field Alert Report (FAR) reviewers by providing risk-based intelligence and insights for review and decision support with a Human-in-the-Loop.

Assist with streamlining an efficient FAR review process by summarizing risk profiles for human exploration that is evaluated with additional data to assist human decisioning. . AI-based machine learning classification of FAR risk into low, medium, and high; provides insights on problem clusters, rare-events, and source variables.","assist field alert report (far) reviewers by providing risk-based intelligence and insights for review and decision support with a human-in-the-loop. assist with streamlining an efficient far review process by summarizing risk profiles for human exploration that is evaluated with additional data to assist human decisioning. . ai-based machine learning classification of far risk into low, medium, and high; provides insights on problem clusters, rare-events, and source variables."
Analytics-Driven Supplement Evaluation (ASE),Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"This AI use case supports the triage and staff assignment process for the review of post-market Change Being Effected (CBE) supplement submissions.

The output of the AI component contributes to the triage and staff assignment process, and summarizes submitted information to support the decision of the reviewers of the CBE 30/0 supplemental submissions.","Using a Convolutional Neural Network (NN) model, in combination with a rules-based approach, produces an output that helps staff triage CBE submission review",Acquisition and/or Development,Neither,10/1/2022,10/1/2022,Unknown,Developed in-house.,Unknown,Unknown,Unknown,Unknown,Unknown,No,Data submitted in applicants' supplemental submissions,Documentation is complete,Unknown,Unknown,No,Unknown,6-12 months,No,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.38095238095238093,"This AI use case supports the triage and staff assignment process for the review of post-market Change Being Effected (CBE) supplement submissions.

The output of the AI component contributes to the triage and staff assignment process, and summarizes submitted information to support the decision of the reviewers of the CBE 30/0 supplemental submissions. . Using a Convolutional Neural Network (NN) model, in combination with a rules-based approach, produces an output that helps staff triage CBE submission review","this ai use case supports the triage and staff assignment process for the review of post-market change being effected (cbe) supplement submissions. the output of the ai component contributes to the triage and staff assignment process, and summarizes submitted information to support the decision of the reviewers of the cbe 30/0 supplemental submissions. . using a convolutional neural network (nn) model, in combination with a rules-based approach, produces an output that helps staff triage cbe submission review"
Augmenting death and cause of death ascertainment in observational data sources,Department of Health and Human Services,HHS,FDA,Health & Medical,None of the above.,"Augment assessment of mortality in electronic health records to support investigations to develop probabilistic estimates for causes of death in the Sentinel system.

The extraction of mortality information improves studies in Sentinel using mortality as an endpoint by enhancing on the quality of mortality data.","Augment assessment of mortality (date of death, cause of death) by applying Natural Language Processing (NLP) on healthcare narrative text and administrative codes.",Operation and Maintenance,Neither,1/1/2022,1/1/2022,12/1/2023,Developed with contracting resources.,75F40119D10037 - 7540119F19002,Unknown,No,No,No,Yes,"`- National Death Index (NDI) data
- State Death Certificate Registery",Documentation has been partially completed,Yes,No – agency does not have access to source code.,Yes,Sentinel,6-12 months,No,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Augment assessment of mortality in electronic health records to support investigations to develop probabilistic estimates for causes of death in the Sentinel system.

The extraction of mortality information improves studies in Sentinel using mortality as an endpoint by enhancing on the quality of mortality data. . Augment assessment of mortality (date of death, cause of death) by applying Natural Language Processing (NLP) on healthcare narrative text and administrative codes.","augment assessment of mortality in electronic health records to support investigations to develop probabilistic estimates for causes of death in the sentinel system. the extraction of mortality information improves studies in sentinel using mortality as an endpoint by enhancing on the quality of mortality data. . augment assessment of mortality (date of death, cause of death) by applying natural language processing (nlp) on healthcare narrative text and administrative codes."
CLAT (Computerized Labeling Assessment Tool) ,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"CLAT uses computer programming, image processing, and AI to assist the human review of drug labeling, including container labels, carton labeling, prescribing information, and Instructions for Use. CLAT assists the human visual comparison of drug labeling against a library of FDA regulations, guidances, standards, and best practices. CLAT identifies potential labeling deviations for human evaluation. 

CLAT is used to improve efficiencies in the manual labeling review process to help the reviewer mitigate medication errors and standardize labeling across therapeutic drug classes and product types. CLAT also assists the reviewer to catch deviations between label versions so no unintended errors have been introduced.","The tool processes images of carton and container labeling to identify minimum requirements on a label, identify availability of objects on an image, color differentiation of strength, missing barcode and orientation, incorrect or missing strength statement, error prone abbreviation, look-alike labels and text prominence. This will assist the reviewer by  improving the efficiency of drug labeling reviews, including container labels, carton labeling, prescribing information, and Instructions for Use. ",Acquisition and/or Development,Neither,9/1/2020,9/1/2020,Unknown,Developed with contracting resources.,Unknown,Unknown,Unknown,Unknown,Unknown,Yes,Label and Container cartons,Documentation is complete,Unknown,Unknown,Yes,CDEROne Analytics: FISMA-high environment,6-12 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"CLAT uses computer programming, image processing, and AI to assist the human review of drug labeling, including container labels, carton labeling, prescribing information, and Instructions for Use. CLAT assists the human visual comparison of drug labeling against a library of FDA regulations, guidances, standards, and best practices. CLAT identifies potential labeling deviations for human evaluation. 

CLAT is used to improve efficiencies in the manual labeling review process to help the reviewer mitigate medication errors and standardize labeling across therapeutic drug classes and product types. CLAT also assists the reviewer to catch deviations between label versions so no unintended errors have been introduced. . The tool processes images of carton and container labeling to identify minimum requirements on a label, identify availability of objects on an image, color differentiation of strength, missing barcode and orientation, incorrect or missing strength statement, error prone abbreviation, look-alike labels and text prominence. This will assist the reviewer by  improving the efficiency of drug labeling reviews, including container labels, carton labeling, prescribing information, and Instructions for Use.","clat uses computer programming, image processing, and ai to assist the human review of drug labeling, including container labels, carton labeling, prescribing information, and instructions for use. clat assists the human visual comparison of drug labeling against a library of fda regulations, guidances, standards, and best practices. clat identifies potential labeling deviations for human evaluation. clat is used to improve efficiencies in the manual labeling review process to help the reviewer mitigate medication errors and standardize labeling across therapeutic drug classes and product types. clat also assists the reviewer to catch deviations between label versions so no unintended errors have been introduced. . the tool processes images of carton and container labeling to identify minimum requirements on a label, identify availability of objects on an image, color differentiation of strength, missing barcode and orientation, incorrect or missing strength statement, error prone abbreviation, look-alike labels and text prominence. this will assist the reviewer by improving the efficiency of drug labeling reviews, including container labels, carton labeling, prescribing information, and instructions for use."
Empirical evaluation of EHR-based signal detection approaches,Department of Health and Human Services,HHS,FDA,Health & Medical,None of the above.,"This supports Electronic Health Record (EHR) signal detection by extracting information using Natural Language Processing (NLP) and expanding the tree based scan statistic (TBSS) methods for EHR-based signal detection.


Extracts data to facilitate the identification of signals for outcomes, improving the manual identification of signals for outcomes derived from electronic health records.",The AI component in this study leverages an Natural Language Processing (NLP) approach to extract data from unstructured clinical notes.,Acquisition and/or Development,Neither,9/1/2023,9/1/2023,Unknown,Developed with contracting resources.,75F40119D10037 - 75F40123F19010,Unknown,Unknown,Unknown,Unknown,Yes,Claims-Electronic Health Record (EHR),Documentation has been partially completed,Unknown,Unknown,Yes,Sentinel ,Less than 6 months,No,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"This supports Electronic Health Record (EHR) signal detection by extracting information using Natural Language Processing (NLP) and expanding the tree based scan statistic (TBSS) methods for EHR-based signal detection.


Extracts data to facilitate the identification of signals for outcomes, improving the manual identification of signals for outcomes derived from electronic health records. . The AI component in this study leverages an Natural Language Processing (NLP) approach to extract data from unstructured clinical notes.","this supports electronic health record (ehr) signal detection by extracting information using natural language processing (nlp) and expanding the tree based scan statistic (tbss) methods for ehr-based signal detection. extracts data to facilitate the identification of signals for outcomes, improving the manual identification of signals for outcomes derived from electronic health records. . the ai component in this study leverages an natural language processing (nlp) approach to extract data from unstructured clinical notes."
FAR-based Facility Signal Detection Tool ,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"Identifies problem clusters for the flagged signals in an objective manner for triage/review. 

Assist with objectively identifying problem clusters for the flagged signals, which then go through a dedicated triage & review process for further actions, and if needed, CMS (inspection) work activities can be opened. ",Identifies problem clusters for the flagged signals in an objective manner for triage/review. ,Operation and Maintenance,Neither,6/1/2019,6/1/2020,6/1/2024,Developed in-house.,Unknown,Unknown,No,No,No,Yes,Internal Field Alert Reports data from LSMV (FAERS tool),Documentation has been partially completed,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CDEROne Analytics: FISMA-high environment,6-12 months,Yes,Yes,No,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Identifies problem clusters for the flagged signals in an objective manner for triage/review. 

Assist with objectively identifying problem clusters for the flagged signals, which then go through a dedicated triage & review process for further actions, and if needed, CMS (inspection) work activities can be opened. . Identifies problem clusters for the flagged signals in an objective manner for triage/review.","identifies problem clusters for the flagged signals in an objective manner for triage/review. assist with objectively identifying problem clusters for the flagged signals, which then go through a dedicated triage & review process for further actions, and if needed, cms (inspection) work activities can be opened. . identifies problem clusters for the flagged signals in an objective manner for triage/review."
MedWatch Dashboard ,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"Assist with consistent monitoring and identification of product risks from MedWatch reporting patterns and report content to support review staff.

Supports efficient, objective, risk-based product quality assessments from MedWatch reports for verification by review staff ",Identify product risk signals from MedWatch reports by using time series analysis to flag products and topic modeling to summarize the comments.,Operation and Maintenance,Neither,1/1/2021,5/1/2021,4/1/2023,Developed in-house.,Unknown,Unknown,No,No,No,Yes,MedWatch data from LSMV and IQVIA data,Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CDEROne Analytics: FISMA-high environment,6-12 months,Yes,Yes,No,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Assist with consistent monitoring and identification of product risks from MedWatch reporting patterns and report content to support review staff.

Supports efficient, objective, risk-based product quality assessments from MedWatch reports for verification by review staff . Identify product risk signals from MedWatch reports by using time series analysis to flag products and topic modeling to summarize the comments.","assist with consistent monitoring and identification of product risks from medwatch reporting patterns and report content to support review staff. supports efficient, objective, risk-based product quality assessments from medwatch reports for verification by review staff . identify product risk signals from medwatch reports by using time series analysis to flag products and topic modeling to summarize the comments."
Quality Surveillance Dashboard (QSD),Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"Extracts unstructured text from documents and pools that with other available data to form a dashboard that enables consistent assessment of CDER regulated manufacturing facilities

Assists in the efficient, objective, risk-based assessments of CDER regulated manufacturing facilities.",Identifies and extracts keywords/phrases from unstructured documents and presents sentences containing keywords/phrases in context.,Operation and Maintenance,Neither,1/1/2019,1/1/2019,3/1/2023,Developed in-house.,Unknown,Unknown,No,No,No,Yes,FDA EIR documents,Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CDEROne Analytics: FISMA-high environment,6-12 months,Yes,Yes,No,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Extracts unstructured text from documents and pools that with other available data to form a dashboard that enables consistent assessment of CDER regulated manufacturing facilities

Assists in the efficient, objective, risk-based assessments of CDER regulated manufacturing facilities. . Identifies and extracts keywords/phrases from unstructured documents and presents sentences containing keywords/phrases in context.","extracts unstructured text from documents and pools that with other available data to form a dashboard that enables consistent assessment of cder regulated manufacturing facilities assists in the efficient, objective, risk-based assessments of cder regulated manufacturing facilities. . identifies and extracts keywords/phrases from unstructured documents and presents sentences containing keywords/phrases in context."
Scalable automated NLP-assisted chart abstraction and feature extraction tool,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"Examine the usability of electronic medical records in conducting pharmacoepidemiology studies through a case study of montelukast use among patients with asthma and neuropsychiatric events

The project will involve an “end-to-end” implementation of a full pharmacoepidemiologic study using one or more product-outcome pairs to demonstrate how NLP and other advanced analytic methods or tools can be used to accurately identify study outcomes and covariates from unstructured Electronic Health Record (EHR) data.","Natural language processing (NLP), a computational linguistics technology, will be used to read, interpret and organize health data that may be contained within unstructured data fields within EHRs. This activity intends to build a semi-automated system that uses advanced methods and tools, including NLP, to enable large-scale outcome identification, confounder feature extraction, and longitudinal contextualization of Electronic Health Record (EHR) data.",Acquisition and/or Development,Neither,10/1/2022,9/1/2022,Unknown,Developed with contracting resources.,75F40119D10037 - 7540119F19002,Unknown,Unknown,Unknown,Unknown,Yes,Claims-Electronic Health Record (EHR) - (structured and unstructured),Documentation has been partially completed,Unknown,Unknown,Yes,Sentinel ,6-12 months,No,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"Examine the usability of electronic medical records in conducting pharmacoepidemiology studies through a case study of montelukast use among patients with asthma and neuropsychiatric events

The project will involve an “end-to-end” implementation of a full pharmacoepidemiologic study using one or more product-outcome pairs to demonstrate how NLP and other advanced analytic methods or tools can be used to accurately identify study outcomes and covariates from unstructured Electronic Health Record (EHR) data. . Natural language processing (NLP), a computational linguistics technology, will be used to read, interpret and organize health data that may be contained within unstructured data fields within EHRs. This activity intends to build a semi-automated system that uses advanced methods and tools, including NLP, to enable large-scale outcome identification, confounder feature extraction, and longitudinal contextualization of Electronic Health Record (EHR) data.","examine the usability of electronic medical records in conducting pharmacoepidemiology studies through a case study of montelukast use among patients with asthma and neuropsychiatric events the project will involve an “end-to-end” implementation of a full pharmacoepidemiologic study using one or more product-outcome pairs to demonstrate how nlp and other advanced analytic methods or tools can be used to accurately identify study outcomes and covariates from unstructured electronic health record (ehr) data. . natural language processing (nlp), a computational linguistics technology, will be used to read, interpret and organize health data that may be contained within unstructured data fields within ehrs. this activity intends to build a semi-automated system that uses advanced methods and tools, including nlp, to enable large-scale outcome identification, confounder feature extraction, and longitudinal contextualization of electronic health record (ehr) data."
Annual Report CMC,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"Objective of this use case was to extract CMC changes reported within unstructured annual report industry submission documents to build a complete repository for downstream analysis

The extracted data are presented in a dashboard (along with other data) for downstream human analysis.",Support information extraction from unstructured documents,Operation and Maintenance,Neither,1/1/2024,1/1/2024,5/1/2024,Developed with both contracting and in-house resources.,75F40124A00003,Unknown,No,No,No,No,Applicant submissions,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CDEROne Analytics: FISMA-high environment,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Objective of this use case was to extract CMC changes reported within unstructured annual report industry submission documents to build a complete repository for downstream analysis

The extracted data are presented in a dashboard (along with other data) for downstream human analysis. . Support information extraction from unstructured documents",objective of this use case was to extract cmc changes reported within unstructured annual report industry submission documents to build a complete repository for downstream analysis the extracted data are presented in a dashboard (along with other data) for downstream human analysis. . support information extraction from unstructured documents
Application-DMF Reference,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"Extracts references to DMFs from marketing application (ANDA/NDA/BLA) submissions. These submission documents may be structured (356H form) or unstructured (eCTD module 1-4). This pipeline parses content from these documents, extracts DMF references (e.g., ANDA123456 references DMF123456), and exposes the data in a structured format for analysis. 

The extracted data are presented in a dashboard, along with additional data, for downstream human analysis. The purpose of this use case is to build a comprehensive inventory of all application to DMF references to support OPQ in reviewing applications since many application to DMF references were previously missing from structured sources.",Support information extraction from unstructured documents,Operation and Maintenance,Neither,5/1/2023,5/1/2023,12/1/2023,Developed with both contracting and in-house resources.,75F40124A00003,Unknown,No,No,No,No,applicant submissions,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CDEROne Analytics: FISMA-high environment,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Extracts references to DMFs from marketing application (ANDA/NDA/BLA) submissions. These submission documents may be structured (356H form) or unstructured (eCTD module 1-4). This pipeline parses content from these documents, extracts DMF references (e.g., ANDA123456 references DMF123456), and exposes the data in a structured format for analysis. 

The extracted data are presented in a dashboard, along with additional data, for downstream human analysis. The purpose of this use case is to build a comprehensive inventory of all application to DMF references to support OPQ in reviewing applications since many application to DMF references were previously missing from structured sources. . Support information extraction from unstructured documents","extracts references to dmfs from marketing application (anda/nda/bla) submissions. these submission documents may be structured (356h form) or unstructured (ectd module 1-4). this pipeline parses content from these documents, extracts dmf references (e.g., anda123456 references dmf123456), and exposes the data in a structured format for analysis. the extracted data are presented in a dashboard, along with additional data, for downstream human analysis. the purpose of this use case is to build a comprehensive inventory of all application to dmf references to support opq in reviewing applications since many application to dmf references were previously missing from structured sources. . support information extraction from unstructured documents"
Artificial Intelligence-based Deduplication Algorithm for Classification of Duplicate Reports in the FDA Adverse Event Reports (FAERS) ,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"The deduplication algorithm is applied to all historical and incoming case report submissions to the FDA Adverse Event Reporting System (FAERS) to identify duplicate individual case safety reports (ICSRs). Unstructured data that is in free text FAERS narratives is processed through a natural language processing system to extract relevant clinical features. Both structured and unstructured data are then used in a probabilistic record linkage approach to identify duplicates. Application of the deduplication algorithm is optimized for processing entire FAERS database to support the data mining analytics process. 

Enhance the efficiency of removing duplicate FAERS reports to support data mining.",Uses Natural Language Processing (NLP) to extract out clinical features from free text narratives in a case safety report and uses them in the identification of duplicative case safety reports in the FAERS database.,Implementation and Assessment,Neither,12/1/2023,9/1/2024,Unknown,Developed with contracting resources.,BAAFY23C2AWP1,Unknown,No,Yes,No,Yes,Adverse Event data in FAERS,Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,6-12 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The deduplication algorithm is applied to all historical and incoming case report submissions to the FDA Adverse Event Reporting System (FAERS) to identify duplicate individual case safety reports (ICSRs). Unstructured data that is in free text FAERS narratives is processed through a natural language processing system to extract relevant clinical features. Both structured and unstructured data are then used in a probabilistic record linkage approach to identify duplicates. Application of the deduplication algorithm is optimized for processing entire FAERS database to support the data mining analytics process. 

Enhance the efficiency of removing duplicate FAERS reports to support data mining. . Uses Natural Language Processing (NLP) to extract out clinical features from free text narratives in a case safety report and uses them in the identification of duplicative case safety reports in the FAERS database.",the deduplication algorithm is applied to all historical and incoming case report submissions to the fda adverse event reporting system (faers) to identify duplicate individual case safety reports (icsrs). unstructured data that is in free text faers narratives is processed through a natural language processing system to extract relevant clinical features. both structured and unstructured data are then used in a probabilistic record linkage approach to identify duplicates. application of the deduplication algorithm is optimized for processing entire faers database to support the data mining analytics process. enhance the efficiency of removing duplicate faers reports to support data mining. . uses natural language processing (nlp) to extract out clinical features from free text narratives in a case safety report and uses them in the identification of duplicative case safety reports in the faers database.
Development of automation tools and data warehouse to facilitate BE assessments,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"Enhancing and streamlining the data entry associated with the review of bioequivalence data in Abbreviated New Drug Application (ANDA) submissions

Reduce manual effort by preparing data for analysis and exporting results as part of the bioequivalence review. This tool will facilitate data collection and Information retrieval from the firm-submitted ANDA packages but will not involve reviewers' evaluation or judgement.",Transform submitted PK data into a usable format so that the reviewer can perform the routine bioequivalence analysis using the authorized pre-written SAS codes. Then it exports results to a draft bioequivalence review  document with the firm-submitted eCTD tables.,Acquisition and/or Development,Neither,5/1/2018,5/1/2018,Unknown,Developed with both contracting and in-house resources.,75F40120F80605,Unknown,Unknown,Unknown,Unknown,No,Unknown,Documentation is widely available,Unknown,Unknown,Yes,CDEROne Analytics: FISMA-high environment,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"Enhancing and streamlining the data entry associated with the review of bioequivalence data in Abbreviated New Drug Application (ANDA) submissions

Reduce manual effort by preparing data for analysis and exporting results as part of the bioequivalence review. This tool will facilitate data collection and Information retrieval from the firm-submitted ANDA packages but will not involve reviewers' evaluation or judgement. . Transform submitted PK data into a usable format so that the reviewer can perform the routine bioequivalence analysis using the authorized pre-written SAS codes. Then it exports results to a draft bioequivalence review  document with the firm-submitted eCTD tables.",enhancing and streamlining the data entry associated with the review of bioequivalence data in abbreviated new drug application (anda) submissions reduce manual effort by preparing data for analysis and exporting results as part of the bioequivalence review. this tool will facilitate data collection and information retrieval from the firm-submitted anda packages but will not involve reviewers' evaluation or judgement. . transform submitted pk data into a usable format so that the reviewer can perform the routine bioequivalence analysis using the authorized pre-written sas codes. then it exports results to a draft bioequivalence review document with the firm-submitted ectd tables.
DMF (Drug Master File) Facilites,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"Extracts facility references from Type II (Drug Substance) DMF manufacturing submissions (i.e., DMF123456 discloses that it uses Facility X for manufacturing and Facility Y for stability testing). These DMF submissions may include structured documents (3938 form) or unstructured documents (eCTD module 3). This pipeline parses content from these documents, extracts facility references and supply chain roles (i.e., FEI 6789 – manufacturing facility), and exposes the data in a structured format for analysis. This allows OPQ to identify discrepancies between facilities reported on marketing applications and those disclosed by DMFs referenced by those applications. 

The extracted data are presented in a dashboard, along with additional data, for downstream human analysis. The ultimate purpose of this use case is to build a comprehensive inventory of facilities reported within DMF submissions as well as the applications that reference those DMFs.",Support information extraction from unstructured documents,Operation and Maintenance,Neither,6/1/2022,6/1/2022,6/1/2023,Developed with both contracting and in-house resources.,75F40124A00003,Unknown,No,No,No,No,Applicant submissions,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CDEROne Analytics: FISMA-high environment,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Extracts facility references from Type II (Drug Substance) DMF manufacturing submissions (i.e., DMF123456 discloses that it uses Facility X for manufacturing and Facility Y for stability testing). These DMF submissions may include structured documents (3938 form) or unstructured documents (eCTD module 3). This pipeline parses content from these documents, extracts facility references and supply chain roles (i.e., FEI 6789 – manufacturing facility), and exposes the data in a structured format for analysis. This allows OPQ to identify discrepancies between facilities reported on marketing applications and those disclosed by DMFs referenced by those applications. 

The extracted data are presented in a dashboard, along with additional data, for downstream human analysis. The ultimate purpose of this use case is to build a comprehensive inventory of facilities reported within DMF submissions as well as the applications that reference those DMFs. . Support information extraction from unstructured documents","extracts facility references from type ii (drug substance) dmf manufacturing submissions (i.e., dmf123456 discloses that it uses facility x for manufacturing and facility y for stability testing). these dmf submissions may include structured documents (3938 form) or unstructured documents (ectd module 3). this pipeline parses content from these documents, extracts facility references and supply chain roles (i.e., fei 6789 – manufacturing facility), and exposes the data in a structured format for analysis. this allows opq to identify discrepancies between facilities reported on marketing applications and those disclosed by dmfs referenced by those applications. the extracted data are presented in a dashboard, along with additional data, for downstream human analysis. the ultimate purpose of this use case is to build a comprehensive inventory of facilities reported within dmf submissions as well as the applications that reference those dmfs. . support information extraction from unstructured documents"
Four Part Harmony Identification,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"Accuracy of language used to report back deficiencies to submitters.

This will reduce the amount of time it takes and reduce staff burden, thus allowing for MDUFA requirements to be met.",This will reduce the amount of time it takes to verify and identify issues with language used for deficiency letters and ensure the adherence to the language necessary.,Acquisition and/or Development,Neither,9/1/2023,9/1/2023,Unknown,Developed in-house.,Unknown,Unknown,Unknown,Unknown,Unknown,Yes,Submission data and documents,Documentation is widely available,Unknown,Unknown,Yes,CEDh,6-12 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"Accuracy of language used to report back deficiencies to submitters.

This will reduce the amount of time it takes and reduce staff burden, thus allowing for MDUFA requirements to be met. . This will reduce the amount of time it takes to verify and identify issues with language used for deficiency letters and ensure the adherence to the language necessary.","accuracy of language used to report back deficiencies to submitters. this will reduce the amount of time it takes and reduce staff burden, thus allowing for mdufa requirements to be met. . this will reduce the amount of time it takes to verify and identify issues with language used for deficiency letters and ensure the adherence to the language necessary."
Information Visualization Platform (InfoViP) to Support Analysis of adverse event reports,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"Information Visualization Platform (InfoVIP) supports post-market surveillance using AI to assist prioritization and analysis of adverse event reports by extracting data from unstructured case narratives and combining it with structured data to support analysis and review of adverse event reports. The tool assists with   case series analyses, creating data visualization, and classifying adverse event reports by information quality.

Improve the efficiency in preparing and analyzing data during the review and evaluation of adverse event reports.",Performs Natural Language Processing (NLP) and applying Machine Learning (ML) algorithm to extract data from unstructured case narratives and combine with structured data to support analysis and review of adverse event reports.,Acquisition and/or Development,Neither,9/1/2019,9/1/2019,Unknown,Developed with contracting resources.,BAAFY23C2AWP1,Unknown,Unknown,Unknown,Unknown,Yes,Adverse Event data in FAERS,Documentation has been partially completed,Unknown,Unknown,No,Unknown,More than 12 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"Information Visualization Platform (InfoVIP) supports post-market surveillance using AI to assist prioritization and analysis of adverse event reports by extracting data from unstructured case narratives and combining it with structured data to support analysis and review of adverse event reports. The tool assists with   case series analyses, creating data visualization, and classifying adverse event reports by information quality.

Improve the efficiency in preparing and analyzing data during the review and evaluation of adverse event reports. . Performs Natural Language Processing (NLP) and applying Machine Learning (ML) algorithm to extract data from unstructured case narratives and combine with structured data to support analysis and review of adverse event reports.","information visualization platform (infovip) supports post-market surveillance using ai to assist prioritization and analysis of adverse event reports by extracting data from unstructured case narratives and combining it with structured data to support analysis and review of adverse event reports. the tool assists with case series analyses, creating data visualization, and classifying adverse event reports by information quality. improve the efficiency in preparing and analyzing data during the review and evaluation of adverse event reports. . performs natural language processing (nlp) and applying machine learning (ml) algorithm to extract data from unstructured case narratives and combine with structured data to support analysis and review of adverse event reports."
LLM-Assisted VAERS Analyses,Department of Health and Human Services,HHS,FDA,Health & Medical,None of the above.,"Build capacity for and assess the application of a LLM to VAERS (Vaccine Adverse Events Reporting System) to provide reviewers adhoc VAERS queries and efficiently generate customized query outputs

Efficiently generate customized query outputs of VAERS queries for reviewers.",To provide reviewers adhoc VAERS queries.,Acquisition and/or Development,Neither,12/1/2023,12/1/2023,Unknown,Developed in-house.,Unknown,Unknown,Unknown,Unknown,Unknown,Yes,Adverse Events data in VAERS,Documentation is widely available,Unknown,Unknown,Yes,CBER HIVE,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"Build capacity for and assess the application of a LLM to VAERS (Vaccine Adverse Events Reporting System) to provide reviewers adhoc VAERS queries and efficiently generate customized query outputs

Efficiently generate customized query outputs of VAERS queries for reviewers. . To provide reviewers adhoc VAERS queries.",build capacity for and assess the application of a llm to vaers (vaccine adverse events reporting system) to provide reviewers adhoc vaers queries and efficiently generate customized query outputs efficiently generate customized query outputs of vaers queries for reviewers. . to provide reviewers adhoc vaers queries.
Pharmacovigilance machine learning to detect excess safety signal,Department of Health and Human Services,HHS,FDA,Government Services (includes Benefits and Service Delivery),None of the above.,"Post-market adverse event reporting

Using a computer algorithm assists FDA in finding ""needle in the haystack"" events that are difficult to uncover with manual review of reports",Generate potential safety signals to investigate further using statistical based algorithms,Operation and Maintenance,Neither,1/1/2015,1/1/2008,1/1/2015,Developed with contracting resources.,Unknown,Unknown,No,No,No,No,Unknown,Unknown,Yes,Unknown,Yes,PV Analyzer,Unknown,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"Post-market adverse event reporting

Using a computer algorithm assists FDA in finding ""needle in the haystack"" events that are difficult to uncover with manual review of reports . Generate potential safety signals to investigate further using statistical based algorithms","post-market adverse event reporting using a computer algorithm assists fda in finding ""needle in the haystack"" events that are difficult to uncover with manual review of reports . generate potential safety signals to investigate further using statistical based algorithms"
Module 3 Facilties,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"Objective of this use case was to identify and extract all drug manufacturing facilities reported within unstructured module 3 submissions from marketing applications to build an comprehensive inventory of drug facilities

The extracted data are presented in a dashboard, along with additional data, for downstream human analysis.",Support information extraction from unstructured documents,Operation and Maintenance,Neither,7/1/2023,7/1/2023,2/1/2024,Developed with both contracting and in-house resources.,75F40124A00003,Unknown,No,No,No,No,applicant submissions,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CDEROne Analytics: FISMA-high environment,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Objective of this use case was to identify and extract all drug manufacturing facilities reported within unstructured module 3 submissions from marketing applications to build an comprehensive inventory of drug facilities

The extracted data are presented in a dashboard, along with additional data, for downstream human analysis. . Support information extraction from unstructured documents","objective of this use case was to identify and extract all drug manufacturing facilities reported within unstructured module 3 submissions from marketing applications to build an comprehensive inventory of drug facilities the extracted data are presented in a dashboard, along with additional data, for downstream human analysis. . support information extraction from unstructured documents"
OPPQ Policy Bot (AKA Policy and Guidance Documents Search) ,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"Helps policy staff in searching documents containing regulations and responding to inquiries, and other policy related work. 

In developing policy and responding to inquiries, OPPQ staff often need to reference  previously published policy documents in order to ensure consistency. This AI-enabled tool will allow policy leads and other staff to quickly identify and query the appropriate documents.","Identifies applicable published MAPPS, guidances, and policy related documents based on queries from users. Includes link to the document for further human review.",Implementation and Assessment,Neither,9/1/2024,9/1/2024,Unknown,Developed with both contracting and in-house resources.,75F40124A00003,Unknown,No,No,No,No,CDER policy documents,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CDEROne Analytics: FISMA-high environment,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Helps policy staff in searching documents containing regulations and responding to inquiries, and other policy related work. 

In developing policy and responding to inquiries, OPPQ staff often need to reference  previously published policy documents in order to ensure consistency. This AI-enabled tool will allow policy leads and other staff to quickly identify and query the appropriate documents. . Identifies applicable published MAPPS, guidances, and policy related documents based on queries from users. Includes link to the document for further human review.","helps policy staff in searching documents containing regulations and responding to inquiries, and other policy related work. in developing policy and responding to inquiries, oppq staff often need to reference previously published policy documents in order to ensure consistency. this ai-enabled tool will allow policy leads and other staff to quickly identify and query the appropriate documents. . identifies applicable published mapps, guidances, and policy related documents based on queries from users. includes link to the document for further human review."
Packaging Materials and Suppliers,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"Objective of this use case aims is to extract data from unstructured sources to build an inventory of drug packaging materials and their suppliers to support staff on conducting drug supply chain analysis

The extracted data are presented in a dashboard for downstream analysis.",Support information extraction from unstructured documents,Operation and Maintenance,Neither,7/1/2023,7/1/2023,9/1/2024,Developed with both contracting and in-house resources.,75F40124A00003,Unknown,No,No,No,No,Applicant submissions,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CDEROne Analytics: FISMA-high environment,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Objective of this use case aims is to extract data from unstructured sources to build an inventory of drug packaging materials and their suppliers to support staff on conducting drug supply chain analysis

The extracted data are presented in a dashboard for downstream analysis. . Support information extraction from unstructured documents",objective of this use case aims is to extract data from unstructured sources to build an inventory of drug packaging materials and their suppliers to support staff on conducting drug supply chain analysis the extracted data are presented in a dashboard for downstream analysis. . support information extraction from unstructured documents
Process Large Amount of Submitted Docket Comments,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"To enhance the automatic process of dockets, we have created an AI/ML tool in CBER/HIVE that automatically download dockets and process them to accelerate the review of docket comments, significantly improving the efficiency and accuracy of our regulatory processes

Efficiently generate customized query outputs of VAERS queries for reviewers.",To provide reviewers adhoc VAERS queries.,Implementation and Assessment,Neither,9/1/2021,11/1/2021,Unknown,Developed in-house.,Unknown,Unknown,No,No,No,No,Public comments for various dockets.,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CBER HIVE,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"To enhance the automatic process of dockets, we have created an AI/ML tool in CBER/HIVE that automatically download dockets and process them to accelerate the review of docket comments, significantly improving the efficiency and accuracy of our regulatory processes

Efficiently generate customized query outputs of VAERS queries for reviewers. . To provide reviewers adhoc VAERS queries.","to enhance the automatic process of dockets, we have created an ai/ml tool in cber/hive that automatically download dockets and process them to accelerate the review of docket comments, significantly improving the efficiency and accuracy of our regulatory processes efficiently generate customized query outputs of vaers queries for reviewers. . to provide reviewers adhoc vaers queries."
Real World Data/Evidence ,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"Identify industry unstructured submissions containing Real World Data/Evidence (RWD/E) by analyzing parsed content for likely indicators of Real World Data/Evidence RWD/E to support congressional reporting.

The extracted data are presented in a dashboard for downstream human analysis.",Supports extracting text from unstructured documents and tagging for documents containing Real World Evidence and Real World Data,Implementation and Assessment,Neither,2/1/2024,2/1/2024,Unknown,Developed with both contracting and in-house resources.,75F40124A00003,Unknown,No,No,No,No,applicant submissions,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CDEROne Analytics: FISMA-high environment,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Identify industry unstructured submissions containing Real World Data/Evidence (RWD/E) by analyzing parsed content for likely indicators of Real World Data/Evidence RWD/E to support congressional reporting.

The extracted data are presented in a dashboard for downstream human analysis. . Supports extracting text from unstructured documents and tagging for documents containing Real World Evidence and Real World Data",identify industry unstructured submissions containing real world data/evidence (rwd/e) by analyzing parsed content for likely indicators of real world data/evidence rwd/e to support congressional reporting. the extracted data are presented in a dashboard for downstream human analysis. . supports extracting text from unstructured documents and tagging for documents containing real world evidence and real world data
Regulatory Starting Material ,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"Extract RSMs and their suppliers from unstructured module 3 industry submissions to create an inventory that will illuminate the upstream supply chain.

The extracted data are presented in a dashboard for downstream analysis.",Support information extraction from unstructured documents,Operation and Maintenance,Neither,9/1/2022,9/1/2022,8/1/2023,Developed with both contracting and in-house resources.,75F40124A00003,Unknown,No,No,No,No,applicant submissions,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CDEROne Analytics: FISMA-high environment,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Extract RSMs and their suppliers from unstructured module 3 industry submissions to create an inventory that will illuminate the upstream supply chain.

The extracted data are presented in a dashboard for downstream analysis. . Support information extraction from unstructured documents",extract rsms and their suppliers from unstructured module 3 industry submissions to create an inventory that will illuminate the upstream supply chain. the extracted data are presented in a dashboard for downstream analysis. . support information extraction from unstructured documents
Resource Capacity Planning,Department of Health and Human Services,HHS,FDA,Mission-Enabling (internal agency support),None of the above.,"Forecasting human drug review program submissions and corresponding FDA workload 

Improve the forecasting of human drug review program expected workload in order to adjust fee revenue and fees to fund resources needed to complete expected workload  ",Forecasts workload submissions across major user fee programs to support fee setting for the human drug review programs,Operation and Maintenance,Neither,1/1/2016,1/1/2017,8/1/2020,Developed with both contracting and in-house resources.,HHSF223201510012B,Unknown,No,No,No,Yes,FDA systems including DARRTS and Panorama,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,CDEROne Analytics: FISMA-high environment,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Forecasting human drug review program submissions and corresponding FDA workload 

Improve the forecasting of human drug review program expected workload in order to adjust fee revenue and fees to fund resources needed to complete expected workload . Forecasts workload submissions across major user fee programs to support fee setting for the human drug review programs",forecasting human drug review program submissions and corresponding fda workload improve the forecasting of human drug review program expected workload in order to adjust fee revenue and fees to fund resources needed to complete expected workload . forecasts workload submissions across major user fee programs to support fee setting for the human drug review programs
Supply Chain Resilience Program,Department of Health and Human Services,HHS,FDA,Emergency Management,None of the above.,"Forecasting demand of medical devices and supplies.

Forecast demand for critical devices during a variety of scenarios (e.g. natural disaster, PHE)  ",Aids in forecasting demand for critical devices under a variety of scenarios.    ,Operation and Maintenance,Neither,2/1/2022,2/1/2022,4/1/2023,Developed with contracting resources.,Unknown,Unknown,No,No,No,Yes,Unknown,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,FiDLE; RSCP and is being moved to CEDh-OSCR,6-12 months,Unknown,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"Forecasting demand of medical devices and supplies.

Forecast demand for critical devices during a variety of scenarios (e.g. natural disaster, PHE) . Aids in forecasting demand for critical devices under a variety of scenarios.","forecasting demand of medical devices and supplies. forecast demand for critical devices during a variety of scenarios (e.g. natural disaster, phe) . aids in forecasting demand for critical devices under a variety of scenarios."
AI Audit Resolution Assistant,Department of Health and Human Services,HHS,HRSA,Mission-Enabling (internal agency support),None of the above.,"The objective of this initiative is to reduce the time for HRSA auditors to review/resolve Single audits. HRSA has experienced an exponential increase in Single audits which is attributable in COVID funding attributable to the Provider Relief Fund programs. 

Success will be measured through the increase in the number of Single audits assigned to each auditor.  e.g., improved technological capabilities should allow HRSA Auditors to more efficiently review and closeout Single audits. ","HRSA will utilize generative AI to streamline the Single Audit resolution process and the creation of Management Decision Letters to formally close out and resolve Single audit findings.  The AI Audit Resolution Assistant (AIARA) includes a vector database comprised of Single audit documents assigned to HRSA, creates a retrieval augmented generation which integrates with a large language model to intelligently summarize audit findings and recommendations, and provides chatbot capability and reduce the cognitive load on HRSA auditors for any audit-specific questions. ",Implementation and Assessment,Neither,1/1/2024,3/1/2024,Unknown,Developed with both contracting and in-house resources.,75R60222F80097,Unknown,No,No,No,No,HRSA did not use agency data to train or fine-tune the large language model. HRSA did use publicly-available data from the Federal Audit Clearinghouse to evaluate the accuracy and precision of LLM outputs. ,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,"At the time of this response, HRSA is evaluating multiple HHS infrastructuure hosting services for Azure OpenAI platform with ATO in place.  A decision on a specific environment is forthcoming.",Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The objective of this initiative is to reduce the time for HRSA auditors to review/resolve Single audits. HRSA has experienced an exponential increase in Single audits which is attributable in COVID funding attributable to the Provider Relief Fund programs. 

Success will be measured through the increase in the number of Single audits assigned to each auditor.  e.g., improved technological capabilities should allow HRSA Auditors to more efficiently review and closeout Single audits. . HRSA will utilize generative AI to streamline the Single Audit resolution process and the creation of Management Decision Letters to formally close out and resolve Single audit findings.  The AI Audit Resolution Assistant (AIARA) includes a vector database comprised of Single audit documents assigned to HRSA, creates a retrieval augmented generation which integrates with a large language model to intelligently summarize audit findings and recommendations, and provides chatbot capability and reduce the cognitive load on HRSA auditors for any audit-specific questions.","the objective of this initiative is to reduce the time for hrsa auditors to review/resolve single audits. hrsa has experienced an exponential increase in single audits which is attributable in covid funding attributable to the provider relief fund programs. success will be measured through the increase in the number of single audits assigned to each auditor. e.g., improved technological capabilities should allow hrsa auditors to more efficiently review and closeout single audits. . hrsa will utilize generative ai to streamline the single audit resolution process and the creation of management decision letters to formally close out and resolve single audit findings. the ai audit resolution assistant (aiara) includes a vector database comprised of single audit documents assigned to hrsa, creates a retrieval augmented generation which integrates with a large language model to intelligently summarize audit findings and recommendations, and provides chatbot capability and reduce the cognitive load on hrsa auditors for any audit-specific questions."
Knowledge Navigator,Department of Health and Human Services,HHS,HRSA,Government Services (includes Benefits and Service Delivery),None of the above.,"The objective is to develop an AI model that can answer detailed and complex questions about the key programmatic document, Application and Program Guidance (APG), that is issued annually before the application cycle opens. The PoC LLM has Applicant and Program Guidance (APG) documents for 10 loan repayments and scholarship programs.

This will allow loan repayment and scholarship analysts and call center agents to  better respond to public inquiries from applicants and participants.  If successful the Knowledge Navigator could be made available to applicants and participants and reduce the volume of calls to the BHW Call Center along with the number of call center agents.  ",BHW has implemented a proof of concept Generative AI (GenAI) Large Language Model (LLM) Knowledge Navigator (KN) to support National Health Service Corps (NHSC) and Nurse Corps loan repayment and scholarship program analysts and call center agents respond to program applicants and participants. ,Acquisition and/or Development,Neither,2/1/2024,2/1/2024,Unknown,Developed with both contracting and in-house resources.,75R60224F34005,Unknown,Unknown,Unknown,Unknown,No,HRSA Application and Program Guidance (APG) documents for 10 HRSA loan repayment and scholarship programs.  All of which are public documents.  Here is an example: https://nhsc.hrsa.gov/sites/default/files/nhsc/loan-repayment/nhsc-students2service-lrp-application-program-guidance.pdf,Documentation is widely available,Unknown,Unknown,Yes,Bureau of Health Workforce Management Information System Solution (BMISS) Platform (UII: 009-000004158,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"The objective is to develop an AI model that can answer detailed and complex questions about the key programmatic document, Application and Program Guidance (APG), that is issued annually before the application cycle opens. The PoC LLM has Applicant and Program Guidance (APG) documents for 10 loan repayments and scholarship programs.

This will allow loan repayment and scholarship analysts and call center agents to  better respond to public inquiries from applicants and participants.  If successful the Knowledge Navigator could be made available to applicants and participants and reduce the volume of calls to the BHW Call Center along with the number of call center agents. . BHW has implemented a proof of concept Generative AI (GenAI) Large Language Model (LLM) Knowledge Navigator (KN) to support National Health Service Corps (NHSC) and Nurse Corps loan repayment and scholarship program analysts and call center agents respond to program applicants and participants.","the objective is to develop an ai model that can answer detailed and complex questions about the key programmatic document, application and program guidance (apg), that is issued annually before the application cycle opens. the poc llm has applicant and program guidance (apg) documents for 10 loan repayments and scholarship programs. this will allow loan repayment and scholarship analysts and call center agents to better respond to public inquiries from applicants and participants. if successful the knowledge navigator could be made available to applicants and participants and reduce the volume of calls to the bhw call center along with the number of call center agents. . bhw has implemented a proof of concept generative ai (genai) large language model (llm) knowledge navigator (kn) to support national health service corps (nhsc) and nurse corps loan repayment and scholarship program analysts and call center agents respond to program applicants and participants."
AI/ML for Study Section Prediction in NICHD,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"The project of AI/ML for Study Section Prediction is to classify the grant applications to specific Study Sections in the Scientific Review Branch in NICHD.  It employs human-in-the-loop development which enables human evaluation and offers the ability to override and alter model outputs.  Source data are from the NIH IMPAC II system via the eRA Review Module. The AI process and output data have no impact on the NIH IMPAC II system. The business area addressed is the alignment of appropriate Study Section for a given grant application to assist the Scientific Review Branch (SRB) at NICHD.

This project adapts the methods and systems developed for the RPAB AI/ML Referral System project to optimize the assignment of grant applications to the most suitable study sections. The AI system will contribute to NIH's mission by improving the efficiency and accuracy of alignment of grant applications to study sections, ensuring that applications are reviewed by the most appropriate researchers, leading to scientific discoveries and health advancement.",Internal: NIH ImpacII and eRA Review Module queries. Results are presented as study section (class) predictions and class probabilities.,Implementation and Assessment,Neither,10/1/2023,10/1/2023,Unknown,Developed in-house.,Unknown,Unknown,No,Yes,No,Yes,"NIH IMPAC II funded and unfunded grant application data is used. Unstructured text from project abstract, specific aims, and title are encoded and vectorized for model training and inference. Fiscal year, activity code, and RCDC terms are transformed via one-hot encoding for use in model training and inference. PII related to individuals associated with the grant is kept intact to preserve the integrity of the use case of grant application referral and the trends of researchers' focus on particular scientific areas.",Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"The project of AI/ML for Study Section Prediction is to classify the grant applications to specific Study Sections in the Scientific Review Branch in NICHD.  It employs human-in-the-loop development which enables human evaluation and offers the ability to override and alter model outputs.  Source data are from the NIH IMPAC II system via the eRA Review Module. The AI process and output data have no impact on the NIH IMPAC II system. The business area addressed is the alignment of appropriate Study Section for a given grant application to assist the Scientific Review Branch (SRB) at NICHD.

This project adapts the methods and systems developed for the RPAB AI/ML Referral System project to optimize the assignment of grant applications to the most suitable study sections. The AI system will contribute to NIH's mission by improving the efficiency and accuracy of alignment of grant applications to study sections, ensuring that applications are reviewed by the most appropriate researchers, leading to scientific discoveries and health advancement. . Internal: NIH ImpacII and eRA Review Module queries. Results are presented as study section (class) predictions and class probabilities.","the project of ai/ml for study section prediction is to classify the grant applications to specific study sections in the scientific review branch in nichd. it employs human-in-the-loop development which enables human evaluation and offers the ability to override and alter model outputs. source data are from the nih impac ii system via the era review module. the ai process and output data have no impact on the nih impac ii system. the business area addressed is the alignment of appropriate study section for a given grant application to assist the scientific review branch (srb) at nichd. this project adapts the methods and systems developed for the rpab ai/ml referral system project to optimize the assignment of grant applications to the most suitable study sections. the ai system will contribute to nih's mission by improving the efficiency and accuracy of alignment of grant applications to study sections, ensuring that applications are reviewed by the most appropriate researchers, leading to scientific discoveries and health advancement. . internal: nih impacii and era review module queries. results are presented as study section (class) predictions and class probabilities."
AI-enabled landscape analysis of New Approach Methodologies (NAMs) in biomedical research literature,Department of Health and Human Services,HHS,NIH,Health & Medical,None of the above.,"Part of NIH Common Fund strategic planning efforts for new program. Complement Animal Research in Experimentation (Complement-ARIE) aims to identify opportunities and use trends for NAMs in biomedical research. Methods development in using genAI for literature searching and extraction applicable to other use cases.

GenAI was used to survey the biomedical research literature to inform potential investment of resources and opportunities for the NIH Common Fund program. Methods are also applicable to other systematic literature review approaches.","Identify trends in use of New Approach Methodologies, gaps, and opportunities for investment by NIH.",Implementation and Assessment,Neither,1/1/2023,1/1/2023,Unknown,Developed with contracting resources.,HHSN273201500010C; GS00Q14OADU417,Unknown,Yes,No,Unknown,No,"Input data are publicly available, published research studies.",Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Part of NIH Common Fund strategic planning efforts for new program. Complement Animal Research in Experimentation (Complement-ARIE) aims to identify opportunities and use trends for NAMs in biomedical research. Methods development in using genAI for literature searching and extraction applicable to other use cases.

GenAI was used to survey the biomedical research literature to inform potential investment of resources and opportunities for the NIH Common Fund program. Methods are also applicable to other systematic literature review approaches. . Identify trends in use of New Approach Methodologies, gaps, and opportunities for investment by NIH.","part of nih common fund strategic planning efforts for new program. complement animal research in experimentation (complement-arie) aims to identify opportunities and use trends for nams in biomedical research. methods development in using genai for literature searching and extraction applicable to other use cases. genai was used to survey the biomedical research literature to inform potential investment of resources and opportunities for the nih common fund program. methods are also applicable to other systematic literature review approaches. . identify trends in use of new approach methodologies, gaps, and opportunities for investment by nih."
Assessment of DTT/NTP research effectiveness,Department of Health and Human Services,HHS,NIH,Health & Medical,None of the above.,"Dextr/LaserAI does not use PII, PHI or demographic data. Dextr is FEDRAMP certified and ATO in the cloud. Data sources for Dextr are published PDFs accessible through open source publications (e.g. PubMed Central) or through our subscriptions (e.g., NLM).

Modification and customization of the DEXTR automated extraction tool has been implemented to identify and capture Division of Translational Toxicology (DTT) and National Toxicology Program (NTP) research citations in the published literature, regulations, and popular press. The DEXTR automated workflow with user-verification supports the identification of these citations that will be used to assess impact and effectiveness of DTT and NTP research. Division Directors and Branch Chiefs require an evidence-based approach to assess the effectiveness of current and past research. Such an approach would be highly valuable for research prioritization. ",Identifies how DTT/NTP research was used or impacted the publication.,Acquisition and/or Development,Neither,10/1/2024,10/1/2024,Unknown,Developed with contracting resources.,Unknown,Unknown,Unknown,Unknown,Unknown,No,"Data sources for Dextr are published PDFs accessible through open source publications (e.g. PubMed Central) or through our subscriptions (e.g., NLM).",Documentation is missing or not available,Unknown,Unknown,Yes,DEXTR,Less than 6 months,No,Yes,No,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"Dextr/LaserAI does not use PII, PHI or demographic data. Dextr is FEDRAMP certified and ATO in the cloud. Data sources for Dextr are published PDFs accessible through open source publications (e.g. PubMed Central) or through our subscriptions (e.g., NLM).

Modification and customization of the DEXTR automated extraction tool has been implemented to identify and capture Division of Translational Toxicology (DTT) and National Toxicology Program (NTP) research citations in the published literature, regulations, and popular press. The DEXTR automated workflow with user-verification supports the identification of these citations that will be used to assess impact and effectiveness of DTT and NTP research. Division Directors and Branch Chiefs require an evidence-based approach to assess the effectiveness of current and past research. Such an approach would be highly valuable for research prioritization. . Identifies how DTT/NTP research was used or impacted the publication.","dextr/laserai does not use pii, phi or demographic data. dextr is fedramp certified and ato in the cloud. data sources for dextr are published pdfs accessible through open source publications (e.g. pubmed central) or through our subscriptions (e.g., nlm). modification and customization of the dextr automated extraction tool has been implemented to identify and capture division of translational toxicology (dtt) and national toxicology program (ntp) research citations in the published literature, regulations, and popular press. the dextr automated workflow with user-verification supports the identification of these citations that will be used to assess impact and effectiveness of dtt and ntp research. division directors and branch chiefs require an evidence-based approach to assess the effectiveness of current and past research. such an approach would be highly valuable for research prioritization. . identifies how dtt/ntp research was used or impacted the publication."
Assisted Referral Tool,Department of Health and Human Services,HHS,NIH,Health & Medical,None of the above.,"Review activities of the Center for Scientific Review (CSR) are organized into Review Branches (RBs). Each RB represents a cluster of study sections around a general scientific area. Grant applications generally are assigned first to an RB, and then to a specific study section within that RB for evaluation of scientific merit.

To provide assistance in assigning appropriate scientific areas for grant applications.","The Assisted Referral Tool (ART) recommends potentially appropriate study sections, based on the scientific content of a user’s grant application.  The user enters the application text and hits the Submit button to get a list of relevant study sections (scientific areas) in two groups, as “Strong” and “Possible” matches. The information provided to ART is only used to recommend study sections and is not stored or persisted.   The recommendations made by ART are solely for the benefit of the user. ",Operation and Maintenance,Neither,1/1/2014,1/1/2014,1/1/2015,Developed in-house.,Unknown,Unknown,Yes,No,No,Yes,IMPACII,Documentation is missing or not available,Yes,No – agency does not have access to source code.,Yes,Center for Scientific Review  General Support System (CSR GSS) ,Less than 6 months,Yes,No,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Review activities of the Center for Scientific Review (CSR) are organized into Review Branches (RBs). Each RB represents a cluster of study sections around a general scientific area. Grant applications generally are assigned first to an RB, and then to a specific study section within that RB for evaluation of scientific merit.

To provide assistance in assigning appropriate scientific areas for grant applications. . The Assisted Referral Tool (ART) recommends potentially appropriate study sections, based on the scientific content of a user’s grant application.  The user enters the application text and hits the Submit button to get a list of relevant study sections (scientific areas) in two groups, as “Strong” and “Possible” matches. The information provided to ART is only used to recommend study sections and is not stored or persisted.   The recommendations made by ART are solely for the benefit of the user.","review activities of the center for scientific review (csr) are organized into review branches (rbs). each rb represents a cluster of study sections around a general scientific area. grant applications generally are assigned first to an rb, and then to a specific study section within that rb for evaluation of scientific merit. to provide assistance in assigning appropriate scientific areas for grant applications. . the assisted referral tool (art) recommends potentially appropriate study sections, based on the scientific content of a user’s grant application. the user enters the application text and hits the submit button to get a list of relevant study sections (scientific areas) in two groups, as “strong” and “possible” matches. the information provided to art is only used to recommend study sections and is not stored or persisted. the recommendations made by art are solely for the benefit of the user."
Automated annotation of study data using knowledge organization systems,Department of Health and Human Services,HHS,NIH,Health & Medical,None of the above.,"Application to support automation of systematic review processes and annotation of extracted information from studies in the published literature.

Automating the annotation of extracted information from environmental health studies will support joining with other structured datasets to serve as training data for predictive model development, validation of new approaches, and more robust analyses on weight of evidence of chemical effects on biological systems.",Annotation of information using knowledge organization systems.,Implementation and Assessment,Neither,1/1/2021,1/1/2022,Unknown,Developed with contracting resources.,GS00Q14OADU417,Unknown,No,No,No,No,"Input data are publicly available, published research studies or publicly available through DTT in CEBS.",Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,More than 12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Application to support automation of systematic review processes and annotation of extracted information from studies in the published literature.

Automating the annotation of extracted information from environmental health studies will support joining with other structured datasets to serve as training data for predictive model development, validation of new approaches, and more robust analyses on weight of evidence of chemical effects on biological systems. . Annotation of information using knowledge organization systems.","application to support automation of systematic review processes and annotation of extracted information from studies in the published literature. automating the annotation of extracted information from environmental health studies will support joining with other structured datasets to serve as training data for predictive model development, validation of new approaches, and more robust analyses on weight of evidence of chemical effects on biological systems. . annotation of information using knowledge organization systems."
Automated Basic-Applied Categorization of extramural grants,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"This machine-learning algorithm is in development. Currently it uses information on NIMH-funded extramural research projects to categorize them as basic or applied research per the federal definitions for each. The data it is trained on is supplied from extramural grant applications and the same fields will be used for subsequent classification of new grants. For new grants, the algorithm will propose a categorization which can be reviewed, confirmed, or edited by an NIMH staff member. The resulting information will be used for internal, analytical purposes and annual reporting of NIMH-funded basic and applied research. The AI will NOT influence funding decisions, require or use PII, or use any other sensitive, personal, or health information.

The algorithm is intended to be consistent in identifying basic and applied research, reduce burden of review by NIMH staff, and provide a complementary perspective to human review.","The algorithm produces categorization of basic and applied research that is used for internal analysis and annual reporting. The algorithm is trained on the following data from extramural grants from fiscal years (FY) 2000-2022 and will use the same for grants in FY2025 and going forward to propose a categorization of basic or applied research: Program Class Code, NIMH Division, NIH Research, Condition, and Disease Categorization (RCDC), Human Subjects Codes, Animal Subjects Code, Clinical Trial NIMH Division, text of grant title, abstracts, specific aims. These data categories are all obtained from NIH IMPACII and will be applied to 100% of NIMH extramural grants, excluding L activity codes. This constitutes approximately 1,500 new grants per FY. Algorithm performance will be assessed each year, comparing the proposed classifications to a manually reviewed sample, and input received from NIMH staff regarding edits to proposed classifications. The frequency of processing is yet to be determined. No anonymization or de-identification processes are employed as personal information is used neither to train, nor classify grants using this algorithm.",Acquisition and/or Development,Neither,6/1/2022,11/1/2022,Unknown,Developed in-house.,Unknown,Unknown,Unknown,Unknown,Unknown,No,"The algorithm is trained on the following data from extramural grants from fiscal years (FY) 2000-2022 and will use the same for grants in FY2025 and going forward to propose a categorization of basic or applied research: Program Class Code, NIMH Division, NIH Research, Condition, and Disease Categorization (RCDC), Human Subjects Codes, Animal Subjects Code, Clinical Trial, NIMH Division, text of grant title, abstracts, and specific aims. These data are all obtained from NIH IMPACII and will be applied to 100% of NIMH extramural grants, excluding L activity codes. This constitutes approximately 1,500 new grants per FY. No anonymization or de-identification processes are employed as personal information is used neither to train nor classify grants using this algorithm.",Documentation is complete,Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,No,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.38095238095238093,"This machine-learning algorithm is in development. Currently it uses information on NIMH-funded extramural research projects to categorize them as basic or applied research per the federal definitions for each. The data it is trained on is supplied from extramural grant applications and the same fields will be used for subsequent classification of new grants. For new grants, the algorithm will propose a categorization which can be reviewed, confirmed, or edited by an NIMH staff member. The resulting information will be used for internal, analytical purposes and annual reporting of NIMH-funded basic and applied research. The AI will NOT influence funding decisions, require or use PII, or use any other sensitive, personal, or health information.

The algorithm is intended to be consistent in identifying basic and applied research, reduce burden of review by NIMH staff, and provide a complementary perspective to human review. . The algorithm produces categorization of basic and applied research that is used for internal analysis and annual reporting. The algorithm is trained on the following data from extramural grants from fiscal years (FY) 2000-2022 and will use the same for grants in FY2025 and going forward to propose a categorization of basic or applied research: Program Class Code, NIMH Division, NIH Research, Condition, and Disease Categorization (RCDC), Human Subjects Codes, Animal Subjects Code, Clinical Trial NIMH Division, text of grant title, abstracts, specific aims. These data categories are all obtained from NIH IMPACII and will be applied to 100% of NIMH extramural grants, excluding L activity codes. This constitutes approximately 1,500 new grants per FY. Algorithm performance will be assessed each year, comparing the proposed classifications to a manually reviewed sample, and input received from NIMH staff regarding edits to proposed classifications. The frequency of processing is yet to be determined. No anonymization or de-identification processes are employed as personal information is used neither to train, nor classify grants using this algorithm.","this machine-learning algorithm is in development. currently it uses information on nimh-funded extramural research projects to categorize them as basic or applied research per the federal definitions for each. the data it is trained on is supplied from extramural grant applications and the same fields will be used for subsequent classification of new grants. for new grants, the algorithm will propose a categorization which can be reviewed, confirmed, or edited by an nimh staff member. the resulting information will be used for internal, analytical purposes and annual reporting of nimh-funded basic and applied research. the ai will not influence funding decisions, require or use pii, or use any other sensitive, personal, or health information. the algorithm is intended to be consistent in identifying basic and applied research, reduce burden of review by nimh staff, and provide a complementary perspective to human review. . the algorithm produces categorization of basic and applied research that is used for internal analysis and annual reporting. the algorithm is trained on the following data from extramural grants from fiscal years (fy) 2000-2022 and will use the same for grants in fy2025 and going forward to propose a categorization of basic or applied research: program class code, nimh division, nih research, condition, and disease categorization (rcdc), human subjects codes, animal subjects code, clinical trial nimh division, text of grant title, abstracts, specific aims. these data categories are all obtained from nih impacii and will be applied to 100% of nimh extramural grants, excluding l activity codes. this constitutes approximately 1,500 new grants per fy. algorithm performance will be assessed each year, comparing the proposed classifications to a manually reviewed sample, and input received from nimh staff regarding edits to proposed classifications. the frequency of processing is yet to be determined. no anonymization or de-identification processes are employed as personal information is used neither to train, nor classify grants using this algorithm."
Automated extraction of study methods and assessment of reliability,Department of Health and Human Services,HHS,NIH,Health & Medical,None of the above.,"Application to support automation of systematic review processes and quality assessment of studies in the published literature (environmental health studies).

This project is focused on automating the identification of studies with reliable/well established methods (e.g., corresponding to regulatory guidelines) from the literature. Supervised and unsupervised methods are being applied to a) extract the methods details from known studies, and b) identify studies whose methods fulfill reliability criteria.   Identification and critical assessment of existing research is required for developing evidence-based conclusions, directing future research, and informing policy decisions by scientists, Branch Chiefs and Division Directors. Study reliability or quality are important considerations in the assessment of published research studies and are very time and labor intensive using manual approaches. ",Input: published environmental health studies; Output: identification of study methods and categorization of reliability/methods quality.,Acquisition and/or Development,Neither,1/1/2017,1/1/2018,Unknown,Developed with contracting resources.,Unknown,Unknown,Unknown,Unknown,Unknown,No,"Input data are publicly available, published research studies or publicly available through DTT in CEBS.",Documentation is complete,Unknown,Unknown,No,Unknown,More than 12 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.38095238095238093,"Application to support automation of systematic review processes and quality assessment of studies in the published literature (environmental health studies).

This project is focused on automating the identification of studies with reliable/well established methods (e.g., corresponding to regulatory guidelines) from the literature. Supervised and unsupervised methods are being applied to a) extract the methods details from known studies, and b) identify studies whose methods fulfill reliability criteria.   Identification and critical assessment of existing research is required for developing evidence-based conclusions, directing future research, and informing policy decisions by scientists, Branch Chiefs and Division Directors. Study reliability or quality are important considerations in the assessment of published research studies and are very time and labor intensive using manual approaches. . Input: published environmental health studies; Output: identification of study methods and categorization of reliability/methods quality.","application to support automation of systematic review processes and quality assessment of studies in the published literature (environmental health studies). this project is focused on automating the identification of studies with reliable/well established methods (e.g., corresponding to regulatory guidelines) from the literature. supervised and unsupervised methods are being applied to a) extract the methods details from known studies, and b) identify studies whose methods fulfill reliability criteria. identification and critical assessment of existing research is required for developing evidence-based conclusions, directing future research, and informing policy decisions by scientists, branch chiefs and division directors. study reliability or quality are important considerations in the assessment of published research studies and are very time and labor intensive using manual approaches. . input: published environmental health studies; output: identification of study methods and categorization of reliability/methods quality."
Automation of Receipt & Referral Process of NIDDK grant applications,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"This use case is centrally reported under OER's Internal Referral Module. NIDDK receives about 100 to 500 applications per week. NIDDK referral staff uses the IMPAC II QVR tool to stratify the applications between divisions based on a “Like Matrix” function to match the Division’s portfolios and the Title, Abstract, and specific aims of grant applications. This process is laborious and manual. Hence, automation was proposed. The scope of the effort is to provide a capability for NIDDK to analyze grant applications using NIDDK-specific business rules, complete the pilot, and the full implementation of the Artificial Intelligence (AI) and Natural Language Processing (NLP) tools for auto assignment of program class codes.

NIDDK receives about 100 to 500 applications per week. The AI and machine learning tool will stratify the applications between programmatic divisions and Program Class Code (PCC) to match the Division’s portfolios based on the Title, Abstract, and specific aims of grant applications. It collectively saves NIDDK Program and Referral Staff 3000+ hours annually on Receipt and Referral activities. In addition, it instantly recommends PCC, allows Division designees to accept or change recommendations, tracks all grants that need PCC assignment, gives real-time information about the application status, and support business rules/generates reports.","Input: Title, Abstract, Public Health Narrative, Specific Aims of grant applications from the FY 2018 to 2022.

Output: Division and Program Class Code (PCC) of grants from FY 23/24.",Operation and Maintenance,Neither,10/1/2021,10/1/2021,10/1/2023,Developed with contracting resources.,2-DK22-560-02 and OER-23-014,Unknown,No,No,No,Yes,"Yes, It used the NIH/NIDDK grants applications data submitted by research investigators. The prediction models use the Title, Abstract, Public Health Narrative, and specific aims of grant applications from the FY 2018 to 2022 to train the models and FY 23 and F24 grants applications data were used for predicting programmatic divisions and Program Class Codes (PCC).",Documentation is complete,Yes,No – agency does not have access to source code.,No,Unknown,6-12 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"This use case is centrally reported under OER's Internal Referral Module. NIDDK receives about 100 to 500 applications per week. NIDDK referral staff uses the IMPAC II QVR tool to stratify the applications between divisions based on a “Like Matrix” function to match the Division’s portfolios and the Title, Abstract, and specific aims of grant applications. This process is laborious and manual. Hence, automation was proposed. The scope of the effort is to provide a capability for NIDDK to analyze grant applications using NIDDK-specific business rules, complete the pilot, and the full implementation of the Artificial Intelligence (AI) and Natural Language Processing (NLP) tools for auto assignment of program class codes.

NIDDK receives about 100 to 500 applications per week. The AI and machine learning tool will stratify the applications between programmatic divisions and Program Class Code (PCC) to match the Division’s portfolios based on the Title, Abstract, and specific aims of grant applications. It collectively saves NIDDK Program and Referral Staff 3000+ hours annually on Receipt and Referral activities. In addition, it instantly recommends PCC, allows Division designees to accept or change recommendations, tracks all grants that need PCC assignment, gives real-time information about the application status, and support business rules/generates reports. . Input: Title, Abstract, Public Health Narrative, Specific Aims of grant applications from the FY 2018 to 2022.

Output: Division and Program Class Code (PCC) of grants from FY 23/24.","this use case is centrally reported under oer's internal referral module. niddk receives about 100 to 500 applications per week. niddk referral staff uses the impac ii qvr tool to stratify the applications between divisions based on a “like matrix” function to match the division’s portfolios and the title, abstract, and specific aims of grant applications. this process is laborious and manual. hence, automation was proposed. the scope of the effort is to provide a capability for niddk to analyze grant applications using niddk-specific business rules, complete the pilot, and the full implementation of the artificial intelligence (ai) and natural language processing (nlp) tools for auto assignment of program class codes. niddk receives about 100 to 500 applications per week. the ai and machine learning tool will stratify the applications between programmatic divisions and program class code (pcc) to match the division’s portfolios based on the title, abstract, and specific aims of grant applications. it collectively saves niddk program and referral staff 3000+ hours annually on receipt and referral activities. in addition, it instantly recommends pcc, allows division designees to accept or change recommendations, tracks all grants that need pcc assignment, gives real-time information about the application status, and support business rules/generates reports. . input: title, abstract, public health narrative, specific aims of grant applications from the fy 2018 to 2022. output: division and program class code (pcc) of grants from fy 23/24."
Best Match: New relevance search for PubMed,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"NLM is the world’s largest biomedical library. Best Match is a relevance search algorithm for NLM’s PubMed – a free search engine for biomedical literature accessed by millions of users around the world every day.

Best Match increases the effectiveness of PubMed searches across the rapidly growing collection of biomedical literature to help users efficiently find the most relevant and high-quality information they need. PubMed comprises more than 37 million citations for biomedical literature from MEDLINE, life science journals, and online books.","This AI technique leverages the intelligence of our users and cutting-edge machine-learning technology as an alternative to the traditional date-sort order that appears in many traditional search engines. Trained with past user searches with dozens of relevance-ranking factors, the Best Match algorithm demonstrates state-of-the-art retrieval performance and an improved user experience. ",Operation and Maintenance,Neither,6/1/2016,6/1/2016,1/1/2023,Developed in-house.,Unknown,Unknown,Yes,No,No,No,"The PubMed literature collection includes articles relevant to biomedicine and the life sciences, broadly defined to encompass the information needs of those working in healthcare and life sciences. The Best Match algorithm was trained on interaction data from clicks on lists of articles returned when users search the PubMed collection with search terms.",Documentation is widely available,Yes,Yes – source code is publicly available.,No,Unknown,6-12 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"NLM is the world’s largest biomedical library. Best Match is a relevance search algorithm for NLM’s PubMed – a free search engine for biomedical literature accessed by millions of users around the world every day.

Best Match increases the effectiveness of PubMed searches across the rapidly growing collection of biomedical literature to help users efficiently find the most relevant and high-quality information they need. PubMed comprises more than 37 million citations for biomedical literature from MEDLINE, life science journals, and online books. . This AI technique leverages the intelligence of our users and cutting-edge machine-learning technology as an alternative to the traditional date-sort order that appears in many traditional search engines. Trained with past user searches with dozens of relevance-ranking factors, the Best Match algorithm demonstrates state-of-the-art retrieval performance and an improved user experience.","nlm is the world’s largest biomedical library. best match is a relevance search algorithm for nlm’s pubmed – a free search engine for biomedical literature accessed by millions of users around the world every day. best match increases the effectiveness of pubmed searches across the rapidly growing collection of biomedical literature to help users efficiently find the most relevant and high-quality information they need. pubmed comprises more than 37 million citations for biomedical literature from medline, life science journals, and online books. . this ai technique leverages the intelligence of our users and cutting-edge machine-learning technology as an alternative to the traditional date-sort order that appears in many traditional search engines. trained with past user searches with dozens of relevance-ranking factors, the best match algorithm demonstrates state-of-the-art retrieval performance and an improved user experience."
Biomedical Citation Selector (BmCS),Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"NLM is the world’s largest biomedical library. MEDLINE is NLM's bibliographic database that contains references to journal articles in life sciences, with a concentration on biomedicine.

Through automation, NLM is able standardize article selection and reduce the amount of time it takes to process MEDLINE articles.",The output is sets of citation records that are classified as relevant to biomedicine and the life sciences. Automation of article selection allows NLM to more efficiently and effectively index and host relevant information for the public. ,Implementation and Assessment,Neither,1/1/2021,6/1/2021,Unknown,Developed in-house.,Unknown,Unknown,No,No,No,Yes,PubMed citation data that was submitted by publishers and stored in the agency database was used.,Documentation is complete,Yes,Yes – source code is publicly available.,No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"NLM is the world’s largest biomedical library. MEDLINE is NLM's bibliographic database that contains references to journal articles in life sciences, with a concentration on biomedicine.

Through automation, NLM is able standardize article selection and reduce the amount of time it takes to process MEDLINE articles. . The output is sets of citation records that are classified as relevant to biomedicine and the life sciences. Automation of article selection allows NLM to more efficiently and effectively index and host relevant information for the public.","nlm is the world’s largest biomedical library. medline is nlm's bibliographic database that contains references to journal articles in life sciences, with a concentration on biomedicine. through automation, nlm is able standardize article selection and reduce the amount of time it takes to process medline articles. . the output is sets of citation records that are classified as relevant to biomedicine and the life sciences. automation of article selection allows nlm to more efficiently and effectively index and host relevant information for the public."
Chatbot for researchers to find data sets for environmental health research efforts,Department of Health and Human Services,HHS,NIH,Health & Medical,None of the above.,"This project is focused on the use of large language models and chatbot technology to provide a way for researchers to find data sets of relevance for environmental health research efforts.

The primary objective is providing a tool that aids researchers in finding useful resources, specifically data sets, thus benefitting research efforts in finding the best data to use quickly.","The AI will use description of data sets that come from both NIH and federal data catalogs.  This will be periodically updated.  As output, the AI will provide rank order listings of recommended data sets.",Acquisition and/or Development,Neither,5/1/2024,5/1/2024,Unknown,Developed with contracting resources.,SOAR,Unknown,Unknown,Unknown,Unknown,No,Input data are publicly available.,Documentation is widely available,Unknown,Unknown,No,Unknown,Less than 6 months,No,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"This project is focused on the use of large language models and chatbot technology to provide a way for researchers to find data sets of relevance for environmental health research efforts.

The primary objective is providing a tool that aids researchers in finding useful resources, specifically data sets, thus benefitting research efforts in finding the best data to use quickly. . The AI will use description of data sets that come from both NIH and federal data catalogs.  This will be periodically updated.  As output, the AI will provide rank order listings of recommended data sets.","this project is focused on the use of large language models and chatbot technology to provide a way for researchers to find data sets of relevance for environmental health research efforts. the primary objective is providing a tool that aids researchers in finding useful resources, specifically data sets, thus benefitting research efforts in finding the best data to use quickly. . the ai will use description of data sets that come from both nih and federal data catalogs. this will be periodically updated. as output, the ai will provide rank order listings of recommended data sets."
Clinical Trial Predictor,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"NIGMS supports basic research that increases our understanding of biological processes and lays the foundation for advances in disease diagnosis, treatment, and prevention. NIGMS-funded scientists investigate how living systems work at a range of levels from molecules and cells to tissues and organs, in research organisms, humans, and populations.

The AI tool predicts whether grant applications may involve clinical trials based on the text of their titles, abstracts, narratives, specific aims, and research strategies. ","The Clinical Trial Predictor uses an ensemble of several NLP and ML algorithms to predict whether applications may involve clinical trials based on the text of their titles, abstracts, narratives, specific aims, and research strategies. The responses are only used to make extramural officers aware of the fact that an application submitted to a NOFO that doesn't allow clinical trials may contain a clinical trial.  The extramural officers then evaluate the identified applications for the presence of a clinical trial.  No decisions are made by the AI, nor is the AI output used as the principle basis of a decision; it is just used to help the extramural officers identify applications containing clinical trials more quickly.",Implementation and Assessment,Neither,1/1/2023,1/1/2023,Unknown,Developed in-house.,Unknown,Unknown,No,No,No,Yes,All data come from the internal NIH IMPAC II database.,Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"NIGMS supports basic research that increases our understanding of biological processes and lays the foundation for advances in disease diagnosis, treatment, and prevention. NIGMS-funded scientists investigate how living systems work at a range of levels from molecules and cells to tissues and organs, in research organisms, humans, and populations.

The AI tool predicts whether grant applications may involve clinical trials based on the text of their titles, abstracts, narratives, specific aims, and research strategies. . The Clinical Trial Predictor uses an ensemble of several NLP and ML algorithms to predict whether applications may involve clinical trials based on the text of their titles, abstracts, narratives, specific aims, and research strategies. The responses are only used to make extramural officers aware of the fact that an application submitted to a NOFO that doesn't allow clinical trials may contain a clinical trial.  The extramural officers then evaluate the identified applications for the presence of a clinical trial.  No decisions are made by the AI, nor is the AI output used as the principle basis of a decision; it is just used to help the extramural officers identify applications containing clinical trials more quickly.","nigms supports basic research that increases our understanding of biological processes and lays the foundation for advances in disease diagnosis, treatment, and prevention. nigms-funded scientists investigate how living systems work at a range of levels from molecules and cells to tissues and organs, in research organisms, humans, and populations. the ai tool predicts whether grant applications may involve clinical trials based on the text of their titles, abstracts, narratives, specific aims, and research strategies. . the clinical trial predictor uses an ensemble of several nlp and ml algorithms to predict whether applications may involve clinical trials based on the text of their titles, abstracts, narratives, specific aims, and research strategies. the responses are only used to make extramural officers aware of the fact that an application submitted to a nofo that doesn't allow clinical trials may contain a clinical trial. the extramural officers then evaluate the identified applications for the presence of a clinical trial. no decisions are made by the ai, nor is the ai output used as the principle basis of a decision; it is just used to help the extramural officers identify applications containing clinical trials more quickly."
ClinicalTrials.gov Protocol Registration and Results System Review Assistant,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"ClinicalTrials.gov is a website and online database of clinical research studies and information about their results. The purpose of ClinicalTrials.gov is to provide information about clinical research studies to the public, researchers, and health care professionals.

This project aims to help ClinicalTrials.gov determine whether the addition of AI could make reviewing study records more efficient and effective.",The classifier takes as input free-text related to a study’s endpoint and returns a prediction of whether one or more issues is present. ,Acquisition and/or Development,Neither,2/1/2021,11/1/2021,Unknown,Developed with contracting resources.,GSA Contract # GS-10F-0064W    NLM BPA Task Order: 75N97019A00005/75N97021F00006,Unknown,Unknown,Unknown,Unknown,No,"We utilized agency-owned data for training and evaluating the model's performance. The data consisted of clinical study record submissions from ClinicalTrials.gov's Protocol Registration and Results System (PRS) that were divided into two groups:

Set A: This group included submissions that did not pass quality control review, meaning they were returned to data submitters for revision and, in certain cases, are versions of the record that were never posted on ClinicalTrials.gov (not accessible to the public). These submissions had a relevant issue identified during manual quality control review, providing examples of endpoints known to contain the specific issue.

Set B: This group included submissions that were posted on ClinicalTrials.gov with endpoints that had never been labeled as having the specific issue. These submissions provided examples of endpoints that were considered not to have the specific issue.The input to the classifiers consisted of endpoint attributes, including Measure Titles, Descriptions, Time Frames, Units of Measure, and labels indicating whether the issue was present. This data did not have any personal identifiers. The data was in a structured format in the PRS, but the specific endpoint attributes were provided as free-text.

We processed over 10,000 endpoints from Set A, which included examples with identified issues, and over 40,000 endpoints from Set B, which had not been flagged with issues. It is important to note that the labeling of issues in the dataset is not exhaustive. Some endpoints may present multiple issues combined and labeled as a single issue, and there is a possibility that some issues were missed by reviewers. Consequently, Set B might contain endpoints that actually have the issue despite not being labeled as such. ",Documentation is complete,Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"ClinicalTrials.gov is a website and online database of clinical research studies and information about their results. The purpose of ClinicalTrials.gov is to provide information about clinical research studies to the public, researchers, and health care professionals.

This project aims to help ClinicalTrials.gov determine whether the addition of AI could make reviewing study records more efficient and effective. . The classifier takes as input free-text related to a study’s endpoint and returns a prediction of whether one or more issues is present.","clinicaltrials.gov is a website and online database of clinical research studies and information about their results. the purpose of clinicaltrials.gov is to provide information about clinical research studies to the public, researchers, and health care professionals. this project aims to help clinicaltrials.gov determine whether the addition of ai could make reviewing study records more efficient and effective. . the classifier takes as input free-text related to a study’s endpoint and returns a prediction of whether one or more issues is present."
Computed Author: author name disambiguation for PubMed,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"NLM is the world’s largest biomedical library. PubMed comprises more than 37 million citations for biomedical literature from MEDLINE, life science journals, and online books.

 NLM developed a machine-learning method to score the features for disambiguating a pair of papers with ambiguous names. Subsequently, agglomerative clustering is employed to collect all papers belonging to the same authors from those classified pairs. Disambiguation performance is evaluated with manual verification of random samples of pairs from clustering results, with a higher accuracy than other state-of-the-art methods. It has been integrated into PubMed to facilitate author name searches.","PubMed users frequently use author names in queries for retrieving scientific literature. However, author name ambiguity (different authors share the same name) may lead to irrelevant retrieval results.",Operation and Maintenance,Neither,5/1/2009,5/1/2009,1/1/2023,Developed in-house.,Unknown,Unknown,No,No,No,No,Searches of the PubMed literature collection using author names were used to create training and testing data sets consisting of article sets.,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"NLM is the world’s largest biomedical library. PubMed comprises more than 37 million citations for biomedical literature from MEDLINE, life science journals, and online books.

 NLM developed a machine-learning method to score the features for disambiguating a pair of papers with ambiguous names. Subsequently, agglomerative clustering is employed to collect all papers belonging to the same authors from those classified pairs. Disambiguation performance is evaluated with manual verification of random samples of pairs from clustering results, with a higher accuracy than other state-of-the-art methods. It has been integrated into PubMed to facilitate author name searches. . PubMed users frequently use author names in queries for retrieving scientific literature. However, author name ambiguity (different authors share the same name) may lead to irrelevant retrieval results.","nlm is the world’s largest biomedical library. pubmed comprises more than 37 million citations for biomedical literature from medline, life science journals, and online books. nlm developed a machine-learning method to score the features for disambiguating a pair of papers with ambiguous names. subsequently, agglomerative clustering is employed to collect all papers belonging to the same authors from those classified pairs. disambiguation performance is evaluated with manual verification of random samples of pairs from clustering results, with a higher accuracy than other state-of-the-art methods. it has been integrated into pubmed to facilitate author name searches. . pubmed users frequently use author names in queries for retrieving scientific literature. however, author name ambiguity (different authors share the same name) may lead to irrelevant retrieval results."
COVID-19 Pandemic Vulnerability Index Dashboard,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"The dashboard creates risk profiles, called PVI scorecards, for every county in the United States, continuously updated with the latest data that summarize and visualize overall disease risk. 

Aims to reduce the impact of COVID-19 on vulnerable populations by aiding equitable decision-making on resource allocation and implementation of local- and state-level interventions. Available at https://covid19pvi.niehs.nih.gov/. ","Inputs are location-specific current infection rates, baseline population concentration, current interventions, and health and environmental vulnerabilities. Outputs are data visualizations and vulnerability estimates.",Operation and Maintenance,Neither,3/1/2020,3/1/2020,3/1/2020,Developed in-house.,Unknown,Unknown,Yes,No,No,No,Source data is derived from publicly available resources detailed here: https://www.niehs.nih.gov/research/programs/coronavirus/covid19pvi/details/.,Documentation is missing or not available,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,No,No,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The dashboard creates risk profiles, called PVI scorecards, for every county in the United States, continuously updated with the latest data that summarize and visualize overall disease risk. 

Aims to reduce the impact of COVID-19 on vulnerable populations by aiding equitable decision-making on resource allocation and implementation of local- and state-level interventions. Available at https://covid19pvi.niehs.nih.gov/. . Inputs are location-specific current infection rates, baseline population concentration, current interventions, and health and environmental vulnerabilities. Outputs are data visualizations and vulnerability estimates.","the dashboard creates risk profiles, called pvi scorecards, for every county in the united states, continuously updated with the latest data that summarize and visualize overall disease risk. aims to reduce the impact of covid-19 on vulnerable populations by aiding equitable decision-making on resource allocation and implementation of local- and state-level interventions. available at https://covid19pvi.niehs.nih.gov/. . inputs are location-specific current infection rates, baseline population concentration, current interventions, and health and environmental vulnerabilities. outputs are data visualizations and vulnerability estimates."
CSR Public Chatbot (CPC),Department of Health and Human Services,HHS,NIH,Health & Medical,None of the above.,"This tool is a chatbot that answers questions from potential applicants and reviewers. Instead of providing lists of links to look through, the chatbot provides answers with links where the answers were drawn from. Source materials are limited to official government web pages, and a disclaimer is included cautioning the user to check the source materials to confirm the accuracy of the answers.

This chatbot is intended to provide answers to the public about questions related to NIH peer review policies and procedures. ","Input: Applicant/Reviewer Questions.
Output: Answers to the questions ",Operation and Maintenance,Neither,1/1/2021,1/1/2021,1/1/2021,Developed in-house.,Unknown,Unknown,Yes,No,No,Yes,Applicant FAQs and publicly available content from public.csr.nih.gov website.,Documentation is missing or not available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Center for Scientific Review  General Support System (CSR GSS),Less than 6 months,Yes,No,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"This tool is a chatbot that answers questions from potential applicants and reviewers. Instead of providing lists of links to look through, the chatbot provides answers with links where the answers were drawn from. Source materials are limited to official government web pages, and a disclaimer is included cautioning the user to check the source materials to confirm the accuracy of the answers.

This chatbot is intended to provide answers to the public about questions related to NIH peer review policies and procedures. . Input: Applicant/Reviewer Questions.
Output: Answers to the questions","this tool is a chatbot that answers questions from potential applicants and reviewers. instead of providing lists of links to look through, the chatbot provides answers with links where the answers were drawn from. source materials are limited to official government web pages, and a disclaimer is included cautioning the user to check the source materials to confirm the accuracy of the answers. this chatbot is intended to provide answers to the public about questions related to nih peer review policies and procedures. . input: applicant/reviewer questions. output: answers to the questions"
DAIT AIDS-Related Research Solution,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"NIAID administers HIV/AIDS grant portfolios.

The incoming grant applications are ranked based on these predictions and more highly-ranked applications are prioritized for review.","The tool uses natural language processing text extraction and classification algorithms to predict both priority (High, Medium, Low) and category (Area of research) for a grant application. After DAIT awards grants, they review all awarded grants to determine if any of the awardees are conducting research that could be considered AIDS-Related. NIH’s Office of AIDS Research (OAR) will fund all or part of the grants if the research is considered AIDS-Related. The DAIT AIDS-Related Research solution (DAIT ARR), categorizes the awarded grants into research topics. This categorization is based on the title and abstract of the grant application.",Operation and Maintenance,Neither,1/1/2017,1/1/2017,1/1/2018,Developed with contracting resources.,Not Reported,Unknown,No,No,No,Yes,"Yes, a dataset was curated to train the model and is evaluated manually by user input.",Documentation is complete,Yes,Yes – source code is publicly available.,Yes,Not Reported,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"NIAID administers HIV/AIDS grant portfolios.

The incoming grant applications are ranked based on these predictions and more highly-ranked applications are prioritized for review. . The tool uses natural language processing text extraction and classification algorithms to predict both priority (High, Medium, Low) and category (Area of research) for a grant application. After DAIT awards grants, they review all awarded grants to determine if any of the awardees are conducting research that could be considered AIDS-Related. NIH’s Office of AIDS Research (OAR) will fund all or part of the grants if the research is considered AIDS-Related. The DAIT AIDS-Related Research solution (DAIT ARR), categorizes the awarded grants into research topics. This categorization is based on the title and abstract of the grant application.","niaid administers hiv/aids grant portfolios. the incoming grant applications are ranked based on these predictions and more highly-ranked applications are prioritized for review. . the tool uses natural language processing text extraction and classification algorithms to predict both priority (high, medium, low) and category (area of research) for a grant application. after dait awards grants, they review all awarded grants to determine if any of the awardees are conducting research that could be considered aids-related. nih’s office of aids research (oar) will fund all or part of the grants if the research is considered aids-related. the dait aids-related research solution (dait arr), categorizes the awarded grants into research topics. this categorization is based on the title and abstract of the grant application."
Detecting Overlapping Science (DOS),Department of Health and Human Services,HHS,NIH,Health & Medical,None of the above.,"This tool detects applications that represent potential duplicate funding (funding the same research in different projects). It examines applications as they are submitted to the NIH and sends a report to relevant personnel in the agency.

Detect and prevent duplicate funding with real-time examination of incoming grant applications.","Input: Text of grant applications.
Output: Pairs of possible duplication.",Operation and Maintenance,Neither,1/1/2023,1/1/2023,1/1/2023,Developed in-house.,Unknown,Unknown,No,No,No,Yes,IMPACII,Documentation is missing or not available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Center for Scientific Review General Support System (CSR GSS),Less than 6 months,Yes,No,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"This tool detects applications that represent potential duplicate funding (funding the same research in different projects). It examines applications as they are submitted to the NIH and sends a report to relevant personnel in the agency.

Detect and prevent duplicate funding with real-time examination of incoming grant applications. . Input: Text of grant applications.
Output: Pairs of possible duplication.",this tool detects applications that represent potential duplicate funding (funding the same research in different projects). it examines applications as they are submitted to the nih and sends a report to relevant personnel in the agency. detect and prevent duplicate funding with real-time examination of incoming grant applications. . input: text of grant applications. output: pairs of possible duplication.
Detection of Implementation Science focus within incoming grant applications,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"Grants management oversight and administration.

Assign the grant application to a particular division for routine grants management oversight and administration.","This tool uses natural language processing and machine learning to calculate an Implementation Science score that is used to predict if a newly submitted grant application proposes to use science that can be categorized as ""Implementation Science.""",Operation and Maintenance,Neither,1/1/2019,1/1/2019,1/1/2020,Developed with contracting resources.,Unknown,Unknown,No,No,No,Yes,Leverages NIH application data from IRDB.,Documentation is complete,No,No – agency does not have access to source code.,Yes,Dimensions for NIH and Analyst Support Infrastructure (ATO Letter - https://nih.sharepoint.com/:b:/s/NHLBI-ITAC-SECURITY/Eb_TarkpZPRCnX5RsGEBgQ8BgK4XC2_hjlsAVeHHLEjdww),Less than 6 months,Yes,No,No,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Grants management oversight and administration.

Assign the grant application to a particular division for routine grants management oversight and administration. . This tool uses natural language processing and machine learning to calculate an Implementation Science score that is used to predict if a newly submitted grant application proposes to use science that can be categorized as ""Implementation Science.""","grants management oversight and administration. assign the grant application to a particular division for routine grants management oversight and administration. . this tool uses natural language processing and machine learning to calculate an implementation science score that is used to predict if a newly submitted grant application proposes to use science that can be categorized as ""implementation science."""
DEXTR - automated data extraction tool,Department of Health and Human Services,HHS,NIH,Health & Medical,None of the above.,"Dextr/LaserAI does not use PII, PHI or demographic data. Dextr is FEDRAMP certified and ATO in the cloud. Data sources for Dextr are published PDFs accessible through open source publications (e.g. PubMed Central) or through our subscriptions (e.g., NLM).

The primary objective of the DEXTR/Laser AI use case is to streamline the process of data extraction from published studies for literature reviews, addressing the key problem of time and labor intensity involved in manual data identification and capture. DEXTR leverages natural language processing (NLP) models, regular expressions and large language models to automatically identify and predict pertinent entities of data extraction within scientific studies. By doing so, it reduces the burden of manual extraction while still supporting user verification, offering a semi-automated approach that ensures accuracy and quality control. One of the key tasks DEXTR enhances is the annotation of studies during the normal workflow, allowing the tool to generate annotated corpora for the development of future models. Dextr also provides automated extraction of information from tables in PDFs expanding the supported data extraction capabilities.  As a dynamic platform for multiple AI projects, DEXTR/LaserAI is continually being optimized and expanded to support a growing number of use cases. This automation not only accelerates data extraction but also aids in creating a robust, scalable system for handling the ever-increasing volume of published research. The anticipated positive outcomes include improved efficiency and accuracy in data extraction, which is critical for developing evidence-based conclusions, guiding future research directions, and informing policy decisions. By enabling faster and more comprehensive reviews of existing literature the tool has the potential to significantly enhance the ability to process large volumes of scientific data.","DEXTR leverages natural language processing (NLP) models, regular expressions and large language models to automatically identify and predict pertinent entities of data extraction within scientific studies.",Implementation and Assessment,Neither,8/1/2020,8/1/2020,Unknown,Developed with contracting resources.,Unknown,Unknown,No,No,No,No,"Data sources for Dextr are published PDFs that are accessible through open source publications (e.g. PubMed Central) or through our subscriptions (e.g., NLM).",Documentation is missing or not available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Dextr is FEDRAMP certified and ATO in the cloud,Less than 6 months,No,No,No,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Dextr/LaserAI does not use PII, PHI or demographic data. Dextr is FEDRAMP certified and ATO in the cloud. Data sources for Dextr are published PDFs accessible through open source publications (e.g. PubMed Central) or through our subscriptions (e.g., NLM).

The primary objective of the DEXTR/Laser AI use case is to streamline the process of data extraction from published studies for literature reviews, addressing the key problem of time and labor intensity involved in manual data identification and capture. DEXTR leverages natural language processing (NLP) models, regular expressions and large language models to automatically identify and predict pertinent entities of data extraction within scientific studies. By doing so, it reduces the burden of manual extraction while still supporting user verification, offering a semi-automated approach that ensures accuracy and quality control. One of the key tasks DEXTR enhances is the annotation of studies during the normal workflow, allowing the tool to generate annotated corpora for the development of future models. Dextr also provides automated extraction of information from tables in PDFs expanding the supported data extraction capabilities.  As a dynamic platform for multiple AI projects, DEXTR/LaserAI is continually being optimized and expanded to support a growing number of use cases. This automation not only accelerates data extraction but also aids in creating a robust, scalable system for handling the ever-increasing volume of published research. The anticipated positive outcomes include improved efficiency and accuracy in data extraction, which is critical for developing evidence-based conclusions, guiding future research directions, and informing policy decisions. By enabling faster and more comprehensive reviews of existing literature the tool has the potential to significantly enhance the ability to process large volumes of scientific data. . DEXTR leverages natural language processing (NLP) models, regular expressions and large language models to automatically identify and predict pertinent entities of data extraction within scientific studies.","dextr/laserai does not use pii, phi or demographic data. dextr is fedramp certified and ato in the cloud. data sources for dextr are published pdfs accessible through open source publications (e.g. pubmed central) or through our subscriptions (e.g., nlm). the primary objective of the dextr/laser ai use case is to streamline the process of data extraction from published studies for literature reviews, addressing the key problem of time and labor intensity involved in manual data identification and capture. dextr leverages natural language processing (nlp) models, regular expressions and large language models to automatically identify and predict pertinent entities of data extraction within scientific studies. by doing so, it reduces the burden of manual extraction while still supporting user verification, offering a semi-automated approach that ensures accuracy and quality control. one of the key tasks dextr enhances is the annotation of studies during the normal workflow, allowing the tool to generate annotated corpora for the development of future models. dextr also provides automated extraction of information from tables in pdfs expanding the supported data extraction capabilities. as a dynamic platform for multiple ai projects, dextr/laserai is continually being optimized and expanded to support a growing number of use cases. this automation not only accelerates data extraction but also aids in creating a robust, scalable system for handling the ever-increasing volume of published research. the anticipated positive outcomes include improved efficiency and accuracy in data extraction, which is critical for developing evidence-based conclusions, guiding future research directions, and informing policy decisions. by enabling faster and more comprehensive reviews of existing literature the tool has the potential to significantly enhance the ability to process large volumes of scientific data. . dextr leverages natural language processing (nlp) models, regular expressions and large language models to automatically identify and predict pertinent entities of data extraction within scientific studies."
Enhance NLM Website Accessibility,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"NLM is pilot testing the use of artificial intelligence (AI) to enhance the accessibility of the NLM website in alignment with Section 508 standards.  This initiative is critical to mitigate the enterprise risk associated with non-compliance with Section 508 standards covering web sites. The AI will focus solely on identifying and resolving accessibility errors associated with Section 508 standards for websites, including suggestions from the U.S. Web Design System framework when possible. The AI is currently being developed behind NIH's firewall for a select internal user base. The tool will not make direct changes to general web design or web content. The AI will integrate with existing web content management systems and accessibility compliance tools (like pa11y).  The AI will use website data (HTML, CSS, JavaScript) and accessibility compliance standards as input and will be deployed in the web development and IT departments, particularly those focused on accessibility and compliance.

The project may reduce costs and time associated with manual accessibility evaluation and remediation. Additionally, it is expected to enhance the accuracy and quality of accessibility features on the website. Importantly, the project aims to improve the user experience by making the site more accessible to a broader audience, thus increasing user access and discovery. Ultimately, this initiative will help reduce the enterprise risk associated with non-compliance and ensure that NLM's website meets the evolving standards for government web accessibility.","Inputs: Website data (HTML, CSS, JavaScript), accessibility standards.

Outputs: Accessibility issue reports, suggested code fixes, task lists for implementation.

Frequency: The AI performs both site-wide scans and single URL audits on demand. ",Acquisition and/or Development,Neither,11/1/2023,5/1/2023,Unknown,Developed with contracting resources.,75N97023A00004/75N97023F00005,Unknown,Unknown,Unknown,Unknown,Yes,"Website data (HTML, CSS, JavaScript), accessibility standards. Measures taken to ensure data accuracy and reliability: grounding, testing, evaluation, NIST framework.",Documentation is complete,Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"NLM is pilot testing the use of artificial intelligence (AI) to enhance the accessibility of the NLM website in alignment with Section 508 standards.  This initiative is critical to mitigate the enterprise risk associated with non-compliance with Section 508 standards covering web sites. The AI will focus solely on identifying and resolving accessibility errors associated with Section 508 standards for websites, including suggestions from the U.S. Web Design System framework when possible. The AI is currently being developed behind NIH's firewall for a select internal user base. The tool will not make direct changes to general web design or web content. The AI will integrate with existing web content management systems and accessibility compliance tools (like pa11y).  The AI will use website data (HTML, CSS, JavaScript) and accessibility compliance standards as input and will be deployed in the web development and IT departments, particularly those focused on accessibility and compliance.

The project may reduce costs and time associated with manual accessibility evaluation and remediation. Additionally, it is expected to enhance the accuracy and quality of accessibility features on the website. Importantly, the project aims to improve the user experience by making the site more accessible to a broader audience, thus increasing user access and discovery. Ultimately, this initiative will help reduce the enterprise risk associated with non-compliance and ensure that NLM's website meets the evolving standards for government web accessibility. . Inputs: Website data (HTML, CSS, JavaScript), accessibility standards.

Outputs: Accessibility issue reports, suggested code fixes, task lists for implementation.

Frequency: The AI performs both site-wide scans and single URL audits on demand.","nlm is pilot testing the use of artificial intelligence (ai) to enhance the accessibility of the nlm website in alignment with section 508 standards. this initiative is critical to mitigate the enterprise risk associated with non-compliance with section 508 standards covering web sites. the ai will focus solely on identifying and resolving accessibility errors associated with section 508 standards for websites, including suggestions from the u.s. web design system framework when possible. the ai is currently being developed behind nih's firewall for a select internal user base. the tool will not make direct changes to general web design or web content. the ai will integrate with existing web content management systems and accessibility compliance tools (like pa11y). the ai will use website data (html, css, javascript) and accessibility compliance standards as input and will be deployed in the web development and it departments, particularly those focused on accessibility and compliance. the project may reduce costs and time associated with manual accessibility evaluation and remediation. additionally, it is expected to enhance the accuracy and quality of accessibility features on the website. importantly, the project aims to improve the user experience by making the site more accessible to a broader audience, thus increasing user access and discovery. ultimately, this initiative will help reduce the enterprise risk associated with non-compliance and ensure that nlm's website meets the evolving standards for government web accessibility. . inputs: website data (html, css, javascript), accessibility standards. outputs: accessibility issue reports, suggested code fixes, task lists for implementation. frequency: the ai performs both site-wide scans and single url audits on demand."
Enhancing Responses to Customer Questions about NLM Products,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"NLM is piloting the creation of an application which uses the GPT family of models from OpenAI (via Azure) and data retrieved from public facing NIH/NLM web domains to answer questions about select NLM products. This is currently being developed behind NIH's firewall for a select internal user base. The artificial intelligence (AI) will generate responses to questions but will not engage in direct conversations with customers. Additional safeguards would make sense in a production environment. The AI will integrate with a knowledge base and use text data from user technical questions and knowledge base articles. It will be deployed within customer service and technical support offices.

Primary Objectives: Provide accurate and timely responses to technical questions about NLM products.

Anticipated Positive Outcomes: Improved efficiency and accuracy in customer service, better user experience, and higher satisfaction rates.","Inputs: User technical questions, knowledge base articles.

Outputs: AI-generated responses, gold standard responses, knowledge base links.

Frequency: Responses generated in real-time based on user inquiries.",Acquisition and/or Development,Neither,11/1/2023,5/1/2023,Unknown,Developed with contracting resources.,75N97023A00004/75N97023F00008,Unknown,Unknown,Unknown,Unknown,Yes,"Website data (HTML, CSS, JavaScript), accessibility standards. Measures taken to ensure data accuracy and reliability: grounding, testing, evaluation, NIST framework.",Documentation is complete,Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"NLM is piloting the creation of an application which uses the GPT family of models from OpenAI (via Azure) and data retrieved from public facing NIH/NLM web domains to answer questions about select NLM products. This is currently being developed behind NIH's firewall for a select internal user base. The artificial intelligence (AI) will generate responses to questions but will not engage in direct conversations with customers. Additional safeguards would make sense in a production environment. The AI will integrate with a knowledge base and use text data from user technical questions and knowledge base articles. It will be deployed within customer service and technical support offices.

Primary Objectives: Provide accurate and timely responses to technical questions about NLM products.

Anticipated Positive Outcomes: Improved efficiency and accuracy in customer service, better user experience, and higher satisfaction rates. . Inputs: User technical questions, knowledge base articles.

Outputs: AI-generated responses, gold standard responses, knowledge base links.

Frequency: Responses generated in real-time based on user inquiries.","nlm is piloting the creation of an application which uses the gpt family of models from openai (via azure) and data retrieved from public facing nih/nlm web domains to answer questions about select nlm products. this is currently being developed behind nih's firewall for a select internal user base. the artificial intelligence (ai) will generate responses to questions but will not engage in direct conversations with customers. additional safeguards would make sense in a production environment. the ai will integrate with a knowledge base and use text data from user technical questions and knowledge base articles. it will be deployed within customer service and technical support offices. primary objectives: provide accurate and timely responses to technical questions about nlm products. anticipated positive outcomes: improved efficiency and accuracy in customer service, better user experience, and higher satisfaction rates. . inputs: user technical questions, knowledge base articles. outputs: ai-generated responses, gold standard responses, knowledge base links. frequency: responses generated in real-time based on user inquiries."
Environmental health research annotation for model development,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"By creating annotated datasets that account for diverse study types, the project will enable the development of AI and machine-learning models that can significantly reduce the time and effort required to identify and capture critical data from environmental health studies. This improvement will streamline tasks for teams conducting literature assessments and reviews, improving their efficiency.

The primary objective of this AI use case is to enhance the development of datasets specifically focused on toxicology and environmental health. The key problem it aims to address is the current scarcity of relevant training datasets or corpora, which limits the ability to create effective AI models for identifying and extracting environmental health data from published studies. This challenge is compounded by the diverse nature of environmental health research, which includes various study designs such as epidemiological studies, in vitro research, exposure assessments, and experimental animal studies.  ","Input: published environmental health studies.

Output: annotated corpus to support model development (e.g., BRAT format).",Acquisition and/or Development,Neither,1/1/2020,1/1/2020,Unknown,Developed with contracting resources.,Unknown,Unknown,Unknown,Unknown,Unknown,No,Unknown,Documentation is missing or not available,Unknown,Unknown,No,Unknown,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.36507936507936506,"By creating annotated datasets that account for diverse study types, the project will enable the development of AI and machine-learning models that can significantly reduce the time and effort required to identify and capture critical data from environmental health studies. This improvement will streamline tasks for teams conducting literature assessments and reviews, improving their efficiency.

The primary objective of this AI use case is to enhance the development of datasets specifically focused on toxicology and environmental health. The key problem it aims to address is the current scarcity of relevant training datasets or corpora, which limits the ability to create effective AI models for identifying and extracting environmental health data from published studies. This challenge is compounded by the diverse nature of environmental health research, which includes various study designs such as epidemiological studies, in vitro research, exposure assessments, and experimental animal studies. . Input: published environmental health studies.

Output: annotated corpus to support model development (e.g., BRAT format).","by creating annotated datasets that account for diverse study types, the project will enable the development of ai and machine-learning models that can significantly reduce the time and effort required to identify and capture critical data from environmental health studies. this improvement will streamline tasks for teams conducting literature assessments and reviews, improving their efficiency. the primary objective of this ai use case is to enhance the development of datasets specifically focused on toxicology and environmental health. the key problem it aims to address is the current scarcity of relevant training datasets or corpora, which limits the ability to create effective ai models for identifying and extracting environmental health data from published studies. this challenge is compounded by the diverse nature of environmental health research, which includes various study designs such as epidemiological studies, in vitro research, exposure assessments, and experimental animal studies. . input: published environmental health studies. output: annotated corpus to support model development (e.g., brat format)."
Expansion of Generative AI (GenAI) Caption Generation for all Collections Videos,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"The artificial intelligence (AI) use case builds on an existing Generative AI (GenAI) workflow in NLM Digital Collections that generates transcripts and captions for all videos stored in the Digital Repository dark archive. The expansion of this workflow will apply to all videos and films targeted for Digital Collections. The AI will not be used for content outside of these collections or for non-video materials, as well as already captioned videos. It will integrate with existing video processing systems to automate the captioning process. The types of data utilized will include video files and associated metadata. The deployment will occur within NLM’s Digital Collections management area, with the primary goals being cost savings by reducing reliance on third-party vendors and standardizing video caption workflows across all collections.

The objective is to expand AI-driven captioning across all videos in the NLM Digital Collections, ensuring standardized and accuracy. ",The inputs consist of video or audio mp4 or mp3 format comes from Collections videos that don't have captions. The AI generated captions will be in text format.,Implementation and Assessment,Neither,2/1/2020,3/1/2020,Unknown,Developed with contracting resources.,HHSN316201200013W/75N97019F00066,Unknown,Yes,No,No,Yes,"The agency-owned data used in this AI use case consists of U-Matic videos in MP4 format, hosted on collections.nlm.nih.gov. Each month, 8 uncaptioned U-Matic videos of various length are selected for processing, where the AI-generated captions are applied. The audio is extracted from the video files and used as input for AWS Transcribe, which is a pre-trained model that generates captions. During the QA process, there is a human reviews process that checks the quality and accuracy of the output captions, and a disclaimer is added to the videos indicating that the captions were AI-generated.",Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,No,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The artificial intelligence (AI) use case builds on an existing Generative AI (GenAI) workflow in NLM Digital Collections that generates transcripts and captions for all videos stored in the Digital Repository dark archive. The expansion of this workflow will apply to all videos and films targeted for Digital Collections. The AI will not be used for content outside of these collections or for non-video materials, as well as already captioned videos. It will integrate with existing video processing systems to automate the captioning process. The types of data utilized will include video files and associated metadata. The deployment will occur within NLM’s Digital Collections management area, with the primary goals being cost savings by reducing reliance on third-party vendors and standardizing video caption workflows across all collections.

The objective is to expand AI-driven captioning across all videos in the NLM Digital Collections, ensuring standardized and accuracy. . The inputs consist of video or audio mp4 or mp3 format comes from Collections videos that don't have captions. The AI generated captions will be in text format.","the artificial intelligence (ai) use case builds on an existing generative ai (genai) workflow in nlm digital collections that generates transcripts and captions for all videos stored in the digital repository dark archive. the expansion of this workflow will apply to all videos and films targeted for digital collections. the ai will not be used for content outside of these collections or for non-video materials, as well as already captioned videos. it will integrate with existing video processing systems to automate the captioning process. the types of data utilized will include video files and associated metadata. the deployment will occur within nlm’s digital collections management area, with the primary goals being cost savings by reducing reliance on third-party vendors and standardizing video caption workflows across all collections. the objective is to expand ai-driven captioning across all videos in the nlm digital collections, ensuring standardized and accuracy. . the inputs consist of video or audio mp4 or mp3 format comes from collections videos that don't have captions. the ai generated captions will be in text format."
Federal IT Acquisition Reform Act (FITARA) Tool,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"NIAID administers contracts to execute mission initiatives.

The tool's classifications inform SOW clarifications/revisions relevant to the FITARA.",The tool classifies contract statement of work (SOW) documents as being IT-related or not. ,Operation and Maintenance,Neither,1/1/2017,1/1/2017,7/1/2017,Developed with contracting resources.,Not Reported,Unknown,No,No,No,Yes,"Yes, a dataset was curated using NIAID SOWs which were manually labelled to train the classification model.",Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Not Reported,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"NIAID administers contracts to execute mission initiatives.

The tool's classifications inform SOW clarifications/revisions relevant to the FITARA. . The tool classifies contract statement of work (SOW) documents as being IT-related or not.",niaid administers contracts to execute mission initiatives. the tool's classifications inform sow clarifications/revisions relevant to the fitara. . the tool classifies contract statement of work (sow) documents as being it-related or not.
Grant Application Subject-Matter Classification Tool,Department of Health and Human Services,HHS,NIH,Health & Medical,None of the above.,"The algorithm is used to recommend a specific scientific area within the Division's portfolio with a specific confidence level. This reduces the time it takes to assign a Program Class Code and personnel assignment to every grant application received by NIEHS. It searches for key words within the title, abstract and specific aims in each grant application to determine the best scientific category. It is not used in combination with any other data sets. It uses grant application data from IMPAC II, which is imported into the Grants Funding Decision Tool (a NIEHS Custom Product).

Natural language processing of grant applications to assign scientific topics and corresponding Program Officer staff.  The subject-matter classification tool improves the efficiency of the grant administration process by automatically assigning incoming applications a subject matter classification based on the text content of their abstracts using a model derived from past human-classified examples. This classification helps route each application to the program officer with the greatest expertise in the inferred field of study.",Training set consists of an Excel spreadsheet containing human-classified grant proposal abstracts. Input file is an Excel table containing new unclassified grant proposal abstracts. Output is input file updated to include tool-assigned subject matter classifications.,Operation and Maintenance,Neither,1/1/2016,1/1/2016,2/1/2020,Developed with contracting resources.,Not Reported,Unknown,No,No,No,No,"Input data is provided to NIEHS by grant applicants, via the IMPAC II database.",Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,No,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The algorithm is used to recommend a specific scientific area within the Division's portfolio with a specific confidence level. This reduces the time it takes to assign a Program Class Code and personnel assignment to every grant application received by NIEHS. It searches for key words within the title, abstract and specific aims in each grant application to determine the best scientific category. It is not used in combination with any other data sets. It uses grant application data from IMPAC II, which is imported into the Grants Funding Decision Tool (a NIEHS Custom Product).

Natural language processing of grant applications to assign scientific topics and corresponding Program Officer staff.  The subject-matter classification tool improves the efficiency of the grant administration process by automatically assigning incoming applications a subject matter classification based on the text content of their abstracts using a model derived from past human-classified examples. This classification helps route each application to the program officer with the greatest expertise in the inferred field of study. . Training set consists of an Excel spreadsheet containing human-classified grant proposal abstracts. Input file is an Excel table containing new unclassified grant proposal abstracts. Output is input file updated to include tool-assigned subject matter classifications.","the algorithm is used to recommend a specific scientific area within the division's portfolio with a specific confidence level. this reduces the time it takes to assign a program class code and personnel assignment to every grant application received by niehs. it searches for key words within the title, abstract and specific aims in each grant application to determine the best scientific category. it is not used in combination with any other data sets. it uses grant application data from impac ii, which is imported into the grants funding decision tool (a niehs custom product). natural language processing of grant applications to assign scientific topics and corresponding program officer staff. the subject-matter classification tool improves the efficiency of the grant administration process by automatically assigning incoming applications a subject matter classification based on the text content of their abstracts using a model derived from past human-classified examples. this classification helps route each application to the program officer with the greatest expertise in the inferred field of study. . training set consists of an excel spreadsheet containing human-classified grant proposal abstracts. input file is an excel table containing new unclassified grant proposal abstracts. output is input file updated to include tool-assigned subject matter classifications."
HIV-related grant classifier tool,Department of Health and Human Services,HHS,NIH,Health & Medical,None of the above.,"A front-end application for scientific staff to input grant information, which then runs an automated algorithm to classify HIV-related grants. 

The tool allows OAR staff to determine the best categorization for a grant application after reviewing data and visualization of confidence levels through a heat map. Natural language processing of grant applications to assign scientific topics and corresponding Program Officer staff.  The subject-matter classification tool improves the efficiency of the grant administration process by automatically assigning incoming applications a subject matter classification based on the text content of their abstracts using a model derived from past human-classified examples. This classification helps route each application to the program officer with the greatest expertise in the inferred field of study.","Input: Grant text data.

Output: Predicted categorization.",Implementation and Assessment,Neither,11/1/2022,12/1/2022,Unknown,Developed in-house.,Unknown,Unknown,No,No,No,No,"Grant text data (Title, Abstract and Specific Aims).",Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"A front-end application for scientific staff to input grant information, which then runs an automated algorithm to classify HIV-related grants. 

The tool allows OAR staff to determine the best categorization for a grant application after reviewing data and visualization of confidence levels through a heat map. Natural language processing of grant applications to assign scientific topics and corresponding Program Officer staff.  The subject-matter classification tool improves the efficiency of the grant administration process by automatically assigning incoming applications a subject matter classification based on the text content of their abstracts using a model derived from past human-classified examples. This classification helps route each application to the program officer with the greatest expertise in the inferred field of study. . Input: Grant text data.

Output: Predicted categorization.","a front-end application for scientific staff to input grant information, which then runs an automated algorithm to classify hiv-related grants. the tool allows oar staff to determine the best categorization for a grant application after reviewing data and visualization of confidence levels through a heat map. natural language processing of grant applications to assign scientific topics and corresponding program officer staff. the subject-matter classification tool improves the efficiency of the grant administration process by automatically assigning incoming applications a subject matter classification based on the text content of their abstracts using a model derived from past human-classified examples. this classification helps route each application to the program officer with the greatest expertise in the inferred field of study. . input: grant text data. output: predicted categorization."
Improving Customer Response,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"NLM is pilot testing the utilization of artificial intelligence (AI) to assist Customer Service Representatives (CSRs) in drafting responses to customer inquiries. It is currently being developed behind NIH's firewall for a select internal user base. The AI is focused on generating draft responses for customer service but will not make final decisions or engage in direct communication with customers. CSRs are instructed to not enter personally identified info into the AI system. In the event this info is entered, the AI system is automatically prompted to not include any personally identified information in its generated response. Additional safeguards would make sense in a production environment or if it was released to the public. The AI will integrate with customer service platforms and databases like Medline Plus and DailyMed. The AI will use text data from customer inquiries and publicly accessible NLM-managed biomedical information services and be deployed within the customer service department. 

The project aims to enhance customer response metrics, increase customer satisfaction, and improve the efficiency of handling customer service inquiries.","Inputs: Customer inquiries, publicly accessible, NLM-managed biomedical information services (such as Medline Plus, DailyMed, etc.).

Outputs: Draft responses, resource links, disclaimers.

Frequency: Responses generated in real-time as inquiries are received.",Acquisition and/or Development,Neither,11/1/2023,5/1/2023,Unknown,Developed with contracting resources.,75N97023A00004/75N97023F00006,Unknown,Unknown,Unknown,Unknown,Yes,"Website data (HTML, CSS, JavaScript), accessibility standards. Measures taken to ensure data accuracy and reliability: grounding, testing, evaluation, NIST framework.",Documentation is complete,Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"NLM is pilot testing the utilization of artificial intelligence (AI) to assist Customer Service Representatives (CSRs) in drafting responses to customer inquiries. It is currently being developed behind NIH's firewall for a select internal user base. The AI is focused on generating draft responses for customer service but will not make final decisions or engage in direct communication with customers. CSRs are instructed to not enter personally identified info into the AI system. In the event this info is entered, the AI system is automatically prompted to not include any personally identified information in its generated response. Additional safeguards would make sense in a production environment or if it was released to the public. The AI will integrate with customer service platforms and databases like Medline Plus and DailyMed. The AI will use text data from customer inquiries and publicly accessible NLM-managed biomedical information services and be deployed within the customer service department. 

The project aims to enhance customer response metrics, increase customer satisfaction, and improve the efficiency of handling customer service inquiries. . Inputs: Customer inquiries, publicly accessible, NLM-managed biomedical information services (such as Medline Plus, DailyMed, etc.).

Outputs: Draft responses, resource links, disclaimers.

Frequency: Responses generated in real-time as inquiries are received.","nlm is pilot testing the utilization of artificial intelligence (ai) to assist customer service representatives (csrs) in drafting responses to customer inquiries. it is currently being developed behind nih's firewall for a select internal user base. the ai is focused on generating draft responses for customer service but will not make final decisions or engage in direct communication with customers. csrs are instructed to not enter personally identified info into the ai system. in the event this info is entered, the ai system is automatically prompted to not include any personally identified information in its generated response. additional safeguards would make sense in a production environment or if it was released to the public. the ai will integrate with customer service platforms and databases like medline plus and dailymed. the ai will use text data from customer inquiries and publicly accessible nlm-managed biomedical information services and be deployed within the customer service department. the project aims to enhance customer response metrics, increase customer satisfaction, and improve the efficiency of handling customer service inquiries. . inputs: customer inquiries, publicly accessible, nlm-managed biomedical information services (such as medline plus, dailymed, etc.). outputs: draft responses, resource links, disclaimers. frequency: responses generated in real-time as inquiries are received."
Improving Metadata Retrieval and Transformation for Metadata Management,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"The Dataset Catalog (catalog) is a catalog of biomedical datasets from various repositories for users to search, discover, retrieve, and connect with datasets to accelerate scientific research. Metadata from included repositories and their datasets are needed to appropriately index them in the catalog. This manual process can be labor intense, and thus NLM is pilot testing the development of a working interface for internal users to retrieve the metadata information from external repositories' websites and then transform the data to a specific format used in internal metadata management for the Dataset Catalog. This is currently being developed behind NIH's firewall for a select internal user base. The artificial intelligence (AI) will automate the retrieval and transformation of metadata but will not replace human oversight in critical metadata curation tasks. Additional safeguards would make sense in a production environment. The AI uses information from publicly available biomedical data repository websites and internal metadata management systems. The AI uses metadata from biomedical repositories and will be deployed within data management and IT departments, particularly those involved in cataloging and metadata management.

Primary Objectives: Streamline metadata retrieval and transformation to improve efficiency and accuracy.

Anticipated Positive Outcomes: Resource savings, enhanced data quality, and improved management of the Data Set Catalog.","Inputs: Metadata from various biomedical repositories, internal schemas.

Outputs: Transformed metadata, Python scripts, user-friendly tools for metadata retrieval.

Frequency: Continuous retrieval and transformation as new metadata is ingested.",Acquisition and/or Development,Neither,11/1/2023,5/1/2023,Unknown,Developed with contracting resources.,75N97023A00004/75N97023F00007,Unknown,Unknown,Unknown,Unknown,Yes,"Website data (HTML, CSS, JavaScript), accessibility standards. Measures taken to ensure data accuracy and reliability: grounding, testing, evaluation, NIST framework.",Documentation is complete,Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"The Dataset Catalog (catalog) is a catalog of biomedical datasets from various repositories for users to search, discover, retrieve, and connect with datasets to accelerate scientific research. Metadata from included repositories and their datasets are needed to appropriately index them in the catalog. This manual process can be labor intense, and thus NLM is pilot testing the development of a working interface for internal users to retrieve the metadata information from external repositories' websites and then transform the data to a specific format used in internal metadata management for the Dataset Catalog. This is currently being developed behind NIH's firewall for a select internal user base. The artificial intelligence (AI) will automate the retrieval and transformation of metadata but will not replace human oversight in critical metadata curation tasks. Additional safeguards would make sense in a production environment. The AI uses information from publicly available biomedical data repository websites and internal metadata management systems. The AI uses metadata from biomedical repositories and will be deployed within data management and IT departments, particularly those involved in cataloging and metadata management.

Primary Objectives: Streamline metadata retrieval and transformation to improve efficiency and accuracy.

Anticipated Positive Outcomes: Resource savings, enhanced data quality, and improved management of the Data Set Catalog. . Inputs: Metadata from various biomedical repositories, internal schemas.

Outputs: Transformed metadata, Python scripts, user-friendly tools for metadata retrieval.

Frequency: Continuous retrieval and transformation as new metadata is ingested.","the dataset catalog (catalog) is a catalog of biomedical datasets from various repositories for users to search, discover, retrieve, and connect with datasets to accelerate scientific research. metadata from included repositories and their datasets are needed to appropriately index them in the catalog. this manual process can be labor intense, and thus nlm is pilot testing the development of a working interface for internal users to retrieve the metadata information from external repositories' websites and then transform the data to a specific format used in internal metadata management for the dataset catalog. this is currently being developed behind nih's firewall for a select internal user base. the artificial intelligence (ai) will automate the retrieval and transformation of metadata but will not replace human oversight in critical metadata curation tasks. additional safeguards would make sense in a production environment. the ai uses information from publicly available biomedical data repository websites and internal metadata management systems. the ai uses metadata from biomedical repositories and will be deployed within data management and it departments, particularly those involved in cataloging and metadata management. primary objectives: streamline metadata retrieval and transformation to improve efficiency and accuracy. anticipated positive outcomes: resource savings, enhanced data quality, and improved management of the data set catalog. . inputs: metadata from various biomedical repositories, internal schemas. outputs: transformed metadata, python scripts, user-friendly tools for metadata retrieval. frequency: continuous retrieval and transformation as new metadata is ingested."
Internal Referral Module ,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"The NIH Extramural Research Program uses the eRA system to efficiently award and manage grants.

The AI system is designed for use by IC ‘internal referral’ officers who manage a pool of applications and are responsible for assigning the applications to program officials. It creates efficiencies by automating the ‘internal referral’ assignments, currently done manually on spreadsheets by the ICs. The inputs are grant applications, the outputs are referrals to Program Officers, Program Class Codes, Organizational units - Divisions and Branches and Scientific Research Clusters.","The Internal Referral initiative automates a manual process by using AI and NLP capabilities to help assign grant applications to NIH Institute and Center Program Officers to make informed decisions.

The inputs are grant applications, the outputs are referrals to Program Officers, Program Class Codes, Organizational units - Divisions and Branches and Scientific Research Clusters.",Operation and Maintenance,Neither,1/1/2018,1/1/2019,2/1/2023,Developed with contracting resources.,"N/A, The work is performed across multiple eRA contracts.",Unknown,No,No,No,Yes,"We used eRA grant application data for all fine tuning and optimization for the models. Specially, we extract the title, abstract, specific aims and public health narrative to train our models for prediction. ",Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",Yes,eRA,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The NIH Extramural Research Program uses the eRA system to efficiently award and manage grants.

The AI system is designed for use by IC ‘internal referral’ officers who manage a pool of applications and are responsible for assigning the applications to program officials. It creates efficiencies by automating the ‘internal referral’ assignments, currently done manually on spreadsheets by the ICs. The inputs are grant applications, the outputs are referrals to Program Officers, Program Class Codes, Organizational units - Divisions and Branches and Scientific Research Clusters. . The Internal Referral initiative automates a manual process by using AI and NLP capabilities to help assign grant applications to NIH Institute and Center Program Officers to make informed decisions.

The inputs are grant applications, the outputs are referrals to Program Officers, Program Class Codes, Organizational units - Divisions and Branches and Scientific Research Clusters.","the nih extramural research program uses the era system to efficiently award and manage grants. the ai system is designed for use by ic ‘internal referral’ officers who manage a pool of applications and are responsible for assigning the applications to program officials. it creates efficiencies by automating the ‘internal referral’ assignments, currently done manually on spreadsheets by the ics. the inputs are grant applications, the outputs are referrals to program officers, program class codes, organizational units - divisions and branches and scientific research clusters. . the internal referral initiative automates a manual process by using ai and nlp capabilities to help assign grant applications to nih institute and center program officers to make informed decisions. the inputs are grant applications, the outputs are referrals to program officers, program class codes, organizational units - divisions and branches and scientific research clusters."
JIT Automated Calculator (JAC),Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"NIGMS supports basic research that increases our understanding of biological processes and lays the foundation for advances in disease diagnosis, treatment, and prevention. NIGMS-funded scientists investigate how living systems work at a range of levels from molecules and cells to tissues and organs, in research organisms, humans, and populations.

NIGMS needs to know how much total support an investigator has to ensure that it is not funding Principal Investigators who are already adequately resourced. The tool can determine how much funding a Principal Investigators has been awarded.",The JIT Automated Calculator (JAC) uses NLP to parse Just-In-Time (JIT) Other Support forms and determine how much outside support PIs are receiving from sources other than the pending application.,Operation and Maintenance,Neither,3/1/2024,3/1/2023,5/1/2023,Developed in-house.,Unknown,Unknown,No,No,No,Yes,All data come from the internal NIH IMPAC II database.,Documentation is missing or not available,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"NIGMS supports basic research that increases our understanding of biological processes and lays the foundation for advances in disease diagnosis, treatment, and prevention. NIGMS-funded scientists investigate how living systems work at a range of levels from molecules and cells to tissues and organs, in research organisms, humans, and populations.

NIGMS needs to know how much total support an investigator has to ensure that it is not funding Principal Investigators who are already adequately resourced. The tool can determine how much funding a Principal Investigators has been awarded. . The JIT Automated Calculator (JAC) uses NLP to parse Just-In-Time (JIT) Other Support forms and determine how much outside support PIs are receiving from sources other than the pending application.","nigms supports basic research that increases our understanding of biological processes and lays the foundation for advances in disease diagnosis, treatment, and prevention. nigms-funded scientists investigate how living systems work at a range of levels from molecules and cells to tissues and organs, in research organisms, humans, and populations. nigms needs to know how much total support an investigator has to ensure that it is not funding principal investigators who are already adequately resourced. the tool can determine how much funding a principal investigators has been awarded. . the jit automated calculator (jac) uses nlp to parse just-in-time (jit) other support forms and determine how much outside support pis are receiving from sources other than the pending application."
Leveraging User-Generated Content for Digital Behavioral Interventions,Department of Health and Human Services,HHS,NIH,Health & Medical,None of the above.,"The project uses AI-based automated text classification technology to build a predictive model to categorize smoking-related tweets into six distinct categories (e.g., quitting strategies). Specifically, the project uses the AI-based BERT pre-trained language model, which will be further fine-tuned using neural network models to capture the contextual nuances of our use case. AI will also be used to select data for the training dataset that will form the basis of the predictive model. Then, we use ChatGPT to classify extracted and labelled tweets according to its sentiment, presence of sarcasm, presence of irony, presence of emojis. 

The primary objective of the project is to classify thousands of tweets to six categories (e.g., strategies for quitting, quitting benefits) and according to their sentiment and presence of irony, sarcasm, and emojis. Tweets that are positive, not ironic, nor sarcastic would then be tested with target audiences for message effectiveness and possible integration in a smoking cessation intervention.","Input: Publicly available tweets.

Output: sentiment (very positive, positive, neutral, negative, very negative), presence of sarcasm (yes, no), presence of irony (yes, no), presence of emojis (yes, no - and if yes, a description of the emojis present in the tweet).",Implementation and Assessment,Neither,1/1/2024,1/1/2024,Unknown,Developed with contracting resources.,Unknown,Unknown,No,No,No,No,Corpus of 5 million tweets extracted for the purposes of the research by a colleague who had access to twitter archives.,Documentation is missing or not available,No,No – agency does not have access to source code.,No,Unknown,More than 12 months,No,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"The project uses AI-based automated text classification technology to build a predictive model to categorize smoking-related tweets into six distinct categories (e.g., quitting strategies). Specifically, the project uses the AI-based BERT pre-trained language model, which will be further fine-tuned using neural network models to capture the contextual nuances of our use case. AI will also be used to select data for the training dataset that will form the basis of the predictive model. Then, we use ChatGPT to classify extracted and labelled tweets according to its sentiment, presence of sarcasm, presence of irony, presence of emojis. 

The primary objective of the project is to classify thousands of tweets to six categories (e.g., strategies for quitting, quitting benefits) and according to their sentiment and presence of irony, sarcasm, and emojis. Tweets that are positive, not ironic, nor sarcastic would then be tested with target audiences for message effectiveness and possible integration in a smoking cessation intervention. . Input: Publicly available tweets.

Output: sentiment (very positive, positive, neutral, negative, very negative), presence of sarcasm (yes, no), presence of irony (yes, no), presence of emojis (yes, no - and if yes, a description of the emojis present in the tweet).","the project uses ai-based automated text classification technology to build a predictive model to categorize smoking-related tweets into six distinct categories (e.g., quitting strategies). specifically, the project uses the ai-based bert pre-trained language model, which will be further fine-tuned using neural network models to capture the contextual nuances of our use case. ai will also be used to select data for the training dataset that will form the basis of the predictive model. then, we use chatgpt to classify extracted and labelled tweets according to its sentiment, presence of sarcasm, presence of irony, presence of emojis. the primary objective of the project is to classify thousands of tweets to six categories (e.g., strategies for quitting, quitting benefits) and according to their sentiment and presence of irony, sarcasm, and emojis. tweets that are positive, not ironic, nor sarcastic would then be tested with target audiences for message effectiveness and possible integration in a smoking cessation intervention. . input: publicly available tweets. output: sentiment (very positive, positive, neutral, negative, very negative), presence of sarcasm (yes, no), presence of irony (yes, no), presence of emojis (yes, no - and if yes, a description of the emojis present in the tweet)."
Medical Text Indexer-NeXt Generation (MTIX) MEDLINE Indexing,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"Machine learning-based system for the automated indexing of MEDLINE citation records using Medical Subject Heading (MeSH) terms. Automated indexing is achieved using a multi-stage neural text ranking approach.

MeSH indexing has been shown to improve information retrieval in PubMed, but manual indexing was not sustainable due to the increasing volume of published biomedical literature. Automated indexing allows for cost-effective and timely indexing of MEDLINE citation records.","The input is PubMed citation data including the article title and abstract, and the output is a set of MeSH terms describing the topic of the article.",Operation and Maintenance,Neither,1/1/2021,1/1/2022,5/1/2023,Developed with contracting resources.,Black Canyon Consulting LLC 47QTCA18D00JA/75N97023F00098,Unknown,No,No,No,Yes,"The MTIX dataset is approximately 10 million PubMed MEDLINE citations published after 2006. It is publicly available data, used for training and evaluation of the MeSH terms predicted by the algorithm. ",Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Machine learning-based system for the automated indexing of MEDLINE citation records using Medical Subject Heading (MeSH) terms. Automated indexing is achieved using a multi-stage neural text ranking approach.

MeSH indexing has been shown to improve information retrieval in PubMed, but manual indexing was not sustainable due to the increasing volume of published biomedical literature. Automated indexing allows for cost-effective and timely indexing of MEDLINE citation records. . The input is PubMed citation data including the article title and abstract, and the output is a set of MeSH terms describing the topic of the article.","machine learning-based system for the automated indexing of medline citation records using medical subject heading (mesh) terms. automated indexing is achieved using a multi-stage neural text ranking approach. mesh indexing has been shown to improve information retrieval in pubmed, but manual indexing was not sustainable due to the increasing volume of published biomedical literature. automated indexing allows for cost-effective and timely indexing of medline citation records. . the input is pubmed citation data including the article title and abstract, and the output is a set of mesh terms describing the topic of the article."
MetaMap,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"NLM is the world’s largest biomedical library. MetaMap is a program providing access from biomedical text to the concepts in the Unified Medical Language System Metathesaurus.

MetaMap uses NLP to provide a link between the text of biomedical literature and the knowledge.","Metamap is used to map biomedical text to the Unified Medical Language System Metathesaurus or, equivalently, to discover Metathesaurus concepts referred to in text. ",Operation and Maintenance,Neither,1/1/1996,1/1/1996,1/1/1998,Developed with contracting resources.,GSA Contract Number:  47QTCA18D00JA,Unknown,No,No,No,Yes,"UMLS Metathesaurus is used by MetaMap to map the incoming text to concepts that exist in the UMLS Metathesaurus.  MEDLINE citations from the MEDLINE baseline were used in the development, training, and testing of MetaMap.   ",Documentation is missing or not available,Yes,Yes – source code is publicly available.,No,Unknown,Less than 6 months,Yes,No,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"NLM is the world’s largest biomedical library. MetaMap is a program providing access from biomedical text to the concepts in the Unified Medical Language System Metathesaurus.

MetaMap uses NLP to provide a link between the text of biomedical literature and the knowledge. . Metamap is used to map biomedical text to the Unified Medical Language System Metathesaurus or, equivalently, to discover Metathesaurus concepts referred to in text.","nlm is the world’s largest biomedical library. metamap is a program providing access from biomedical text to the concepts in the unified medical language system metathesaurus. metamap uses nlp to provide a link between the text of biomedical literature and the knowledge. . metamap is used to map biomedical text to the unified medical language system metathesaurus or, equivalently, to discover metathesaurus concepts referred to in text."
MTIX,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"NLM is the world’s largest biomedical library. MEDLINE is NLM's bibliographic database that contains references to journal articles in life sciences, with a concentration on biomedicine.

This is a machine learning-based system for the automated indexing of MEDLINE articles with Medical Subject Headings (MeSH) terms. Automated indexing is achieved using a multi-stage neural text ranking approach. ",Automated indexing allows for cost-effective and timely indexing of MEDLINE articles.,Implementation and Assessment,Neither,1/1/2021,1/1/2022,Unknown,Developed with contracting resources.,Black Canyon Consulting LLC 47QTCA18D00JA/75N97023F00098,Unknown,No,No,No,Yes,"The MTIX dataset is approximately 10 million PubMed MEDLINE citations published after 2006. It is publicly available data, used for training and evaluation of the MeSH terms predicted by the algorithm. ",Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"NLM is the world’s largest biomedical library. MEDLINE is NLM's bibliographic database that contains references to journal articles in life sciences, with a concentration on biomedicine.

This is a machine learning-based system for the automated indexing of MEDLINE articles with Medical Subject Headings (MeSH) terms. Automated indexing is achieved using a multi-stage neural text ranking approach. . Automated indexing allows for cost-effective and timely indexing of MEDLINE articles.","nlm is the world’s largest biomedical library. medline is nlm's bibliographic database that contains references to journal articles in life sciences, with a concentration on biomedicine. this is a machine learning-based system for the automated indexing of medline articles with medical subject headings (mesh) terms. automated indexing is achieved using a multi-stage neural text ranking approach. . automated indexing allows for cost-effective and timely indexing of medline articles."
NanCI: Connecting Scientists,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"NCI is helping to connect scientists.

NanCI is the app for biomedical scientists that helps discover scientific papers, build a career network, and explore events. ",NanCI is a new mobile application that uses machine learning algorithms to match users’ interests and provide a unique experience by recommending tailored content.,Operation and Maintenance,Neither,1/1/2021,1/1/2022,1/1/2024,Developed with contracting resources.,Google & Barnacle.ai via STRIDES mechanism,Unknown,No,No,Unknown,No,Unknown,Documentation is missing or not available,No,No – agency does not have access to source code.,No,Unknown,6-12 months,No,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"NCI is helping to connect scientists.

NanCI is the app for biomedical scientists that helps discover scientific papers, build a career network, and explore events. . NanCI is a new mobile application that uses machine learning algorithms to match users’ interests and provide a unique experience by recommending tailored content.","nci is helping to connect scientists. nanci is the app for biomedical scientists that helps discover scientific papers, build a career network, and explore events. . nanci is a new mobile application that uses machine learning algorithms to match users’ interests and provide a unique experience by recommending tailored content."
NBS Virtual Assistant,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"The NIH Business System (NBS) is NIH's financial system.

Streamlined Service Desk Operations. The AI-powered chatbot can handle routine user inquiries, reducing the burden on IT service desk. Provides self-service options for users to resolve issues independently.","The NIH Business System (NBS) Assistant is an AI-powered chatbot  to redefine how NBS users engage, and to offer seamless support and personalized assistance across various NBS workstreams. NBS is NIH's financial system.",Implementation and Assessment,Neither,1/1/2024,7/1/2024,Unknown,Developed with contracting resources.,Software - 75N97024F00051. System Integrator - 75N97022F00084,Unknown,No,No,No,No,"G-Invoicing: This dataset includes detailed information related to the G-Invoicing process, such as transaction records and invoicing workflows documentation.

User Provisioning: This includes data related to user access management, including roles, permissions, and user activity logs within the NIH systems.

PRISM-Related Job Aids and Documents: This dataset comprises job aids, guides, and documentation related to PRISM, which supports various administrative and operational functions at NIH.",Documentation is complete,No,"Yes – agency has access to source code, but it is not public.",Yes,NBS AI will be part of the NBS Cloud (NBSC) ATO. System name is NBSC.,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The NIH Business System (NBS) is NIH's financial system.

Streamlined Service Desk Operations. The AI-powered chatbot can handle routine user inquiries, reducing the burden on IT service desk. Provides self-service options for users to resolve issues independently. . The NIH Business System (NBS) Assistant is an AI-powered chatbot  to redefine how NBS users engage, and to offer seamless support and personalized assistance across various NBS workstreams. NBS is NIH's financial system.","the nih business system (nbs) is nih's financial system. streamlined service desk operations. the ai-powered chatbot can handle routine user inquiries, reducing the burden on it service desk. provides self-service options for users to resolve issues independently. . the nih business system (nbs) assistant is an ai-powered chatbot to redefine how nbs users engage, and to offer seamless support and personalized assistance across various nbs workstreams. nbs is nih's financial system."
"NCI-DOE Collaboration, MOSSAIC project (Modeling Outcomes using Surveillance Data and Scalable AI for Cancer)",Department of Health and Human Services,HHS,NIH,Health & Medical,None of the above.,"MOSSAIC applies deep learning natural language processing (NLP) and foundation models to population-based cancer data collected by NCI's Surveillance, Epidemiology, and End Results (SEER) program. DOE's Oak Ridge National Lab (ORNL) has data use agreements (DUAs) with multiple SEER registries to access and train models using SEER data. To date, the MOSSAIC pathology-coding API is deployed in 22 SEER registries, moving the US towards near real-time cancer incidence reporting. Other APIs are still in the validation phase or research and development phase.

MOSSAIC enhances the infrastructure of the SEER cancer registries by providing tools that can increase the efficiency and accuracy of manual data abstraction by automatically extracting cancer surveillance data elements.  SEER registries receive millions of unstructured clinical text documents that must be manually reviewed, leading to a lag in reporting of US cancer incidence trends.  Automated tools such as those developed by MOSSAIC will help us achieve near real-time incidence trends and ultimately a more meaningful report card on the status of cancer in the US.","Input: unstructured (free text) cancer pathology reports.

Output: varies depending on the algorithm but generally a predicted class (eg, tumor site) and associated relative confidence score that can be used to tune accuracy.",Implementation and Assessment,Neither,1/1/2017,10/1/2017,Unknown,Developed with contracting resources.,Developed under an Interagency Agreement with the US Department of Energy,Unknown,No,Yes,No,No,"Data is owned by the NCI SEER registries, which are funded by the NCI",Documentation is complete,No,Yes – source code is publicly available.,No,Unknown,Less than 6 months,No,No,No,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"MOSSAIC applies deep learning natural language processing (NLP) and foundation models to population-based cancer data collected by NCI's Surveillance, Epidemiology, and End Results (SEER) program. DOE's Oak Ridge National Lab (ORNL) has data use agreements (DUAs) with multiple SEER registries to access and train models using SEER data. To date, the MOSSAIC pathology-coding API is deployed in 22 SEER registries, moving the US towards near real-time cancer incidence reporting. Other APIs are still in the validation phase or research and development phase.

MOSSAIC enhances the infrastructure of the SEER cancer registries by providing tools that can increase the efficiency and accuracy of manual data abstraction by automatically extracting cancer surveillance data elements.  SEER registries receive millions of unstructured clinical text documents that must be manually reviewed, leading to a lag in reporting of US cancer incidence trends.  Automated tools such as those developed by MOSSAIC will help us achieve near real-time incidence trends and ultimately a more meaningful report card on the status of cancer in the US. . Input: unstructured (free text) cancer pathology reports.

Output: varies depending on the algorithm but generally a predicted class (eg, tumor site) and associated relative confidence score that can be used to tune accuracy.","mossaic applies deep learning natural language processing (nlp) and foundation models to population-based cancer data collected by nci's surveillance, epidemiology, and end results (seer) program. doe's oak ridge national lab (ornl) has data use agreements (duas) with multiple seer registries to access and train models using seer data. to date, the mossaic pathology-coding api is deployed in 22 seer registries, moving the us towards near real-time cancer incidence reporting. other apis are still in the validation phase or research and development phase. mossaic enhances the infrastructure of the seer cancer registries by providing tools that can increase the efficiency and accuracy of manual data abstraction by automatically extracting cancer surveillance data elements. seer registries receive millions of unstructured clinical text documents that must be manually reviewed, leading to a lag in reporting of us cancer incidence trends. automated tools such as those developed by mossaic will help us achieve near real-time incidence trends and ultimately a more meaningful report card on the status of cancer in the us. . input: unstructured (free text) cancer pathology reports. output: varies depending on the algorithm but generally a predicted class (eg, tumor site) and associated relative confidence score that can be used to tune accuracy."
NHLBI Chat,Department of Health and Human Services,HHS,NIH,Health & Medical,None of the above.,"NHLBI Chat is a secure LLM tooling providing access to the Azure OpenAI API so that all NHLBI staff can explore generative AI for their day-to-day need.

NHLBI Chat is a secure LLM tooling providing access to the Azure OpenAI API so that all NHLBI staff can explore generative AI for their day-to-day need.",The Azure OpenAI API accepts text as input and return text as output. Users enter text through a chat interface in a website.,Operation and Maintenance,Neither,5/1/2023,5/1/2023,9/1/2024,Developed in-house.,Unknown,Unknown,No,No,No,No,Unknown,Documentation is missing or not available,Yes,Yes – source code is publicly available.,Yes,NHLBI Chat - https://trac.nhlbi.nih.gov/trac/openTracRequest?tracIDNumber=47124&type=simple ,Less than 6 months,Yes,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"NHLBI Chat is a secure LLM tooling providing access to the Azure OpenAI API so that all NHLBI staff can explore generative AI for their day-to-day need.

NHLBI Chat is a secure LLM tooling providing access to the Azure OpenAI API so that all NHLBI staff can explore generative AI for their day-to-day need. . The Azure OpenAI API accepts text as input and return text as output. Users enter text through a chat interface in a website.",nhlbi chat is a secure llm tooling providing access to the azure openai api so that all nhlbi staff can explore generative ai for their day-to-day need. nhlbi chat is a secure llm tooling providing access to the azure openai api so that all nhlbi staff can explore generative ai for their day-to-day need. . the azure openai api accepts text as input and return text as output. users enter text through a chat interface in a website.
NIAID GenAI Toolkit,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"The NIAID GenAI Chat and Document Bot are Azure-hosted GenAI applications that leverage commercially available large language models from the Azure OpenAI Service to assist NIAID employees in a variety of task such as content generation, summarization, etc.

The Azure-hosted NIAID GenAI Chat and Document Bot helps employees be more efficient with a wide variety of administrative tasks, such as summarizing/querying documents, drafting emails, and creating presentation outlines?, etc. ","Input: natural text in the form of user questions, user uploaded documents. 

Output: Generated text in the form of answers to user questions, generated answers (summaries/queries) based on user documents. ",Operation and Maintenance,Neither,2/1/2024,2/1/2024,7/1/2024,Developed with contracting resources.,Unknown,Unknown,No,No,No,Yes,"No, training, fine-tuning, and performance evaluation was not conducted.",Documentation is missing or not available,Yes,Yes – source code is publicly available.,Yes,Not Reported,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The NIAID GenAI Chat and Document Bot are Azure-hosted GenAI applications that leverage commercially available large language models from the Azure OpenAI Service to assist NIAID employees in a variety of task such as content generation, summarization, etc.

The Azure-hosted NIAID GenAI Chat and Document Bot helps employees be more efficient with a wide variety of administrative tasks, such as summarizing/querying documents, drafting emails, and creating presentation outlines?, etc. . Input: natural text in the form of user questions, user uploaded documents. 

Output: Generated text in the form of answers to user questions, generated answers (summaries/queries) based on user documents.","the niaid genai chat and document bot are azure-hosted genai applications that leverage commercially available large language models from the azure openai service to assist niaid employees in a variety of task such as content generation, summarization, etc. the azure-hosted niaid genai chat and document bot helps employees be more efficient with a wide variety of administrative tasks, such as summarizing/querying documents, drafting emails, and creating presentation outlines?, etc. . input: natural text in the form of user questions, user uploaded documents. output: generated text in the form of answers to user questions, generated answers (summaries/queries) based on user documents."
NIAMS AI Chatbot Pilot,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"To develop a secured AI Chatbot environment where NIAMS staff can 1) conduct general research, 2) help staff in writing grants/publications/papers (e.g. foreign researcher with their English grammar, sentence structure, etc.), 3) scientific and IT code generation, 4) summarizing meeting minutes and 5) providing a platform to learn how AI can be used in their daily work tasks.

Aside from what is described in 'Use Case Scope', other expected benefits are to assess the cost vs benefit value of having our own chatbot vs paying per user per month to use the NIH Enterprise versions of MS Copilot and OpenAI ChatGPT. This pilot will also be used to understand and develop a tiered business case on which NIAMS staff would fit which chatbot (NIAMS chatbot, MS copilot and OpenAI ChatGPT).","Inputs: Text input, text-based documents or PDF uploaded by users entered into a prompt. Amount or frequency of data are based on how often users use the chatbot, which will be determined during or after the pilot. 

Outputs: Text summaries, information, recommendations and classifications. Output will be presented in text form. The frequency the AI produces the results is based on how often users use the chatbot.",Acquisition and/or Development,Neither,8/1/2024,8/1/2024,Unknown,Developed with contracting resources.,47QTCA19D00A8,Unknown,Unknown,Unknown,Unknown,No,Unknown,Documentation is missing or not available,Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.38095238095238093,"To develop a secured AI Chatbot environment where NIAMS staff can 1) conduct general research, 2) help staff in writing grants/publications/papers (e.g. foreign researcher with their English grammar, sentence structure, etc.), 3) scientific and IT code generation, 4) summarizing meeting minutes and 5) providing a platform to learn how AI can be used in their daily work tasks.

Aside from what is described in 'Use Case Scope', other expected benefits are to assess the cost vs benefit value of having our own chatbot vs paying per user per month to use the NIH Enterprise versions of MS Copilot and OpenAI ChatGPT. This pilot will also be used to understand and develop a tiered business case on which NIAMS staff would fit which chatbot (NIAMS chatbot, MS copilot and OpenAI ChatGPT). . Inputs: Text input, text-based documents or PDF uploaded by users entered into a prompt. Amount or frequency of data are based on how often users use the chatbot, which will be determined during or after the pilot. 

Outputs: Text summaries, information, recommendations and classifications. Output will be presented in text form. The frequency the AI produces the results is based on how often users use the chatbot.","to develop a secured ai chatbot environment where niams staff can 1) conduct general research, 2) help staff in writing grants/publications/papers (e.g. foreign researcher with their english grammar, sentence structure, etc.), 3) scientific and it code generation, 4) summarizing meeting minutes and 5) providing a platform to learn how ai can be used in their daily work tasks. aside from what is described in 'use case scope', other expected benefits are to assess the cost vs benefit value of having our own chatbot vs paying per user per month to use the nih enterprise versions of ms copilot and openai chatgpt. this pilot will also be used to understand and develop a tiered business case on which niams staff would fit which chatbot (niams chatbot, ms copilot and openai chatgpt). . inputs: text input, text-based documents or pdf uploaded by users entered into a prompt. amount or frequency of data are based on how often users use the chatbot, which will be determined during or after the pilot. outputs: text summaries, information, recommendations and classifications. output will be presented in text form. the frequency the ai produces the results is based on how often users use the chatbot."
NICHD RPAB AI/ML Application Referral System,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"The NICHD RPAB AI/ML Application Referral System leverages the advanced AI/ML technologies to predict extramural branch assignments of new, incoming NICHD grant applications.  It employs human-in-the-loop development which enables human evaluation and offers the ability to override and alter model outputs.  Source data are from the NIH IMPAC II system and stored in the existing NICHD IT application CHIRP (Child Health Information Retrieval Program).  The AI process and output data have no impact on the NIH IMPAC II system. The business area addressed is the internal referral assignment performed by RPAB.

The primary objective is to enhance the efficiency, accuracy, and consistency of grant application referral assignments, while reducing the burden on Subject Matter Experts in RPAB. The AI system is expected to streamline the process of internal referral of new grant applications.",Internal: NIH ImpacII and CHIRP results are presented as class predictions and class probabilities.,Implementation and Assessment,Neither,10/1/2023,10/1/2023,Unknown,Developed in-house.,Unknown,Unknown,No,Yes,No,Yes,"NIH IMPAC II funded and unfunded grant application data is used. Unstructured text from project abstract, specific aims, and title are encoded and vectorized for model training and inference. Fiscal year, activity code, and RCDC terms are transformed via one-hot encoding for use in model training and inference. PII related to individuals associated with the grant is kept intact to preserve the integrity of the use case of grant application referral and the trends of researchers' focus on particular scientific areas.",Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"The NICHD RPAB AI/ML Application Referral System leverages the advanced AI/ML technologies to predict extramural branch assignments of new, incoming NICHD grant applications.  It employs human-in-the-loop development which enables human evaluation and offers the ability to override and alter model outputs.  Source data are from the NIH IMPAC II system and stored in the existing NICHD IT application CHIRP (Child Health Information Retrieval Program).  The AI process and output data have no impact on the NIH IMPAC II system. The business area addressed is the internal referral assignment performed by RPAB.

The primary objective is to enhance the efficiency, accuracy, and consistency of grant application referral assignments, while reducing the burden on Subject Matter Experts in RPAB. The AI system is expected to streamline the process of internal referral of new grant applications. . Internal: NIH ImpacII and CHIRP results are presented as class predictions and class probabilities.","the nichd rpab ai/ml application referral system leverages the advanced ai/ml technologies to predict extramural branch assignments of new, incoming nichd grant applications. it employs human-in-the-loop development which enables human evaluation and offers the ability to override and alter model outputs. source data are from the nih impac ii system and stored in the existing nichd it application chirp (child health information retrieval program). the ai process and output data have no impact on the nih impac ii system. the business area addressed is the internal referral assignment performed by rpab. the primary objective is to enhance the efficiency, accuracy, and consistency of grant application referral assignments, while reducing the burden on subject matter experts in rpab. the ai system is expected to streamline the process of internal referral of new grant applications. . internal: nih impacii and chirp results are presented as class predictions and class probabilities."
NLP Automated Referral,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"Publicly reported under OER. NIGMS worked with eRA for over a year to incorporate natural language processing (NLP) and machine learning (ML) algorithms into eRA's Internal Referral Module (IRM).  IRM is now capable of using ML/NLP to assign applications directly to program officers (POs) based on the titles, abstracts, narratives, and specific aims of those applications, as well as the application histories of the PIs. The current process runs in the background on eRA's AWS cloud instances, and assigns the majority of new NIGMS applications to POs as soon as they are referred to NIGMS by CSR.  ML/NLP is also used to provide POs with the three most relevant NIH institutions to applications to facilitate IC fit determinations.

Automated referral allows NIGMS to retain institutional referral knowledge by training on historical data, eliminates delays in referral by assigning applications as soon as they come in, and reduces burden on staff members and allows them to allocate more of their time to other high value tasks.","The tools uses IMPAC II application data, including titles, abstracts, narratives and specific aims to produce a top three of most relevant ICs and POs. ",Operation and Maintenance,Neither,8/1/2018,12/1/2019,8/1/2020,Developed in-house.,Unknown,Unknown,No,No,No,Yes,All data come from the internal NIH IMPAC II database.,Documentation is missing or not available,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,No,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Publicly reported under OER. NIGMS worked with eRA for over a year to incorporate natural language processing (NLP) and machine learning (ML) algorithms into eRA's Internal Referral Module (IRM).  IRM is now capable of using ML/NLP to assign applications directly to program officers (POs) based on the titles, abstracts, narratives, and specific aims of those applications, as well as the application histories of the PIs. The current process runs in the background on eRA's AWS cloud instances, and assigns the majority of new NIGMS applications to POs as soon as they are referred to NIGMS by CSR.  ML/NLP is also used to provide POs with the three most relevant NIH institutions to applications to facilitate IC fit determinations.

Automated referral allows NIGMS to retain institutional referral knowledge by training on historical data, eliminates delays in referral by assigning applications as soon as they come in, and reduces burden on staff members and allows them to allocate more of their time to other high value tasks. . The tools uses IMPAC II application data, including titles, abstracts, narratives and specific aims to produce a top three of most relevant ICs and POs.","publicly reported under oer. nigms worked with era for over a year to incorporate natural language processing (nlp) and machine learning (ml) algorithms into era's internal referral module (irm). irm is now capable of using ml/nlp to assign applications directly to program officers (pos) based on the titles, abstracts, narratives, and specific aims of those applications, as well as the application histories of the pis. the current process runs in the background on era's aws cloud instances, and assigns the majority of new nigms applications to pos as soon as they are referred to nigms by csr. ml/nlp is also used to provide pos with the three most relevant nih institutions to applications to facilitate ic fit determinations. automated referral allows nigms to retain institutional referral knowledge by training on historical data, eliminates delays in referral by assigning applications as soon as they come in, and reduces burden on staff members and allows them to allocate more of their time to other high value tasks. . the tools uses impac ii application data, including titles, abstracts, narratives and specific aims to produce a top three of most relevant ics and pos."
Person-level disambiguation for PubMed authors and NIH grant applicants,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"The Office of Portfolio Analysis employs AI-based approaches to support analysis of the biomedical research landscape and inform data-driven decision-making by NIH leadership and extramural research administrators in other federal agencies.

High-quality disambiguation is required to correctly link researchers to their grants and outputs including articles, patents, and clinical trials. ","The NIH Office of Portfolio Analysis developed a disambiguation solution that used article level metadata to assign 24.5M unique papers from the PubMed database to 16.0M unique author names, then used a novel neural network model to determine whether author-publication pairs refer to variant representations of the same person. ",Operation and Maintenance,Neither,2/1/2021,2/1/2021,2/1/2023,Developed with contracting resources.,Unknown,Unknown,No,No,No,No,Unknown,Documentation is missing or not available,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,More than 12 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"The Office of Portfolio Analysis employs AI-based approaches to support analysis of the biomedical research landscape and inform data-driven decision-making by NIH leadership and extramural research administrators in other federal agencies.

High-quality disambiguation is required to correctly link researchers to their grants and outputs including articles, patents, and clinical trials. . The NIH Office of Portfolio Analysis developed a disambiguation solution that used article level metadata to assign 24.5M unique papers from the PubMed database to 16.0M unique author names, then used a novel neural network model to determine whether author-publication pairs refer to variant representations of the same person.","the office of portfolio analysis employs ai-based approaches to support analysis of the biomedical research landscape and inform data-driven decision-making by nih leadership and extramural research administrators in other federal agencies. high-quality disambiguation is required to correctly link researchers to their grants and outputs including articles, patents, and clinical trials. . the nih office of portfolio analysis developed a disambiguation solution that used article level metadata to assign 24.5m unique papers from the pubmed database to 16.0m unique author names, then used a novel neural network model to determine whether author-publication pairs refer to variant representations of the same person."
Scientific summaries tool,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"The goal is to generate justifications for personnel actions including appointments, conversions, pay adjustments, and hiring/retention incentives for physicians, scientists, and veterinarians.

Within NIAID DIR, we have team that drafts justifications for personnel actions based on the research being performed. This tool will be created to help them quickly and effectively prepare justifications for personnel actions for investigators in specific research fields.","Inputs: scientific publications, CV/Bib, BSC submissions and outcome memos, prior justifications, and, clinical protocols.

Outputs: Scientific summary.",Implementation and Assessment,Neither,7/1/2024,7/1/2024,Unknown,Developed in-house.,Unknown,Unknown,No,No,No,No,Unknown,Documentation is widely available,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Not Reported,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"The goal is to generate justifications for personnel actions including appointments, conversions, pay adjustments, and hiring/retention incentives for physicians, scientists, and veterinarians.

Within NIAID DIR, we have team that drafts justifications for personnel actions based on the research being performed. This tool will be created to help them quickly and effectively prepare justifications for personnel actions for investigators in specific research fields. . Inputs: scientific publications, CV/Bib, BSC submissions and outcome memos, prior justifications, and, clinical protocols.

Outputs: Scientific summary.","the goal is to generate justifications for personnel actions including appointments, conversions, pay adjustments, and hiring/retention incentives for physicians, scientists, and veterinarians. within niaid dir, we have team that drafts justifications for personnel actions based on the research being performed. this tool will be created to help them quickly and effectively prepare justifications for personnel actions for investigators in specific research fields. . inputs: scientific publications, cv/bib, bsc submissions and outcome memos, prior justifications, and, clinical protocols. outputs: scientific summary."
Semantic group prediction in the UMLS Metathesaurus,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"Developing AI approaches (neural networks, heuristics) to predict the semantic groups of new terms to be added to the Unified Medical Language System (UMLS) Metathesaurus.   This is used during new concept integration into the UMLS Metathesaurus and is only intended to interact with the new concepts that will be added and existing UMLS Metathesaurus concepts.  There is no training or intended use of this product for sensitive data.  The hybrid system combining deep learning and heuristics should be able to effectively support UMLS editing and quality assurance. 

Support UMLS Metathesaurus editing.  This should dramatically reduce the time needed to integrate new concepts into the UMLS Metathesaurus and increase the consistency. ","Input: UMLS Metathesaurus (new concepts).

Output: Semantic group(s) for the new concept. Concepts are typically text words or phrases for example """"Lung Cancer"""", """"heart attack"""". This is used yearly when it is time to add new concepts to the UMLS Metathesaurus. ",Acquisition and/or Development,Neither,7/1/2021,11/1/2021,Unknown,Developed with contracting resources.,GSA Contract Number: 47QTCA18D00JA,Unknown,Unknown,Unknown,Unknown,Yes,"UMLS Metathesaurus was used for development, training, and testing of the developed software. ",Documentation is missing or not available,Unknown,Unknown,No,Unknown,Less than 6 months,Yes,No,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"Developing AI approaches (neural networks, heuristics) to predict the semantic groups of new terms to be added to the Unified Medical Language System (UMLS) Metathesaurus.   This is used during new concept integration into the UMLS Metathesaurus and is only intended to interact with the new concepts that will be added and existing UMLS Metathesaurus concepts.  There is no training or intended use of this product for sensitive data.  The hybrid system combining deep learning and heuristics should be able to effectively support UMLS editing and quality assurance. 

Support UMLS Metathesaurus editing.  This should dramatically reduce the time needed to integrate new concepts into the UMLS Metathesaurus and increase the consistency. . Input: UMLS Metathesaurus (new concepts).

Output: Semantic group(s) for the new concept. Concepts are typically text words or phrases for example """"Lung Cancer"""", """"heart attack"""". This is used yearly when it is time to add new concepts to the UMLS Metathesaurus.","developing ai approaches (neural networks, heuristics) to predict the semantic groups of new terms to be added to the unified medical language system (umls) metathesaurus. this is used during new concept integration into the umls metathesaurus and is only intended to interact with the new concepts that will be added and existing umls metathesaurus concepts. there is no training or intended use of this product for sensitive data. the hybrid system combining deep learning and heuristics should be able to effectively support umls editing and quality assurance. support umls metathesaurus editing. this should dramatically reduce the time needed to integrate new concepts into the umls metathesaurus and increase the consistency. . input: umls metathesaurus (new concepts). output: semantic group(s) for the new concept. concepts are typically text words or phrases for example """"lung cancer"""", """"heart attack"""". this is used yearly when it is time to add new concepts to the umls metathesaurus."
Similarity-based Application and Investigator Matching (SAIM),Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"NIGMS supports basic research that increases our understanding of biological processes and lays the foundation for advances in disease diagnosis, treatment, and prevention. NIGMS-funded scientists investigate how living systems work at a range of levels from molecules and cells to tissues and organs, in research organisms, humans, and populations.

NIGMS needs to know how much total support an investigator has to ensure that it is not funding Principal Investigators who are already adequately resourced. The tool can determine if an application has significant overlap (Just-In-Time Other Support) with an application that is funded by another agency.",The Similarity-based Application and Investigator Matching (SAIM) system uses NLP to find non-NIH grants awarded to NIGMS Principal Investigators.,Operation and Maintenance,Neither,6/1/2023,6/1/2023,1/1/2024,Developed in-house.,Unknown,Unknown,No,No,No,Yes,"Data for this project come from the internal NIH IMPAC II database, the publicly available NSF grants database, the publicly available SBIR grants database, and the publicly available HHS grants database, TAGGS.",Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"NIGMS supports basic research that increases our understanding of biological processes and lays the foundation for advances in disease diagnosis, treatment, and prevention. NIGMS-funded scientists investigate how living systems work at a range of levels from molecules and cells to tissues and organs, in research organisms, humans, and populations.

NIGMS needs to know how much total support an investigator has to ensure that it is not funding Principal Investigators who are already adequately resourced. The tool can determine if an application has significant overlap (Just-In-Time Other Support) with an application that is funded by another agency. . The Similarity-based Application and Investigator Matching (SAIM) system uses NLP to find non-NIH grants awarded to NIGMS Principal Investigators.","nigms supports basic research that increases our understanding of biological processes and lays the foundation for advances in disease diagnosis, treatment, and prevention. nigms-funded scientists investigate how living systems work at a range of levels from molecules and cells to tissues and organs, in research organisms, humans, and populations. nigms needs to know how much total support an investigator has to ensure that it is not funding principal investigators who are already adequately resourced. the tool can determine if an application has significant overlap (just-in-time other support) with an application that is funded by another agency. . the similarity-based application and investigator matching (saim) system uses nlp to find non-nih grants awarded to nigms principal investigators."
SingleCite: Improving single citation search in PubMed,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"NLM is the world’s largest biomedical library. SingleCite is search algorithm designed to improve single citation searches in the PubMed database.

SingleCite helps increase the effectiveness of PubMed searches by making a user’s search for a specific document in PubMed more successful.","SingleCite predicts the probability of a retrieved document being the target of a query based on predefined variables. This search is important for scholarly databases, such as PubMed.",Operation and Maintenance,Neither,6/1/2017,6/1/2017,1/1/2023,Developed in-house.,Unknown,Unknown,Yes,No,No,No,PubMed literature collectionAn earlier tool that perfomed the same task,Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,No,No,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"NLM is the world’s largest biomedical library. SingleCite is search algorithm designed to improve single citation searches in the PubMed database.

SingleCite helps increase the effectiveness of PubMed searches by making a user’s search for a specific document in PubMed more successful. . SingleCite predicts the probability of a retrieved document being the target of a query based on predefined variables. This search is important for scholarly databases, such as PubMed.","nlm is the world’s largest biomedical library. singlecite is search algorithm designed to improve single citation searches in the pubmed database. singlecite helps increase the effectiveness of pubmed searches by making a user’s search for a specific document in pubmed more successful. . singlecite predicts the probability of a retrieved document being the target of a query based on predefined variables. this search is important for scholarly databases, such as pubmed."
SRDMS NLP COI,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"NIAID administers grant portfolios in the extramural research program areas including Allergy, Immunology, & Transplantation, Infectious Diseases, and HIV/AIDS.

Detect individuals who may pose a conflict-of-interest during the grant review process. ",The tool allows NIAID’s Scientific Review Program team to more easily identify conflicts of interest (COI) between grant reviewers and applicants using NLP methods.,Operation and Maintenance,Neither,1/1/2019,1/1/2019,1/1/2019,Developed with contracting resources.,Unknown,Unknown,No,No,No,Yes,Yes - Labeled dataset of grant applications and associated conflicts of interest is used to calculate pipeline evaluation metrics.,Documentation is complete,Yes,Yes – source code is publicly available.,Yes,Not Reported,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"NIAID administers grant portfolios in the extramural research program areas including Allergy, Immunology, & Transplantation, Infectious Diseases, and HIV/AIDS.

Detect individuals who may pose a conflict-of-interest during the grant review process. . The tool allows NIAID’s Scientific Review Program team to more easily identify conflicts of interest (COI) between grant reviewers and applicants using NLP methods.","niaid administers grant portfolios in the extramural research program areas including allergy, immunology, & transplantation, infectious diseases, and hiv/aids. detect individuals who may pose a conflict-of-interest during the grant review process. . the tool allows niaid’s scientific review program team to more easily identify conflicts of interest (coi) between grant reviewers and applicants using nlp methods."
Stem Cell Auto Coder,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"NIGMS supports basic research that increases our understanding of biological processes and lays the foundation for advances in disease diagnosis, treatment, and prevention. NIGMS-funded scientists investigate how living systems work at a range of levels from molecules and cells to tissues and organs, in research organisms, humans, and populations.

Manually coding Stem Cell Research subcategories is a time intensive process. NIGMS sped up this process by using automation.","The Stem Cell Auto Coder uses NLP and ML to predict the Stem Cell Research subcategories of an application: human embryonic, non-human embryonic, human induced pluripotent, non-human induced pluripotent, human non-embryonic, and non-human non-embryonic.",Implementation and Assessment,Neither,2/1/2023,2/1/2023,Unknown,Developed in-house.,Unknown,Unknown,No,No,No,Yes,All data come from the internal NIH IMPAC II database.,Documentation is missing or not available,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"NIGMS supports basic research that increases our understanding of biological processes and lays the foundation for advances in disease diagnosis, treatment, and prevention. NIGMS-funded scientists investigate how living systems work at a range of levels from molecules and cells to tissues and organs, in research organisms, humans, and populations.

Manually coding Stem Cell Research subcategories is a time intensive process. NIGMS sped up this process by using automation. . The Stem Cell Auto Coder uses NLP and ML to predict the Stem Cell Research subcategories of an application: human embryonic, non-human embryonic, human induced pluripotent, non-human induced pluripotent, human non-embryonic, and non-human non-embryonic.","nigms supports basic research that increases our understanding of biological processes and lays the foundation for advances in disease diagnosis, treatment, and prevention. nigms-funded scientists investigate how living systems work at a range of levels from molecules and cells to tissues and organs, in research organisms, humans, and populations. manually coding stem cell research subcategories is a time intensive process. nigms sped up this process by using automation. . the stem cell auto coder uses nlp and ml to predict the stem cell research subcategories of an application: human embryonic, non-human embryonic, human induced pluripotent, non-human induced pluripotent, human non-embryonic, and non-human non-embryonic."
Study Section Clustering Tool (SSCT),Department of Health and Human Services,HHS,NIH,Health & Medical,None of the above.,"This tool suggests how study sections might be organized into groups of similar scientific fields to decide how applications might be grouped into new study sections. Subject matter experts ultimately finalize the study section groupings. 

This tool helps to ensure that applications are grouped into study sections in a way that maps on to the current state of the science.  ","Input: Text of grant applications.

Output: Lists of study sections that should be grouped together for further review.  ",Operation and Maintenance,Neither,1/1/2022,1/1/2022,1/1/2023,Developed with contracting resources.,Unknown,Unknown,No,No,No,Yes,IMPACII,Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Center for Scientific Review  General Support System (CSR GSS) ,Less than 6 months,Yes,No,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"This tool suggests how study sections might be organized into groups of similar scientific fields to decide how applications might be grouped into new study sections. Subject matter experts ultimately finalize the study section groupings. 

This tool helps to ensure that applications are grouped into study sections in a way that maps on to the current state of the science. . Input: Text of grant applications.

Output: Lists of study sections that should be grouped together for further review.",this tool suggests how study sections might be organized into groups of similar scientific fields to decide how applications might be grouped into new study sections. subject matter experts ultimately finalize the study section groupings. this tool helps to ensure that applications are grouped into study sections in a way that maps on to the current state of the science. . input: text of grant applications. output: lists of study sections that should be grouped together for further review.
Synonymy prediction in the UMLS Metathesaurus,Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"Developing artificial intelligence (AI) approaches (neural networks) to predict synonymy among biomedical concepts in the Unified Medical Language System (UMLS) Metathesaurus, with application to the insertion of new concepts into the UMLS Metathesaurus.  This is used during new concept integration into the UMLS Metathesaurus and is only intended to interact with the new concepts that will be added and existing UMLS Metathesaurus concepts.  There is no training or intended use of this product for sensitive data. Preliminary results indicate the performance of the system should be sufficient to support the insertion of new terms into the UMLS Metathesaurus. 

Support UMLS Metathesaurus editing.  This should dramatically reduce the time needed to integrate new concepts into the UMLS Metathesaurus and increase the consistency. ","Input: UMLS Metathesaurus (new concepts).

Output: Synonymy with existing Metathesaurus concepts. Concepts are typically text words or phrases for example """"Lung Cancer"""", """"heart attack"""". This is used yearly when it is time to add new concepts to the UMLS Metathesaurus.   ",Acquisition and/or Development,Neither,9/1/2019,3/1/2020,Unknown,Developed with contracting resources.,GSA Contract Number: 47QTCA18D00JA,Unknown,Unknown,Unknown,Unknown,Yes,"UMLS Metathesaurus was used for development, training, and testing of the developed software. ",Documentation is missing or not available,Unknown,Unknown,No,Unknown,Less than 6 months,Yes,No,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"Developing artificial intelligence (AI) approaches (neural networks) to predict synonymy among biomedical concepts in the Unified Medical Language System (UMLS) Metathesaurus, with application to the insertion of new concepts into the UMLS Metathesaurus.  This is used during new concept integration into the UMLS Metathesaurus and is only intended to interact with the new concepts that will be added and existing UMLS Metathesaurus concepts.  There is no training or intended use of this product for sensitive data. Preliminary results indicate the performance of the system should be sufficient to support the insertion of new terms into the UMLS Metathesaurus. 

Support UMLS Metathesaurus editing.  This should dramatically reduce the time needed to integrate new concepts into the UMLS Metathesaurus and increase the consistency. . Input: UMLS Metathesaurus (new concepts).

Output: Synonymy with existing Metathesaurus concepts. Concepts are typically text words or phrases for example """"Lung Cancer"""", """"heart attack"""". This is used yearly when it is time to add new concepts to the UMLS Metathesaurus.","developing artificial intelligence (ai) approaches (neural networks) to predict synonymy among biomedical concepts in the unified medical language system (umls) metathesaurus, with application to the insertion of new concepts into the umls metathesaurus. this is used during new concept integration into the umls metathesaurus and is only intended to interact with the new concepts that will be added and existing umls metathesaurus concepts. there is no training or intended use of this product for sensitive data. preliminary results indicate the performance of the system should be sufficient to support the insertion of new terms into the umls metathesaurus. support umls metathesaurus editing. this should dramatically reduce the time needed to integrate new concepts into the umls metathesaurus and increase the consistency. . input: umls metathesaurus (new concepts). output: synonymy with existing metathesaurus concepts. concepts are typically text words or phrases for example """"lung cancer"""", """"heart attack"""". this is used yearly when it is time to add new concepts to the umls metathesaurus."
TB Case Browser Image Text Detection,Department of Health and Human Services,HHS,NIH,Health & Medical,None of the above.,"The NIAID TB Portals Program is a multi-national collaboration for tuberculosis (TB) data sharing and analysis to advance TB research.

A tool that uses AWS Recognition managed AI service to detect text in images which could be potentially Personally Identifiable Information (PII)/ Protected Health Information (PHI) in TB Portals data. ",The tool detects and counts instances of text and if above a threshold throws a warning to the user who is uploading the image. User must then affirmatively bypass to upload.,Operation and Maintenance,Neither,1/1/2019,1/1/2019,1/1/2019,Developed with contracting resources.,Unknown,Unknown,No,No,No,Yes,"Yes, used to evaluate performance.",Documentation is complete,Yes,Yes – source code is publicly available.,Yes,Not Reported,Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The NIAID TB Portals Program is a multi-national collaboration for tuberculosis (TB) data sharing and analysis to advance TB research.

A tool that uses AWS Recognition managed AI service to detect text in images which could be potentially Personally Identifiable Information (PII)/ Protected Health Information (PHI) in TB Portals data. . The tool detects and counts instances of text and if above a threshold throws a warning to the user who is uploading the image. User must then affirmatively bypass to upload.",the niaid tb portals program is a multi-national collaboration for tuberculosis (tb) data sharing and analysis to advance tb research. a tool that uses aws recognition managed ai service to detect text in images which could be potentially personally identifiable information (pii)/ protected health information (phi) in tb portals data. . the tool detects and counts instances of text and if above a threshold throws a warning to the user who is uploading the image. user must then affirmatively bypass to upload.
Tool for PO Lookup Assignment (TPAL),Department of Health and Human Services,HHS,NIH,Government Services (includes Benefits and Service Delivery),None of the above.,"Similar to publicly reported IRM use case. The Tool for PO Assignment Lookup (TPAL) provides NIGMS Program staff with the ability to automatically search for?appropriate?Program Officers (POs), including their corresponding Program Area Codes (PACs), and NIH ICs?to assign?or reassign?project proposals, or to provide advice to prospective applicants. TPAL uses natural language processing and machine learning to predict the three most relevant NIGMS POs, including their corresponding PACs, and the three most relevant NIH ICs from the project description/abstract text entered into a textbox. Note that because the PO and IC prediction algorithms are independent of each other, TPAL will always try to provide the most relevant program? officers, even when NIGMS is not one of the top three recommended NIH ICs.  

There are many occasions in which Division Directors, Branch Chiefs, and Program Officers wish to receive suggestions for the most appropriate people to talk to about a project proposal or where to send a proposal that might not be appropriate for NIGMS.","Input: Free form text in an online textbox. 

Output: Top three most relevant ICs and POs and their probabilities. ",Operation and Maintenance,Neither,3/1/2020,3/1/2020,7/1/2020,Developed in-house.,Unknown,Unknown,No,No,No,Yes,All data comes from the internal NIH IMPAC II database.,Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Similar to publicly reported IRM use case. The Tool for PO Assignment Lookup (TPAL) provides NIGMS Program staff with the ability to automatically search for?appropriate?Program Officers (POs), including their corresponding Program Area Codes (PACs), and NIH ICs?to assign?or reassign?project proposals, or to provide advice to prospective applicants. TPAL uses natural language processing and machine learning to predict the three most relevant NIGMS POs, including their corresponding PACs, and the three most relevant NIH ICs from the project description/abstract text entered into a textbox. Note that because the PO and IC prediction algorithms are independent of each other, TPAL will always try to provide the most relevant program? officers, even when NIGMS is not one of the top three recommended NIH ICs.  

There are many occasions in which Division Directors, Branch Chiefs, and Program Officers wish to receive suggestions for the most appropriate people to talk to about a project proposal or where to send a proposal that might not be appropriate for NIGMS. . Input: Free form text in an online textbox. 

Output: Top three most relevant ICs and POs and their probabilities.","similar to publicly reported irm use case. the tool for po assignment lookup (tpal) provides nigms program staff with the ability to automatically search for?appropriate?program officers (pos), including their corresponding program area codes (pacs), and nih ics?to assign?or reassign?project proposals, or to provide advice to prospective applicants. tpal uses natural language processing and machine learning to predict the three most relevant nigms pos, including their corresponding pacs, and the three most relevant nih ics from the project description/abstract text entered into a textbox. note that because the po and ic prediction algorithms are independent of each other, tpal will always try to provide the most relevant program? officers, even when nigms is not one of the top three recommended nih ics. there are many occasions in which division directors, branch chiefs, and program officers wish to receive suggestions for the most appropriate people to talk to about a project proposal or where to send a proposal that might not be appropriate for nigms. . input: free form text in an online textbox. output: top three most relevant ics and pos and their probabilities."
Transformative Research Award Anonymization Check (TRAAC),Department of Health and Human Services,HHS,NIH,Health & Medical,None of the above.,"This tool helps to determine whether grant applications for NIH Director's Transformative Research Awards are properly anonymized by quickly detecting information that might identify the applicant in the Specific Aims or Research Strategy sections. 

This tool automatically screens many pages and highlights any text that might allow identification of the applicants. The highlighted text is then examined by subject matter experts. This streamlines content screening and aids both NIH staff and the external scientific community in making more rapid and efficient determinations about whether anonymity has been compromised. ","Input: PDF files including text and images from two sections of the NIH grant applications.

Output: Spreadsheets containing records of identified text/phrases as well as PDF files with text/phrases highlighted. ",Operation and Maintenance,Neither,1/1/2021,1/1/2021,1/1/2022,Developed in-house.,Unknown,Unknown,Yes,No,No,Yes,IMPACII ,Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Center for Scientific Review  General Support System (CSR GSS) ,Less than 6 months,Yes,No,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"This tool helps to determine whether grant applications for NIH Director's Transformative Research Awards are properly anonymized by quickly detecting information that might identify the applicant in the Specific Aims or Research Strategy sections. 

This tool automatically screens many pages and highlights any text that might allow identification of the applicants. The highlighted text is then examined by subject matter experts. This streamlines content screening and aids both NIH staff and the external scientific community in making more rapid and efficient determinations about whether anonymity has been compromised. . Input: PDF files including text and images from two sections of the NIH grant applications.

Output: Spreadsheets containing records of identified text/phrases as well as PDF files with text/phrases highlighted.",this tool helps to determine whether grant applications for nih director's transformative research awards are properly anonymized by quickly detecting information that might identify the applicant in the specific aims or research strategy sections. this tool automatically screens many pages and highlights any text that might allow identification of the applicants. the highlighted text is then examined by subject matter experts. this streamlines content screening and aids both nih staff and the external scientific community in making more rapid and efficient determinations about whether anonymity has been compromised. . input: pdf files including text and images from two sections of the nih grant applications. output: spreadsheets containing records of identified text/phrases as well as pdf files with text/phrases highlighted.
Informal generative AI research for OIDP's HIV.gov,Department of Health and Human Services,HHS,OASH,Education & Workforce,None of the above.,"OIDP's HIV.gov monitors (1) new and emerging AI tools/policies, generative AI use in the field of communications, and (2) the impact of generative AI on HIV federal content delivery through major search engines. This work is done by an ORISE fellow in conjunction with the HIV.gov communications team. The information is synthesized and discussed during a weekly communications meeting to determine any possible application or if additional follow-up is needed. The purpose of this activity is to determine potential tools that we may be able to use to extend the reach of our work. For example, we have been monitoring tools that potentially can synthesize weekly content into a short narrated video.",This practice helps us guide our planning.,Implementation and Assessment,Neither,11/1/2022,11/1/2022,Unknown,Developed with both contracting and in-house resources.,Unknown,Unknown,No,No,No,No,Unknown,Documentation has been partially completed,No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"OIDP's HIV.gov monitors (1) new and emerging AI tools/policies, generative AI use in the field of communications, and (2) the impact of generative AI on HIV federal content delivery through major search engines. This work is done by an ORISE fellow in conjunction with the HIV.gov communications team. The information is synthesized and discussed during a weekly communications meeting to determine any possible application or if additional follow-up is needed. The purpose of this activity is to determine potential tools that we may be able to use to extend the reach of our work. For example, we have been monitoring tools that potentially can synthesize weekly content into a short narrated video. . This practice helps us guide our planning.","oidp's hiv.gov monitors (1) new and emerging ai tools/policies, generative ai use in the field of communications, and (2) the impact of generative ai on hiv federal content delivery through major search engines. this work is done by an orise fellow in conjunction with the hiv.gov communications team. the information is synthesized and discussed during a weekly communications meeting to determine any possible application or if additional follow-up is needed. the purpose of this activity is to determine potential tools that we may be able to use to extend the reach of our work. for example, we have been monitoring tools that potentially can synthesize weekly content into a short narrated video. . this practice helps us guide our planning."
Presenting/providing technical assistance on HIV.gov's cautious & transparent use of generative AI,Department of Health and Human Services,HHS,OASH,Education & Workforce,None of the above.,"OIDP's HIV.gov hosts generative AI workshops, 1-on-1 trainings, and limited generative AI labs to share our cautious and transparent use of generative AI to create communications content. These activities also include information on generative AI basics and information on federal guidance. Our limited AI-generated content is disclosed with a watermark or a tag saying, ""Created by AI, Reviewed By Humans."" These trainings have occurred at 5 national HIV-related conferences and include multiple 1-1 training sessions and workshops that reinforce the importance of following best practices, being transparent, and being cautious. We have trained over 500 people on the cautious and transparent use of generative AI.","These offerings help our audiences understand the importance of cautious and transparent use, federal resources, and additionally promote information about our office's HIV programs.",Operation and Maintenance,Neither,3/1/2023,3/1/2023,3/1/2023,Developed with both contracting and in-house resources.,Unknown,Unknown,No,No,No,No,Unknown,Documentation is widely available,No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"OIDP's HIV.gov hosts generative AI workshops, 1-on-1 trainings, and limited generative AI labs to share our cautious and transparent use of generative AI to create communications content. These activities also include information on generative AI basics and information on federal guidance. Our limited AI-generated content is disclosed with a watermark or a tag saying, ""Created by AI, Reviewed By Humans."" These trainings have occurred at 5 national HIV-related conferences and include multiple 1-1 training sessions and workshops that reinforce the importance of following best practices, being transparent, and being cautious. We have trained over 500 people on the cautious and transparent use of generative AI. . These offerings help our audiences understand the importance of cautious and transparent use, federal resources, and additionally promote information about our office's HIV programs.","oidp's hiv.gov hosts generative ai workshops, 1-on-1 trainings, and limited generative ai labs to share our cautious and transparent use of generative ai to create communications content. these activities also include information on generative ai basics and information on federal guidance. our limited ai-generated content is disclosed with a watermark or a tag saying, ""created by ai, reviewed by humans."" these trainings have occurred at 5 national hiv-related conferences and include multiple 1-1 training sessions and workshops that reinforce the importance of following best practices, being transparent, and being cautious. we have trained over 500 people on the cautious and transparent use of generative ai. . these offerings help our audiences understand the importance of cautious and transparent use, federal resources, and additionally promote information about our office's hiv programs."
Utilizing HHSGPT Pilot for writing/editing ,Department of Health and Human Services,HHS,OASH,Mission-Enabling (internal agency support),None of the above.,"Limited OIDP staff have been selected to have access to HHSGPT, an internal generative AI tool in its pilot phase. Team members have been using it for editing, brainstorming, idea generating, drafting text, revisions, gap analysis, organization/structure, evaluations, etc. HHSGPT has been used to assess national strategies, such as the 2026-2030 vaccine national strategy, the 2026-2030 viral hepatitis strategy, and others. When utilizing HHSGPT, we work to report back how this use has reduced labor burden and note any interesting findings when using the tool. Use reporting is done on an Excel file. ","HHSGPT uses a closed tool to generate suggestions, responses, edits, and information that may be useful for OIDP tasks related to infectious diseases or HIV information/policies. Team members have been using it for editing, brainstorming, idea generating, drafting text, revisions, gap analysis, organization/structure, evaluations, etc.  ",Operation and Maintenance,Neither,10/1/2023,10/1/2023,10/1/2023,Developed in-house.,Unknown,Unknown,No,No,No,No,HHS,Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",Yes,Not Reported,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Limited OIDP staff have been selected to have access to HHSGPT, an internal generative AI tool in its pilot phase. Team members have been using it for editing, brainstorming, idea generating, drafting text, revisions, gap analysis, organization/structure, evaluations, etc. HHSGPT has been used to assess national strategies, such as the 2026-2030 vaccine national strategy, the 2026-2030 viral hepatitis strategy, and others. When utilizing HHSGPT, we work to report back how this use has reduced labor burden and note any interesting findings when using the tool. Use reporting is done on an Excel file. . HHSGPT uses a closed tool to generate suggestions, responses, edits, and information that may be useful for OIDP tasks related to infectious diseases or HIV information/policies. Team members have been using it for editing, brainstorming, idea generating, drafting text, revisions, gap analysis, organization/structure, evaluations, etc.","limited oidp staff have been selected to have access to hhsgpt, an internal generative ai tool in its pilot phase. team members have been using it for editing, brainstorming, idea generating, drafting text, revisions, gap analysis, organization/structure, evaluations, etc. hhsgpt has been used to assess national strategies, such as the 2026-2030 vaccine national strategy, the 2026-2030 viral hepatitis strategy, and others. when utilizing hhsgpt, we work to report back how this use has reduced labor burden and note any interesting findings when using the tool. use reporting is done on an excel file. . hhsgpt uses a closed tool to generate suggestions, responses, edits, and information that may be useful for oidp tasks related to infectious diseases or hiv information/policies. team members have been using it for editing, brainstorming, idea generating, drafting text, revisions, gap analysis, organization/structure, evaluations, etc."
HHSGPT,Department of Health and Human Services,HHS,OCIO,Government Services (includes Benefits and Service Delivery),None of the above.,"ChatGPT is an AI-powered chatbot developed by OpenAI, and it can be integrated with Azure, Microsoft's cloud computing platform. ChatGPT leverages the power of OpenAI's language model to provide natural language understanding and generation capabilities.

Scalability: As an AI-powered chatbot, ChatGPT can handle multiple conversations simultaneously, scaling up to meet high demand without the need for additional human resources.
Increased Efficiency: ChatGPT can automate repetitive tasks, freeing up human agents to focus on more complex or value-added activities, improving overall efficiency.

Persona Benefits:
Assist data scientists in data exploration, model prototyping, analysis, collaboration, and accessing relevant knowledge, enhancing their productivity and efficiency.

Assist policy analysts by providing instant insights, answering complex policy-related questions, and facilitating interactive discussions, enabling faster and more informed decision-making. 
Assist in writing large policy documents by providing suggestions, generating coherent content, and offering language refinement, streamlining the writing process and ensuring high-quality outputs.","At present, HHSGPT is undergoing a pilot phase throughout the entire HHS organization under an IATT.",Acquisition and/or Development,Neither,9/1/2023,9/1/2023,Unknown,Developed in-house.,Unknown,Unknown,Unknown,Unknown,Unknown,No,Unknown,Documentation has been partially completed,Unknown,Unknown,Yes,HHSGPT,Less than 6 months,Yes,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.38095238095238093,"ChatGPT is an AI-powered chatbot developed by OpenAI, and it can be integrated with Azure, Microsoft's cloud computing platform. ChatGPT leverages the power of OpenAI's language model to provide natural language understanding and generation capabilities.

Scalability: As an AI-powered chatbot, ChatGPT can handle multiple conversations simultaneously, scaling up to meet high demand without the need for additional human resources.
Increased Efficiency: ChatGPT can automate repetitive tasks, freeing up human agents to focus on more complex or value-added activities, improving overall efficiency.

Persona Benefits:
Assist data scientists in data exploration, model prototyping, analysis, collaboration, and accessing relevant knowledge, enhancing their productivity and efficiency.

Assist policy analysts by providing instant insights, answering complex policy-related questions, and facilitating interactive discussions, enabling faster and more informed decision-making. 
Assist in writing large policy documents by providing suggestions, generating coherent content, and offering language refinement, streamlining the writing process and ensuring high-quality outputs. . At present, HHSGPT is undergoing a pilot phase throughout the entire HHS organization under an IATT.","chatgpt is an ai-powered chatbot developed by openai, and it can be integrated with azure, microsoft's cloud computing platform. chatgpt leverages the power of openai's language model to provide natural language understanding and generation capabilities. scalability: as an ai-powered chatbot, chatgpt can handle multiple conversations simultaneously, scaling up to meet high demand without the need for additional human resources. increased efficiency: chatgpt can automate repetitive tasks, freeing up human agents to focus on more complex or value-added activities, improving overall efficiency. persona benefits: assist data scientists in data exploration, model prototyping, analysis, collaboration, and accessing relevant knowledge, enhancing their productivity and efficiency. assist policy analysts by providing instant insights, answering complex policy-related questions, and facilitating interactive discussions, enabling faster and more informed decision-making. assist in writing large policy documents by providing suggestions, generating coherent content, and offering language refinement, streamlining the writing process and ensuring high-quality outputs. . at present, hhsgpt is undergoing a pilot phase throughout the entire hhs organization under an iatt."
Counterparty Risk Anomaly Detection ,Department of Housing and Urban Development,HUD,Ginnie Mae ,Mission-Enabling,None of the above.,"Ginnie Mae is responsible for analyzing counterparty risk profiles of mortgage issuers who participate in Ginnie Mae’s program. Ginnie Mae analyzes data from multiple sources to identify potential risks and areas of focus. To enhance the identification of data patterns, Ginnie Mae uses machine learning algorithms, specifically clustering and genetic techniques. These algorithms detect potential risk areas, enabling a focused approach to subsequent analysis by Ginnie Mae staff. ",Data patterns ,Operation and Maintenance,Neither,8/1/2020,8/1/2020,5/1/2022,Developed with contracting resources.,GSQ1117BJ0043 ,No,No,No,Yes,Yes,"This is not a self-learning system.  Although the data is not used to train, fine-tune, and/or evaluate performance, the following sources are used by the ML model to perform analysis: 
- GNMA Investor Reported MBS portfolio data aggregated on an issuer level 
- MBFRF (Mortgage Banking Financial Reporting Form) Data for counterparty financial information ","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Ginnie Mae Reporting and Feedback System (RFS) ,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.","No - Use Case is not Rights or Safety impacting, section 5 does not apply",Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"Ginnie Mae is responsible for analyzing counterparty risk profiles of mortgage issuers who participate in Ginnie Mae’s program. Ginnie Mae analyzes data from multiple sources to identify potential risks and areas of focus. To enhance the identification of data patterns, Ginnie Mae uses machine learning algorithms, specifically clustering and genetic techniques. These algorithms detect potential risk areas, enabling a focused approach to subsequent analysis by Ginnie Mae staff. . Data patterns","ginnie mae is responsible for analyzing counterparty risk profiles of mortgage issuers who participate in ginnie mae’s program. ginnie mae analyzes data from multiple sources to identify potential risks and areas of focus. to enhance the identification of data patterns, ginnie mae uses machine learning algorithms, specifically clustering and genetic techniques. these algorithms detect potential risk areas, enabling a focused approach to subsequent analysis by ginnie mae staff. . data patterns"
Subledger Data Quality Machine Learning,Department of Housing and Urban Development,HUD,Ginnie Mae ,Mission-Enabling,None of the above.,"Ginnie Mae analyzes Master Sub-Servicer (MSS) transaction data on a monthly cadence. Through its use of machine learning models, Ginnie Mae has enhanced its ability to identify data inconsistencies and exceptions associated with its MSS transaction data. By automating the analysis of a significant number of MSS transactions, the machine learning algorithms enhance the efficiency and accuracy of key reporting processes. ",The AI solution identifies data anomalies. ,Operation and Maintenance,Neither,11/20/2020,11/20/2020,4/21/2021,Developed with contracting resources.,GS11Q16BJC0005 ,No,No,No,Yes,No,"This is not a self-learning or self-refining system.  Although the data is not used to train, fine-tune, and/or evaluate performance, the following sources are used:  
- Transaction data from Ginnie Mae Master Sub-Servicers for the defaulted Single-Family non-pooled assets (this data does not include any PII). ","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Ginnie Mae Financial Accounting System  ,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.","No - Use Case is not Rights or Safety impacting, section 5 does not apply",Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"Ginnie Mae analyzes Master Sub-Servicer (MSS) transaction data on a monthly cadence. Through its use of machine learning models, Ginnie Mae has enhanced its ability to identify data inconsistencies and exceptions associated with its MSS transaction data. By automating the analysis of a significant number of MSS transactions, the machine learning algorithms enhance the efficiency and accuracy of key reporting processes. . The AI solution identifies data anomalies.","ginnie mae analyzes master sub-servicer (mss) transaction data on a monthly cadence. through its use of machine learning models, ginnie mae has enhanced its ability to identify data inconsistencies and exceptions associated with its mss transaction data. by automating the analysis of a significant number of mss transactions, the machine learning algorithms enhance the efficiency and accuracy of key reporting processes. . the ai solution identifies data anomalies."
Automating Draft Counterparty Credit Narrative Reports,Department of Housing and Urban Development,HUD,Ginnie Mae ,Mission-Enabling,None of the above.,"Ginnie Mae performs counterparty credit reviews of mortgage issuers who participate in Ginnie Mae’s program. The reviews are based on written analysis and financial data. As an initial step in counterparty credit reviews, Ginnie Mae uses Natural Language Generation (NLG) to implement coded rules and generate draft narratives. This application of AI enables processing efficiency and reduces errors that can occur in a manual process. ",Draft memos/reports ,Operation and Maintenance,Neither,2/21/2021,3/1/2021,4/1/2022,Developed with contracting resources.,47QFDA19A0008 ,No,No,No,Yes,Yes,"This is not a self-learning or self-refining system.  The reports are generated from business rules and format established by Ginnie Mae staff.  Although the data is not used to train, fine-tune, and/or evaluate performance, the following sources are used:  
-GNMA Investor Reported MBS portfolio data aggregated on an issuer level 
- MBFRF (Mortgage Banking Financial Reporting Form) Data for counterparty financial information ","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Ginnie Mae Reporting and Feedback System (RFS) ,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.","No - Use Case is not Rights or Safety impacting, section 5 does not apply",Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"Ginnie Mae performs counterparty credit reviews of mortgage issuers who participate in Ginnie Mae’s program. The reviews are based on written analysis and financial data. As an initial step in counterparty credit reviews, Ginnie Mae uses Natural Language Generation (NLG) to implement coded rules and generate draft narratives. This application of AI enables processing efficiency and reduces errors that can occur in a manual process. . Draft memos/reports","ginnie mae performs counterparty credit reviews of mortgage issuers who participate in ginnie mae’s program. the reviews are based on written analysis and financial data. as an initial step in counterparty credit reviews, ginnie mae uses natural language generation (nlg) to implement coded rules and generate draft narratives. this application of ai enables processing efficiency and reduces errors that can occur in a manual process. . draft memos/reports"
Voice of the Customer,Department of Housing and Urban Development,HUD,Office of the Chief Financial Officer (OCFO),Mission-Enabling,None of the above.,"The Office of the Chief Financial Officer, Customer Experience Team works across the department to develop a deep understanding of who our customers are and how we can best serve their needs. The Voice of the Customer application brings the best-in-class voice transcription, speech, and text analytics to customer feedback surveys,  contact center calls and chats. It allows HUD to unlock critical insights that provide deeper understanding of how to support and manage our program/service delivery and contact center providers, and ultimately to improve customers’ experiences.","A data analytics dashboard that transforms customer feedback into succinct, easy-to-understand metrics, such as customer satisfaction rates across different customer interaction and call center topics.",Operation and Maintenance,Neither,1/15/2024,9/30/2022,8/30/2024,Developed with contracting resources.,86614922C00009,No,No,Yes,Yes,Other,"This system allows HUD to build surveys of varying complexity, distribute surveys via different delivery methods, centrally manage customer feedback data, and enable enterprise-wide qualitative and quantitative data analysis on both data collected through the system and data imported into the system.  The Voice of the Customer data can come in through a variety of ways. It can be a web/paper survey, a phone call, an email, or anything having all that separate data on one platform will help OCFO’s Customer Experience team hear what customers have to say holistically. The gathered feedback is both customer and employee experience data as it pertains to their interactions with HUD. Contact information will be requested, but is voluntary/optional, and is only collected/used for future research. This would include gathering a person’s name and contact information they are willing to provide (email or phone number). This data would be used for research purposes only to further investigate how to improve customers’ experiences with HUD.  These interactions would include moments when the customer/employee interacts with HUD (For example, a customer visits the HUD website for information, or an employee visits the intranet for information). The data would be collected via surveys or conversations regarding their experience within that specific moment and/or overall experience.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,Yes,P351 Voice of the Customer (VoC),Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.","No - Use Case is not Rights or Safety impacting, section 5 does not apply",Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"The Office of the Chief Financial Officer, Customer Experience Team works across the department to develop a deep understanding of who our customers are and how we can best serve their needs. The Voice of the Customer application brings the best-in-class voice transcription, speech, and text analytics to customer feedback surveys,  contact center calls and chats. It allows HUD to unlock critical insights that provide deeper understanding of how to support and manage our program/service delivery and contact center providers, and ultimately to improve customers’ experiences. . A data analytics dashboard that transforms customer feedback into succinct, easy-to-understand metrics, such as customer satisfaction rates across different customer interaction and call center topics.","the office of the chief financial officer, customer experience team works across the department to develop a deep understanding of who our customers are and how we can best serve their needs. the voice of the customer application brings the best-in-class voice transcription, speech, and text analytics to customer feedback surveys, contact center calls and chats. it allows hud to unlock critical insights that provide deeper understanding of how to support and manage our program/service delivery and contact center providers, and ultimately to improve customers’ experiences. . a data analytics dashboard that transforms customer feedback into succinct, easy-to-understand metrics, such as customer satisfaction rates across different customer interaction and call center topics."
Quantitative Text Analysis,Department of Housing and Urban Development,HUD,Office of the Chief Financial Officer (OCFO),Mission-Enabling,Summarizing the key points of a lengthy report using AI.,"Analysis of large amounts of text for patterns (commonly appearing words, common word clusters, etc.)","The outputs are charts and graphs showing which words appear most commonly across the documents, which words appear clustered together most often, and the sentiment of the words, etc. ",Retired,Neither,11/1/2023,11/1/2023,12/1/2023,Developed with contracting resources.,"GS-00F-275CA-86615121A00004, TO 86615122F0020",No,No,No,Yes,No,Unknown,Unknown,No,Unknown,Unknown,Leveraged Audient Group contract service to produce output,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,"No - Use Case is not Rights or Safety impacting, section 5 does not apply",Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"Analysis of large amounts of text for patterns (commonly appearing words, common word clusters, etc.) . The outputs are charts and graphs showing which words appear most commonly across the documents, which words appear clustered together most often, and the sentiment of the words, etc.","analysis of large amounts of text for patterns (commonly appearing words, common word clusters, etc.) . the outputs are charts and graphs showing which words appear most commonly across the documents, which words appear clustered together most often, and the sentiment of the words, etc."
Translation of Digital Media,Department of Housing and Urban Development,HUD,Office of Public Affairs (OPA),Mission-Enabling,None of the above.,"OPA is using Google Translate to translate information on public-facing websites. On every HUD.gov webpage, there is an “Español” button that calls Google Translate to translate the page into Spanish. From there, a Google Translate banner appears on the page allowing for the translation into any available language. This is a free, off-the-shelf solution with no customization. It is also an interim solution to improve language access at HUD while a permanent solution is being developed in line with HUD’s Language Access Plan in response to Executive Order 13166. ",Translated content on HUD.gov,Operation and Maintenance,Both,5/1/2024,5/1/2024,5/1/2024,Developed in-house.,Unknown,No,Yes,No,Yes,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Google Translate,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Yes,"The Office of Public Affairs completed an AI Impact Assessment and identified the following risks for machine translation of digital media:

- There are known challenges with AI translation when it comes to fully understanding cultural backgrounds and nuances (for example, translating idioms, proverbs, humor, colloquialisms, slang, and sarcasm) and this can result in awkward translations. 
- Text submitted to the Google Translate API is sent to Google's servers for processing.
- Initially there was data to suggest that there was gender bias evident in translations, specifically by mostly defaulting to male translations.",Agency CAIO has waived this minimum practice and reported such waiver to OMB.,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,"Other – Immediate human intervention is not practicable; however, an appropriate fail-safe has been implemented. ",Agency CAIO has waived this minimum practice and reported such waiver to OMB.,Agency CAIO has waived this minimum practice and reported such waiver to OMB.,Agency CAIO has waived this minimum practice and reported such waiver to OMB.,Both,0.746031746031746,"OPA is using Google Translate to translate information on public-facing websites. On every HUD.gov webpage, there is an “Español” button that calls Google Translate to translate the page into Spanish. From there, a Google Translate banner appears on the page allowing for the translation into any available language. This is a free, off-the-shelf solution with no customization. It is also an interim solution to improve language access at HUD while a permanent solution is being developed in line with HUD’s Language Access Plan in response to Executive Order 13166. . Translated content on HUD.gov . The Office of Public Affairs completed an AI Impact Assessment and identified the following risks for machine translation of digital media:

- There are known challenges with AI translation when it comes to fully understanding cultural backgrounds and nuances (for example, translating idioms, proverbs, humor, colloquialisms, slang, and sarcasm) and this can result in awkward translations. 
- Text submitted to the Google Translate API is sent to Google's servers for processing.
- Initially there was data to suggest that there was gender bias evident in translations, specifically by mostly defaulting to male translations.","opa is using google translate to translate information on public-facing websites. on every hud.gov webpage, there is an “español” button that calls google translate to translate the page into spanish. from there, a google translate banner appears on the page allowing for the translation into any available language. this is a free, off-the-shelf solution with no customization. it is also an interim solution to improve language access at hud while a permanent solution is being developed in line with hud’s language access plan in response to executive order 13166. . translated content on hud.gov . the office of public affairs completed an ai impact assessment and identified the following risks for machine translation of digital media: - there are known challenges with ai translation when it comes to fully understanding cultural backgrounds and nuances (for example, translating idioms, proverbs, humor, colloquialisms, slang, and sarcasm) and this can result in awkward translations. - text submitted to the google translate api is sent to google's servers for processing. - initially there was data to suggest that there was gender bias evident in translations, specifically by mostly defaulting to male translations."
NextGen Advanced Methods: Air Traffic Control System Command Center (ATCSCC) Webinar Speech2Text and Analysis,National Aeronautics and Space Administration,NASA,Ames Research Center,Natural Language Processing,None of the above.,"The Advanced Methods project explores the use of innovative and emerging technologies to drive post operational analysis of Traffic Management for aircraft. Technologies such as machine learning, (ML), artificial intelligence (AI), and advanced data analytics for use in improving the FAA's traffic flow management. In this specific use case, our aim is to use deep learning to convert live ATCSCC webinar meeting conversation to text, and then apply natural language processing to the converted text data for later analysis and review.",Text from speech to text and NLP of air traffic management content.,In production,Neither,1/1/2021,1/1/2021,Unknown,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Yes,Unknown,Yes,Yes,Yes,Operated in an approved enclave,ATCSCC,Less than 6 months,No,Yes,Unknown,This project leveraged external NLP work,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The Advanced Methods project explores the use of innovative and emerging technologies to drive post operational analysis of Traffic Management for aircraft. Technologies such as machine learning, (ML), artificial intelligence (AI), and advanced data analytics for use in improving the FAA's traffic flow management. In this specific use case, our aim is to use deep learning to convert live ATCSCC webinar meeting conversation to text, and then apply natural language processing to the converted text data for later analysis and review. . Text from speech to text and NLP of air traffic management content.","the advanced methods project explores the use of innovative and emerging technologies to drive post operational analysis of traffic management for aircraft. technologies such as machine learning, (ml), artificial intelligence (ai), and advanced data analytics for use in improving the faa's traffic flow management. in this specific use case, our aim is to use deep learning to convert live atcscc webinar meeting conversation to text, and then apply natural language processing to the converted text data for later analysis and review. . text from speech to text and nlp of air traffic management content."
NextGen Data Analytics: Letters of Agreement,National Aeronautics and Space Administration,NASA,Ames Research Center,Natural Language Processing,None of the above.,"Today, operation constraints are documented via Standard Operating Procedures (SOP) and Letters of Agreement (LOA) and are not made available to the public in a consistent manner. SOPs are specific to an air traffic control facility and specify the procedures necessary for safe operation in the sector. LOA outline agreements establishing procedures and responsibilities between two parties (including crossing restrictions, holding patters, emergency procedure coordination, etc.) The LOA/SOPs are published internally as scanned PDFs and are the responsibility of the facility to maintain. To reduce the manual effort of tagging the documents for ease of reference, there is an opportunity to use modern data analytics and machine learning to produce and disseminate constraints in a standardized manner. 
Providing LOAs or SOPs to stakeholders will enable flight planners (pilots and vendors) to study or ingest this information and thereby plan flight trajectories that remain consistent with air traffic constraints. It is also fundamental to Next Gen capabilities to share accurate data for purposes of creating new noise abatement procedures; improve NAS information for common situational awareness and alignment to implement new tools to assist in future time-based flow management.",Test-based digitized SOPs and LOAs.,In production,Neither,1/1/2021,1/1/2021,Unknown,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Yes,Letter of agreement dataset from FAA,Yes,Yes,Yes,Operated in an approved enclave,Checking,Less than 6 months,No,Yes,Unknown,This project used external text mining capabilities,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Today, operation constraints are documented via Standard Operating Procedures (SOP) and Letters of Agreement (LOA) and are not made available to the public in a consistent manner. SOPs are specific to an air traffic control facility and specify the procedures necessary for safe operation in the sector. LOA outline agreements establishing procedures and responsibilities between two parties (including crossing restrictions, holding patters, emergency procedure coordination, etc.) The LOA/SOPs are published internally as scanned PDFs and are the responsibility of the facility to maintain. To reduce the manual effort of tagging the documents for ease of reference, there is an opportunity to use modern data analytics and machine learning to produce and disseminate constraints in a standardized manner. 
Providing LOAs or SOPs to stakeholders will enable flight planners (pilots and vendors) to study or ingest this information and thereby plan flight trajectories that remain consistent with air traffic constraints. It is also fundamental to Next Gen capabilities to share accurate data for purposes of creating new noise abatement procedures; improve NAS information for common situational awareness and alignment to implement new tools to assist in future time-based flow management. . Test-based digitized SOPs and LOAs.","today, operation constraints are documented via standard operating procedures (sop) and letters of agreement (loa) and are not made available to the public in a consistent manner. sops are specific to an air traffic control facility and specify the procedures necessary for safe operation in the sector. loa outline agreements establishing procedures and responsibilities between two parties (including crossing restrictions, holding patters, emergency procedure coordination, etc.) the loa/sops are published internally as scanned pdfs and are the responsibility of the facility to maintain. to reduce the manual effort of tagging the documents for ease of reference, there is an opportunity to use modern data analytics and machine learning to produce and disseminate constraints in a standardized manner. providing loas or sops to stakeholders will enable flight planners (pilots and vendors) to study or ingest this information and thereby plan flight trajectories that remain consistent with air traffic constraints. it is also fundamental to next gen capabilities to share accurate data for purposes of creating new noise abatement procedures; improve nas information for common situational awareness and alignment to implement new tools to assist in future time-based flow management. . test-based digitized sops and loas."
AEGIS: Autonomous Exploration for Gathering Increased Science,National Aeronautics and Space Administration,NASA,Jet Propulsion Laboratory,Deep Learning,None of the above.,"AEGIS enables intelligent targeting and data acquisition by planetary rovers. It uses computer vision techniques to identify targets (e.g., rocks) in wide angles images of the rover's surrounding terrain. If targets are found that match scientists specificiations, they are then measured autonomously using remote sensing instruments. AEGIS was first used on the MER Mission. It is currently in use on the MSL Mission to acquire data for the ChemCam instrument. It is planned for use in Spring of 2022 on the M2020 Mission to acquire data for the SuperCam instrument.","Recommendations of relevant objects, e.g., Mars rocks, for scientific examination.",In production,Neither,1/1/2006,1/1/2007,1/1/2010,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Unknown,public Mars science data related to sensors for optimizing scientific measurements,Unknown,Yes,Yes,No.  Engineering Review and Resease instead.,Mars2020 Rover,Unknown,Unknown,Unknown,Unknown,Mars 2020 team built on prior rovers and shared across different facets of Mars 2020.,Yes.  Peer review,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"AEGIS enables intelligent targeting and data acquisition by planetary rovers. It uses computer vision techniques to identify targets (e.g., rocks) in wide angles images of the rover's surrounding terrain. If targets are found that match scientists specificiations, they are then measured autonomously using remote sensing instruments. AEGIS was first used on the MER Mission. It is currently in use on the MSL Mission to acquire data for the ChemCam instrument. It is planned for use in Spring of 2022 on the M2020 Mission to acquire data for the SuperCam instrument. . Recommendations of relevant objects, e.g., Mars rocks, for scientific examination.","aegis enables intelligent targeting and data acquisition by planetary rovers. it uses computer vision techniques to identify targets (e.g., rocks) in wide angles images of the rover's surrounding terrain. if targets are found that match scientists specificiations, they are then measured autonomously using remote sensing instruments. aegis was first used on the mer mission. it is currently in use on the msl mission to acquire data for the chemcam instrument. it is planned for use in spring of 2022 on the m2020 mission to acquire data for the supercam instrument. . recommendations of relevant objects, e.g., mars rocks, for scientific examination."
ASPEN Mission Planner,National Aeronautics and Space Administration,NASA,Jet Propulsion Laboratory,Statistical Methods,None of the above.,"Based on AI techniques, ASPEN is a modular, reconfigurable application framework which is capable of supporting a wide variety of planning and scheduling applications. ASPEN provides a set of reusable software components that implement the elements commonly found in complex planning/scheduling systems, including: an expressive modeling language, a resource management system, a temporal reasoning system, and a graphical interface.  ASPEN has been used for many space missions including: Modified Antarctic Mapping Mission, Orbital Express, Earth Observing One, and ESA's Rosetta Orbitter.",Plan and schedule recommendations to optimize Science from Scientific Sensors,In mission,Neither,1/1/1997,1/1/1998,1/1/2002,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Yes,"Data related to optimizing sensors and flight hardware.  Note This is a mission-embedded software component subject to system engineering and mission planning reviews, not the IT ATO process.  PII, SAOP, HISP, etc. do not apply.",Yes,Yes,Yes,No.  Engineering Review and Resease instead.,Multiple approved space-based systems,Unknown,No,Unknown,Unknown,This capability has been re-used among many missions,Yes.  Peer review,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Based on AI techniques, ASPEN is a modular, reconfigurable application framework which is capable of supporting a wide variety of planning and scheduling applications. ASPEN provides a set of reusable software components that implement the elements commonly found in complex planning/scheduling systems, including: an expressive modeling language, a resource management system, a temporal reasoning system, and a graphical interface.  ASPEN has been used for many space missions including: Modified Antarctic Mapping Mission, Orbital Express, Earth Observing One, and ESA's Rosetta Orbitter. . Plan and schedule recommendations to optimize Science from Scientific Sensors","based on ai techniques, aspen is a modular, reconfigurable application framework which is capable of supporting a wide variety of planning and scheduling applications. aspen provides a set of reusable software components that implement the elements commonly found in complex planning/scheduling systems, including: an expressive modeling language, a resource management system, a temporal reasoning system, and a graphical interface. aspen has been used for many space missions including: modified antarctic mapping mission, orbital express, earth observing one, and esa's rosetta orbitter. . plan and schedule recommendations to optimize science from scientific sensors"
CLASP Coverage Planning & Scheduling,National Aeronautics and Space Administration,NASA,Jet Propulsion Laboratory,Statistical Methods,None of the above.,"The Compressed Large-scale Activity Scheduling and Planning (CLASP) project is a long-range scheduler for space-based or aerial instruments that can be modeled as pushbrooms 1D line sensors dragged across the surface of the body being observed. It addresses the problem of choosing the orientation and on/off times of a pushbroom instrument or collection of pushbroom instruments such that the schedule covers as many target points as possible, but without oversubscribing memory and energy. Orientation and time of observation is derived from geometric computations that CLASP performs using the SPICE ephemeris toolkit.  CLASP allows mission planning teams to start with a baseline mission concept and simulate the mission's science return using models of science observations, spacecraft operations, downlink, and spacecraft trajectory. This analysis can then be folded back into many aspects of mission design -- including trajectory, spacecraft design, operations concept, and downlink concept. The long planning horizons allow this analysis to span an entire mission.  Actively in use for optimized scheduling for the NISAR Mission, ECOSTRESS mission (study of water needs for plant areas), EMIT mission (mineralogy of arid dusty regions), OCO-3 (atmospheric CO2) and more as well as used for numerous missions analysis and studies (e.g. 100+).","Estimates of scientific mission outcomes / results, based on optimized scheduling of spacecraft and sensors.",In mission,Neither,1/1/2006,1/1/2006,1/1/2018,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Yes,Mission planning and execution data from multiple missions.,Yes,Yes,Yes,No.  Engineering Review and Resease instead.,Deployed on multiple remote sensing spacecraft,Unknown,No,Unknown,Unknown,This capability has been re-used among many missions,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The Compressed Large-scale Activity Scheduling and Planning (CLASP) project is a long-range scheduler for space-based or aerial instruments that can be modeled as pushbrooms 1D line sensors dragged across the surface of the body being observed. It addresses the problem of choosing the orientation and on/off times of a pushbroom instrument or collection of pushbroom instruments such that the schedule covers as many target points as possible, but without oversubscribing memory and energy. Orientation and time of observation is derived from geometric computations that CLASP performs using the SPICE ephemeris toolkit.  CLASP allows mission planning teams to start with a baseline mission concept and simulate the mission's science return using models of science observations, spacecraft operations, downlink, and spacecraft trajectory. This analysis can then be folded back into many aspects of mission design -- including trajectory, spacecraft design, operations concept, and downlink concept. The long planning horizons allow this analysis to span an entire mission.  Actively in use for optimized scheduling for the NISAR Mission, ECOSTRESS mission (study of water needs for plant areas), EMIT mission (mineralogy of arid dusty regions), OCO-3 (atmospheric CO2) and more as well as used for numerous missions analysis and studies (e.g. 100+). . Estimates of scientific mission outcomes / results, based on optimized scheduling of spacecraft and sensors.","the compressed large-scale activity scheduling and planning (clasp) project is a long-range scheduler for space-based or aerial instruments that can be modeled as pushbrooms 1d line sensors dragged across the surface of the body being observed. it addresses the problem of choosing the orientation and on/off times of a pushbroom instrument or collection of pushbroom instruments such that the schedule covers as many target points as possible, but without oversubscribing memory and energy. orientation and time of observation is derived from geometric computations that clasp performs using the spice ephemeris toolkit. clasp allows mission planning teams to start with a baseline mission concept and simulate the mission's science return using models of science observations, spacecraft operations, downlink, and spacecraft trajectory. this analysis can then be folded back into many aspects of mission design -- including trajectory, spacecraft design, operations concept, and downlink concept. the long planning horizons allow this analysis to span an entire mission. actively in use for optimized scheduling for the nisar mission, ecostress mission (study of water needs for plant areas), emit mission (mineralogy of arid dusty regions), oco-3 (atmospheric co2) and more as well as used for numerous missions analysis and studies (e.g. 100+). . estimates of scientific mission outcomes / results, based on optimized scheduling of spacecraft and sensors."
Enhanced AutoNav for Perseverance Rover on Mars,National Aeronautics and Space Administration,NASA,Jet Propulsion Laboratory,Classification,None of the above.,"AutoNav on the Perseverance Rover autonomously plans a safe path based on stereo navigation camera images, based on multiple technologies including a tree search for decision making, Dijkstra algorithm for global path planning, stereo processing for 3D terrain reconstruction, and Approximate Clearance Evaluation (ACE) for safety checks. It is deployed on Perserverance rover and being used for autonomous driving on Mars.",Recommended navigation path for Mars Rover.,In production,Neither,1/1/2015,1/1/2018,7/1/2020,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Yes,Sensor and imagery data from prior Mars missions and from terrestrial-based physical simulation sites.,Yes,Yes,Yes,No.  Engineering Review and Resease instead.,Mars2020 Rover,Less than 6 months,No,Unknown,Unknown,Reused selected techniques from prior Mars rovers,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"AutoNav on the Perseverance Rover autonomously plans a safe path based on stereo navigation camera images, based on multiple technologies including a tree search for decision making, Dijkstra algorithm for global path planning, stereo processing for 3D terrain reconstruction, and Approximate Clearance Evaluation (ACE) for safety checks. It is deployed on Perserverance rover and being used for autonomous driving on Mars. . Recommended navigation path for Mars Rover.","autonav on the perseverance rover autonomously plans a safe path based on stereo navigation camera images, based on multiple technologies including a tree search for decision making, dijkstra algorithm for global path planning, stereo processing for 3d terrain reconstruction, and approximate clearance evaluation (ace) for safety checks. it is deployed on perserverance rover and being used for autonomous driving on mars. . recommended navigation path for mars rover."
Mars2020 Rover (Perseverance),National Aeronautics and Space Administration,NASA,Jet Propulsion Laboratory,Other,None of the above.,"Research, experiments, and engineering to empower future rovers with onboard autonomy; planning, scheduling & execution; path planning; onboard science; image processing; terrain classification; fault diagnosis; and location estimation.  This is a multi-faceted effort and includes experimentation and demonstrations on-site at JPL's simulated mars navigation yard.  The M2020 onboard scheduler incrementally constructs a feasible schedule by iterating through activities in priority-first order. When considering each activity it computes the valid time intervals for placement, taking into account preheating, maintenance heating, and wake/sleep of the rover as required. After an activity is placed (other than a preheat/maintenance or wake/sleep), the activity is never reconsidered by the scheduler for deletion or moving. Therefore the scheduler can be considered non backtracking, and only searches in the sense that it computes valid timeline intervals for legal activity placement.  Meta Search: Because the onboard scheduler will be invoked many times in a given sol (Martian Day) with a range of possible contexts (due to execution variations), its non backtracking nature leaves its vulnerable to brittleness. In order to mitigate this potential brittleness, the Copilot systems perform a monte carlo based stochastic analysis to set meta parameters of the scheduler - primarily activity priority but also potentially preferred time and temporal constraints.",Mission-priority-based recommendations for scheduling Mars2020 Rover activities.,In production,Neither,1/1/2015,1/1/2018,7/1/2020,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Yes,Multiple forms of rover mission data,Yes,Yes,Yes,No.  Engineering Review and Resease instead.,Mars2020 Rover,Less than 6 months,No,no,Unknown,Reused selected techniques from prior Mars rovers,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Research, experiments, and engineering to empower future rovers with onboard autonomy; planning, scheduling & execution; path planning; onboard science; image processing; terrain classification; fault diagnosis; and location estimation.  This is a multi-faceted effort and includes experimentation and demonstrations on-site at JPL's simulated mars navigation yard.  The M2020 onboard scheduler incrementally constructs a feasible schedule by iterating through activities in priority-first order. When considering each activity it computes the valid time intervals for placement, taking into account preheating, maintenance heating, and wake/sleep of the rover as required. After an activity is placed (other than a preheat/maintenance or wake/sleep), the activity is never reconsidered by the scheduler for deletion or moving. Therefore the scheduler can be considered non backtracking, and only searches in the sense that it computes valid timeline intervals for legal activity placement.  Meta Search: Because the onboard scheduler will be invoked many times in a given sol (Martian Day) with a range of possible contexts (due to execution variations), its non backtracking nature leaves its vulnerable to brittleness. In order to mitigate this potential brittleness, the Copilot systems perform a monte carlo based stochastic analysis to set meta parameters of the scheduler - primarily activity priority but also potentially preferred time and temporal constraints. . Mission-priority-based recommendations for scheduling Mars2020 Rover activities.","research, experiments, and engineering to empower future rovers with onboard autonomy; planning, scheduling & execution; path planning; onboard science; image processing; terrain classification; fault diagnosis; and location estimation. this is a multi-faceted effort and includes experimentation and demonstrations on-site at jpl's simulated mars navigation yard. the m2020 onboard scheduler incrementally constructs a feasible schedule by iterating through activities in priority-first order. when considering each activity it computes the valid time intervals for placement, taking into account preheating, maintenance heating, and wake/sleep of the rover as required. after an activity is placed (other than a preheat/maintenance or wake/sleep), the activity is never reconsidered by the scheduler for deletion or moving. therefore the scheduler can be considered non backtracking, and only searches in the sense that it computes valid timeline intervals for legal activity placement. meta search: because the onboard scheduler will be invoked many times in a given sol (martian day) with a range of possible contexts (due to execution variations), its non backtracking nature leaves its vulnerable to brittleness. in order to mitigate this potential brittleness, the copilot systems perform a monte carlo based stochastic analysis to set meta parameters of the scheduler - primarily activity priority but also potentially preferred time and temporal constraints. . mission-priority-based recommendations for scheduling mars2020 rover activities."
MLNav (Machine Learning Navigation),National Aeronautics and Space Administration,NASA,Jet Propulsion Laboratory,Deep Learning,None of the above.,"Accelerates path planning of rovers and other types of vehicles through ML-based heuristics, while guaranteeing safety through conventional, model-based collision checking. Integrated with M2020's Enhanced AutoNav (ENav) and tested with the real terrain data from Mars on ENav simulator. Uses U-net for the ML-based heuristics, trained by simulation-generated terrain data.",Path planning recommendations for Mars2020 Rover,In production,Neither,10/1/2019,9/30/2021,7/1/2020,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Yes,Simulated terrain data for Mars,Yes,Yes,Yes,No.  Engineering Review and Resease instead.,Mars2020 Rover,Less than 6 months,No,No,Unknown,No,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Accelerates path planning of rovers and other types of vehicles through ML-based heuristics, while guaranteeing safety through conventional, model-based collision checking. Integrated with M2020's Enhanced AutoNav (ENav) and tested with the real terrain data from Mars on ENav simulator. Uses U-net for the ML-based heuristics, trained by simulation-generated terrain data. . Path planning recommendations for Mars2020 Rover","accelerates path planning of rovers and other types of vehicles through ml-based heuristics, while guaranteeing safety through conventional, model-based collision checking. integrated with m2020's enhanced autonav (enav) and tested with the real terrain data from mars on enav simulator. uses u-net for the ml-based heuristics, trained by simulation-generated terrain data. . path planning recommendations for mars2020 rover"
Perseverance Rover on Mars - Terrain Relative Navigation,National Aeronautics and Space Administration,NASA,Jet Propulsion Laboratory,Deep Learning,None of the above.,"3D machine vision via dual cameras to inform convolutional neural networks for rover navigation path planning.  Physics / momentum - based Terrain Relative Navigation (TRN).  Enhanced AutoNavigation (Enav) plans feasible paths using multiple techniques, to include random forests.  Approximate Clearance Evaluation (ACE) assesses obstacles to deterimine if the suspension of the rover can clear them (drive over them) or needs to route around them.  Additional, more complex AIML techniques are being planned and tested for future rovers at JPL.  TRN was also used to provide precision landing for the Perseverance Rover in the entry, descent, and landing process to get the rover to the surface of Mars.",Real-time terrain-relative navigation recommendations.,In production,Neither,1/1/2004,10/1/2014,2/18/2021,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Yes,Simulated terrain data for Mars,Yes,Yes,Yes,No.  Engineering Review and Resease instead.,Mars2020 Rover,Less than 6 months,No,No,Unknown,Built on prior TRN work,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"3D machine vision via dual cameras to inform convolutional neural networks for rover navigation path planning.  Physics / momentum - based Terrain Relative Navigation (TRN).  Enhanced AutoNavigation (Enav) plans feasible paths using multiple techniques, to include random forests.  Approximate Clearance Evaluation (ACE) assesses obstacles to deterimine if the suspension of the rover can clear them (drive over them) or needs to route around them.  Additional, more complex AIML techniques are being planned and tested for future rovers at JPL.  TRN was also used to provide precision landing for the Perseverance Rover in the entry, descent, and landing process to get the rover to the surface of Mars. . Real-time terrain-relative navigation recommendations.","3d machine vision via dual cameras to inform convolutional neural networks for rover navigation path planning. physics / momentum - based terrain relative navigation (trn). enhanced autonavigation (enav) plans feasible paths using multiple techniques, to include random forests. approximate clearance evaluation (ace) assesses obstacles to deterimine if the suspension of the rover can clear them (drive over them) or needs to route around them. additional, more complex aiml techniques are being planned and tested for future rovers at jpl. trn was also used to provide precision landing for the perseverance rover in the entry, descent, and landing process to get the rover to the surface of mars. . real-time terrain-relative navigation recommendations."
SPOC (Soil Property and Object Classification),National Aeronautics and Space Administration,NASA,Jet Propulsion Laboratory,Deep Learning,None of the above.,"Using a convolutional neural network (CNN), SPOC (Soil Property and Object Classification) takes rover images and classifies the terrain type (e.g., sand, soil) from visual appearance. This ability enables rover to drive more safely. It is trained by labeled images from MER (Mars Exploration Rover), MSL (Mars Science Laboratory), and Mars 2020 rovers, annotated by tens of thousands of citizen scientsts through the AI4Mars project. SPOC deployed on MSL's ground operation system and onboard test on M2020 is being considered.  SPOC is one of many inputs to navigation.","Terrain image classifications such as soil, sand, rock, etc.",In production,Neither,1/1/2016,1/1/2022,7/1/2020,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Yes,"As an embedded system on the Mars Rover, ATO, PII, SAOP, HISP, etc. do not apply",Yes,Yes,Yes,No.  Engineering Review and Resease instead.,Multiple approved space-based systems,Less than 1 Year,No,Yes,Unknown,Reused for many scientific missions,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"Using a convolutional neural network (CNN), SPOC (Soil Property and Object Classification) takes rover images and classifies the terrain type (e.g., sand, soil) from visual appearance. This ability enables rover to drive more safely. It is trained by labeled images from MER (Mars Exploration Rover), MSL (Mars Science Laboratory), and Mars 2020 rovers, annotated by tens of thousands of citizen scientsts through the AI4Mars project. SPOC deployed on MSL's ground operation system and onboard test on M2020 is being considered.  SPOC is one of many inputs to navigation. . Terrain image classifications such as soil, sand, rock, etc.","using a convolutional neural network (cnn), spoc (soil property and object classification) takes rover images and classifies the terrain type (e.g., sand, soil) from visual appearance. this ability enables rover to drive more safely. it is trained by labeled images from mer (mars exploration rover), msl (mars science laboratory), and mars 2020 rovers, annotated by tens of thousands of citizen scientsts through the ai4mars project. spoc deployed on msl's ground operation system and onboard test on m2020 is being considered. spoc is one of many inputs to navigation. . terrain image classifications such as soil, sand, rock, etc."
Volcano SensorWeb,National Aeronautics and Space Administration,NASA,Jet Propulsion Laboratory,AIML Platform/Environment,None of the above.,"The Sensor Web Project uses a network of sensors linked by software and the internet to an autonomous satellite observation response capability. This system of systems is designed with a flexible, modular, architecture to facilitate expansion in sensors, customization of trigger conditions, and customization of responses.  This system has been used to implement a global surveillance program to study volcanos. We have also run sensorweb tests to study flooding, cryosphere events, and atmospheric phenomena.  Specifically, in our application, we use low resolution, high coverage sensors to trigger observations by high resolution instruments. Note that there are many other rationales to network sensors into a sensorweb. For example automated response might enable observation using complementary instruments such as imaging radar, infra-red, visible, etc. Or automated response might be used to apply more assets to increase the frequency of observation to improve the temporal resolution of available data.  Our sensorweb project is being used to monitor the Earth's 50 most active volcanos. We have also run sensorweb experiments to monitor flooding, wildfires, and cryospheric events (snowfall and melt, lake freezing and thawing, sea ice formation and breakup.)",Detection and classification of volcano-related features.,In production,Neither,1/1/2003,1/1/2003,1/1/2003,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Yes,Terrestrial and space-based remote sensing satellite images to train the model,Yes,Yes,Yes,No.  Engineering Review and Resease instead.,Deployed on multiple remote sensing spacecraft,Less than 6 months,No,Yes,Unknown,This capability has been re-used among many missions,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The Sensor Web Project uses a network of sensors linked by software and the internet to an autonomous satellite observation response capability. This system of systems is designed with a flexible, modular, architecture to facilitate expansion in sensors, customization of trigger conditions, and customization of responses.  This system has been used to implement a global surveillance program to study volcanos. We have also run sensorweb tests to study flooding, cryosphere events, and atmospheric phenomena.  Specifically, in our application, we use low resolution, high coverage sensors to trigger observations by high resolution instruments. Note that there are many other rationales to network sensors into a sensorweb. For example automated response might enable observation using complementary instruments such as imaging radar, infra-red, visible, etc. Or automated response might be used to apply more assets to increase the frequency of observation to improve the temporal resolution of available data.  Our sensorweb project is being used to monitor the Earth's 50 most active volcanos. We have also run sensorweb experiments to monitor flooding, wildfires, and cryospheric events (snowfall and melt, lake freezing and thawing, sea ice formation and breakup.) . Detection and classification of volcano-related features.","the sensor web project uses a network of sensors linked by software and the internet to an autonomous satellite observation response capability. this system of systems is designed with a flexible, modular, architecture to facilitate expansion in sensors, customization of trigger conditions, and customization of responses. this system has been used to implement a global surveillance program to study volcanos. we have also run sensorweb tests to study flooding, cryosphere events, and atmospheric phenomena. specifically, in our application, we use low resolution, high coverage sensors to trigger observations by high resolution instruments. note that there are many other rationales to network sensors into a sensorweb. for example automated response might enable observation using complementary instruments such as imaging radar, infra-red, visible, etc. or automated response might be used to apply more assets to increase the frequency of observation to improve the temporal resolution of available data. our sensorweb project is being used to monitor the earth's 50 most active volcanos. we have also run sensorweb experiments to monitor flooding, wildfires, and cryospheric events (snowfall and melt, lake freezing and thawing, sea ice formation and breakup.) . detection and classification of volcano-related features."
NASA OCIO STI Concept Tagging Service,National Aeronautics and Space Administration,NASA,Langley Research Center,Natural Language Processing,None of the above.,An API (application program interface) for exposing topic models created with the STI (Scientific & Technical Information) concept training repository.,API standards,In production,Neither,1/1/2021,1/1/2021,Unknown,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Yes,Unknown,Yes,Yes,Yes,No.  Engineering Review and Resease instead.,STI,Less than 6 months,No,Yes,Unknown,This project re-used open source and prior NLP work,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,An API (application program interface) for exposing topic models created with the STI (Scientific & Technical Information) concept training repository. . API standards,an api (application program interface) for exposing topic models created with the sti (scientific & technical information) concept training repository. . api standards
Autonomous WAiting Room Evaluation (AWARE),National Aeronautics and Space Administration,NASA,Langley Research Center,Deep Learning,None of the above.,"Using an existing security camera and YOLO Machine Learning model to detect and count number of people waiting for service at Langley's Badge & Pass Office. When a predetermined threshold of people is exceeded, automated texts and emails are sent to request additional help at the service counters.",Count of current customers in waiting area,In production,Neither,1/1/2023,7/1/2023,Unknown,Developed with both contracting and in-house resources.,Unknown,No,Yes,No,No,No,Unknown,No,Yes,Yes,Yes,AWARE,Less than 1 Year,No,No,Unknown,"This project re-used prior work in identifying vehicles, objects or animals in a NASA environment",Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Using an existing security camera and YOLO Machine Learning model to detect and count number of people waiting for service at Langley's Badge & Pass Office. When a predetermined threshold of people is exceeded, automated texts and emails are sent to request additional help at the service counters. . Count of current customers in waiting area","using an existing security camera and yolo machine learning model to detect and count number of people waiting for service at langley's badge & pass office. when a predetermined threshold of people is exceeded, automated texts and emails are sent to request additional help at the service counters. . count of current customers in waiting area"
Onboard Planner for Mars2020 Rover (Perseverance),National Aeronautics and Space Administration,NASA,Jet Propulsion Laboratory,Other,None of the above.,"The M2020 onboard scheduler incrementally constructs a feasible schedule by iterating through activities in priority-first order. When considering each activity it computes the valid time intervals for placement, taking into account preheating, maintenance heating, and wake/sleep of the rover as required. After an activity is placed (other than a preheat/maintenance or wake/sleep), the activity is never reconsidered by the scheduler for deletion or moving. Therefore the scheduler can be considered non backtracking, and only searches in the sense that it computes valid timeline intervals for legal activity placement.  Meta Search: Because the onboard scheduler will be invoked many times in a given sol (Martian Day) with a range of possible contexts (due to execution variations), its non backtracking nature leaves its vulnerable to brittleness. In order to mitigate this potential brittleness, the Copilot systems perform a monte carlo based stochastic analysis to set meta parameters of the scheduler - primarily activity priority but also potentially preferred time and temporal constraints.      Also: Research, experiments, and engineering to empower future rovers with onboard autonomy; planning, scheduling & execution; path planning; onboard science; image processing; terrain classification; fault diagnosis; and location estimation.  This is a multi-faceted effort and includes experimentation and demonstrations on-site at JPL's simulated mars navigation yard.",Planning and scheduling recommendations,In mission,Neither,1/1/2016,1/1/2018,7/1/2020,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Yes,"As a Mission-embedded system on Mars Rover, ATO, public dissemination, SAOP, HISP, etc. do not apply",Yes,Yes,Yes,No.  Engineering Review and Resease instead.,Multiple approved space-based systems,Less than 6 months,No,No,Unknown,This work built-upon prior rovers,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The M2020 onboard scheduler incrementally constructs a feasible schedule by iterating through activities in priority-first order. When considering each activity it computes the valid time intervals for placement, taking into account preheating, maintenance heating, and wake/sleep of the rover as required. After an activity is placed (other than a preheat/maintenance or wake/sleep), the activity is never reconsidered by the scheduler for deletion or moving. Therefore the scheduler can be considered non backtracking, and only searches in the sense that it computes valid timeline intervals for legal activity placement.  Meta Search: Because the onboard scheduler will be invoked many times in a given sol (Martian Day) with a range of possible contexts (due to execution variations), its non backtracking nature leaves its vulnerable to brittleness. In order to mitigate this potential brittleness, the Copilot systems perform a monte carlo based stochastic analysis to set meta parameters of the scheduler - primarily activity priority but also potentially preferred time and temporal constraints.      Also: Research, experiments, and engineering to empower future rovers with onboard autonomy; planning, scheduling & execution; path planning; onboard science; image processing; terrain classification; fault diagnosis; and location estimation.  This is a multi-faceted effort and includes experimentation and demonstrations on-site at JPL's simulated mars navigation yard. . Planning and scheduling recommendations","the m2020 onboard scheduler incrementally constructs a feasible schedule by iterating through activities in priority-first order. when considering each activity it computes the valid time intervals for placement, taking into account preheating, maintenance heating, and wake/sleep of the rover as required. after an activity is placed (other than a preheat/maintenance or wake/sleep), the activity is never reconsidered by the scheduler for deletion or moving. therefore the scheduler can be considered non backtracking, and only searches in the sense that it computes valid timeline intervals for legal activity placement. meta search: because the onboard scheduler will be invoked many times in a given sol (martian day) with a range of possible contexts (due to execution variations), its non backtracking nature leaves its vulnerable to brittleness. in order to mitigate this potential brittleness, the copilot systems perform a monte carlo based stochastic analysis to set meta parameters of the scheduler - primarily activity priority but also potentially preferred time and temporal constraints. also: research, experiments, and engineering to empower future rovers with onboard autonomy; planning, scheduling & execution; path planning; onboard science; image processing; terrain classification; fault diagnosis; and location estimation. this is a multi-faceted effort and includes experimentation and demonstrations on-site at jpl's simulated mars navigation yard. . planning and scheduling recommendations"
"SensorWeb: Volcano, Flood, Wildfire, and others.",National Aeronautics and Space Administration,NASA,Jet Propulsion Laboratory,Statistical Methods,None of the above.,"The Sensor Web Project uses a network of sensors linked by software and the internet to an autonomous satellite observation response capability. This system of systems is designed with a flexible, modular, architecture to facilitate expansion in sensors, customization of trigger conditions, and customization of responses.  This system has been used to implement a global surveillance program to study volcanos. We have also run sensorweb tests to study flooding, cryosphere events, and atmospheric phenomena.  Specifically, in our application, we use low resolution, high coverage sensors to trigger observations by high resolution instruments. Note that there are many other rationales to network sensors into a sensorweb. For example automated response might enable observation using complementary instruments such as imaging radar, infra-red, visible, etc. Or automated response might be used to apply more assets to increase the frequency of observation to improve the temporal resolution of available data.  Our sensorweb project is being used to monitor the Earth's 50 most active volcanos. We have also run sensorweb experiments to monitor flooding, wildfires, and cryospheric events (snowfall and melt, lake freezing and thawing, sea ice formation and breakup.)","Identification and labelling of terrain, climate and weather features",In mission,Neither,1/1/2003,1/1/2003,1/1/2003,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Yes,Remote sensing data from many scientific instruments,Yes,Yes,Yes,No.  Engineering Review and Resease instead.,Multiple approved space-based systems,Less than 6 months,No,No,Unknown,This capability has been re-used among many missions,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The Sensor Web Project uses a network of sensors linked by software and the internet to an autonomous satellite observation response capability. This system of systems is designed with a flexible, modular, architecture to facilitate expansion in sensors, customization of trigger conditions, and customization of responses.  This system has been used to implement a global surveillance program to study volcanos. We have also run sensorweb tests to study flooding, cryosphere events, and atmospheric phenomena.  Specifically, in our application, we use low resolution, high coverage sensors to trigger observations by high resolution instruments. Note that there are many other rationales to network sensors into a sensorweb. For example automated response might enable observation using complementary instruments such as imaging radar, infra-red, visible, etc. Or automated response might be used to apply more assets to increase the frequency of observation to improve the temporal resolution of available data.  Our sensorweb project is being used to monitor the Earth's 50 most active volcanos. We have also run sensorweb experiments to monitor flooding, wildfires, and cryospheric events (snowfall and melt, lake freezing and thawing, sea ice formation and breakup.) . Identification and labelling of terrain, climate and weather features","the sensor web project uses a network of sensors linked by software and the internet to an autonomous satellite observation response capability. this system of systems is designed with a flexible, modular, architecture to facilitate expansion in sensors, customization of trigger conditions, and customization of responses. this system has been used to implement a global surveillance program to study volcanos. we have also run sensorweb tests to study flooding, cryosphere events, and atmospheric phenomena. specifically, in our application, we use low resolution, high coverage sensors to trigger observations by high resolution instruments. note that there are many other rationales to network sensors into a sensorweb. for example automated response might enable observation using complementary instruments such as imaging radar, infra-red, visible, etc. or automated response might be used to apply more assets to increase the frequency of observation to improve the temporal resolution of available data. our sensorweb project is being used to monitor the earth's 50 most active volcanos. we have also run sensorweb experiments to monitor flooding, wildfires, and cryospheric events (snowfall and melt, lake freezing and thawing, sea ice formation and breakup.) . identification and labelling of terrain, climate and weather features"
"Global, Seasonal Mars Frost Maps",National Aeronautics and Space Administration,NASA,Jet Propulsion Laboratory,Deep Learning,None of the above.,Global frost Martian maps dervied from five remote sensing datasets and processed with tools like CNNs and other data science techniques. Publicly available on JMARS,Frost maps,In production,Neither,5/1/2021,10/1/2021,11/1/2024,Developed with both contracting and in-house resources.,Unknown,No,Yes,No,No,Yes,"HiRISE, CTX, THEMIS, MCS, CRISM",Yes,Yes,No – agency does not have access to source code.,No.  Engineering Review and Resease instead.,Multiple approved space-based systems,Less than 6 months,No,Yes,Unknown,This capability has been re-used among many missions,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,Global frost Martian maps dervied from five remote sensing datasets and processed with tools like CNNs and other data science techniques. Publicly available on JMARS . Frost maps,global frost martian maps dervied from five remote sensing datasets and processed with tools like cnns and other data science techniques. publicly available on jmars . frost maps
Purchase Card Management System (PCMS),National Aeronautics and Space Administration,NASA,Jet Propulsion Laboratory,NLP,None of the above.,Purchase card application uses ML model to suggest if a purchase may be a taggable asset or a chemical.,Metadata tagging recommendation,In production,Neither,11/13/2018,11/13/2018,11/15/2018,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,No,The training data set relies on historical records related to the description of purchased items along with the corresponding Asset-Tag/Chemical flag to determine the suggested item categorization,Yes,Yes,Yes,No.  Engineering Review and Resease instead.,Unknown,Less than 6 months,No,No,Unknown,We may share this technique with other relevant use cases,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,Purchase card application uses ML model to suggest if a purchase may be a taggable asset or a chemical. . Metadata tagging recommendation,purchase card application uses ml model to suggest if a purchase may be a taggable asset or a chemical. . metadata tagging recommendation
New Technology and Software Reporting (NTR),National Aeronautics and Space Administration,NASA,Jet Propulsion Laboratory,NLP,None of the above.,"NTR application uses ML model to suggest a Technology Category (e.g. Aerospace, Robotics) for the new technology being reported",Category classification,In production,Neither,5/9/2024,7/2/2024,8/22/2024,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,No,The training data set relies on historical records related to the description of the New Technolgy Reported along with the corresponding Technology Category field to determine the suggested categorization,Yes,Yes,Yes,No.  Engineering Review and Resease instead.,Unknown,Less than 6 months,No,No,Unknown,We may share this technique with other relevant use cases,Yes,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"NTR application uses ML model to suggest a Technology Category (e.g. Aerospace, Robotics) for the new technology being reported . Category classification","ntr application uses ml model to suggest a technology category (e.g. aerospace, robotics) for the new technology being reported . category classification"
AWS Assisted Software Development,National Science Foundation,NSF,OCIO,Mission-Enabling,None of the above.,"Within software development, AI-driven tools, such as AWS CodeWhisperer, Amazon Q and Bedrock, and integration with development workflow, promise to elevate developer productivity and refine code quality. .","The objective of this pilot is to explore potential integration of these AWS services with DIS Developers IDEs, Jenkins automation, Bitbucket, laying the groundwork for Secure AI Assisted Development at NSF",Acquisition and/or Development,Neither,7/15/1905,7/15/1905,Unknown,Unknown,Unknown,No,No,No,Yes,No,Training data will be derived from both AWS pre trained models as well as Agency generated in the case of Bedrock tuning to NSF use cases,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,No,Unknown,Unknown,No,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"Within software development, AI-driven tools, such as AWS CodeWhisperer, Amazon Q and Bedrock, and integration with development workflow, promise to elevate developer productivity and refine code quality. . . The objective of this pilot is to explore potential integration of these AWS services with DIS Developers IDEs, Jenkins automation, Bitbucket, laying the groundwork for Secure AI Assisted Development at NSF","within software development, ai-driven tools, such as aws codewhisperer, amazon q and bedrock, and integration with development workflow, promise to elevate developer productivity and refine code quality. . . the objective of this pilot is to explore potential integration of these aws services with dis developers ides, jenkins automation, bitbucket, laying the groundwork for secure ai assisted development at nsf"
Data Access Alternatives: Artificial Intelligence Supported Interfaces,National Science Foundation,NSF,SBE/NCSES,Mission-Enabling,Searching for information using AI.,The key objective of this effort is to create and test machine-learning-backed or “artificial intelligence” (AI)-backed user experiences with federal statistical data. This experience shall improve on the current state of user interactions based on obtaining answers to questions via search engines or emailing federal staff or contractors. ,This project seeks to develop and pilot an AI chat bot (or the like) that answers users text queries submitted via an interface. Answers will be obtained from public statistical data of federal statistical agencies in this project.,Initiated,Neither,7/16/1905,Unknown,Unknown,Developed with both contracting and in-house resources.,Unknown,No,Unknown,Unknown,Yes,Other,Unknown,Unknown,Yes,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.2857142857142857,The key objective of this effort is to create and test machine-learning-backed or “artificial intelligence” (AI)-backed user experiences with federal statistical data. This experience shall improve on the current state of user interactions based on obtaining answers to questions via search engines or emailing federal staff or contractors. . This project seeks to develop and pilot an AI chat bot (or the like) that answers users text queries submitted via an interface. Answers will be obtained from public statistical data of federal statistical agencies in this project.,the key objective of this effort is to create and test machine-learning-backed or “artificial intelligence” (ai)-backed user experiences with federal statistical data. this experience shall improve on the current state of user interactions based on obtaining answers to questions via search engines or emailing federal staff or contractors. . this project seeks to develop and pilot an ai chat bot (or the like) that answers users text queries submitted via an interface. answers will be obtained from public statistical data of federal statistical agencies in this project.
USA Class,Office of Personnel Management,OPM,HRS/HRSES/Federal Classification Center,Mission-Enabling,Unknown,"The purpose of this project is to build an application to help managers and HR specialists (classifiers) improve the quality and speed the development of position descriptions (PDs). Developing a PD can be a lengthy process that requires significant back-and-forth between managers and classifiers. The length of time required to develop a PD is often cited as a major pain point and bottleneck in federal hiring, workforce planning, organizational design, and performance management processes. To address the challenge today, managers will simply copy whole PDs or significant sections of PDs which weakens the accuracy of the new PD. Agencies have also turned to the use of “standard” PDs which tend to be too generic to be effective for use in recruitment and performance.

This project seeks to solve the “blank-page-syndrome” by suggesting draft major duties, factor evaluation statements, and other elements of the PD for managers and classifiers to use to develop PDs that meet classification standards.

This project is expected to significantly decrease the time required to develop PDs, leading to a decrease in time to hire and other related HR processes. A decrease in the time to hire means agencies can be more competitive in attracting top talent to their organizations, enhancing their ability to meet their missions. A decrease in the time required to develop position descriptions is also expected to lead to cost savings for agencies – freeing up manager and HR specialist time for other tasks. Additionally, the streamlined generation of PDs will enable agencies to move away from “standard” PD processes which will allow for better organizational design and workforce planning outcomes. The project is expected to also lead to higher-quality PDs that adhere to classification standards. This will improve position and performance management and talent acquisition in agencies, ensuring organizations are structured and staffed appropriately to meet their missions.
","USA Class will provide draft PD content generated through large language models (LLMs) and informed by a PD library curated by OPM experts and user responses to prompt questions about the position’s purpose, regular and recurring work, and knowledge required. The questions are intended to be straightforward and easily answered by the manager in a few minutes. The application will then send this information in a prompt to the LLM and generates suggested major duty titles and work tasks. The user (manager or classifier) can edit the draft content suggested by the system and makes the decision whether to include it in the PD or not. Future iterations will include a certification by a classifier and others that they accept/approve the final content of the full PD.",Acquisition and/or Development,"Rights-Impacting
",3/1/2024,3/1/2024,Unknown,Developed with both contracting and in-house resources.,24322624F0027,No,Unknown,Unknown,Unknown,Yes,"The AI system uses a RAG (Retrieval-Augmented-Generation) architecture including GPT-4o (accessed through OPM’s Azure Open AI environment) that leverages manager-provided responses to a few brief questions as well as a library of curated and validated federal position descriptions. The Position Description library consists of approximately 2,800 PDs developed by OPM/HRS’s Federal Classification Center for internal and external customer agencies.","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Unknown,Unknown,Yes,USAData Analytics,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,"We are aware that AI models can perpetuate existing biases present in the training data, leading to unfair or discriminatory classification outcomes. 

It could be a risk to user adoption that Federal managers and HR professionals do not understand how USA Class uses generative AI and whether or not it is safe and effective for them to use.

USA Class could produce incomplete, inaccurate, or inconsistent suggestions due to limitations in training data or other gaps. 

USA Class users could rely too much on the suggested position description content and not review and edit or adjust their prompts enough to get as precise results as are possible leading to overly generic or incomplete PDs. ",Yes – by an agency AI oversight board not directly involved in the system’s development,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,These activities are expected as the current prototype matures in the pilot process.,None of the above,Yes,Rights-Impacting,0.6190476190476191,"The purpose of this project is to build an application to help managers and HR specialists (classifiers) improve the quality and speed the development of position descriptions (PDs). Developing a PD can be a lengthy process that requires significant back-and-forth between managers and classifiers. The length of time required to develop a PD is often cited as a major pain point and bottleneck in federal hiring, workforce planning, organizational design, and performance management processes. To address the challenge today, managers will simply copy whole PDs or significant sections of PDs which weakens the accuracy of the new PD. Agencies have also turned to the use of “standard” PDs which tend to be too generic to be effective for use in recruitment and performance.

This project seeks to solve the “blank-page-syndrome” by suggesting draft major duties, factor evaluation statements, and other elements of the PD for managers and classifiers to use to develop PDs that meet classification standards.

This project is expected to significantly decrease the time required to develop PDs, leading to a decrease in time to hire and other related HR processes. A decrease in the time to hire means agencies can be more competitive in attracting top talent to their organizations, enhancing their ability to meet their missions. A decrease in the time required to develop position descriptions is also expected to lead to cost savings for agencies – freeing up manager and HR specialist time for other tasks. Additionally, the streamlined generation of PDs will enable agencies to move away from “standard” PD processes which will allow for better organizational design and workforce planning outcomes. The project is expected to also lead to higher-quality PDs that adhere to classification standards. This will improve position and performance management and talent acquisition in agencies, ensuring organizations are structured and staffed appropriately to meet their missions. . USA Class will provide draft PD content generated through large language models (LLMs) and informed by a PD library curated by OPM experts and user responses to prompt questions about the position’s purpose, regular and recurring work, and knowledge required. The questions are intended to be straightforward and easily answered by the manager in a few minutes. The application will then send this information in a prompt to the LLM and generates suggested major duty titles and work tasks. The user (manager or classifier) can edit the draft content suggested by the system and makes the decision whether to include it in the PD or not. Future iterations will include a certification by a classifier and others that they accept/approve the final content of the full PD. . We are aware that AI models can perpetuate existing biases present in the training data, leading to unfair or discriminatory classification outcomes. 

It could be a risk to user adoption that Federal managers and HR professionals do not understand how USA Class uses generative AI and whether or not it is safe and effective for them to use.

USA Class could produce incomplete, inaccurate, or inconsistent suggestions due to limitations in training data or other gaps. 

USA Class users could rely too much on the suggested position description content and not review and edit or adjust their prompts enough to get as precise results as are possible leading to overly generic or incomplete PDs.","the purpose of this project is to build an application to help managers and hr specialists (classifiers) improve the quality and speed the development of position descriptions (pds). developing a pd can be a lengthy process that requires significant back-and-forth between managers and classifiers. the length of time required to develop a pd is often cited as a major pain point and bottleneck in federal hiring, workforce planning, organizational design, and performance management processes. to address the challenge today, managers will simply copy whole pds or significant sections of pds which weakens the accuracy of the new pd. agencies have also turned to the use of “standard” pds which tend to be too generic to be effective for use in recruitment and performance. this project seeks to solve the “blank-page-syndrome” by suggesting draft major duties, factor evaluation statements, and other elements of the pd for managers and classifiers to use to develop pds that meet classification standards. this project is expected to significantly decrease the time required to develop pds, leading to a decrease in time to hire and other related hr processes. a decrease in the time to hire means agencies can be more competitive in attracting top talent to their organizations, enhancing their ability to meet their missions. a decrease in the time required to develop position descriptions is also expected to lead to cost savings for agencies – freeing up manager and hr specialist time for other tasks. additionally, the streamlined generation of pds will enable agencies to move away from “standard” pd processes which will allow for better organizational design and workforce planning outcomes. the project is expected to also lead to higher-quality pds that adhere to classification standards. this will improve position and performance management and talent acquisition in agencies, ensuring organizations are structured and staffed appropriately to meet their missions. . usa class will provide draft pd content generated through large language models (llms) and informed by a pd library curated by opm experts and user responses to prompt questions about the position’s purpose, regular and recurring work, and knowledge required. the questions are intended to be straightforward and easily answered by the manager in a few minutes. the application will then send this information in a prompt to the llm and generates suggested major duty titles and work tasks. the user (manager or classifier) can edit the draft content suggested by the system and makes the decision whether to include it in the pd or not. future iterations will include a certification by a classifier and others that they accept/approve the final content of the full pd. . we are aware that ai models can perpetuate existing biases present in the training data, leading to unfair or discriminatory classification outcomes. it could be a risk to user adoption that federal managers and hr professionals do not understand how usa class uses generative ai and whether or not it is safe and effective for them to use. usa class could produce incomplete, inaccurate, or inconsistent suggestions due to limitations in training data or other gaps. usa class users could rely too much on the suggested position description content and not review and edit or adjust their prompts enough to get as precise results as are possible leading to overly generic or incomplete pds."
Insight,Social Security Administration,SSA,"Office of Analytics, Review, and Oversight",Government Services (includes Benefits and Service Delivery),None of the above.,"This AI use case is decision support software used by hearings and appeals-level Disability Program adjudicators to help maximize the quality, speed, and consistency of their decision making.  The use case analyzes the free text of disability decisions and other case data to offer adjudicators real-time alerts on potential quality issues and case-specific reference information within a web application.  The use case increases decision quality and reduces human error.","Insight provides 43 Quality flags, when raised, indicating various inconsistencies, omissions, or Policy non-compliant conditions that exist in the disability decision. ",Operation and Maintenance,Neither,7/24/2015,10/1/2016,10/1/2017,Developed with both contracting and in-house resources.,ITSSC,Yes,No,Yes,Yes,Yes,Manually curated data sets based on random sampling methodology of hearing and appeals case processing management information,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Insight,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"This AI use case is decision support software used by hearings and appeals-level Disability Program adjudicators to help maximize the quality, speed, and consistency of their decision making.  The use case analyzes the free text of disability decisions and other case data to offer adjudicators real-time alerts on potential quality issues and case-specific reference information within a web application.  The use case increases decision quality and reduces human error. . Insight provides 43 Quality flags, when raised, indicating various inconsistencies, omissions, or Policy non-compliant conditions that exist in the disability decision.","this ai use case is decision support software used by hearings and appeals-level disability program adjudicators to help maximize the quality, speed, and consistency of their decision making. the use case analyzes the free text of disability decisions and other case data to offer adjudicators real-time alerts on potential quality issues and case-specific reference information within a web application. the use case increases decision quality and reduces human error. . insight provides 43 quality flags, when raised, indicating various inconsistencies, omissions, or policy non-compliant conditions that exist in the disability decision."
Intelligent Medical Language Analysis Generation (IMAGEN),Social Security Administration,SSA,Office of the Chief Information Officer,Government Services (includes Benefits and Service Delivery),Searching for information using AI.,"This AI use case helps employees visualize, search, and more easily identify relevant clinical content in medical records.  The use case improves efficiency and accuracy of disability determinations/decisions.","Document annotations on medical records to aid in navigation such as identifying patient encounters, tests, dates, and other relevant information. Possible impairment and SSA Medical Listing Codes are also identified. ",Operation and Maintenance,"Rights-Impacting
",1/1/2019,1/1/2019,8/20/2021,Developed with both contracting and in-house resources.,Agency-wide IT Support Services Contract.  (ITSSC),Yes,No,Yes,Yes,Yes,Historically adjudicated disability claims and outcomes. Manually annotated documents,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,IMAGEN,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Rights-Impacting,0.6031746031746031,"This AI use case helps employees visualize, search, and more easily identify relevant clinical content in medical records.  The use case improves efficiency and accuracy of disability determinations/decisions. . Document annotations on medical records to aid in navigation such as identifying patient encounters, tests, dates, and other relevant information. Possible impairment and SSA Medical Listing Codes are also identified.","this ai use case helps employees visualize, search, and more easily identify relevant clinical content in medical records. the use case improves efficiency and accuracy of disability determinations/decisions. . document annotations on medical records to aid in navigation such as identifying patient encounters, tests, dates, and other relevant information. possible impairment and ssa medical listing codes are also identified."
Duplicate Identification Process,Social Security Administration,SSA,Office of the Chief Information Officer,Government Services (includes Benefits and Service Delivery),None of the above.,"This AI use case identifies, flags, and marks duplicates, reducing the amount of time spent to review cases for hearings.","This AI use case identifies, flags, and marks duplicates, reducing the amount of time spent to review cases for hearings.",Operation and Maintenance,Neither,8/20/2018,10/1/2018,9/1/2019,Developed with contracting resources.,"28321324D00060010
ITSSC - Accenture",Yes,No,Yes,Yes,No,No Agency data used for training,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",No,No – agency does not have access to source code.,Yes,DIP,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"This AI use case identifies, flags, and marks duplicates, reducing the amount of time spent to review cases for hearings. . This AI use case identifies, flags, and marks duplicates, reducing the amount of time spent to review cases for hearings.","this ai use case identifies, flags, and marks duplicates, reducing the amount of time spent to review cases for hearings. . this ai use case identifies, flags, and marks duplicates, reducing the amount of time spent to review cases for hearings."
Handwriting Recognition of Forms; Intelligent Document Processing Platform - Forms Classification and Data Extraction ,Social Security Administration,SSA,Office of the Chief Information Officer,Government Services (includes Benefits and Service Delivery),None of the above.,"This AI use case classify, identify and extract structured, semi-structured forms containing printed and handwritten fields. The use case increases automation, reduces clerical errors and reduces customer wait times.",The output of extracted data fields on the form is represented in JSON format that can be available to downstream systems using API and through other integration mechanism such as queues.,Operation and Maintenance,Neither,3/1/2020,11/1/2020,1/2/2021,Developed with contracting resources.,"28321324D00060010
ITSSC - Accenture",Yes,No,Yes,Yes,No,No Agency data used for training,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,Yes,"Form Data Extraction

",Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"This AI use case classify, identify and extract structured, semi-structured forms containing printed and handwritten fields. The use case increases automation, reduces clerical errors and reduces customer wait times. . The output of extracted data fields on the form is represented in JSON format that can be available to downstream systems using API and through other integration mechanism such as queues.","this ai use case classify, identify and extract structured, semi-structured forms containing printed and handwritten fields. the use case increases automation, reduces clerical errors and reduces customer wait times. . the output of extracted data fields on the form is represented in json format that can be available to downstream systems using api and through other integration mechanism such as queues."
Modernized Development Worksheet (MDW),Social Security Administration,SSA,"Office of Analytics, Review, and Oversight",Government Services (includes Benefits and Service Delivery),None of the above.,This AI use case reviews data during case development so it can be better categorized. The use case reduces processing time and customer wait times.,Labels and categorizes new or updated MDW requests,Operation and Maintenance,Neither,7/11/1905,Current model: 2019-2023,12/1/2023,Developed in-house.,Unknown,Yes,No,Yes,No,Yes,Model trained from case development worksheets prepared by SSA employees.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,This AI use case reviews data during case development so it can be better categorized. The use case reduces processing time and customer wait times. . Labels and categorizes new or updated MDW requests,this ai use case reviews data during case development so it can be better categorized. the use case reduces processing time and customer wait times. . labels and categorizes new or updated mdw requests
Anomalous iClaim Predictive Model,Social Security Administration,SSA,"Office of Analytics, Review, and Oversight",Government Services (includes Benefits and Service Delivery),None of the above.,This AI use case identifies high-risk iClaims. These claims are then sent to Operations for further review before additional action is taken to adjudicate the claims.,Risk scores for recently submitted iClaims.,Operation and Maintenance,"Rights-Impacting
",10/1/2015,10/1/2015,3/20/2016,Developed in-house.,Unknown,Yes,No,Yes,No,Yes,Disclosing this information may impede the agency's ability to effectively combat fraud.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Anti-Fraud Infrastructure,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Rights-Impacting,0.5714285714285714,This AI use case identifies high-risk iClaims. These claims are then sent to Operations for further review before additional action is taken to adjudicate the claims. . Risk scores for recently submitted iClaims.,this ai use case identifies high-risk iclaims. these claims are then sent to operations for further review before additional action is taken to adjudicate the claims. . risk scores for recently submitted iclaims.
Pre-Effectuation Review / Targeted Denial Review Models,Social Security Administration,SSA,"Office of Analytics, Review, and Oversight",Government Services (includes Benefits and Service Delivery),None of the above.,This AI use case identifies high likelihood of error in certain claims and refers them for review. This use case reduces incorrect benefits payment.,Probability scores via an in-line mainframe process,Operation and Maintenance,"Rights-Impacting
",1/1/1996,Current model: 2020-2024,12/1/2024,Developed in-house.,Unknown,Yes,No,Yes,No,Yes,Models used data from disability program data sets.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Rights-Impacting,0.5555555555555556,This AI use case identifies high likelihood of error in certain claims and refers them for review. This use case reduces incorrect benefits payment. . Probability scores via an in-line mainframe process,this ai use case identifies high likelihood of error in certain claims and refers them for review. this use case reduces incorrect benefits payment. . probability scores via an in-line mainframe process
Representative Payee Misuse Model,Social Security Administration,SSA,"Office of Analytics, Review, and Oversight",Government Services (includes Benefits and Service Delivery),None of the above.,This AI use case identifies possible representative payee fraud and flags for review. This use case reduces representative payee fraud.,The system outputs numerical scores based on likelihood of representative payee misuse.,Operation and Maintenance,"Rights-Impacting
",1/1/2009,1/1/2009,9/30/2023,Developed in-house.,Unknown,Yes,No,Yes,No,Yes,Models used data from earnings and beneficiary data.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,Yes,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Rights-Impacting,0.5714285714285714,This AI use case identifies possible representative payee fraud and flags for review. This use case reduces representative payee fraud. . The system outputs numerical scores based on likelihood of representative payee misuse.,this ai use case identifies possible representative payee fraud and flags for review. this use case reduces representative payee fraud. . the system outputs numerical scores based on likelihood of representative payee misuse.
Continuing Disability Review Model,Social Security Administration,SSA,"Office of Analytics, Review, and Oversight",Government Services (includes Benefits and Service Delivery),None of the above.,This AI use case identifies disability cases with the greatest likelihood of medical improvement and flag them for a continuing disability review to improve the accuracy of CDRs.,"The system outputs numerical scores for each disability case: the score represents the likelihood of medical improvements. The probabilities are binned according to low, medium, or high. ",Operation and Maintenance,"Rights-Impacting
",1/1/1993,1/1/1993,9/30/2018,Developed in-house.,Unknown,Yes,No,Yes,No,Yes,Model used data from disability program data sets,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Unknown,Yes,Yes,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Rights-Impacting,0.5555555555555556,"This AI use case identifies disability cases with the greatest likelihood of medical improvement and flag them for a continuing disability review to improve the accuracy of CDRs. . The system outputs numerical scores for each disability case: the score represents the likelihood of medical improvements. The probabilities are binned according to low, medium, or high.","this ai use case identifies disability cases with the greatest likelihood of medical improvement and flag them for a continuing disability review to improve the accuracy of cdrs. . the system outputs numerical scores for each disability case: the score represents the likelihood of medical improvements. the probabilities are binned according to low, medium, or high."
Social Security Income Redetermination Model,Social Security Administration,SSA,"Office of Analytics, Review, and Oversight",Government Services (includes Benefits and Service Delivery),None of the above.,This AI use case identifies SSI overpayment cases that have highest expected overpayments for review. This use case contains excess overpayments by prioritizing technician review,"The system outputs numerical scores for individuals, comprised of likelihood and amount of overpayment. ",Operation and Maintenance,"Rights-Impacting
",1/1/1995,1/1/1995,9/9/2024,Developed in-house.,Unknown,Yes,No,Yes,No,Yes,Model used data from disability program data sets,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Unknown,Yes,Yes,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Rights-Impacting,0.5555555555555556,"This AI use case identifies SSI overpayment cases that have highest expected overpayments for review. This use case contains excess overpayments by prioritizing technician review . The system outputs numerical scores for individuals, comprised of likelihood and amount of overpayment.","this ai use case identifies ssi overpayment cases that have highest expected overpayments for review. this use case contains excess overpayments by prioritizing technician review . the system outputs numerical scores for individuals, comprised of likelihood and amount of overpayment."
Proactive Triage and Analysis of Hearings (PATH),Social Security Administration,SSA,"Office of Analytics, Review, and Oversight",Government Services (includes Benefits and Service Delivery),None of the above.,This AI use case flags high likelihood favorable claims and refers them to human adjudicators for further review to determine eligibility for on-the-record hearing decisions.,The system outputs sorted case listings based on likelihood of favorable hearing decision.,Operation and Maintenance,"Rights-Impacting
",1/1/2018,1/1/2018,11/1/2024,Developed in-house.,Unknown,Yes,No,Yes,No,Yes,Model used data from disability program data sets,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,Yes,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Rights-Impacting,0.5714285714285714,This AI use case flags high likelihood favorable claims and refers them to human adjudicators for further review to determine eligibility for on-the-record hearing decisions. . The system outputs sorted case listings based on likelihood of favorable hearing decision.,this ai use case flags high likelihood favorable claims and refers them to human adjudicators for further review to determine eligibility for on-the-record hearing decisions. . the system outputs sorted case listings based on likelihood of favorable hearing decision.
Quick Disability Determinations Model,Social Security Administration,SSA,Office of Retirement and Disability Policy,Government Services (includes Benefits and Service Delivery),None of the above.,Used to screen initial applications to identify cases where a favorable disability determination is highly likely and medical evidence is readily available to prioritize this workload and expedite case processing. ," System output consists of probability model scores for each separate scoring service, Scoring Service for Allowance and Scoring Service for Processing Time, and the overall QDD Score.  ",Operation and Maintenance,"Rights-Impacting
",9/25/2003,9/15/2004,2/1/2008,Developed with contracting resources.,SS0004400360001,Yes,No,Yes,Yes,Yes,Model used data from disability program data sets,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,No – agency does not have access to source code.,Yes,eDIB (electronic disability),6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Rights-Impacting,0.5873015873015873,"Used to screen initial applications to identify cases where a favorable disability determination is highly likely and medical evidence is readily available to prioritize this workload and expedite case processing. . System output consists of probability model scores for each separate scoring service, Scoring Service for Allowance and Scoring Service for Processing Time, and the overall QDD Score.","used to screen initial applications to identify cases where a favorable disability determination is highly likely and medical evidence is readily available to prioritize this workload and expedite case processing. . system output consists of probability model scores for each separate scoring service, scoring service for allowance and scoring service for processing time, and the overall qdd score."
Mobile Wage Reporting (MOBWR) ,Social Security Administration,SSA,Office of the Chief Information Officer,Government Services (includes Benefits and Service Delivery),Inputting large amounts of data from paper forms into a digital system using AI.,Uses AI to extract text/data from scanned images/documents representing pay stubs or payroll information to enable faster processing.,Gross pay and pay date from the paystub image,Operation and Maintenance,Neither,11/4/2020,3/1/2021,5/14/2022,Developed with contracting resources.,"6801, 10128, 10191, 10325",Yes,No,Yes,Yes,No,No Agency data used for training,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,Yes,Supplemental Security Income Record Maintenance System (SSIRMS),Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,Uses AI to extract text/data from scanned images/documents representing pay stubs or payroll information to enable faster processing. . Gross pay and pay date from the paystub image,uses ai to extract text/data from scanned images/documents representing pay stubs or payroll information to enable faster processing. . gross pay and pay date from the paystub image
SSA 800# Public Question & Answer Bot,Social Security Administration,SSA,Office of Transformation,Government Services (includes Benefits and Service Delivery),Searching for information using AI.,"The bot answers caller’s questions using approved FAQs found on SSA.GOV. Support is provided in both English and Spanish. 

Calls answered solely by the bot reduce the wait for other callers needing agent service.",Returns the answer snippet content for the applicable FAQ topic that should contain the same language as if the askers were to navigate to the FAQs webpage themselves on ssa.gov.,Operation and Maintenance,Neither,2/13/2023,4/3/2023,6/5/2023,Developed with contracting resources.,Unknown,Yes,Yes,No,No,Yes,The ssa.gov FAQs webpage content is exported to be sourced by the FAQ Question & Answer bot.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,No – agency does not have access to source code.,Yes,Enterprise Contact Center,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"The bot answers caller’s questions using approved FAQs found on SSA.GOV. Support is provided in both English and Spanish. 

Calls answered solely by the bot reduce the wait for other callers needing agent service. . Returns the answer snippet content for the applicable FAQ topic that should contain the same language as if the askers were to navigate to the FAQs webpage themselves on ssa.gov.",the bot answers caller’s questions using approved faqs found on ssa.gov. support is provided in both english and spanish. calls answered solely by the bot reduce the wait for other callers needing agent service. . returns the answer snippet content for the applicable faq topic that should contain the same language as if the askers were to navigate to the faqs webpage themselves on ssa.gov.
Generative AI for Analytic Insights,Social Security Administration,SSA,"Office of Analytics, Review, and Oversight",Government Services (includes Benefits and Service Delivery),None of the above.,"Generative AI model to analyze SSA's National 800 Number transcripts to provide insights into caller intent, call resolution, and call topics. These insights could highlight pain points in SSA’s policies and processes and focus efforts to improve customer service, deflect callers whose business cannot be resolved using the 800 Number, and ultimately decrease wait times.",Varies by use case,Operation and Maintenance,Neither,10/1/2022,10/1/2022,9/30/2023,Developed with contracting resources.,Unknown,Yes,No,Yes,No,No,No Agency data used for training,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,Yes,*** Provisional ATO in progress,6-12 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"Generative AI model to analyze SSA's National 800 Number transcripts to provide insights into caller intent, call resolution, and call topics. These insights could highlight pain points in SSA’s policies and processes and focus efforts to improve customer service, deflect callers whose business cannot be resolved using the 800 Number, and ultimately decrease wait times. . Varies by use case","generative ai model to analyze ssa's national 800 number transcripts to provide insights into caller intent, call resolution, and call topics. these insights could highlight pain points in ssa’s policies and processes and focus efforts to improve customer service, deflect callers whose business cannot be resolved using the 800 number, and ultimately decrease wait times. . varies by use case"
PolicyNet Plain Language Search Engine,Social Security Administration,SSA,Office of the Chief Information Officer,Government Services (includes Benefits and Service Delivery),Searching for information using AI.,"Search will employ learning to rank functionality based on user entered thumbs up/thumbs down data (data capture began in FY24 - learning to rank will be implemented in FY25).  Users across all internal SSA components and offices may use the system to find relevant information on SSA policies, procedures or instructions. Learning to rank will help users find relevant results faster, which supports better customer service. 
","Based on the user thumbs up/thumbs down selections, search results will be ranked higher (or lower) within the results output.  ",Operation and Maintenance,Neither,3/1/2024,8/19/2024,9/21/2024,Developed with contracting resources.,ITSSC,Yes,No,No,Yes,No,Model used disability program regulations and user provided data,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,Yes – source code is publicly available.,Yes,21st Century PolicyNet ,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"Search will employ learning to rank functionality based on user entered thumbs up/thumbs down data (data capture began in FY24 - learning to rank will be implemented in FY25).  Users across all internal SSA components and offices may use the system to find relevant information on SSA policies, procedures or instructions. Learning to rank will help users find relevant results faster, which supports better customer service. . Based on the user thumbs up/thumbs down selections, search results will be ranked higher (or lower) within the results output.","search will employ learning to rank functionality based on user entered thumbs up/thumbs down data (data capture began in fy24 - learning to rank will be implemented in fy25). users across all internal ssa components and offices may use the system to find relevant information on ssa policies, procedures or instructions. learning to rank will help users find relevant results faster, which supports better customer service. . based on the user thumbs up/thumbs down selections, search results will be ranked higher (or lower) within the results output."
Speech to Text Video Transcription,Social Security Administration,SSA,Office of the Chief Information Officer,Mission-Enabling,Transcribing and summarizing a recorded meeting or interview using AI.,Generate transcript from recorded audio of disability hearings,Transcript,Operation and Maintenance,Neither,10/1/2023,10/1/2023,5/6/2024,Developed with contracting resources.,Microsoft/Accenture,Yes,No,Yes,No,No,No Agency data used for training,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,Yes,Hearing Recording and Transcription,Unknown,Yes,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,Generate transcript from recorded audio of disability hearings . Transcript,generate transcript from recorded audio of disability hearings . transcript
Customer Insight Tool,Social Security Administration,SSA,Office of Transformation,Government Services (includes Benefits and Service Delivery),None of the above.,"Survey analysis tool that evaluates sentiment and categorization.
Current capabilities include: 
1. Machine Translation - using algorithms to automatically translate text from one language to another
2. Sentiment Analysis - classifies text into negative, positive and neutral sentiment
3. Categorization - system-generated groupings of conceptually-related words and phrases based on phrase-level feedback
4. Suggested Actions - uncovers explicit suggestions from feedback text
5. Customer Effort - aimed to recover explicit mentions of effort in feedback text","Survey analysis tool that evaluates sentiment and categorization.
Current capabilities include: 
1. Machine Translation - using algorithms to automatically translate text from one language to another
2. Sentiment Analysis - classifies text into negative, positive and neutral sentiment
3. Categorization - system-generated groupings of conceptually-related words and phrases based on phrase-level feedback
4. Suggested Actions - uncovers explicit suggestions from feedback text
5. Customer Effort - aimed to recover explicit mentions of effort in feedback text",Operation and Maintenance,Neither,8/1/2021,9/26/2020,8/1/2021,Developed with contracting resources.,Inter-Agency Agreement (IAA-21-0017),Yes,No,Yes,No,No,No Agency data used for training,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,Yes,Customer Feedback Tool (CXFT),Less than 6 months,Yes,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"Survey analysis tool that evaluates sentiment and categorization.
Current capabilities include: 
1. Machine Translation - using algorithms to automatically translate text from one language to another
2. Sentiment Analysis - classifies text into negative, positive and neutral sentiment
3. Categorization - system-generated groupings of conceptually-related words and phrases based on phrase-level feedback
4. Suggested Actions - uncovers explicit suggestions from feedback text
5. Customer Effort - aimed to recover explicit mentions of effort in feedback text . Survey analysis tool that evaluates sentiment and categorization.
Current capabilities include: 
1. Machine Translation - using algorithms to automatically translate text from one language to another
2. Sentiment Analysis - classifies text into negative, positive and neutral sentiment
3. Categorization - system-generated groupings of conceptually-related words and phrases based on phrase-level feedback
4. Suggested Actions - uncovers explicit suggestions from feedback text
5. Customer Effort - aimed to recover explicit mentions of effort in feedback text","survey analysis tool that evaluates sentiment and categorization. current capabilities include: 1. machine translation - using algorithms to automatically translate text from one language to another 2. sentiment analysis - classifies text into negative, positive and neutral sentiment 3. categorization - system-generated groupings of conceptually-related words and phrases based on phrase-level feedback 4. suggested actions - uncovers explicit suggestions from feedback text 5. customer effort - aimed to recover explicit mentions of effort in feedback text . survey analysis tool that evaluates sentiment and categorization. current capabilities include: 1. machine translation - using algorithms to automatically translate text from one language to another 2. sentiment analysis - classifies text into negative, positive and neutral sentiment 3. categorization - system-generated groupings of conceptually-related words and phrases based on phrase-level feedback 4. suggested actions - uncovers explicit suggestions from feedback text 5. customer effort - aimed to recover explicit mentions of effort in feedback text"
Benefit and Earnings Public Use File,Social Security Administration,SSA,Office of Retirement and Disability Policy,Government Services (includes Benefits and Service Delivery),None of the above.,Generate high fidelity synthetic benefits and earnings data for external researches to support policy research and decision making while protecting privacy. ,Synthetic microdata file aimed for public use by policy researchers,Operation and Maintenance,Neither,9/1/2023,Unknown,Unknown,Developed in-house.,Unknown,No,Yes,Yes,Yes,Yes,Model used data from disability program data sets,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Unknown,No,No,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,Generate high fidelity synthetic benefits and earnings data for external researches to support policy research and decision making while protecting privacy. . Synthetic microdata file aimed for public use by policy researchers,generate high fidelity synthetic benefits and earnings data for external researches to support policy research and decision making while protecting privacy. . synthetic microdata file aimed for public use by policy researchers
Therapy Chatbot - Text-Based Mental Health Support for SSA Employees,Social Security Administration,SSA,Office of Human Resources,Health & Medical,None of the above.,This AI use case is a therapy chatbot that employees can access on their own personal devices.,Synthetic text output that serves discussion with users,Operation and Maintenance,Safety-Impacting,Unknown,7/1/2023,9/1/2023,Developed with contracting resources.,Unknown,No,No,Yes,Unknown,No,No Agency data used for training,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Unknown,Unknown,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Safety-Impacting,0.47619047619047616,This AI use case is a therapy chatbot that employees can access on their own personal devices. . Synthetic text output that serves discussion with users,this ai use case is a therapy chatbot that employees can access on their own personal devices. . synthetic text output that serves discussion with users
Agency Support Companion (ASC) chatbot,Social Security Administration,SSA,Office of the Chief Information Officer,Mission-Enabling,Improving the quality of written communications using AI tools.,"Generative AI model to create content, summarization and research capabilities",Chatbot responses to users as well as management information reports for executives,Acquisition and/or Development,Neither,7/15/2024,8/14/2024,Unknown,Developed with contracting resources.,ITSSC,No,No,No,No,No,No Agency data used for training,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Agency Support Companion,Unknown,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Generative AI model to create content, summarization and research capabilities . Chatbot responses to users as well as management information reports for executives","generative ai model to create content, summarization and research capabilities . chatbot responses to users as well as management information reports for executives"
Intelligent generation of modernized code.,Social Security Administration,SSA,Office of the Chief Information Officer,Government Services (includes Benefits and Service Delivery),None of the above.,"The module uses AI techniques such as cluster analysis, pattern matching, templates, and programming language models to generate modernized code in a new target language.
These AI techniques enable it to perform efficient data analysis, processing, and intelligent generation of new code. AI powered approach provides structured standardized code that’s highly maintainable.


",Transformed legacy code into modernized language,Operation and Maintenance,Neither,10/1/2024,7/1/2024,11/1/2014,Developed with contracting resources.,ITSSC,No,No,No,No,No,No Agency data used for training,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,ISAT,Less than 6 months,Yes,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"The module uses AI techniques such as cluster analysis, pattern matching, templates, and programming language models to generate modernized code in a new target language.
These AI techniques enable it to perform efficient data analysis, processing, and intelligent generation of new code. AI powered approach provides structured standardized code that’s highly maintainable. . Transformed legacy code into modernized language","the module uses ai techniques such as cluster analysis, pattern matching, templates, and programming language models to generate modernized code in a new target language. these ai techniques enable it to perform efficient data analysis, processing, and intelligent generation of new code. ai powered approach provides structured standardized code that’s highly maintainable. . transformed legacy code into modernized language"
Data Governance Product (DGP) -  Data cataloging and predictive analysis of data assets. ,Social Security Administration,SSA,Office of the Chief Information Officer,Government Services (includes Benefits and Service Delivery),None of the above.,"Vendor built in AI engine for data cataloging to automates the discovery, classification, and curation of data assets. And also provides predictive insights for data asset analytics. Uses Natural Language Processing (NLP) enables non-technical stakeholders to contribute to rule creation and facilitates the conversion of plain text into regular expression.
",Efficient data catalog search results. Plain English conversation with the users to create technical regular expression rule creation,Operation and Maintenance,Neither,5/1/2020,7/11/2022,10/1/2024,Developed with contracting resources.,contract number GS-35F-267DA,No,No,Yes,Yes,Yes,Model used data from internal sources for performance and evaluation purposes,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Data Governance Product (DGP),Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Vendor built in AI engine for data cataloging to automates the discovery, classification, and curation of data assets. And also provides predictive insights for data asset analytics. Uses Natural Language Processing (NLP) enables non-technical stakeholders to contribute to rule creation and facilitates the conversion of plain text into regular expression. . Efficient data catalog search results. Plain English conversation with the users to create technical regular expression rule creation","vendor built in ai engine for data cataloging to automates the discovery, classification, and curation of data assets. and also provides predictive insights for data asset analytics. uses natural language processing (nlp) enables non-technical stakeholders to contribute to rule creation and facilitates the conversion of plain text into regular expression. . efficient data catalog search results. plain english conversation with the users to create technical regular expression rule creation"
AI input in Translation,Department of State,STATE,A/PRI/LS,Mission-Enabling ,None of the above.,Leveraging machine-based tools to streamline workflows in translation work. Machine output is carefully post-edited by professional human translators to ensure highest quality product. ,Very rough suggested language that is carefully reviewed and edited by human professional translators. ,Implementation and Assessment,Neither,1/19/2024,1/19/2024,1/19/2024,Developed with both contracting and in-house resources.,Unknown,No,Unknown,No,Unknown,Unknown,In-house curated translation memory for training and fine-tuning models,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Yes,Unknown,Unknown,No,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3333333333333333,Leveraging machine-based tools to streamline workflows in translation work. Machine output is carefully post-edited by professional human translators to ensure highest quality product. . Very rough suggested language that is carefully reviewed and edited by human professional translators.,leveraging machine-based tools to streamline workflows in translation work. machine output is carefully post-edited by professional human translators to ensure highest quality product. . very rough suggested language that is carefully reviewed and edited by human professional translators.
BudgetChat AI Tool,Department of State,STATE,BP/RPBI,Mission-Enabling (internal agency support),Inputting large amounts of data from paper forms into a digital system using AI.,"BudgetChat (aka BudgetSearch) is expected to consolidate budget documents to make searching and summarizing them easier, thereby streamlining access and analysis of initiatives over multiple years and programs.","Summary of information that will be verified, reviewed, and edited by humans.",Acquisition and/or Development,Neither,7/26/2024,9/12/2024,12/1/2024,Developed with both contracting and in-house resources.,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Budget documents that have been published from previous and current year. ,Yes,Unknown,Unknown,Unknown,Unknown,Unknown,Yes,Unknown,Yes,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.30158730158730157,"BudgetChat (aka BudgetSearch) is expected to consolidate budget documents to make searching and summarizing them easier, thereby streamlining access and analysis of initiatives over multiple years and programs. . Summary of information that will be verified, reviewed, and edited by humans.","budgetchat (aka budgetsearch) is expected to consolidate budget documents to make searching and summarizing them easier, thereby streamlining access and analysis of initiatives over multiple years and programs. . summary of information that will be verified, reviewed, and edited by humans."
Travel.State.Gov (TSG) Enhanced Search and Chatbot  ,Department of State,STATE,CA/CST,Other,Unknown,A new chatbot and enhanced search on the TSG website processing existing FAQs and resources. The chatbot provides predefined responses. ,Improved outcomes.,Acquisition and/or Development,Neither,2/1/2024,2/1/2024,Unknown,Developed with both contracting and in-house resources.,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Data is posted on the site at www.travel.state.gov,The data is public facing and documented in www.travel.state.gov.,Unknown,Unknown,Unknown,Unknown,Unknown,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.31746031746031744,A new chatbot and enhanced search on the TSG website processing existing FAQs and resources. The chatbot provides predefined responses. . Improved outcomes.,a new chatbot and enhanced search on the tsg website processing existing faqs and resources. the chatbot provides predefined responses. . improved outcomes.
CodeGen - AI-assisted IT Application Development,Department of State,STATE,CA/CST,Other; Mission-Enabling (internal agency support),None of the above.,"CodeGen is an application designed for seamless integration with Large Language Models (LLMs) that aims to enhance the process that developers follow to build, test, and deploy software. Through use of API, it provides a secure suite of capabilities that fast-tracks developers during code generation, translation, analysis, optimization, and debugging.",Improved outcomes.,Acquisition and/or Development,Neither,6/9/2023,8/22/2023,Unknown,Developed with both contracting and in-house resources.,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Application source code and documentation.,Application source code and documentation. ,Unknown,Unknown,Unknown,Unknown,Unknown,Yes,Unknown,Unknown,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.30158730158730157,"CodeGen is an application designed for seamless integration with Large Language Models (LLMs) that aims to enhance the process that developers follow to build, test, and deploy software. Through use of API, it provides a secure suite of capabilities that fast-tracks developers during code generation, translation, analysis, optimization, and debugging. . Improved outcomes.","codegen is an application designed for seamless integration with large language models (llms) that aims to enhance the process that developers follow to build, test, and deploy software. through use of api, it provides a secure suite of capabilities that fast-tracks developers during code generation, translation, analysis, optimization, and debugging. . improved outcomes."
Violence Against Civilians Model,Department of State,STATE,CSO,Other,Unknown,"A machine learning model that uses open-source political, social, and economic datasets to forecast mass civilian killings for the upcoming quarter and year for each country globally in order to inform conflict prevention. ",Forecasted quarterly and yearly estimates of indicators. ,Implementation and Assessment,Neither,1/23/2023,1/23/2023,1/23/2023,Developed with both contracting and in-house resources.,Unknown,No,Unknown,No,No,Unknown,Unknown,Documentation is complete,No,Unknown,Unknown,Unknown,Unknown,Yes,Other,Other,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.38095238095238093,"A machine learning model that uses open-source political, social, and economic datasets to forecast mass civilian killings for the upcoming quarter and year for each country globally in order to inform conflict prevention. . Forecasted quarterly and yearly estimates of indicators.","a machine learning model that uses open-source political, social, and economic datasets to forecast mass civilian killings for the upcoming quarter and year for each country globally in order to inform conflict prevention. . forecasted quarterly and yearly estimates of indicators."
Mass Mobilization Model,Department of State,STATE,CSO,Other,Unknown,"A machine learning forecasting model that uses open-source political, social, and economic datasets to predict mass mobilizations (protests and riots) for each country globally. ",Forecasted yearly estimates.,Implementation and Assessment,Neither,1/1/2023,1/1/2024,1/1/2024,Developed with both contracting and in-house resources.,Unknown,No,Unknown,No,No,Unknown,Unknown,Documentation is complete,No,Unknown,Unknown,Unknown,Unknown,Yes,Other,Other,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.38095238095238093,"A machine learning forecasting model that uses open-source political, social, and economic datasets to predict mass mobilizations (protests and riots) for each country globally. . Forecasted yearly estimates.","a machine learning forecasting model that uses open-source political, social, and economic datasets to predict mass mobilizations (protests and riots) for each country globally. . forecasted yearly estimates."
ForeignAssistance.gov Processing for Mismatched Data,Department of State,STATE,F,Other,Unknown,Created a custom Natural Language Processing (NLP) model to recommend information for tagging and identify mismatched data. ,Potential mismatches and suggested tags.,Acquisition and/or Development,Neither,6/1/2023,7/24/2023,7/6/2023,Developed with both contracting and in-house resources.,Unknown,No,Unknown,Unknown,Unknown,Unknown,Foreign Assistance Obligations and Disbursements.,Yes,Unknown,Unknown,Unknown,Unknown,Unknown,No,Unknown,Unknown,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3492063492063492,Created a custom Natural Language Processing (NLP) model to recommend information for tagging and identify mismatched data. . Potential mismatches and suggested tags.,created a custom natural language processing (nlp) model to recommend information for tagging and identify mismatched data. . potential mismatches and suggested tags.
J Reports Data Collection & Management Tool (DCT),Department of State,STATE,J,Diplomacy & Trade; Mission-Enabling (internal agency support),Unknown,"The DCT is an AI-driven research assistant designed to increase the quality and maintain the integrity of select Congressional reports while reducing the labor hours it takes to generate them. The tool automates the summarization, translation, and categorization of user-submitted content into a searchable database, replacing the manual tracking and organizing of research.  Users can upload materials throughout the year and easily retrieve content during drafting season. ","The DCT provides AI-generated summaries and translations of user-submitted research.  It also suggests categories to organize the research according to instructions. The DCT uses optical character recognition (OCR) to digitize images and documents (e.g., PDFs).",Implementation and Assessment,Neither,4/1/2023,6/20/2023,11/29/2023,Developed with both contracting and in-house resources.,Unknown,No,Unknown,No,No,Unknown,"DCT Evaluation Data: An internal research dataset of user-submitted articles, reviewed and labeled by subject matter experts to facilitate model evaluation.",Documentation is widely available,Yes,Unknown,Data.State-SBU,Data.State-SBU,Unknown,Yes,Yes,Yes,Unknown,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"The DCT is an AI-driven research assistant designed to increase the quality and maintain the integrity of select Congressional reports while reducing the labor hours it takes to generate them. The tool automates the summarization, translation, and categorization of user-submitted content into a searchable database, replacing the manual tracking and organizing of research.  Users can upload materials throughout the year and easily retrieve content during drafting season. . The DCT provides AI-generated summaries and translations of user-submitted research.  It also suggests categories to organize the research according to instructions. The DCT uses optical character recognition (OCR) to digitize images and documents (e.g., PDFs).","the dct is an ai-driven research assistant designed to increase the quality and maintain the integrity of select congressional reports while reducing the labor hours it takes to generate them. the tool automates the summarization, translation, and categorization of user-submitted content into a searchable database, replacing the manual tracking and organizing of research. users can upload materials throughout the year and easily retrieve content during drafting season. . the dct provides ai-generated summaries and translations of user-submitted research. it also suggests categories to organize the research according to instructions. the dct uses optical character recognition (ocr) to digitize images and documents (e.g., pdfs)."
FOIA 360 AI Matching Tool,Department of State,STATE,M/SS/CFA,Other,Unknown,This system aims to make the Department of State's FOIA process better by spotting similar requests and documents to cut down on duplication. It also helps find patterns in both current and new requests to speed up response times for the public.,Improved outcomes.,Implementation and Assessment,Neither,3/2/2023,6/5/2023,4/1/2024,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Unknown,FOIA request text from FOIA Express and documents captured from the Department's Virtual FOIA Reading Room are used for training and testing,Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",Yes,ElasticSearch,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,This system aims to make the Department of State's FOIA process better by spotting similar requests and documents to cut down on duplication. It also helps find patterns in both current and new requests to speed up response times for the public. . Improved outcomes.,this system aims to make the department of state's foia process better by spotting similar requests and documents to cut down on duplication. it also helps find patterns in both current and new requests to speed up response times for the public. . improved outcomes.
StateChat,Department of State,STATE,M/SS/CFA,Mission-Enabling (internal agency support),Unknown,StateChat is the Department's enterprise Generative AI-powered chatbot.,"StateChat is a chatbot, its outputs are textual in a chat interface.",Implementation and Assessment,Neither,1/1/2024,2/1/2024,3/1/2024,Developed with both contracting and in-house resources.,Unknown,No,No,Yes,Yes,Yes,The model is not trained or fine-tuned with agency data; it is provided various types of agency data as context.  ,Documentation is widely available,Yes,Unknown,Yes,StateChat,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"StateChat is the Department's enterprise Generative AI-powered chatbot. . StateChat is a chatbot, its outputs are textual in a chat interface.","statechat is the department's enterprise generative ai-powered chatbot. . statechat is a chatbot, its outputs are textual in a chat interface."
NLP to pull key information from unstructured text,Department of State,STATE,PM,Mission-Enabling (internal agency support),Unknown,Natural language processing (NLP) to pull key information from unstructured text.  Use NLP to extract information such as country names and agreement dates from dozens of pages of unstructured PDF document.,Dataset with key information from unstructured data.,Implementation and Assessment,Neither,4/1/2022,4/15/2022,9/30/2022,Developed with both contracting and in-house resources.,Unknown,No,Unknown,No,No,Unknown,Unknown,Documentation is complete,Unknown,Unknown,Unknown,Unknown,Less than 6 months,Unknown,Unknown,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3492063492063492,Natural language processing (NLP) to pull key information from unstructured text.  Use NLP to extract information such as country names and agreement dates from dozens of pages of unstructured PDF document. . Dataset with key information from unstructured data.,natural language processing (nlp) to pull key information from unstructured text. use nlp to extract information such as country names and agreement dates from dozens of pages of unstructured pdf document. . dataset with key information from unstructured data.
Storyzy,Department of State,STATE,R/GEC,Other,None of the above.,Improve detected use of synthetic content.- Synthetic content is computer-generated data that mimics real-world data.,Improved outcomes.,Operation and Maintenance,Neither,6/1/2023,7/17/2023,12/1/2023,Developed with both contracting and in-house resources.,Unknown,No,Unknown,No,No,Unknown,Open source data from social media.,No,No,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,Improve detected use of synthetic content.- Synthetic content is computer-generated data that mimics real-world data. . Improved outcomes.,improve detected use of synthetic content.- synthetic content is computer-generated data that mimics real-world data. . improved outcomes.
Digital Media Analytics Platform,Department of State,STATE,R/GPA/RA,Mission-Enabling (internal agency support),Unknown,Using open-source neural machine translation models to translate global media articles and Department and public foreign social media posts into English. ,Trend analysis,Implementation and Assessment,Neither,11/1/2022,12/1/2022,1/1/2024,Developed with both contracting and in-house resources.,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Yes,"Yes – agency has access to source code, but it is not public.",Unknown,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3333333333333333,Using open-source neural machine translation models to translate global media articles and Department and public foreign social media posts into English. . Trend analysis,using open-source neural machine translation models to translate global media articles and department and public foreign social media posts into english. . trend analysis
Collections Chatbot,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"This project implemented the SBSE chatbot, which provides self-service Frequently Asked Questions (FAQ) to taxpayers regarding payments, payment plans, financial relief and offer in compromise topics. Three new topics are currently in development and scheduled to be incorporated into the chatbot on 3/13/2025: Automated Substitute for Return (ASFR), Centralized Lien Operation (CLO), and Withholding Compliance (WHC). 

The Intent Engine (using Natural Language Processing (NLP) and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests).  The answers provided to the taxpayers are not created by Generative AI. The responses of the chatbot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response.","The NLP algorithm functions by classifying user inputs into predefined topics using advanced natural language processing techniques. It evaluates the input against a set of trained models and assigns a response based on a confidence score calculated by the algorithm. Only responses associated with these pre-established topics are provided, ensuring accuracy and consistency. The algorithm does not generate new or ad-hoc answers outside the defined topics, making it reliable for controlled environments where precision and adherence to predetermined content are critical.",Operation and Maintenance,Neither,4/1/2021,6/1/2021,12/10/2021,Developed with contracting resources.,Contract # NNG15SD27B,No,No,No,Yes,Yes,Unknown,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,eGain Suite,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"This project implemented the SBSE chatbot, which provides self-service Frequently Asked Questions (FAQ) to taxpayers regarding payments, payment plans, financial relief and offer in compromise topics. Three new topics are currently in development and scheduled to be incorporated into the chatbot on 3/13/2025: Automated Substitute for Return (ASFR), Centralized Lien Operation (CLO), and Withholding Compliance (WHC). 

The Intent Engine (using Natural Language Processing (NLP) and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests).  The answers provided to the taxpayers are not created by Generative AI. The responses of the chatbot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response. . The NLP algorithm functions by classifying user inputs into predefined topics using advanced natural language processing techniques. It evaluates the input against a set of trained models and assigns a response based on a confidence score calculated by the algorithm. Only responses associated with these pre-established topics are provided, ensuring accuracy and consistency. The algorithm does not generate new or ad-hoc answers outside the defined topics, making it reliable for controlled environments where precision and adherence to predetermined content are critical.","this project implemented the sbse chatbot, which provides self-service frequently asked questions (faq) to taxpayers regarding payments, payment plans, financial relief and offer in compromise topics. three new topics are currently in development and scheduled to be incorporated into the chatbot on 3/13/2025: automated substitute for return (asfr), centralized lien operation (clo), and withholding compliance (whc). the intent engine (using natural language processing (nlp) and ai algorithms) classifies taxpayers' utterances (questions) into intents (requests). the answers provided to the taxpayers are not created by generative ai. the responses of the chatbot and business logic is predetermined by content owners. this ai tool aims to classify/navigate to a correct predetermined response. . the nlp algorithm functions by classifying user inputs into predefined topics using advanced natural language processing techniques. it evaluates the input against a set of trained models and assigns a response based on a confidence score calculated by the algorithm. only responses associated with these pre-established topics are provided, ensuring accuracy and consistency. the algorithm does not generate new or ad-hoc answers outside the defined topics, making it reliable for controlled environments where precision and adherence to predetermined content are critical."
Systran Translation Software,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"Systran (SYSTRAN Pure neural Server) language translation software provides language translation capability. It is a Commercial Off-the-Shelf (COTS) product, and version 9 is deployed. Users only use their documentation that is in either English or Non-English, and Systran will translate the content to target language. No documents are stored in Systran. Users are typically IRS staff requiring translation of Non-English to English for cursory evaluation (Triage) of relevancy to case work.  Some users are translating English to a Non-English language as a beginning for manual translation. The Neural Network based translation engine can be augmented with dictionary and translation memories to bolster domain specific language accuracy, and dictionary created during a MITRE task order was added in c. 2020.",Translated content (in target language picked by user) of the source (input) text submitted by the user.  User then can read and review the translation on either a graphical user interface (GUI) field or export. ,Operation and Maintenance,Neither,9/1/2020,9/1/2020,9/1/2020,Developed with contracting resources.,2043FY-22-F-00078 P00002,No,No,No,Yes,No,Publication 850 and other official translation pairs received from IRS' Human Translation team were used to add this information as translation memories data source to base translation engine.  All such information is non-sensitive. ,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",No,No – agency does not have access to source code.,Yes,RAS-1,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Systran (SYSTRAN Pure neural Server) language translation software provides language translation capability. It is a Commercial Off-the-Shelf (COTS) product, and version 9 is deployed. Users only use their documentation that is in either English or Non-English, and Systran will translate the content to target language. No documents are stored in Systran. Users are typically IRS staff requiring translation of Non-English to English for cursory evaluation (Triage) of relevancy to case work.  Some users are translating English to a Non-English language as a beginning for manual translation. The Neural Network based translation engine can be augmented with dictionary and translation memories to bolster domain specific language accuracy, and dictionary created during a MITRE task order was added in c. 2020. . Translated content (in target language picked by user) of the source (input) text submitted by the user.  User then can read and review the translation on either a graphical user interface (GUI) field or export.","systran (systran pure neural server) language translation software provides language translation capability. it is a commercial off-the-shelf (cots) product, and version 9 is deployed. users only use their documentation that is in either english or non-english, and systran will translate the content to target language. no documents are stored in systran. users are typically irs staff requiring translation of non-english to english for cursory evaluation (triage) of relevancy to case work. some users are translating english to a non-english language as a beginning for manual translation. the neural network based translation engine can be augmented with dictionary and translation memories to bolster domain specific language accuracy, and dictionary created during a mitre task order was added in c. 2020. . translated content (in target language picked by user) of the source (input) text submitted by the user. user then can read and review the translation on either a graphical user interface (gui) field or export."
Automated Underreporter (AUR) Voicebot,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"The AUR Voice Bot will authenticate taxpayers using “Shared Secrets” and answer questions requiring account look ups and/or updates. It will give answers to provide confirmation we received their response, the current status of their case, and grant requests for additional time to respond if they are eligible.",The system produces a predefined response that provides taxpayers with information related to their account status or current case details after authentication. It also helps route calls to appropriate service areas or escalate to live agents when necessary.,Operation and Maintenance,Neither,2/9/2023,9/25/2023,5/22/2024,Developed with contracting resources.,2032H5-19-F-00641,No,No,No,Yes,Yes,"Uses historical taxpayer interaction data, such as account inquiries and case resolution logs, to train and evaluate performance. Metadata from AUR case files and voice interaction transcripts.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",No,No – agency does not have access to source code.,Yes,Nuance Digital Contact Center Platform (DCCP),Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The AUR Voice Bot will authenticate taxpayers using “Shared Secrets” and answer questions requiring account look ups and/or updates. It will give answers to provide confirmation we received their response, the current status of their case, and grant requests for additional time to respond if they are eligible. . The system produces a predefined response that provides taxpayers with information related to their account status or current case details after authentication. It also helps route calls to appropriate service areas or escalate to live agents when necessary.","the aur voice bot will authenticate taxpayers using “shared secrets” and answer questions requiring account look ups and/or updates. it will give answers to provide confirmation we received their response, the current status of their case, and grant requests for additional time to respond if they are eligible. . the system produces a predefined response that provides taxpayers with information related to their account status or current case details after authentication. it also helps route calls to appropriate service areas or escalate to live agents when necessary."
Employee Resource Center (ERC) Chatbot,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"This project implemented the ERC chatbot, which provides self-service FAQs to IRS employees regarding administrative support, benefit topics, credit card services, HRConnect questions, retirement, insurance, and various other topics related to the ERC. The Intent Engine (using NLP and AI algorithms) classifies users' utterances (questions) into intents (requests).  The answers provided to the taxpayers are not created by Generative AI. The responses of the chatbot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response.","The NLP algorithm functions by classifying user inputs into predefined topics using advanced natural language processing techniques. It evaluates the input against a set of trained models and assigns a response based on a confidence score calculated by the algorithm. Only responses associated with these pre-established topics are provided, ensuring accuracy and consistency. The algorithm does not generate new or ad-hoc answers outside the defined topics, making it reliable for controlled environments where precision and adherence to predetermined content are critical.",Operation and Maintenance,Neither,9/30/2022,3/1/2023,7/20/2023,Developed with contracting resources.,Contract # NNG15SD27B,No,No,No,Yes,Yes,Project utilized data provided by content Subject Matter Experts to evaluate the accuracy of responses.,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,eGain Suite,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"This project implemented the ERC chatbot, which provides self-service FAQs to IRS employees regarding administrative support, benefit topics, credit card services, HRConnect questions, retirement, insurance, and various other topics related to the ERC. The Intent Engine (using NLP and AI algorithms) classifies users' utterances (questions) into intents (requests).  The answers provided to the taxpayers are not created by Generative AI. The responses of the chatbot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response. . The NLP algorithm functions by classifying user inputs into predefined topics using advanced natural language processing techniques. It evaluates the input against a set of trained models and assigns a response based on a confidence score calculated by the algorithm. Only responses associated with these pre-established topics are provided, ensuring accuracy and consistency. The algorithm does not generate new or ad-hoc answers outside the defined topics, making it reliable for controlled environments where precision and adherence to predetermined content are critical.","this project implemented the erc chatbot, which provides self-service faqs to irs employees regarding administrative support, benefit topics, credit card services, hrconnect questions, retirement, insurance, and various other topics related to the erc. the intent engine (using nlp and ai algorithms) classifies users' utterances (questions) into intents (requests). the answers provided to the taxpayers are not created by generative ai. the responses of the chatbot and business logic is predetermined by content owners. this ai tool aims to classify/navigate to a correct predetermined response. . the nlp algorithm functions by classifying user inputs into predefined topics using advanced natural language processing techniques. it evaluates the input against a set of trained models and assigns a response based on a confidence score calculated by the algorithm. only responses associated with these pre-established topics are provided, ensuring accuracy and consistency. the algorithm does not generate new or ad-hoc answers outside the defined topics, making it reliable for controlled environments where precision and adherence to predetermined content are critical."
Robotic Process Automation for Form 941-X Extraction,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"What is the purpose/objective of this AI project? 
Intelligent Data extraction from scanned 941X forms
At a high level, how does it accomplish its purpose? 
Digital processing of Form 941X using Artificial Intelligence and Intelligent Document Processing to read scanned forms, extract data in usable format and distribute the data to downstream systems.
What is the decision or judgment that this model automates? 
Learning and improving character recognition while extracting the data. Decision is about recognizing the extracted character correctly.
Are humans in the loop in reviewing the AI output prior to taking action on the results? If so, how? 
Yes, When the character extraction confidence scores are lower, the form is diverted to UiPath Action Center, where a Human in the loop reviews the extracted data and confirms/corrects before submitting.
What benefits are anticipated to come from this project? 
Speed, volume, and Accuracy in data extraction from scanned taxpayer forms
What is the direct or ultimate impact of this decision? 
Speed of data extraction from paper forms and clearing the backlog of 941x forms to be processed.
What is the population impacted by this decision?
Data Transcribes can transcribe a lot more forms at a much faster rate than manual process.",The system will extract text from the forms (pdf's) translating handwritten text into a digital format. After the extraction process the system presents what was extracted to the end user to then validate before anything is sent to any downstream systems.,Operation and Maintenance,Neither,9/11/2023,9/11/2023,11/20/2023,Developed with both contracting and in-house resources.,2032H5-21-F-00495 Blue Tech Number / 2032H5-22-F-00922 P00003 Accenture Contract ,No,No,No,Yes,No,"The forms (Form 940, Form 941, and Form 1040) are owned by the IRS.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Yes,Robotic Process Automation Platform,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"What is the purpose/objective of this AI project? 
Intelligent Data extraction from scanned 941X forms
At a high level, how does it accomplish its purpose? 
Digital processing of Form 941X using Artificial Intelligence and Intelligent Document Processing to read scanned forms, extract data in usable format and distribute the data to downstream systems.
What is the decision or judgment that this model automates? 
Learning and improving character recognition while extracting the data. Decision is about recognizing the extracted character correctly.
Are humans in the loop in reviewing the AI output prior to taking action on the results? If so, how? 
Yes, When the character extraction confidence scores are lower, the form is diverted to UiPath Action Center, where a Human in the loop reviews the extracted data and confirms/corrects before submitting.
What benefits are anticipated to come from this project? 
Speed, volume, and Accuracy in data extraction from scanned taxpayer forms
What is the direct or ultimate impact of this decision? 
Speed of data extraction from paper forms and clearing the backlog of 941x forms to be processed.
What is the population impacted by this decision?
Data Transcribes can transcribe a lot more forms at a much faster rate than manual process. . The system will extract text from the forms (pdf's) translating handwritten text into a digital format. After the extraction process the system presents what was extracted to the end user to then validate before anything is sent to any downstream systems.","what is the purpose/objective of this ai project? intelligent data extraction from scanned 941x forms at a high level, how does it accomplish its purpose? digital processing of form 941x using artificial intelligence and intelligent document processing to read scanned forms, extract data in usable format and distribute the data to downstream systems. what is the decision or judgment that this model automates? learning and improving character recognition while extracting the data. decision is about recognizing the extracted character correctly. are humans in the loop in reviewing the ai output prior to taking action on the results? if so, how? yes, when the character extraction confidence scores are lower, the form is diverted to uipath action center, where a human in the loop reviews the extracted data and confirms/corrects before submitting. what benefits are anticipated to come from this project? speed, volume, and accuracy in data extraction from scanned taxpayer forms what is the direct or ultimate impact of this decision? speed of data extraction from paper forms and clearing the backlog of 941x forms to be processed. what is the population impacted by this decision? data transcribes can transcribe a lot more forms at a much faster rate than manual process. . the system will extract text from the forms (pdf's) translating handwritten text into a digital format. after the extraction process the system presents what was extracted to the end user to then validate before anything is sent to any downstream systems."
Taxpayer Services Chatbot,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"This project implemented the W&I chatbot, which provides self-service Frequently Asked Questions (FAQ) to taxpayers regarding refunds and Advanced Child Tax Credit topics. The Intent Engine (using Natural Language Processing (NLP) and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests). The answers provided to the taxpayers are not created by Generative AI. The responses of the chatbot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response.","The NLP algorithm functions by classifying user inputs into predefined topics using advanced natural language processing techniques. It evaluates the input against a set of trained models and assigns a response based on a confidence score calculated by the algorithm. Only responses associated with these pre-established topics are provided, ensuring accuracy and consistency. The algorithm does not generate new or ad-hoc answers outside the defined topics, making it reliable for controlled environments where precision and adherence to predetermined content are critical.

",Operation and Maintenance,Neither,2/1/2022,6/9/2022,3/14/2023,Developed with contracting resources.,Contract # NNG15SD27B,No,No,No,Yes,Yes,Project utilized data provided by content Subject Matter Experts to evaluate the accuracy of responses.,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,eGain Suite,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"This project implemented the W&I chatbot, which provides self-service Frequently Asked Questions (FAQ) to taxpayers regarding refunds and Advanced Child Tax Credit topics. The Intent Engine (using Natural Language Processing (NLP) and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests). The answers provided to the taxpayers are not created by Generative AI. The responses of the chatbot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response. . The NLP algorithm functions by classifying user inputs into predefined topics using advanced natural language processing techniques. It evaluates the input against a set of trained models and assigns a response based on a confidence score calculated by the algorithm. Only responses associated with these pre-established topics are provided, ensuring accuracy and consistency. The algorithm does not generate new or ad-hoc answers outside the defined topics, making it reliable for controlled environments where precision and adherence to predetermined content are critical.","this project implemented the w&i chatbot, which provides self-service frequently asked questions (faq) to taxpayers regarding refunds and advanced child tax credit topics. the intent engine (using natural language processing (nlp) and ai algorithms) classifies taxpayers' utterances (questions) into intents (requests). the answers provided to the taxpayers are not created by generative ai. the responses of the chatbot and business logic is predetermined by content owners. this ai tool aims to classify/navigate to a correct predetermined response. . the nlp algorithm functions by classifying user inputs into predefined topics using advanced natural language processing techniques. it evaluates the input against a set of trained models and assigns a response based on a confidence score calculated by the algorithm. only responses associated with these pre-established topics are provided, ensuring accuracy and consistency. the algorithm does not generate new or ad-hoc answers outside the defined topics, making it reliable for controlled environments where precision and adherence to predetermined content are critical."
Winnie Chatbot for Employee IT FAQs,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"This project implemented the Winnie chatbot, which provides self-service Frequently Asked Questions (FAQ) to IRS staff regarding various Information Technology (IT) topics. The Intent Engine (using Natural Language Processing (NLP) and AI algorithms) classifies users' utterances (questions) into intents (requests). The answers provided to the users are not created by Generative AI. The responses of the chatbot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response.","The NLP algorithm functions by classifying user inputs into predefined topics using advanced natural language processing techniques. It evaluates the input against a set of trained models and assigns a response based on a confidence score calculated by the algorithm. Only responses associated with these pre-established topics are provided, ensuring accuracy and consistency. The algorithm does not generate new or ad-hoc answers outside the defined topics, making it reliable for controlled environments where precision and adherence to predetermined content are critical.",Operation and Maintenance,Neither,12/5/2022,9/26/2023,9/26/2023,Developed with contracting resources.,Contract # NNG15SD27B,No,No,No,Yes,No,Project utilized data provided by content Subject Matter Experts to evaluate the accuracy of responses.,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,eGain Suite,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"This project implemented the Winnie chatbot, which provides self-service Frequently Asked Questions (FAQ) to IRS staff regarding various Information Technology (IT) topics. The Intent Engine (using Natural Language Processing (NLP) and AI algorithms) classifies users' utterances (questions) into intents (requests). The answers provided to the users are not created by Generative AI. The responses of the chatbot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response. . The NLP algorithm functions by classifying user inputs into predefined topics using advanced natural language processing techniques. It evaluates the input against a set of trained models and assigns a response based on a confidence score calculated by the algorithm. Only responses associated with these pre-established topics are provided, ensuring accuracy and consistency. The algorithm does not generate new or ad-hoc answers outside the defined topics, making it reliable for controlled environments where precision and adherence to predetermined content are critical.","this project implemented the winnie chatbot, which provides self-service frequently asked questions (faq) to irs staff regarding various information technology (it) topics. the intent engine (using natural language processing (nlp) and ai algorithms) classifies users' utterances (questions) into intents (requests). the answers provided to the users are not created by generative ai. the responses of the chatbot and business logic is predetermined by content owners. this ai tool aims to classify/navigate to a correct predetermined response. . the nlp algorithm functions by classifying user inputs into predefined topics using advanced natural language processing techniques. it evaluates the input against a set of trained models and assigns a response based on a confidence score calculated by the algorithm. only responses associated with these pre-established topics are provided, ensuring accuracy and consistency. the algorithm does not generate new or ad-hoc answers outside the defined topics, making it reliable for controlled environments where precision and adherence to predetermined content are critical."
DATA Act Bot for Procurement Data Matching,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"Purpose:  The Digital Accountability and Transparency Act (DATA) Act Bot automates verifying that IRS Federal Procurement Data System (FPDS) reporting matches the information in contract documents (e.g. dollar amounts, dates, location of work). Supervised learning and natural language processing are used to extract unstructured information from contract documents. F1 accuracy scores are used to measure performance of validation models for each specific data element.

Benefits: The AI improves operational efficiency by validating consistency and accuracy of contract metadata.","AI model validates that contract spending information reported to USASpending.gov matches contract documents. The system validates the consistency of contract metadata such as contract number, modification number, dollar amounts, contract work / place of performance location address, and contract dates.",Operation and Maintenance,Neither,9/26/2019,9/26/2019,3/31/2022,Developed with contracting resources.,2032H822F00222,No,No,No,Yes,Yes,"IRS contract documents were used to train the model. Project team members manually annotated or created labeled data. Created dataset of information extracted from contract documents (e.g. dates, dollar amounts, addresses). This dataset is used to improve the accuracy and reliability of contract spending reporting.","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Integrated Financial System / Procurement for the Public Sector,More than 12 months,Other,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"Purpose:  The Digital Accountability and Transparency Act (DATA) Act Bot automates verifying that IRS Federal Procurement Data System (FPDS) reporting matches the information in contract documents (e.g. dollar amounts, dates, location of work). Supervised learning and natural language processing are used to extract unstructured information from contract documents. F1 accuracy scores are used to measure performance of validation models for each specific data element.

Benefits: The AI improves operational efficiency by validating consistency and accuracy of contract metadata. . AI model validates that contract spending information reported to USASpending.gov matches contract documents. The system validates the consistency of contract metadata such as contract number, modification number, dollar amounts, contract work / place of performance location address, and contract dates.","purpose: the digital accountability and transparency act (data) act bot automates verifying that irs federal procurement data system (fpds) reporting matches the information in contract documents (e.g. dollar amounts, dates, location of work). supervised learning and natural language processing are used to extract unstructured information from contract documents. f1 accuracy scores are used to measure performance of validation models for each specific data element. benefits: the ai improves operational efficiency by validating consistency and accuracy of contract metadata. . ai model validates that contract spending information reported to usaspending.gov matches contract documents. the system validates the consistency of contract metadata such as contract number, modification number, dollar amounts, contract work / place of performance location address, and contract dates."
Taxpayer Services Form 1040 Voicebot,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"This project is implementing 1040 IVR voice bot for Taxpayer Sevices (TS), formally known as Wage & Investments (W&I), which allows them to check their individual tax information, such as Late Filing Season Individual Income Tax Return Menu, Non-Filing Season Individual Income Tax Return Menu. Taxpayer could be directed to the appropriate Customer Service Representative based on their request. . The Intent Engine (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests).  The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response.",Provide caller with information based on business logic/rules.,Operation and Maintenance,Neither,2/21/2023,11/13/2023,3/5/2024,Developed with contracting resources.,2032H5-19-F-00641,No,No,No,Yes,Yes,Call summary/call detail records,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,Nuance Digital Contact Center Platform (DCCP),Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"This project is implementing 1040 IVR voice bot for Taxpayer Sevices (TS), formally known as Wage & Investments (W&I), which allows them to check their individual tax information, such as Late Filing Season Individual Income Tax Return Menu, Non-Filing Season Individual Income Tax Return Menu. Taxpayer could be directed to the appropriate Customer Service Representative based on their request. . The Intent Engine (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests).  The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response. . Provide caller with information based on business logic/rules.","this project is implementing 1040 ivr voice bot for taxpayer sevices (ts), formally known as wage & investments (w&i), which allows them to check their individual tax information, such as late filing season individual income tax return menu, non-filing season individual income tax return menu. taxpayer could be directed to the appropriate customer service representative based on their request. . the intent engine (using nlp and ai algorithms) classifies taxpayers' utterances (questions) into intents (requests). the responses of the voicebot and business logic is predetermined by content owners. this ai tool aims to classify/navigate to a correct predetermined response. . provide caller with information based on business logic/rules."
Where's My Refund and Where's My Amended Return Voicebot,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"This project is implementing Where's My Refund (WMR) and Where's My Ammended Return (WMAR) voice bots for Accounts Management (AM). WMR voicebot allows taxpayers to obtain the status of their refund and fact of filing information. Taxpayers can initiate a Refund Trace, or re-issuance of their refund check, for their current year refunds. WMAR voicebot allows taxpayers to request the status of their amended tax return. The Intent Engine (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests). The answers provided to the taxpayers are not created by Generative AI. The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response.",Provider caller with status on their WMR/WMAR.,Operation and Maintenance,Neither,4/1/2022,3/7/2023,1/4/2024,Developed with contracting resources.,2032H5-19-F-00641,No,No,No,Yes,Yes,Call summary and call detail data.,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,Nuance Digital Contact Center Platform (DCCP),Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"This project is implementing Where's My Refund (WMR) and Where's My Ammended Return (WMAR) voice bots for Accounts Management (AM). WMR voicebot allows taxpayers to obtain the status of their refund and fact of filing information. Taxpayers can initiate a Refund Trace, or re-issuance of their refund check, for their current year refunds. WMAR voicebot allows taxpayers to request the status of their amended tax return. The Intent Engine (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests). The answers provided to the taxpayers are not created by Generative AI. The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response. . Provider caller with status on their WMR/WMAR.","this project is implementing where's my refund (wmr) and where's my ammended return (wmar) voice bots for accounts management (am). wmr voicebot allows taxpayers to obtain the status of their refund and fact of filing information. taxpayers can initiate a refund trace, or re-issuance of their refund check, for their current year refunds. wmar voicebot allows taxpayers to request the status of their amended tax return. the intent engine (using nlp and ai algorithms) classifies taxpayers' utterances (questions) into intents (requests). the answers provided to the taxpayers are not created by generative ai. the responses of the voicebot and business logic is predetermined by content owners. this ai tool aims to classify/navigate to a correct predetermined response. . provider caller with status on their wmr/wmar."
Simulation of the Nation (SimoN) Synthetic Data Engine,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"The Internal Revenue Service (IRS) Enterprise Services (ES), Enterprise Systems Testing (EST) is the testing authority for Information Technology (IT) and provides enterprise-wide testing solutions and critical support across the enterprise for numerous systems and applications.  

The Synthetic Data Engine, or Simulation of the Nation (SimoN) is designed to generate volumes of synthetic tax data to represent a wide range of use cases, without copying Federal Tax Information (FTI) from production systems is an IRS directive.  SimoN is also able to age data to support multi-year tax code concerns for more comprehensive testing sequences spanning multiple tax years. This will improve the volume, diversity and quality of test data being created and processed in a systemic manner. To date, SimoN has produced over 700,000 unique and valid Individual Master File (IMF) tax returns, and over 50,000 unique and valid Business Master File (BMF) returns for testing. SimoN also supports Social Security Administration (SSA) interconnects and SS4 interconnects which allow the data produced to be fully linked between business and individuals. Over 320 dimensions of socioeconomic data are synthesized and validated against United States (U.S.) Census metrics to ensure a validated and realistic synthetic dataset. 

The SimoN solution consumes publicly available statistical data about businesses and the population of the U.S. This includes tax household composition, income and education levels and other data points. The solution combines statistical data with Artificial Intelligence (AI) and Machine Learning (ML) to generate businesses and simulated households with a variety of tax scenarios. The solution then stores the business and population data in a database.  The data is extracted to files for submission to the test environment systems simulating eFile and SSA feeds with no review of data created.

At no point, is Production data (i.e., Personally Identifiable Information (PII), Federal Taxpayer Information (FTI) or Controlled Unclassified Information (CUI) used as input to the Artificial Intelligence/Machine Language (AI/ML) process of SimoN, nor does the simulated output data feed Production systems. In addition to generating synthetic data, this system is designed to simulate the aging of the population. The aging model includes a variety of socioeconomic events, such as births, deaths, marriages, job changes and address changes. SimoN also has multiple levels of automated testing and data validations to ensure the data produced is usable by IRS test systems and simulates businesses and population.","SimoN is capable of outputting 3 tax years of IMF and BMF returns starting in Tax Year 2022. With each XML Schema Definition (XSD) that is released under the Modernized e-File (MeF), SimoN implements each XSD version and provides full regression support for testing in successive years. Each XSD version has automated tests that run to ensure verifications can be run to reduce the likelihood of AI generated synthetic returns that might have data anomalies. 

Also, to help seed test systems with synthetic individuals and synthetic businesses, SimoN can output the Data Master 1 (DM1) file simulating a feed from Social Security Administration (SSA) and Application for Employer Identification Number file (Form SS-4) containing transcribed form information so that the test system can be initialized with the appropriate backend reference data for validation of IMF and BMF entities, as well as the ability to validate incoming synthetic tax returns.",Operation and Maintenance,Neither,4/4/2021,10/1/2021,1/31/2024,Developed with contracting resources.,2032H5-22-C-00097,No,No,No,Yes,No,"None, all data used for unsupervised model training is based on public data sets from the Census Bureau.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,6-12 months,Yes,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"The Internal Revenue Service (IRS) Enterprise Services (ES), Enterprise Systems Testing (EST) is the testing authority for Information Technology (IT) and provides enterprise-wide testing solutions and critical support across the enterprise for numerous systems and applications.  

The Synthetic Data Engine, or Simulation of the Nation (SimoN) is designed to generate volumes of synthetic tax data to represent a wide range of use cases, without copying Federal Tax Information (FTI) from production systems is an IRS directive.  SimoN is also able to age data to support multi-year tax code concerns for more comprehensive testing sequences spanning multiple tax years. This will improve the volume, diversity and quality of test data being created and processed in a systemic manner. To date, SimoN has produced over 700,000 unique and valid Individual Master File (IMF) tax returns, and over 50,000 unique and valid Business Master File (BMF) returns for testing. SimoN also supports Social Security Administration (SSA) interconnects and SS4 interconnects which allow the data produced to be fully linked between business and individuals. Over 320 dimensions of socioeconomic data are synthesized and validated against United States (U.S.) Census metrics to ensure a validated and realistic synthetic dataset. 

The SimoN solution consumes publicly available statistical data about businesses and the population of the U.S. This includes tax household composition, income and education levels and other data points. The solution combines statistical data with Artificial Intelligence (AI) and Machine Learning (ML) to generate businesses and simulated households with a variety of tax scenarios. The solution then stores the business and population data in a database.  The data is extracted to files for submission to the test environment systems simulating eFile and SSA feeds with no review of data created.

At no point, is Production data (i.e., Personally Identifiable Information (PII), Federal Taxpayer Information (FTI) or Controlled Unclassified Information (CUI) used as input to the Artificial Intelligence/Machine Language (AI/ML) process of SimoN, nor does the simulated output data feed Production systems. In addition to generating synthetic data, this system is designed to simulate the aging of the population. The aging model includes a variety of socioeconomic events, such as births, deaths, marriages, job changes and address changes. SimoN also has multiple levels of automated testing and data validations to ensure the data produced is usable by IRS test systems and simulates businesses and population. . SimoN is capable of outputting 3 tax years of IMF and BMF returns starting in Tax Year 2022. With each XML Schema Definition (XSD) that is released under the Modernized e-File (MeF), SimoN implements each XSD version and provides full regression support for testing in successive years. Each XSD version has automated tests that run to ensure verifications can be run to reduce the likelihood of AI generated synthetic returns that might have data anomalies. 

Also, to help seed test systems with synthetic individuals and synthetic businesses, SimoN can output the Data Master 1 (DM1) file simulating a feed from Social Security Administration (SSA) and Application for Employer Identification Number file (Form SS-4) containing transcribed form information so that the test system can be initialized with the appropriate backend reference data for validation of IMF and BMF entities, as well as the ability to validate incoming synthetic tax returns.","the internal revenue service (irs) enterprise services (es), enterprise systems testing (est) is the testing authority for information technology (it) and provides enterprise-wide testing solutions and critical support across the enterprise for numerous systems and applications. the synthetic data engine, or simulation of the nation (simon) is designed to generate volumes of synthetic tax data to represent a wide range of use cases, without copying federal tax information (fti) from production systems is an irs directive. simon is also able to age data to support multi-year tax code concerns for more comprehensive testing sequences spanning multiple tax years. this will improve the volume, diversity and quality of test data being created and processed in a systemic manner. to date, simon has produced over 700,000 unique and valid individual master file (imf) tax returns, and over 50,000 unique and valid business master file (bmf) returns for testing. simon also supports social security administration (ssa) interconnects and ss4 interconnects which allow the data produced to be fully linked between business and individuals. over 320 dimensions of socioeconomic data are synthesized and validated against united states (u.s.) census metrics to ensure a validated and realistic synthetic dataset. the simon solution consumes publicly available statistical data about businesses and the population of the u.s. this includes tax household composition, income and education levels and other data points. the solution combines statistical data with artificial intelligence (ai) and machine learning (ml) to generate businesses and simulated households with a variety of tax scenarios. the solution then stores the business and population data in a database. the data is extracted to files for submission to the test environment systems simulating efile and ssa feeds with no review of data created. at no point, is production data (i.e., personally identifiable information (pii), federal taxpayer information (fti) or controlled unclassified information (cui) used as input to the artificial intelligence/machine language (ai/ml) process of simon, nor does the simulated output data feed production systems. in addition to generating synthetic data, this system is designed to simulate the aging of the population. the aging model includes a variety of socioeconomic events, such as births, deaths, marriages, job changes and address changes. simon also has multiple levels of automated testing and data validations to ensure the data produced is usable by irs test systems and simulates businesses and population. . simon is capable of outputting 3 tax years of imf and bmf returns starting in tax year 2022. with each xml schema definition (xsd) that is released under the modernized e-file (mef), simon implements each xsd version and provides full regression support for testing in successive years. each xsd version has automated tests that run to ensure verifications can be run to reduce the likelihood of ai generated synthetic returns that might have data anomalies. also, to help seed test systems with synthetic individuals and synthetic businesses, simon can output the data master 1 (dm1) file simulating a feed from social security administration (ssa) and application for employer identification number file (form ss-4) containing transcribed form information so that the test system can be initialized with the appropriate backend reference data for validation of imf and bmf entities, as well as the ability to validate incoming synthetic tax returns."
Machine Translation,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"To better facilitate and automate content translation efforts at the IRS, a cloud-based commercial off-the-shelf (COTS) solution is being leveraged for use by the Linguistic, Policy, Tools and Services (LPTS) Team. 

The Machine Translation (MT) application leverages AWS Neural Translate and is being assessed and evaluated by the primary users, the Linguistic Policy, Tools and Services (LPTS) organization, through integration with their existing processes. The MT application focuses on translation for existing text and labels into Spanish with the ultimate goal of becoming an enterprise solution for a variety of non-English translations.",The MT application focuses on translation for existing text and labels into Spanish and other languages with the ultimate goal of becoming an enterprise solution for a variety of non-English translations.,Operation and Maintenance,Neither,3/1/2022,10/3/2022,9/25/2023,Developed with contracting resources.,"2032H5-22-P-00059,2032H5-24-F-00147",No,No,No,Yes,No,Custom IRS Pub 850 glossary of English and Spanish tax terms and phrases developed by the IRS is periodically uploaded to allow the MT Application to provide the most consistent translations possible.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",No,"Yes – agency has access to source code, but it is not public.",Yes,Integrated Enterprise Portal,6-12 months,Yes,Other,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"To better facilitate and automate content translation efforts at the IRS, a cloud-based commercial off-the-shelf (COTS) solution is being leveraged for use by the Linguistic, Policy, Tools and Services (LPTS) Team. 

The Machine Translation (MT) application leverages AWS Neural Translate and is being assessed and evaluated by the primary users, the Linguistic Policy, Tools and Services (LPTS) organization, through integration with their existing processes. The MT application focuses on translation for existing text and labels into Spanish with the ultimate goal of becoming an enterprise solution for a variety of non-English translations. . The MT application focuses on translation for existing text and labels into Spanish and other languages with the ultimate goal of becoming an enterprise solution for a variety of non-English translations.","to better facilitate and automate content translation efforts at the irs, a cloud-based commercial off-the-shelf (cots) solution is being leveraged for use by the linguistic, policy, tools and services (lpts) team. the machine translation (mt) application leverages aws neural translate and is being assessed and evaluated by the primary users, the linguistic policy, tools and services (lpts) organization, through integration with their existing processes. the mt application focuses on translation for existing text and labels into spanish with the ultimate goal of becoming an enterprise solution for a variety of non-english translations. . the mt application focuses on translation for existing text and labels into spanish and other languages with the ultimate goal of becoming an enterprise solution for a variety of non-english translations."
Economic Impact Payment (EIP) FAQ Voicebot,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"This project developed self-service application that responds to general taxpayer inquiries regarding the Economic Impact Payment (EIP) and includes options to route the call to live assistor during normal business hours.
The Intent Engine/AI System (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests). The answers provided to the taxpayers are not created by Generative AI. The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response.",Provide caller with information on Economic Impact Payment (EIP).,Operation and Maintenance,Neither,2/1/2021,4/9/2021,5/5/2021,Developed with contracting resources.,2032H5-19-F-00641,No,No,No,Yes,Yes,Call summary and call detail data.,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,Nuance Digital Contact Center Platform (DCCP),Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"This project developed self-service application that responds to general taxpayer inquiries regarding the Economic Impact Payment (EIP) and includes options to route the call to live assistor during normal business hours.
The Intent Engine/AI System (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests). The answers provided to the taxpayers are not created by Generative AI. The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response. . Provide caller with information on Economic Impact Payment (EIP).",this project developed self-service application that responds to general taxpayer inquiries regarding the economic impact payment (eip) and includes options to route the call to live assistor during normal business hours. the intent engine/ai system (using nlp and ai algorithms) classifies taxpayers' utterances (questions) into intents (requests). the answers provided to the taxpayers are not created by generative ai. the responses of the voicebot and business logic is predetermined by content owners. this ai tool aims to classify/navigate to a correct predetermined response. . provide caller with information on economic impact payment (eip).
One-Time Payment Voicebot,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"This project developed self-service application that responds to general taxpayer inquiries regarding the One Time Payment (OTP) and includes options to route the call to live assistor during normal business hours.
The Intent Engine (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests).  The answers provided to the taxpayers are not created by Generative AI. The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response.",The system generates predefined responses that address common taxpayer inquiries about one-time payments. It also provides routing to live agents during business hours if the taxpayer's issue cannot be resolved by the voicebot.,Operation and Maintenance,Neither,4/1/2021,6/1/2021,1/4/2022,Developed with contracting resources.,2032H5-19-F-00641,No,No,No,Yes,Yes,Training data includes logs from prior taxpayer inquiries regarding one-time payments and payment plans. ,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",No,No – agency does not have access to source code.,Yes,Nuance Digital Contact Center Platform (DCCP),Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"This project developed self-service application that responds to general taxpayer inquiries regarding the One Time Payment (OTP) and includes options to route the call to live assistor during normal business hours.
The Intent Engine (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests).  The answers provided to the taxpayers are not created by Generative AI. The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response. . The system generates predefined responses that address common taxpayer inquiries about one-time payments. It also provides routing to live agents during business hours if the taxpayer's issue cannot be resolved by the voicebot.",this project developed self-service application that responds to general taxpayer inquiries regarding the one time payment (otp) and includes options to route the call to live assistor during normal business hours. the intent engine (using nlp and ai algorithms) classifies taxpayers' utterances (questions) into intents (requests). the answers provided to the taxpayers are not created by generative ai. the responses of the voicebot and business logic is predetermined by content owners. this ai tool aims to classify/navigate to a correct predetermined response. . the system generates predefined responses that address common taxpayer inquiries about one-time payments. it also provides routing to live agents during business hours if the taxpayer's issue cannot be resolved by the voicebot.
FAQs / Notice Clarifications Voicebot,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"This project developed self-service application that responds to general taxpayer inquiries regarding the Notice Clarifications and includes options to route the call to live assistor during normal business hours.
The Intent Engine (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests). The answers provided to the taxpayers are not created by Generative AI. The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response.","The system produces predefined answers to taxpayer questions about notices and other clarifications. If the question cannot be resolved automatically, the system escalates to a live agent or prompts the taxpayer for further clarification. ",Operation and Maintenance,Neither,4/1/2021,6/1/2021,1/4/2022,Developed with contracting resources.,2032H5-19-F-00641,No,No,No,Yes,Yes,The model uses interaction data from FAQs and Notice clarifications previously provided by live agents. Metadata from taxpayer inquiries about IRS notices and FAQs.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",No,No – agency does not have access to source code.,Yes,Nuance Digital Contact Center Platform (DCCP),Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"This project developed self-service application that responds to general taxpayer inquiries regarding the Notice Clarifications and includes options to route the call to live assistor during normal business hours.
The Intent Engine (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests). The answers provided to the taxpayers are not created by Generative AI. The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response. . The system produces predefined answers to taxpayer questions about notices and other clarifications. If the question cannot be resolved automatically, the system escalates to a live agent or prompts the taxpayer for further clarification.","this project developed self-service application that responds to general taxpayer inquiries regarding the notice clarifications and includes options to route the call to live assistor during normal business hours. the intent engine (using nlp and ai algorithms) classifies taxpayers' utterances (questions) into intents (requests). the answers provided to the taxpayers are not created by generative ai. the responses of the voicebot and business logic is predetermined by content owners. this ai tool aims to classify/navigate to a correct predetermined response. . the system produces predefined answers to taxpayer questions about notices and other clarifications. if the question cannot be resolved automatically, the system escalates to a live agent or prompts the taxpayer for further clarification."
Advance Child Tax Credit (AdvCTC) Voicebot,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"This project developed self-service application that answers common taxpayer questions about the Advance Child Care Tax Credit.
The Intent Engine (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests). The answers provided to the taxpayers are not created by Generative AI. The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response.",The system generates predefined responses that address common taxpayer inquiries about Advance Child Care Tax Credit. It also provides routing to live agents during business hours if the taxpayer's issue cannot be resolved by the voicebot.,Operation and Maintenance,Neither,11/17/2021,11/17/2021,2/18/2022,Developed with contracting resources.,2032H5-19-F-00641,No,No,No,Yes,Yes,Call summary and call detail data.,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,Nuance Digital Contact Center Platform (DCCP),Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"This project developed self-service application that answers common taxpayer questions about the Advance Child Care Tax Credit.
The Intent Engine (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests). The answers provided to the taxpayers are not created by Generative AI. The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response. . The system generates predefined responses that address common taxpayer inquiries about Advance Child Care Tax Credit. It also provides routing to live agents during business hours if the taxpayer's issue cannot be resolved by the voicebot.",this project developed self-service application that answers common taxpayer questions about the advance child care tax credit. the intent engine (using nlp and ai algorithms) classifies taxpayers' utterances (questions) into intents (requests). the answers provided to the taxpayers are not created by generative ai. the responses of the voicebot and business logic is predetermined by content owners. this ai tool aims to classify/navigate to a correct predetermined response. . the system generates predefined responses that address common taxpayer inquiries about advance child care tax credit. it also provides routing to live agents during business hours if the taxpayer's issue cannot be resolved by the voicebot.
Small Business/Self Employed (SBSE) FAQ Voicebot,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"This project developed self-service application that resides on ACI platform which answers general taxpayer inquiries regarding the One Time Payment (OTP), the Notice Clarifications and includes options to route the call to live assistor during normal business hours.
The Intent Engine (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests). The answers provided to the taxpayers are not created by Generative AI. The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response.",The system outputs include predefined responses to FAQs related to notices and one-time payments. It routes call to live agents during business hours if necessary and provides guidance based on the identified intent of the taxpayer. ,Operation and Maintenance,Neither,1/27/2023,6/15/2023,8/4/2023,Developed with contracting resources.,2032H5-19-F-00641,No,No,No,Yes,Yes,The model uses data from FAQs and historical inquiries handled by live agents.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",No,No – agency does not have access to source code.,Yes,Nuance Digital Contact Center Platform (DCCP),Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"This project developed self-service application that resides on ACI platform which answers general taxpayer inquiries regarding the One Time Payment (OTP), the Notice Clarifications and includes options to route the call to live assistor during normal business hours.
The Intent Engine (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests). The answers provided to the taxpayers are not created by Generative AI. The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response. . The system outputs include predefined responses to FAQs related to notices and one-time payments. It routes call to live agents during business hours if necessary and provides guidance based on the identified intent of the taxpayer.","this project developed self-service application that resides on aci platform which answers general taxpayer inquiries regarding the one time payment (otp), the notice clarifications and includes options to route the call to live assistor during normal business hours. the intent engine (using nlp and ai algorithms) classifies taxpayers' utterances (questions) into intents (requests). the answers provided to the taxpayers are not created by generative ai. the responses of the voicebot and business logic is predetermined by content owners. this ai tool aims to classify/navigate to a correct predetermined response. . the system outputs include predefined responses to faqs related to notices and one-time payments. it routes call to live agents during business hours if necessary and provides guidance based on the identified intent of the taxpayer."
Insolvency and Offer in Compromise Voicebot,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"This project developed self-service application that responds to general taxpayer inquiries regarding the OIC Program as well as insolvent taxpayers with their most requested services with the options to route the call to live assistor during normal business hours.
The Intent Engine (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests). The answers provided to the taxpayers are not created by Generative AI. The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response.","The outputs consist of responses to questions about Offers in Compromise (OIC) (deployed 5/31/24) and Insolvency (deployed 6/14/24), based on predefined FAQs. When the system cannot resolve an inquiry, it either prompts the user for clarification or routes the call to a live assistor.",Operation and Maintenance,Neither,12/6/2023,2/1/2024,5/31/2024,Developed with contracting resources.,2032H5-19-F-00641,No,No,No,Yes,Yes,Data from taxpayer interactions about insolvency options and Offers in Compromise (OIC) queries. Metadata from insolvency-related queries and OIC case files. ,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",No,No – agency does not have access to source code.,Yes,Nuance Digital Contact Center Platform (DCCP),Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"This project developed self-service application that responds to general taxpayer inquiries regarding the OIC Program as well as insolvent taxpayers with their most requested services with the options to route the call to live assistor during normal business hours.
The Intent Engine (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests). The answers provided to the taxpayers are not created by Generative AI. The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response. . The outputs consist of responses to questions about Offers in Compromise (OIC) (deployed 5/31/24) and Insolvency (deployed 6/14/24), based on predefined FAQs. When the system cannot resolve an inquiry, it either prompts the user for clarification or routes the call to a live assistor.","this project developed self-service application that responds to general taxpayer inquiries regarding the oic program as well as insolvent taxpayers with their most requested services with the options to route the call to live assistor during normal business hours. the intent engine (using nlp and ai algorithms) classifies taxpayers' utterances (questions) into intents (requests). the answers provided to the taxpayers are not created by generative ai. the responses of the voicebot and business logic is predetermined by content owners. this ai tool aims to classify/navigate to a correct predetermined response. . the outputs consist of responses to questions about offers in compromise (oic) (deployed 5/31/24) and insolvency (deployed 6/14/24), based on predefined faqs. when the system cannot resolve an inquiry, it either prompts the user for clarification or routes the call to a live assistor."
Financial Relief Voicebot,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"This project developed self-service application that responds to general taxpayer inquiries regarding the Financial Relief and includes options to route the call to live assistor during normal business hours.
The Intent Engine (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests). The answers provided to the taxpayers are not created by Generative AI. The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response.","The system generates predefined answers to inquiries about financial relief options for taxpayers. The system navigates through predefined responses, asks clarifying questions if needed, and routes calls to live agents for complex issues.",Acquisition and/or Development,Neither,1/8/2024,2/1/2024,Unknown,Developed with both contracting and in-house resources.,2032H5-19-F-00641,No,Unknown,Unknown,Unknown,Yes,Historical data from financial relief inquiries and associated case resolutions. Metadata from taxpayer requests for financial relief and payment plans. ,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",Unknown,Unknown,Yes,Nuance Digital Contact Center Platform (DCCP),Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"This project developed self-service application that responds to general taxpayer inquiries regarding the Financial Relief and includes options to route the call to live assistor during normal business hours.
The Intent Engine (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests). The answers provided to the taxpayers are not created by Generative AI. The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response. . The system generates predefined answers to inquiries about financial relief options for taxpayers. The system navigates through predefined responses, asks clarifying questions if needed, and routes calls to live agents for complex issues.","this project developed self-service application that responds to general taxpayer inquiries regarding the financial relief and includes options to route the call to live assistor during normal business hours. the intent engine (using nlp and ai algorithms) classifies taxpayers' utterances (questions) into intents (requests). the answers provided to the taxpayers are not created by generative ai. the responses of the voicebot and business logic is predetermined by content owners. this ai tool aims to classify/navigate to a correct predetermined response. . the system generates predefined answers to inquiries about financial relief options for taxpayers. the system navigates through predefined responses, asks clarifying questions if needed, and routes calls to live agents for complex issues."
Integrate Google Doc AI for Digitalization,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"This Digital Enablement Platform (DEP) implementation has the goal to increase access to high quality data extraction, reduce reliance on paper and manually intensive processes utilizing artificial intelligence. Therefore, eliminating the need for transcription and manual data corrections by the Business.
DEP Artificial Intelligence (AI) will include updates to the existing business unit processes to leverage the UiPath Robotic Process Automation (RPA) platform. The UiPath RPA platform will also integrate with Google Document AI to implement a new extraction engine that will work in conjunction with and in support of the DEP.
The UiPath Automation Suite software for RPA platform in the Integrated Enterprise Portal (IEP) Amazon Web Services (AWS) provides support for multiple optical character recognition (OCRs), included the native UiPath OCR engine for intelligent document processing. Eliminating the need for transcription and manual data corrections by the Business.
Humans in the loop will validate AI OCR classification and extraction within the UiPath Action Center. Any extraction confidence levels that fail both UiPath and Google DocAI confidence level requirements necessitate human verification as well as 10 percent of all documents that are processed through the DEP AI system.
The benefit of the DEP AI system revolves around the of transition to being a modern, digitally capable, customer-centric agency. It will ensure taxpayers file accurate returns as well as allowing for the IRS to identify inconsistencies and streaming correspondence. 
The DEP AI aims to enhance efficiency by transitioning to end-to-end digital processes, eliminating paper backlogs, and improving accuracy in translating return data. These digital tools will be accessible to individuals with disabilities and offered in taxpayers’ preferred languages. Additionally, this commitment to digitalization reduces the environmental impact by minimizing paper usage and storage. Overall, the decision benefits taxpayers, improves operations, and contributes to environmental conservation efforts.
IRS business units tied to paper-driven processes as well as U.S. taxpayers.",The output is a metadata file that contains the data that has been extracted from the images we have scanned.,Implementation and Assessment,Neither,11/20/2023,11/20/2023,6/30/2024,Developed with contracting resources.,TIRNO-17-D-00004 WR2730,No,No,No,Yes,Yes,"We have used CP2000 letter responses, Form 709 tax data, Form 94X tax data, SCOIC (offer in compromise) data.","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Digital Enablement Platform,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"This Digital Enablement Platform (DEP) implementation has the goal to increase access to high quality data extraction, reduce reliance on paper and manually intensive processes utilizing artificial intelligence. Therefore, eliminating the need for transcription and manual data corrections by the Business.
DEP Artificial Intelligence (AI) will include updates to the existing business unit processes to leverage the UiPath Robotic Process Automation (RPA) platform. The UiPath RPA platform will also integrate with Google Document AI to implement a new extraction engine that will work in conjunction with and in support of the DEP.
The UiPath Automation Suite software for RPA platform in the Integrated Enterprise Portal (IEP) Amazon Web Services (AWS) provides support for multiple optical character recognition (OCRs), included the native UiPath OCR engine for intelligent document processing. Eliminating the need for transcription and manual data corrections by the Business.
Humans in the loop will validate AI OCR classification and extraction within the UiPath Action Center. Any extraction confidence levels that fail both UiPath and Google DocAI confidence level requirements necessitate human verification as well as 10 percent of all documents that are processed through the DEP AI system.
The benefit of the DEP AI system revolves around the of transition to being a modern, digitally capable, customer-centric agency. It will ensure taxpayers file accurate returns as well as allowing for the IRS to identify inconsistencies and streaming correspondence. 
The DEP AI aims to enhance efficiency by transitioning to end-to-end digital processes, eliminating paper backlogs, and improving accuracy in translating return data. These digital tools will be accessible to individuals with disabilities and offered in taxpayers’ preferred languages. Additionally, this commitment to digitalization reduces the environmental impact by minimizing paper usage and storage. Overall, the decision benefits taxpayers, improves operations, and contributes to environmental conservation efforts.
IRS business units tied to paper-driven processes as well as U.S. taxpayers. . The output is a metadata file that contains the data that has been extracted from the images we have scanned.","this digital enablement platform (dep) implementation has the goal to increase access to high quality data extraction, reduce reliance on paper and manually intensive processes utilizing artificial intelligence. therefore, eliminating the need for transcription and manual data corrections by the business. dep artificial intelligence (ai) will include updates to the existing business unit processes to leverage the uipath robotic process automation (rpa) platform. the uipath rpa platform will also integrate with google document ai to implement a new extraction engine that will work in conjunction with and in support of the dep. the uipath automation suite software for rpa platform in the integrated enterprise portal (iep) amazon web services (aws) provides support for multiple optical character recognition (ocrs), included the native uipath ocr engine for intelligent document processing. eliminating the need for transcription and manual data corrections by the business. humans in the loop will validate ai ocr classification and extraction within the uipath action center. any extraction confidence levels that fail both uipath and google docai confidence level requirements necessitate human verification as well as 10 percent of all documents that are processed through the dep ai system. the benefit of the dep ai system revolves around the of transition to being a modern, digitally capable, customer-centric agency. it will ensure taxpayers file accurate returns as well as allowing for the irs to identify inconsistencies and streaming correspondence. the dep ai aims to enhance efficiency by transitioning to end-to-end digital processes, eliminating paper backlogs, and improving accuracy in translating return data. these digital tools will be accessible to individuals with disabilities and offered in taxpayers’ preferred languages. additionally, this commitment to digitalization reduces the environmental impact by minimizing paper usage and storage. overall, the decision benefits taxpayers, improves operations, and contributes to environmental conservation efforts. irs business units tied to paper-driven processes as well as u.s. taxpayers. . the output is a metadata file that contains the data that has been extracted from the images we have scanned."
Modernization Accelerator - Legacy Applications Chatbot & Code Conversion,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"The IRS Enterprise has many systems that use a legacy coding language, which is much older than typical modern coding languages. This makes it difficult to find skilled and knowledgeable people to maintain or modernize applications. A large language model (LLM) can help speed up this translation so we can more rapidly modernize while retaining the functional objectives of the legacy code. This use case will consist of an iterative multi-phase approach to help accelerate the modernization of sample legacy code by building a Generative AI (GenAI) Sandbox environment that will allow us to ingest a subset of IRS legacy documentation to build a LLM GenAI chatbot. This will accelerate knowledge discovery for future legacy modernization initiatives. In addition, we will leverage GenAI to ingest a subset of IRS Assembler Language Code (ALC) legacy code to evaluate, validate, and optimize GenAI capabilities to not only transform legacy code into English readable pseudocode, and convert it into a modern language. This initial use case will be limited to IRS use only within a sandbox environment and not intended for production.

Existing Code to Java
LLM within IRS boundary","The AI accelerator outputs a document that describes the functionality of the ALC code ingested; identifying loops and conditions as well as key architectural and functional diagrams that can assist a developer understanding complex legacy code.
The Retrieval Augmented Generation (RAG) Chatbot will generate responses grounded in the files that are ingested, including confidence scores, to aide business analysis efforts",Implementation and Assessment,Neither,10/1/2023,3/15/2024,10/31/2024,Developed with contracting resources.,TIRNO-17-D-00004 WR2730,No,No,No,Yes,No,"No models are being trained. Outputs are being calibrated through prompt engineering. The inputs will be supporting documentation available to the public in PDF format, as well as a subset of ALC code (.ASM files) approved for ingestion. No Personally Identifiable Information (PII) data will be used. Any additional inputs used will go through appropriate review process.

ASM code is a shorthand for machine language. Each ASM language is designed for a specific computer architecture, and sometimes an operating system.","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The IRS Enterprise has many systems that use a legacy coding language, which is much older than typical modern coding languages. This makes it difficult to find skilled and knowledgeable people to maintain or modernize applications. A large language model (LLM) can help speed up this translation so we can more rapidly modernize while retaining the functional objectives of the legacy code. This use case will consist of an iterative multi-phase approach to help accelerate the modernization of sample legacy code by building a Generative AI (GenAI) Sandbox environment that will allow us to ingest a subset of IRS legacy documentation to build a LLM GenAI chatbot. This will accelerate knowledge discovery for future legacy modernization initiatives. In addition, we will leverage GenAI to ingest a subset of IRS Assembler Language Code (ALC) legacy code to evaluate, validate, and optimize GenAI capabilities to not only transform legacy code into English readable pseudocode, and convert it into a modern language. This initial use case will be limited to IRS use only within a sandbox environment and not intended for production.

Existing Code to Java
LLM within IRS boundary . The AI accelerator outputs a document that describes the functionality of the ALC code ingested; identifying loops and conditions as well as key architectural and functional diagrams that can assist a developer understanding complex legacy code.
The Retrieval Augmented Generation (RAG) Chatbot will generate responses grounded in the files that are ingested, including confidence scores, to aide business analysis efforts","the irs enterprise has many systems that use a legacy coding language, which is much older than typical modern coding languages. this makes it difficult to find skilled and knowledgeable people to maintain or modernize applications. a large language model (llm) can help speed up this translation so we can more rapidly modernize while retaining the functional objectives of the legacy code. this use case will consist of an iterative multi-phase approach to help accelerate the modernization of sample legacy code by building a generative ai (genai) sandbox environment that will allow us to ingest a subset of irs legacy documentation to build a llm genai chatbot. this will accelerate knowledge discovery for future legacy modernization initiatives. in addition, we will leverage genai to ingest a subset of irs assembler language code (alc) legacy code to evaluate, validate, and optimize genai capabilities to not only transform legacy code into english readable pseudocode, and convert it into a modern language. this initial use case will be limited to irs use only within a sandbox environment and not intended for production. existing code to java llm within irs boundary . the ai accelerator outputs a document that describes the functionality of the alc code ingested; identifying loops and conditions as well as key architectural and functional diagrams that can assist a developer understanding complex legacy code. the retrieval augmented generation (rag) chatbot will generate responses grounded in the files that are ingested, including confidence scores, to aide business analysis efforts"
Modernization Accelerator - International Business Machines (IBM) Cognitive Data Mapper (CDM) and AWS SageMaker+LLAMA,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"The Business Masterfile Modernization (BMF Mod) project will reproduce the legacy BMF system, written in 60-year-old mainframe technology, into a modern, cloud-based system. BMF Mod will also add additional features to the system as new technology permits.

Note that this software will only be used in an isolated development (sandbox) environment, to assist developers in programming the new modernized system. It will not be promoted into Production. 

These Artificial Intelligence/Machine Language (AI/ML) tools analyze the existing legacy BMF system code, after it has been prepared by other non-AI/ML International Business Machines (IBM) tools, to automatically produce: Business logic in natural language, which describe what actions or data transformations the code performs: Modernized Java code to implement the business rules; Microservice recommendations, meaning that code snippets are grouped together by function and access patterns, to split very large programs that are hard to understand into smaller chunks that can be more easily maintained.

These tools take in code from the existing BMF system, after it has been prepared by other non-AI/ML IBM tools, and analyze the code to produce the business logic, code, and microservice recommendations.
IBM Cognitive Data Mapper (CDM) uses Natural Language Processing (NLP) Machine Learning (ML) models to automatically analyze the source system's data and metadata, map to a target model or business glossary, and/or classify the information. CDM will be used for automated data mapping to automatically identify and establish relationships between data elements in different data sources and source systems.
Amazon Web Services (AWS) SageMaker with LLAMA accepts prepared code from other non-AI/ML IBM tools, then further analyzes the code to extract business logic and convert the code to modern Java.

The tools analyze existing code from the BMF system, which are very large programs, and suggest how to translate the code into smaller, more manageable and understandable business logic, code, and microservices.

All output business logic, code, and microservice recommendations will be reviewed by both IBM and IRS employees prior to being used in development of the new modernized BMF system. The outputs will be put into a spreadsheet that can be shared with subject matter experts, who will then manually check accuracy of the work. Code will also be packaged and extensively tested to ensure accuracy.

These AI/ML tools will accelerate our development process, allowing us to deliver a modernized system a few years faster than otherwise. The output business logic and microservice recommendations are also very useful as documentation artifacts, helping to understand how the BMF system functions.

Approval will allow BMF Mod work to continue. Without approval we would need to extend our modernization plans to account for performing these functions manually.

While these AI/ML tools are only intended to be used by IBM consultants working on the BMF Mod program, its impacts will benefit all BMF stakeholders including IRS business operating divisions, Treasury, and taxpayers.","IBM CDM uses Natural Language Processing (NLP) Machine Learning (ML) models to automatically analyze the source system's data and metadata, map to a target model or business glossary, and/or classify the information. CDM will be used for automated data mapping to automatically identify and establish relationships between data elements in different data sources and source systems.
AWS SageMaker with LLAMA accepts prepared code from other non-AI/ML IBM tools, then further analyzes the code to extract business logic and convert the code to modern Java.",Acquisition and/or Development,Neither,6/6/2024,6/14/2024,Unknown,Developed with contracting resources.,2032H5-24-F-00181,No,Unknown,Unknown,Unknown,Yes,Existing legacy Business Master File code,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"The Business Masterfile Modernization (BMF Mod) project will reproduce the legacy BMF system, written in 60-year-old mainframe technology, into a modern, cloud-based system. BMF Mod will also add additional features to the system as new technology permits.

Note that this software will only be used in an isolated development (sandbox) environment, to assist developers in programming the new modernized system. It will not be promoted into Production. 

These Artificial Intelligence/Machine Language (AI/ML) tools analyze the existing legacy BMF system code, after it has been prepared by other non-AI/ML International Business Machines (IBM) tools, to automatically produce: Business logic in natural language, which describe what actions or data transformations the code performs: Modernized Java code to implement the business rules; Microservice recommendations, meaning that code snippets are grouped together by function and access patterns, to split very large programs that are hard to understand into smaller chunks that can be more easily maintained.

These tools take in code from the existing BMF system, after it has been prepared by other non-AI/ML IBM tools, and analyze the code to produce the business logic, code, and microservice recommendations.
IBM Cognitive Data Mapper (CDM) uses Natural Language Processing (NLP) Machine Learning (ML) models to automatically analyze the source system's data and metadata, map to a target model or business glossary, and/or classify the information. CDM will be used for automated data mapping to automatically identify and establish relationships between data elements in different data sources and source systems.
Amazon Web Services (AWS) SageMaker with LLAMA accepts prepared code from other non-AI/ML IBM tools, then further analyzes the code to extract business logic and convert the code to modern Java.

The tools analyze existing code from the BMF system, which are very large programs, and suggest how to translate the code into smaller, more manageable and understandable business logic, code, and microservices.

All output business logic, code, and microservice recommendations will be reviewed by both IBM and IRS employees prior to being used in development of the new modernized BMF system. The outputs will be put into a spreadsheet that can be shared with subject matter experts, who will then manually check accuracy of the work. Code will also be packaged and extensively tested to ensure accuracy.

These AI/ML tools will accelerate our development process, allowing us to deliver a modernized system a few years faster than otherwise. The output business logic and microservice recommendations are also very useful as documentation artifacts, helping to understand how the BMF system functions.

Approval will allow BMF Mod work to continue. Without approval we would need to extend our modernization plans to account for performing these functions manually.

While these AI/ML tools are only intended to be used by IBM consultants working on the BMF Mod program, its impacts will benefit all BMF stakeholders including IRS business operating divisions, Treasury, and taxpayers. . IBM CDM uses Natural Language Processing (NLP) Machine Learning (ML) models to automatically analyze the source system's data and metadata, map to a target model or business glossary, and/or classify the information. CDM will be used for automated data mapping to automatically identify and establish relationships between data elements in different data sources and source systems.
AWS SageMaker with LLAMA accepts prepared code from other non-AI/ML IBM tools, then further analyzes the code to extract business logic and convert the code to modern Java.","the business masterfile modernization (bmf mod) project will reproduce the legacy bmf system, written in 60-year-old mainframe technology, into a modern, cloud-based system. bmf mod will also add additional features to the system as new technology permits. note that this software will only be used in an isolated development (sandbox) environment, to assist developers in programming the new modernized system. it will not be promoted into production. these artificial intelligence/machine language (ai/ml) tools analyze the existing legacy bmf system code, after it has been prepared by other non-ai/ml international business machines (ibm) tools, to automatically produce: business logic in natural language, which describe what actions or data transformations the code performs: modernized java code to implement the business rules; microservice recommendations, meaning that code snippets are grouped together by function and access patterns, to split very large programs that are hard to understand into smaller chunks that can be more easily maintained. these tools take in code from the existing bmf system, after it has been prepared by other non-ai/ml ibm tools, and analyze the code to produce the business logic, code, and microservice recommendations. ibm cognitive data mapper (cdm) uses natural language processing (nlp) machine learning (ml) models to automatically analyze the source system's data and metadata, map to a target model or business glossary, and/or classify the information. cdm will be used for automated data mapping to automatically identify and establish relationships between data elements in different data sources and source systems. amazon web services (aws) sagemaker with llama accepts prepared code from other non-ai/ml ibm tools, then further analyzes the code to extract business logic and convert the code to modern java. the tools analyze existing code from the bmf system, which are very large programs, and suggest how to translate the code into smaller, more manageable and understandable business logic, code, and microservices. all output business logic, code, and microservice recommendations will be reviewed by both ibm and irs employees prior to being used in development of the new modernized bmf system. the outputs will be put into a spreadsheet that can be shared with subject matter experts, who will then manually check accuracy of the work. code will also be packaged and extensively tested to ensure accuracy. these ai/ml tools will accelerate our development process, allowing us to deliver a modernized system a few years faster than otherwise. the output business logic and microservice recommendations are also very useful as documentation artifacts, helping to understand how the bmf system functions. approval will allow bmf mod work to continue. without approval we would need to extend our modernization plans to account for performing these functions manually. while these ai/ml tools are only intended to be used by ibm consultants working on the bmf mod program, its impacts will benefit all bmf stakeholders including irs business operating divisions, treasury, and taxpayers. . ibm cdm uses natural language processing (nlp) machine learning (ml) models to automatically analyze the source system's data and metadata, map to a target model or business glossary, and/or classify the information. cdm will be used for automated data mapping to automatically identify and establish relationships between data elements in different data sources and source systems. aws sagemaker with llama accepts prepared code from other non-ai/ml ibm tools, then further analyzes the code to extract business logic and convert the code to modern java."
Joint Committee on Taxation Research Aide,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,The Joint Committee on Taxation Review (JCTR) Research Aide is a generative AI (GenAI) system that uses Retrieval Augmented Generation (RAG) search to answer questions on JCTR policies and procedures that are provided in natural language. This tool has a use case specific user interface (UI) that uses the Certara platform via Application Programming Interface (API) to perform the RAG search and generation.,"This system uses an embedding model to produce a vector representation of texts for contextual searching, and Large Language Model to generate a response to user inquiries. Outputs of the tool include the model generated response to a user's inquiry, as well as the text from relevant documents identified in the retrieval stage and used by the model in generating said response.",Acquisition and/or Development,Neither,1/15/2024,1/15/2024,Unknown,Developed with contracting resources.,2032H8-23-F-00166,No,Unknown,Unknown,Unknown,Yes,Text from the sections of the Internal Revenue Manual relevant to the Joint Committee (JC) (4.36) and instructional/training documents provided by the IRS to JC examiners and staff.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",Unknown,Unknown,Yes,RAS-1,Less than 6 months,Yes,Other,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"The Joint Committee on Taxation Review (JCTR) Research Aide is a generative AI (GenAI) system that uses Retrieval Augmented Generation (RAG) search to answer questions on JCTR policies and procedures that are provided in natural language. This tool has a use case specific user interface (UI) that uses the Certara platform via Application Programming Interface (API) to perform the RAG search and generation. . This system uses an embedding model to produce a vector representation of texts for contextual searching, and Large Language Model to generate a response to user inquiries. Outputs of the tool include the model generated response to a user's inquiry, as well as the text from relevant documents identified in the retrieval stage and used by the model in generating said response.","the joint committee on taxation review (jctr) research aide is a generative ai (genai) system that uses retrieval augmented generation (rag) search to answer questions on jctr policies and procedures that are provided in natural language. this tool has a use case specific user interface (ui) that uses the certara platform via application programming interface (api) to perform the rag search and generation. . this system uses an embedding model to produce a vector representation of texts for contextual searching, and large language model to generate a response to user inquiries. outputs of the tool include the model generated response to a user's inquiry, as well as the text from relevant documents identified in the retrieval stage and used by the model in generating said response."
Internal Revenue Manual Research Aid (IRMA),Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,IRMA - Internal Revenue Manual (IRM) Research Aide is a generative AI (GenAI) system that uses Retrieval Augmented Generation (RAG) search to answer questions on IRM policies and procedures that are provided in natural language. This tool has a use case specific user interface (UI) that uses the Certara platform via Application Programming Interface (API) to perform the RAG search and generation.,"This system uses an embedding model to produce a vector representation of texts for contextual searching, and Large Language Model to generate a response to user inquiries. Outputs of the tool include the model generated response to a user's inquiry, as well as the text from relevant documents identified in the retrieval stage and used by the model in generating said response.",Acquisition and/or Development,Neither,5/9/2024,5/10/2024,Unknown,Developed with contracting resources.,2032H8-23-F-00166,No,Unknown,Unknown,Unknown,Yes,All text from the sections and subsections of the unredacted Internal Revenue Manual.,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Unknown,Unknown,Yes,RAS-1,Less than 6 months,Yes,Other,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"IRMA - Internal Revenue Manual (IRM) Research Aide is a generative AI (GenAI) system that uses Retrieval Augmented Generation (RAG) search to answer questions on IRM policies and procedures that are provided in natural language. This tool has a use case specific user interface (UI) that uses the Certara platform via Application Programming Interface (API) to perform the RAG search and generation. . This system uses an embedding model to produce a vector representation of texts for contextual searching, and Large Language Model to generate a response to user inquiries. Outputs of the tool include the model generated response to a user's inquiry, as well as the text from relevant documents identified in the retrieval stage and used by the model in generating said response.","irma - internal revenue manual (irm) research aide is a generative ai (genai) system that uses retrieval augmented generation (rag) search to answer questions on irm policies and procedures that are provided in natural language. this tool has a use case specific user interface (ui) that uses the certara platform via application programming interface (api) to perform the rag search and generation. . this system uses an embedding model to produce a vector representation of texts for contextual searching, and large language model to generate a response to user inquiries. outputs of the tool include the model generated response to a user's inquiry, as well as the text from relevant documents identified in the retrieval stage and used by the model in generating said response."
Data Integration using Informatica Data Management Cloud,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"Informatica Intelligent Cloud Services Overview 

Informatica Intelligent Cloud Services (IICS) is a cloud-native data integration and management platform that enables organizations to connect, access, transform, and govern data across hybrid and multi-cloud environments. IICS leverages artificial intelligence and machine learning to automate and optimize data processes, enhance data quality, and accelerate data insights. IICS offers a range of services and features, such as: 

Data Integration: A service that enables users to design, develop, and deploy data integration workflows and tasks across cloud and on-premises sources and targets, using a graphical user interface or a code-based environment. Data Integration supports various data integration patterns, such as batch, real-time, streaming, and change data capture. 

The solution enables Integrated Production Model (IPM) to load data directly to the cloud from on-premises sources.  Without the solution, there's no native support to load data to Databricks, causing extra work and less-than-optimal solutions to maintain and develop.  Any custom needed IPM data is impacted by this solution.","The Informatica Artificial Intelligence (AI), named Claire, can make the following types of recommendations during mapping design:
- Recommend transformation types to include in the mapping.
- Recommend additional sources based on primary key and foreign key relationships.
- Recommend joining or unioning additional sources.
- Make recommendations for objects in a mapping inventory.
- Recommend mapplets or user-defined functions.
- Make recommendations to standardize your data
- Make recommendations to mask sensitive data.
- Recommend rule specifications.",Acquisition and/or Development,Neither,4/1/2024,11/1/2023,Unknown,Developed with contracting resources.,2032H5-21-F-00059 / NNG15SD22B,No,Unknown,Unknown,Unknown,Yes,The source information comes from the Informatica mapping development work.,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Unknown,Unknown,No,Unknown,More than 12 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"Informatica Intelligent Cloud Services Overview 

Informatica Intelligent Cloud Services (IICS) is a cloud-native data integration and management platform that enables organizations to connect, access, transform, and govern data across hybrid and multi-cloud environments. IICS leverages artificial intelligence and machine learning to automate and optimize data processes, enhance data quality, and accelerate data insights. IICS offers a range of services and features, such as: 

Data Integration: A service that enables users to design, develop, and deploy data integration workflows and tasks across cloud and on-premises sources and targets, using a graphical user interface or a code-based environment. Data Integration supports various data integration patterns, such as batch, real-time, streaming, and change data capture. 

The solution enables Integrated Production Model (IPM) to load data directly to the cloud from on-premises sources.  Without the solution, there's no native support to load data to Databricks, causing extra work and less-than-optimal solutions to maintain and develop.  Any custom needed IPM data is impacted by this solution. . The Informatica Artificial Intelligence (AI), named Claire, can make the following types of recommendations during mapping design:
- Recommend transformation types to include in the mapping.
- Recommend additional sources based on primary key and foreign key relationships.
- Recommend joining or unioning additional sources.
- Make recommendations for objects in a mapping inventory.
- Recommend mapplets or user-defined functions.
- Make recommendations to standardize your data
- Make recommendations to mask sensitive data.
- Recommend rule specifications.","informatica intelligent cloud services overview informatica intelligent cloud services (iics) is a cloud-native data integration and management platform that enables organizations to connect, access, transform, and govern data across hybrid and multi-cloud environments. iics leverages artificial intelligence and machine learning to automate and optimize data processes, enhance data quality, and accelerate data insights. iics offers a range of services and features, such as: data integration: a service that enables users to design, develop, and deploy data integration workflows and tasks across cloud and on-premises sources and targets, using a graphical user interface or a code-based environment. data integration supports various data integration patterns, such as batch, real-time, streaming, and change data capture. the solution enables integrated production model (ipm) to load data directly to the cloud from on-premises sources. without the solution, there's no native support to load data to databricks, causing extra work and less-than-optimal solutions to maintain and develop. any custom needed ipm data is impacted by this solution. . the informatica artificial intelligence (ai), named claire, can make the following types of recommendations during mapping design: - recommend transformation types to include in the mapping. - recommend additional sources based on primary key and foreign key relationships. - recommend joining or unioning additional sources. - make recommendations for objects in a mapping inventory. - recommend mapplets or user-defined functions. - make recommendations to standardize your data - make recommendations to mask sensitive data. - recommend rule specifications."
Answering Employee Questions with Natural Language Processing in IRWorks,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"The purpose of the Artificial Intelligence (AI) element of the IRWorks platform (a ServiceNow implementation for the IRS) is to utilize natural language to respond to common user questions about IRWorks knowledge base (KB) articles and service catalogue items in the IRWorks Employee Center. This AI element is interpretive.
At a high level, this is accomplished by using language models to train the Virtual Agent (a ServiceNow product) to respond to user questions.
The decision/judgement that this AI element uses is to search existing Knowledge Base (KBs)/service catalogue items, in conjunction with the user's defined access within IRWorks, to automate Tier 1 support by outputting (in natural language) answers and/or associated links that align to the customer's (natural language) query.
Yes, humans are involved in reviewing the output - the AI element does not produce actionable items but produces information or links that are provided to the querying user for them to review and/or take manual actions.
The anticipated benefits of this AI element of the IRWorks platform are to reduce costs of Tier 1 support and provide a better user experience. 
The direct/ultimate impact of this decision is to reduce costs of Tier 1 support and provide a better user experience.
The population impacted by this decision is all IRWorks users, about 90K+ IRS employees.",Improve efficiency and optimize outputs for end users requesting services/information from within ServiceNow.,Operation and Maintenance,Neither,4/30/2023,4/30/2023,4/30/2023,Developed with contracting resources.,47QTCA22D00B2; GS35F0414R; 47QTCA22D00B2,No,No,No,Yes,Yes,IRS IRWorks Knowledge Management,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,"Yes – agency has access to source code, but it is not public.",Yes,IRWorks,Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The purpose of the Artificial Intelligence (AI) element of the IRWorks platform (a ServiceNow implementation for the IRS) is to utilize natural language to respond to common user questions about IRWorks knowledge base (KB) articles and service catalogue items in the IRWorks Employee Center. This AI element is interpretive.
At a high level, this is accomplished by using language models to train the Virtual Agent (a ServiceNow product) to respond to user questions.
The decision/judgement that this AI element uses is to search existing Knowledge Base (KBs)/service catalogue items, in conjunction with the user's defined access within IRWorks, to automate Tier 1 support by outputting (in natural language) answers and/or associated links that align to the customer's (natural language) query.
Yes, humans are involved in reviewing the output - the AI element does not produce actionable items but produces information or links that are provided to the querying user for them to review and/or take manual actions.
The anticipated benefits of this AI element of the IRWorks platform are to reduce costs of Tier 1 support and provide a better user experience. 
The direct/ultimate impact of this decision is to reduce costs of Tier 1 support and provide a better user experience.
The population impacted by this decision is all IRWorks users, about 90K+ IRS employees. . Improve efficiency and optimize outputs for end users requesting services/information from within ServiceNow.","the purpose of the artificial intelligence (ai) element of the irworks platform (a servicenow implementation for the irs) is to utilize natural language to respond to common user questions about irworks knowledge base (kb) articles and service catalogue items in the irworks employee center. this ai element is interpretive. at a high level, this is accomplished by using language models to train the virtual agent (a servicenow product) to respond to user questions. the decision/judgement that this ai element uses is to search existing knowledge base (kbs)/service catalogue items, in conjunction with the user's defined access within irworks, to automate tier 1 support by outputting (in natural language) answers and/or associated links that align to the customer's (natural language) query. yes, humans are involved in reviewing the output - the ai element does not produce actionable items but produces information or links that are provided to the querying user for them to review and/or take manual actions. the anticipated benefits of this ai element of the irworks platform are to reduce costs of tier 1 support and provide a better user experience. the direct/ultimate impact of this decision is to reduce costs of tier 1 support and provide a better user experience. the population impacted by this decision is all irworks users, about 90k+ irs employees. . improve efficiency and optimize outputs for end users requesting services/information from within servicenow."
Compliance Data Warehouse Data Access Log Monitoring,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"The purpose of this AI project is to provide insights into access and usage patterns improving understanding of Compliance Data Warehouse (CDW) data usage and Personally Identifiable Information (PII) exposure risks. It does so by comparing a user's activity to a distribution of similar users, based on access groups, and provides a score based on how far from expected normal behavior that user's activity performed. This model is only used to inform humans rather than automating a decision. This model automates the manual analysis of log data to identify and rank anomalous user activity for further investigation. Humans review the AI output prior to taking action on the result. They do so by reviewing AI outputs (i.e., model scores, feature ranking, user's historic log data) in a Tableau Dashboard and automated emailed reports. The benefits of this project are the automated multivariate analysis of CDW usage data in order to flag anomalous user activity that may otherwise go unnoticed via manual univariate analysis. The direct impact of this decision is the ability to routinely identify and evaluate high-risk events to avoid future security risks. No population is directly impacted by this AI as it is only used to inform the RAAS Infrastructure Security Team in order to conduct further outreach and investigation of anomalous user activity.","The AI model outputs a table of scored query activity per user per day where a higher score is interpreted as more anomalous activity, as well as feature ranking and score for each given output row where the score of a feature is the distance from the normal value for the population for that feature. The output of the broader system also includes supplemental score data such as flagging reoccurring anomalous behavior and the user's activity score percentile compared to the group they were modeled against. The model scores in addition to contextual information (i.e., discovery directory information and entitlement information) and supplemental statistics are displayed in a dashboard for the intended users.",Acquisition and/or Development,Neither,2/8/2024,2/8/2024,Unknown,Developed with contracting resources.,2032H5-22-F-00432,No,Unknown,Unknown,Unknown,Other,"Our team produces an aggregated set of metrics from the raw CDW production log data. This is merged with internally available employee information for identifying users by business unit, and entitlement information to identify Common and Special Access Groups (CAGs/SAGs).","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",Unknown,Unknown,Yes,RAS-1,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"The purpose of this AI project is to provide insights into access and usage patterns improving understanding of Compliance Data Warehouse (CDW) data usage and Personally Identifiable Information (PII) exposure risks. It does so by comparing a user's activity to a distribution of similar users, based on access groups, and provides a score based on how far from expected normal behavior that user's activity performed. This model is only used to inform humans rather than automating a decision. This model automates the manual analysis of log data to identify and rank anomalous user activity for further investigation. Humans review the AI output prior to taking action on the result. They do so by reviewing AI outputs (i.e., model scores, feature ranking, user's historic log data) in a Tableau Dashboard and automated emailed reports. The benefits of this project are the automated multivariate analysis of CDW usage data in order to flag anomalous user activity that may otherwise go unnoticed via manual univariate analysis. The direct impact of this decision is the ability to routinely identify and evaluate high-risk events to avoid future security risks. No population is directly impacted by this AI as it is only used to inform the RAAS Infrastructure Security Team in order to conduct further outreach and investigation of anomalous user activity. . The AI model outputs a table of scored query activity per user per day where a higher score is interpreted as more anomalous activity, as well as feature ranking and score for each given output row where the score of a feature is the distance from the normal value for the population for that feature. The output of the broader system also includes supplemental score data such as flagging reoccurring anomalous behavior and the user's activity score percentile compared to the group they were modeled against. The model scores in addition to contextual information (i.e., discovery directory information and entitlement information) and supplemental statistics are displayed in a dashboard for the intended users.","the purpose of this ai project is to provide insights into access and usage patterns improving understanding of compliance data warehouse (cdw) data usage and personally identifiable information (pii) exposure risks. it does so by comparing a user's activity to a distribution of similar users, based on access groups, and provides a score based on how far from expected normal behavior that user's activity performed. this model is only used to inform humans rather than automating a decision. this model automates the manual analysis of log data to identify and rank anomalous user activity for further investigation. humans review the ai output prior to taking action on the result. they do so by reviewing ai outputs (i.e., model scores, feature ranking, user's historic log data) in a tableau dashboard and automated emailed reports. the benefits of this project are the automated multivariate analysis of cdw usage data in order to flag anomalous user activity that may otherwise go unnoticed via manual univariate analysis. the direct impact of this decision is the ability to routinely identify and evaluate high-risk events to avoid future security risks. no population is directly impacted by this ai as it is only used to inform the raas infrastructure security team in order to conduct further outreach and investigation of anomalous user activity. . the ai model outputs a table of scored query activity per user per day where a higher score is interpreted as more anomalous activity, as well as feature ranking and score for each given output row where the score of a feature is the distance from the normal value for the population for that feature. the output of the broader system also includes supplemental score data such as flagging reoccurring anomalous behavior and the user's activity score percentile compared to the group they were modeled against. the model scores in addition to contextual information (i.e., discovery directory information and entitlement information) and supplemental statistics are displayed in a dashboard for the intended users."
Ask-CFO Research Aide,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"Ask-CFO is an extension of the Research Aide developed using the Certara Composer platform originally designed for the Joint Committee on Taxation Research Aide (JCR) and Internal Revenue Manual Research Aid (IRMA) use cases. It is a Retrieval Augmented Generation (RAG) search on a number of Chief Financial Officer (CFO) specific documents. In addition to the existing research aide capability and user interface (UI), extensions will be developed to incorporate AI Agents that can perform actions such as document summary comparison over fiscal year, or extract information from tables.","This system uses an embedding model to produce a vector representation of texts for contextual searching, and Large Language Model to generate a response to user inquiries. Outputs of the tool include the model generated response to a user's inquiry, as well as the text from relevant documents identified in the retrieval stage and used by the model in generating said response.",Acquisition and/or Development,Neither,8/1/2024,8/1/2024,Unknown,Developed with contracting resources.,2032H823F00166,No,Unknown,Unknown,Unknown,No,Our team was provided with a sample of documents and reports used and produced by the office of the CFO. Text from these documents is used in this use-case.,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Unknown,Unknown,Yes,RAS-1,Less than 6 months,Yes,Other,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"Ask-CFO is an extension of the Research Aide developed using the Certara Composer platform originally designed for the Joint Committee on Taxation Research Aide (JCR) and Internal Revenue Manual Research Aid (IRMA) use cases. It is a Retrieval Augmented Generation (RAG) search on a number of Chief Financial Officer (CFO) specific documents. In addition to the existing research aide capability and user interface (UI), extensions will be developed to incorporate AI Agents that can perform actions such as document summary comparison over fiscal year, or extract information from tables. . This system uses an embedding model to produce a vector representation of texts for contextual searching, and Large Language Model to generate a response to user inquiries. Outputs of the tool include the model generated response to a user's inquiry, as well as the text from relevant documents identified in the retrieval stage and used by the model in generating said response.","ask-cfo is an extension of the research aide developed using the certara composer platform originally designed for the joint committee on taxation research aide (jcr) and internal revenue manual research aid (irma) use cases. it is a retrieval augmented generation (rag) search on a number of chief financial officer (cfo) specific documents. in addition to the existing research aide capability and user interface (ui), extensions will be developed to incorporate ai agents that can perform actions such as document summary comparison over fiscal year, or extract information from tables. . this system uses an embedding model to produce a vector representation of texts for contextual searching, and large language model to generate a response to user inquiries. outputs of the tool include the model generated response to a user's inquiry, as well as the text from relevant documents identified in the retrieval stage and used by the model in generating said response."
Redaction Studio - Redaction Assistant,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"Redaction Studio is a Commercial Off The Shelf (COTS) application that allows Criminal Investigation (CI) Special Agents to efficiently and securely redact body worn camera footage. The application provides an ""AI masking"" feature called Redaction Assistant that will attempt to automatically identify and mask (i.e. redact) items like faces. The masks proposed by the feature can be reviewed, adjusted, or removed by the Special Agent using the application. IRS:CI is not training or tuning a model, we are just proposing that Agents be able to use an ""AI""/ML feature.

Efficient redaction of body worn camera footage is needed to comply with Executive Order 14047 which requires the expedited public release of body worn camera footage following incidents involving serious bodily injury or death.

Axon, the creator of Redaction Studio, describes the assistant like this:
""The Redaction Assistant is an advanced AI tool within Redaction Studio that significantly speeds up the redaction process. By scanning through footage, it finds and suggests areas that may need redaction, such as faces or personal information, saving time and enhancing accuracy. Redaction Assistant is an Axon Redaction studio add-on and might not be available in all agencies.""

Some details on using it can be reviewed here:
https://my.axon.com/s/article/Using-Redaction-Assistant?language=en_US","Axon, the creator of Redaction Studio describes the assistant like this:
""The Redaction Assistant is an advanced AI tool within Redaction Studio that significantly speeds up the redaction process. By scanning through footage, it finds and suggests areas that may need redaction, such as faces or personal information, saving time and enhancing accuracy.""",Implementation and Assessment,Neither,1/1/2023,7/16/2024,11/30/2024,Developed with contracting resources.,2032H5-24-D-00007,No,No,No,Yes,Other,"This use case does not require datasets and does not involve training, fine-tuning or evaluation of a model.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,Yes,"CI-1 ,CI-2",Less than 6 months,Other,Other,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"Redaction Studio is a Commercial Off The Shelf (COTS) application that allows Criminal Investigation (CI) Special Agents to efficiently and securely redact body worn camera footage. The application provides an ""AI masking"" feature called Redaction Assistant that will attempt to automatically identify and mask (i.e. redact) items like faces. The masks proposed by the feature can be reviewed, adjusted, or removed by the Special Agent using the application. IRS:CI is not training or tuning a model, we are just proposing that Agents be able to use an ""AI""/ML feature.

Efficient redaction of body worn camera footage is needed to comply with Executive Order 14047 which requires the expedited public release of body worn camera footage following incidents involving serious bodily injury or death.

Axon, the creator of Redaction Studio, describes the assistant like this:
""The Redaction Assistant is an advanced AI tool within Redaction Studio that significantly speeds up the redaction process. By scanning through footage, it finds and suggests areas that may need redaction, such as faces or personal information, saving time and enhancing accuracy. Redaction Assistant is an Axon Redaction studio add-on and might not be available in all agencies.""

Some details on using it can be reviewed here:
https://my.axon.com/s/article/Using-Redaction-Assistant?language=en_US . Axon, the creator of Redaction Studio describes the assistant like this:
""The Redaction Assistant is an advanced AI tool within Redaction Studio that significantly speeds up the redaction process. By scanning through footage, it finds and suggests areas that may need redaction, such as faces or personal information, saving time and enhancing accuracy.""","redaction studio is a commercial off the shelf (cots) application that allows criminal investigation (ci) special agents to efficiently and securely redact body worn camera footage. the application provides an ""ai masking"" feature called redaction assistant that will attempt to automatically identify and mask (i.e. redact) items like faces. the masks proposed by the feature can be reviewed, adjusted, or removed by the special agent using the application. irs:ci is not training or tuning a model, we are just proposing that agents be able to use an ""ai""/ml feature. efficient redaction of body worn camera footage is needed to comply with executive order 14047 which requires the expedited public release of body worn camera footage following incidents involving serious bodily injury or death. axon, the creator of redaction studio, describes the assistant like this: ""the redaction assistant is an advanced ai tool within redaction studio that significantly speeds up the redaction process. by scanning through footage, it finds and suggests areas that may need redaction, such as faces or personal information, saving time and enhancing accuracy. redaction assistant is an axon redaction studio add-on and might not be available in all agencies."" some details on using it can be reviewed here: https://my.axon.com/s/article/using-redaction-assistant?language=en_us . axon, the creator of redaction studio describes the assistant like this: ""the redaction assistant is an advanced ai tool within redaction studio that significantly speeds up the redaction process. by scanning through footage, it finds and suggests areas that may need redaction, such as faces or personal information, saving time and enhancing accuracy."""
Taxpayer 90 (Palantir prototype),Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"Palantir AIP will interact with synthetic data and public information (e.g. IRM and tax forms) to summarize documentation and parse through information more quickly. AIP is enabled within SaaS platform (only within SaaS, no firewall changes/proxy exceptions needed), shared with Selection aNd Analytic Platform (SNAP), but AIP is only permissioned to users with the Taxpayer 90 Business Entitlement Access Request System (BEARS) entitlement. Data is contained within the Software as a Service (SaaS) tenant.",Summary of synthetic data and public information (e.g. Internal Revenue Manual (IRM) and tax forms).,Acquisition and/or Development,Neither,10/30/2024,9/4/2024,Unknown,Developed with contracting resources.,2032H5-18-A-00029,No,Unknown,Unknown,Unknown,No,Test data provided only for prototype development,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,Yes,SNAP-TP90,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"Palantir AIP will interact with synthetic data and public information (e.g. IRM and tax forms) to summarize documentation and parse through information more quickly. AIP is enabled within SaaS platform (only within SaaS, no firewall changes/proxy exceptions needed), shared with Selection aNd Analytic Platform (SNAP), but AIP is only permissioned to users with the Taxpayer 90 Business Entitlement Access Request System (BEARS) entitlement. Data is contained within the Software as a Service (SaaS) tenant. . Summary of synthetic data and public information (e.g. Internal Revenue Manual (IRM) and tax forms).","palantir aip will interact with synthetic data and public information (e.g. irm and tax forms) to summarize documentation and parse through information more quickly. aip is enabled within saas platform (only within saas, no firewall changes/proxy exceptions needed), shared with selection and analytic platform (snap), but aip is only permissioned to users with the taxpayer 90 business entitlement access request system (bears) entitlement. data is contained within the software as a service (saas) tenant. . summary of synthetic data and public information (e.g. internal revenue manual (irm) and tax forms)."
Expanded Systran Translation Software,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"Internal Revenue Service-Criminal Investigation (IRS-CI) is looking to acquire a high-volume enterprise license for Systran.  This is the same product that Research, Applied Analytics and Statistics (RAAS) currently uses, but will be expanded to allow high volume processing through multiple Graphic Processing Units (GPUs), enabling large quantities of documents to be translated in a relatively short time.  IRS-CI needs this capability to translate seized data in foreign languages.  ",The output is a PDF document containing the translation of the documents.,Acquisition and/or Development,Neither,3/15/2024,11/5/2024,Unknown,Developed with contracting resources.,No contract number as acquisition is still pending,No,Unknown,Unknown,Unknown,Other,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,Yes,"CI-1, CI-2",Less than 6 months,Yes,Other,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,"Secure storage of content at rest and in flight, we are using Google Cloud resource that will be hardened and secured in the IRS environment for production deployment",Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"Internal Revenue Service-Criminal Investigation (IRS-CI) is looking to acquire a high-volume enterprise license for Systran.  This is the same product that Research, Applied Analytics and Statistics (RAAS) currently uses, but will be expanded to allow high volume processing through multiple Graphic Processing Units (GPUs), enabling large quantities of documents to be translated in a relatively short time.  IRS-CI needs this capability to translate seized data in foreign languages. . The output is a PDF document containing the translation of the documents. . Secure storage of content at rest and in flight, we are using Google Cloud resource that will be hardened and secured in the IRS environment for production deployment","internal revenue service-criminal investigation (irs-ci) is looking to acquire a high-volume enterprise license for systran. this is the same product that research, applied analytics and statistics (raas) currently uses, but will be expanded to allow high volume processing through multiple graphic processing units (gpus), enabling large quantities of documents to be translated in a relatively short time. irs-ci needs this capability to translate seized data in foreign languages. . the output is a pdf document containing the translation of the documents. . secure storage of content at rest and in flight, we are using google cloud resource that will be hardened and secured in the irs environment for production deployment"
Generative AI for Form 1040X: Tax Examiner Assistant,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"Develop a workflow assistant that leverages generative AI to consolidate and analyze complex policies from various documents (e.g., IRMs, SERP, desk guides). This tool will leverage Large Language Model (LLM) to provide real-time, accurate guidance for Tax Examiners handling individual amended returns. Future iterations will incorporate additional features that may include generating correspondence for taxpayers that prioritizes their actions. Key stakeholders, including Research, Applied Analytics, and Statistics (RAAS), Information Technology (IT), Submission Processing, and Accounts Management, will collaborate to enhance existing RAAS capabilities, integrate new documents, and fine-tune the solution for this specific use case. The tool will be made available to a select pool of Tax Examiners, who will verify and validate the outputs of the tool prior to finalizing the processing of the amended returns. Benefits and impacts include operational efficiency, reduce processing times, and provide timely, accurate guidance.",The 1040X Tax Examiner Assistant tool produces a comprehensive response with a plain text summary of the answer with relevant resources and links to the resources. The tool also outputs a feedback prompt where Tax Examiners can provide user feedback.,Acquisition and/or Development,Neither,9/3/2024,9/12/2024,Unknown,Developed with contracting resources.,2032H5-24-P-00086,No,Unknown,Unknown,Unknown,No,"Text and excerpts from publicly available, non-sensitive data from Internal Revenue Manual (IRM) 21, IRM 3, and Document 6209.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"Develop a workflow assistant that leverages generative AI to consolidate and analyze complex policies from various documents (e.g., IRMs, SERP, desk guides). This tool will leverage Large Language Model (LLM) to provide real-time, accurate guidance for Tax Examiners handling individual amended returns. Future iterations will incorporate additional features that may include generating correspondence for taxpayers that prioritizes their actions. Key stakeholders, including Research, Applied Analytics, and Statistics (RAAS), Information Technology (IT), Submission Processing, and Accounts Management, will collaborate to enhance existing RAAS capabilities, integrate new documents, and fine-tune the solution for this specific use case. The tool will be made available to a select pool of Tax Examiners, who will verify and validate the outputs of the tool prior to finalizing the processing of the amended returns. Benefits and impacts include operational efficiency, reduce processing times, and provide timely, accurate guidance. . The 1040X Tax Examiner Assistant tool produces a comprehensive response with a plain text summary of the answer with relevant resources and links to the resources. The tool also outputs a feedback prompt where Tax Examiners can provide user feedback.","develop a workflow assistant that leverages generative ai to consolidate and analyze complex policies from various documents (e.g., irms, serp, desk guides). this tool will leverage large language model (llm) to provide real-time, accurate guidance for tax examiners handling individual amended returns. future iterations will incorporate additional features that may include generating correspondence for taxpayers that prioritizes their actions. key stakeholders, including research, applied analytics, and statistics (raas), information technology (it), submission processing, and accounts management, will collaborate to enhance existing raas capabilities, integrate new documents, and fine-tune the solution for this specific use case. the tool will be made available to a select pool of tax examiners, who will verify and validate the outputs of the tool prior to finalizing the processing of the amended returns. benefits and impacts include operational efficiency, reduce processing times, and provide timely, accurate guidance. . the 1040x tax examiner assistant tool produces a comprehensive response with a plain text summary of the answer with relevant resources and links to the resources. the tool also outputs a feedback prompt where tax examiners can provide user feedback."
Automated Collection System (ACS) Voicebot,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,"This project developed self-service application in four phases that allows taxpayer to mitigate various issues regarding payments, balance dues, account history etc. 
SBSE Phase 1-4:
Automated Collection System (ACS) Conversational Interactive Voice Response (IVR) (ACI) Phase 1, Voice Balance Due (VDB): Taxpayers may set up payment plans, extensions, monthly payment amounts, payment dates, revise agreements and form of payment. (Prod Date: 6/14/22)
ACI Phase 2, Location and Transcript: Provides taxpayers with information regarding mailing addresses for forms based off the caller’s zip code. Transcript conducts enhanced business operating division routing (EBR) checks and enables taxpayers to obtain a transcript of their tax return, and/or transcript of their account. (Prod Date: 8/23/22)
ACI Phase 3, Payoff and View Credit: Allows taxpayers to find out the amount of any outstanding balances with the IRS. This includes Get Payoff Date, Mailing Instructions and the ability to transfer to VBD. (Prod Date: 9/28/22)
ACI Phase 4, View Debit: Authorizes and allows the taxpayer to  choose any tax year on record and the application will provide a history of their account. (Prod Date: 1/5/23)
The Intent Engine (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests).  The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response.","The AI outputs include responses tailored to taxpayer queries, such as payment plan setup, balance inquires, mailing address information, transcript requests, and account history details. If the AI cannot classify the input confidently, it prompts for clarification or escalates to a live agent.",Operation and Maintenance,Neither,5/3/2021,1/18/2022,6/14/2022,Developed with contracting resources.,2032H5-19-F-00641,No,No,No,Yes,Yes,"System uses historical data from ACS call logs, payments plans, and account balance queries. ","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.",No,No – agency does not have access to source code.,Yes,Nuance Digital Contact Center Platform (DCCP),Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"This project developed self-service application in four phases that allows taxpayer to mitigate various issues regarding payments, balance dues, account history etc. 
SBSE Phase 1-4:
Automated Collection System (ACS) Conversational Interactive Voice Response (IVR) (ACI) Phase 1, Voice Balance Due (VDB): Taxpayers may set up payment plans, extensions, monthly payment amounts, payment dates, revise agreements and form of payment. (Prod Date: 6/14/22)
ACI Phase 2, Location and Transcript: Provides taxpayers with information regarding mailing addresses for forms based off the caller’s zip code. Transcript conducts enhanced business operating division routing (EBR) checks and enables taxpayers to obtain a transcript of their tax return, and/or transcript of their account. (Prod Date: 8/23/22)
ACI Phase 3, Payoff and View Credit: Allows taxpayers to find out the amount of any outstanding balances with the IRS. This includes Get Payoff Date, Mailing Instructions and the ability to transfer to VBD. (Prod Date: 9/28/22)
ACI Phase 4, View Debit: Authorizes and allows the taxpayer to  choose any tax year on record and the application will provide a history of their account. (Prod Date: 1/5/23)
The Intent Engine (using NLP and AI algorithms) classifies taxpayers' utterances (questions) into intents (requests).  The responses of the voicebot and business logic is predetermined by content owners. This AI tool aims to classify/navigate to a correct predetermined response. . The AI outputs include responses tailored to taxpayer queries, such as payment plan setup, balance inquires, mailing address information, transcript requests, and account history details. If the AI cannot classify the input confidently, it prompts for clarification or escalates to a live agent.","this project developed self-service application in four phases that allows taxpayer to mitigate various issues regarding payments, balance dues, account history etc. sbse phase 1-4: automated collection system (acs) conversational interactive voice response (ivr) (aci) phase 1, voice balance due (vdb): taxpayers may set up payment plans, extensions, monthly payment amounts, payment dates, revise agreements and form of payment. (prod date: 6/14/22) aci phase 2, location and transcript: provides taxpayers with information regarding mailing addresses for forms based off the caller’s zip code. transcript conducts enhanced business operating division routing (ebr) checks and enables taxpayers to obtain a transcript of their tax return, and/or transcript of their account. (prod date: 8/23/22) aci phase 3, payoff and view credit: allows taxpayers to find out the amount of any outstanding balances with the irs. this includes get payoff date, mailing instructions and the ability to transfer to vbd. (prod date: 9/28/22) aci phase 4, view debit: authorizes and allows the taxpayer to choose any tax year on record and the application will provide a history of their account. (prod date: 1/5/23) the intent engine (using nlp and ai algorithms) classifies taxpayers' utterances (questions) into intents (requests). the responses of the voicebot and business logic is predetermined by content owners. this ai tool aims to classify/navigate to a correct predetermined response. . the ai outputs include responses tailored to taxpayer queries, such as payment plan setup, balance inquires, mailing address information, transcript requests, and account history details. if the ai cannot classify the input confidently, it prompts for clarification or escalates to a live agent."
Customer Experience Analytics,Department of the Treasury,TREAS,Internal Revenue Service,Mission-Enabling,None of the above.,Extract insights from data sources in the customer service domain to better understand service interactions and create new opportunities to improve the customer experience.,Summary of call transcripts between IRS and Taxpayer,Acquisition and/or Development,Neither,10/1/2023,10/2/2023,Unknown,Developed with contracting resources.,2032H8-21-F-00179,No,Unknown,Unknown,Unknown,Yes,Call Transcripts,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,Yes,RAS-1,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,Extract insights from data sources in the customer service domain to better understand service interactions and create new opportunities to improve the customer experience. . Summary of call transcripts between IRS and Taxpayer,extract insights from data sources in the customer service domain to better understand service interactions and create new opportunities to improve the customer experience. . summary of call transcripts between irs and taxpayer
Machine Learning for Peace,United States Agency for International Development,USAID,"USAID/Bureau for Democracy, Human Rights, and Governance",Diplomacy & Trade,None of the above.,"DRG is partnering with a seven-member consortium, including researchers at Duke University’s DevLab, to build an Artificial Intelligence (AI) tool that can forecast closing civic spaces -- patterns of events that result in curtailed freedoms and impaired democracy.","Event labels, forecasts",Operation and Maintenance,Neither,7/10/1905,7/10/1905,7/14/1905,Developed with contracting resources.,GS-10F-0294V/7200AA23M00014,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"DRG is partnering with a seven-member consortium, including researchers at Duke University’s DevLab, to build an Artificial Intelligence (AI) tool that can forecast closing civic spaces -- patterns of events that result in curtailed freedoms and impaired democracy. . Event labels, forecasts","drg is partnering with a seven-member consortium, including researchers at duke university’s devlab, to build an artificial intelligence (ai) tool that can forecast closing civic spaces -- patterns of events that result in curtailed freedoms and impaired democracy. . event labels, forecasts"
Qure.AI integrated in mobile X-ray screening to improve community TB case finding,United States Agency for International Development,USAID,USAID/Vietnam,Health & Medical,None of the above.,The USAID Support to End TB project applies Qure.AI in mobile X-ray van to improve the quality of X-ray screening under community active TB case finding efforts in Vietnam. AI system is also critical to shorten the screening time and improve TB detection accuracy.,Image classification,Operation and Maintenance,Both,7/12/1905,7/13/1905,7/13/1905,Developed with contracting resources.,72044020CA00002,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,,Unknown,Unknown,No - Some individual decisions or actions require direct human oversight.,Unknown,Unknown,Unknown,Both,0.5396825396825397,The USAID Support to End TB project applies Qure.AI in mobile X-ray van to improve the quality of X-ray screening under community active TB case finding efforts in Vietnam. AI system is also critical to shorten the screening time and improve TB detection accuracy. . Image classification,the usaid support to end tb project applies qure.ai in mobile x-ray van to improve the quality of x-ray screening under community active tb case finding efforts in vietnam. ai system is also critical to shorten the screening time and improve tb detection accuracy. . image classification
USING PREDICTIVE ANALYTICS TO IMPROVE CARE,United States Agency for International Development,USAID,"USAID/Bureau for Inclusive Growth, Partnerships, and Innovation",Health & Medical,None of the above.,"1) identify high-risk patients for targeted intervention - in this specific use case it's children who are at risk of malnutrition but it is meant to eventually be applicable in other settings beyond health, 2) identify frontline health workers / caregivers in need of additional follow up and support from a community health facilitator (in this project it's a community facilitator but outside of this project it would likely be a supervisor in the future)",,Implementation and Assessment,Both,7/11/1905,7/1/2020,10/1/2024,Developed with contracting resources.,7200AA20FA00023,No,No,No,No,Yes,Unknown,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,Yes – source code is publicly available.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Planned or in-progress.,"For this particular use case - nutrition outcome monitoring and prediction, one of the key challenges is stable and continued availability of children and their caregivers to enable continuous tracking of changes in their anthropometric measures. From a technical standpoint, the key risk for using AI in any targeted health intervention is the risk that it negatively impacts the baseline intervention (i.e. redirects resources in a way that negatively impacts the program's impact). This risk is avoided by strictly supplementing implementing partners' existing operations.",Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"For this study, Dimagi has built a predictive algorithm to generate scores to identify those at highest risk for worsening malnutrition outcomes in order to target an additional intervention. The algorithm is run once at the beginning of the study to identify those to enroll in the top-up intervention, the efficacy of the model is being monitored and evaluated via an RCT.",Unknown,Unknown,Both,0.6349206349206349,"1) identify high-risk patients for targeted intervention - in this specific use case it's children who are at risk of malnutrition but it is meant to eventually be applicable in other settings beyond health, 2) identify frontline health workers / caregivers in need of additional follow up and support from a community health facilitator (in this project it's a community facilitator but outside of this project it would likely be a supervisor in the future) . For this particular use case - nutrition outcome monitoring and prediction, one of the key challenges is stable and continued availability of children and their caregivers to enable continuous tracking of changes in their anthropometric measures. From a technical standpoint, the key risk for using AI in any targeted health intervention is the risk that it negatively impacts the baseline intervention (i.e. redirects resources in a way that negatively impacts the program's impact). This risk is avoided by strictly supplementing implementing partners' existing operations.","1) identify high-risk patients for targeted intervention - in this specific use case it's children who are at risk of malnutrition but it is meant to eventually be applicable in other settings beyond health, 2) identify frontline health workers / caregivers in need of additional follow up and support from a community health facilitator (in this project it's a community facilitator but outside of this project it would likely be a supervisor in the future) . for this particular use case - nutrition outcome monitoring and prediction, one of the key challenges is stable and continued availability of children and their caregivers to enable continuous tracking of changes in their anthropometric measures. from a technical standpoint, the key risk for using ai in any targeted health intervention is the risk that it negatively impacts the baseline intervention (i.e. redirects resources in a way that negatively impacts the program's impact). this risk is avoided by strictly supplementing implementing partners' existing operations."
Market segmentation,United States Agency for International Development,USAID,USAID/Bureau for Global Health (GH),Diplomacy & Trade,None of the above.,"Use of analytical approaches to increase the understanding of the market size and characteristics for private HIV treatment and services. Utilizes existing data to profile people living with HIV/AIDS (PLHIV) accessing HIV services and treatment based on willingness to pay (WTP) to create profiles incorporating demographic dimensions, geographic dimensions, and temporal dimensions to help define the size of the market and for HIV treatment services for PLHIV. Data sources include geospatial datasets, market data on comparable products, and micro-economic data. The use of novel data sources creates outputs in the form of patient profiles or segments at the global, regional, and/or country level to better understand which patients would be willing to pay for HIV treatments or services. Use of the market segmentation activity with development of a country-agnostic approach helps profile and estimate the size of a specific population group of PLHIV based on their willingness to pay for HIV services that can be applied to any PEPFAR-priority country context.","Reports, visualization",Retired,Neither,Unknown,Unknown,Unknown,Developed with contracting resources.,7200AA19CA0004,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"Use of analytical approaches to increase the understanding of the market size and characteristics for private HIV treatment and services. Utilizes existing data to profile people living with HIV/AIDS (PLHIV) accessing HIV services and treatment based on willingness to pay (WTP) to create profiles incorporating demographic dimensions, geographic dimensions, and temporal dimensions to help define the size of the market and for HIV treatment services for PLHIV. Data sources include geospatial datasets, market data on comparable products, and micro-economic data. The use of novel data sources creates outputs in the form of patient profiles or segments at the global, regional, and/or country level to better understand which patients would be willing to pay for HIV treatments or services. Use of the market segmentation activity with development of a country-agnostic approach helps profile and estimate the size of a specific population group of PLHIV based on their willingness to pay for HIV services that can be applied to any PEPFAR-priority country context. . Reports, visualization","use of analytical approaches to increase the understanding of the market size and characteristics for private hiv treatment and services. utilizes existing data to profile people living with hiv/aids (plhiv) accessing hiv services and treatment based on willingness to pay (wtp) to create profiles incorporating demographic dimensions, geographic dimensions, and temporal dimensions to help define the size of the market and for hiv treatment services for plhiv. data sources include geospatial datasets, market data on comparable products, and micro-economic data. the use of novel data sources creates outputs in the form of patient profiles or segments at the global, regional, and/or country level to better understand which patients would be willing to pay for hiv treatments or services. use of the market segmentation activity with development of a country-agnostic approach helps profile and estimate the size of a specific population group of plhiv based on their willingness to pay for hiv services that can be applied to any pepfar-priority country context. . reports, visualization"
Automated interpretation of Line Probe Assay strips using computer vision,United States Agency for International Development,USAID,USAID/India,Health & Medical,None of the above.,"The AI automates the interpretation of Line Probe Assay (LPA) results, enhancing accuracy, efficiency, and consistency in detecting drug-resistant TB. It reduces manual errors, accelerates diagnosis, and integrates with the Ni-kshay platform to support timely treatment decisions for TB patients.","The AI system detects the bands present on the LPA strip and outputs the sensitivity to different drugs, automatically classifying the test results as resistant or susceptible. It generates a final diagnostic report that helps clinicians make informed treatment decisions based on the presence or absence of drug resistance",Acquisition and/or Development,Both,6/1/2020,7/1/2020,Unknown,Developed with contracting resources., 72038620CA00006,No,No,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,No,"- Misinterpretation of Results: Incorrect interpretation of AI outputs could lead to delayed or incorrect treatment. This was identified through validation studies and expert reviews.
- Integration Disruption: The AI model is hosted in Ni-kshay (Official Health Management Information System for TB in India). Any outage or disruption in Ni-kshay could impact the AI's availability and functionality. This risk was identified during system integration and stress testing.",Yes – by another appropriate agency office that was not directly involved in the system’s development,"Established Process of Machine Learning Operations: Alongside automated testing and drift detection, model re-training and re-deployments are supported by continuous integration pipelines that are managed by machine learning and data engineers on the platform, adapting work done by data science team into repeatable scripts for re-training and re-testing a model once deployed.",No - Some individual decisions or actions require direct human oversight.,Unknown,Unknown,Unknown,Both,0.6031746031746031,"The AI automates the interpretation of Line Probe Assay (LPA) results, enhancing accuracy, efficiency, and consistency in detecting drug-resistant TB. It reduces manual errors, accelerates diagnosis, and integrates with the Ni-kshay platform to support timely treatment decisions for TB patients. . The AI system detects the bands present on the LPA strip and outputs the sensitivity to different drugs, automatically classifying the test results as resistant or susceptible. It generates a final diagnostic report that helps clinicians make informed treatment decisions based on the presence or absence of drug resistance . - Misinterpretation of Results: Incorrect interpretation of AI outputs could lead to delayed or incorrect treatment. This was identified through validation studies and expert reviews.
- Integration Disruption: The AI model is hosted in Ni-kshay (Official Health Management Information System for TB in India). Any outage or disruption in Ni-kshay could impact the AI's availability and functionality. This risk was identified during system integration and stress testing.","the ai automates the interpretation of line probe assay (lpa) results, enhancing accuracy, efficiency, and consistency in detecting drug-resistant tb. it reduces manual errors, accelerates diagnosis, and integrates with the ni-kshay platform to support timely treatment decisions for tb patients. . the ai system detects the bands present on the lpa strip and outputs the sensitivity to different drugs, automatically classifying the test results as resistant or susceptible. it generates a final diagnostic report that helps clinicians make informed treatment decisions based on the presence or absence of drug resistance . - misinterpretation of results: incorrect interpretation of ai outputs could lead to delayed or incorrect treatment. this was identified through validation studies and expert reviews. - integration disruption: the ai model is hosted in ni-kshay (official health management information system for tb in india). any outage or disruption in ni-kshay could impact the ai's availability and functionality. this risk was identified during system integration and stress testing."
Prediction of risk for Loss to Follow Up (LFU) among TB patients using AI,United States Agency for International Development,USAID,USAID/India,Health & Medical,None of the above.,"The AI predicts the likelihood of adverse treatment outcomes in TB patients, such as treatment failure or mortality. By identifying high-risk patients early, It facilitates timely interventions, improves patient outcomes, optimizes resource allocation, and enhances the effectiveness of TB treatment programs.","AI model outputs a risk score for each patient that is converted to a HIGH-RISK or LOW-RISK label after comparing it with a district-wise threshold. This helps prioritize patients for timely interventions, improving treatment outcomes and reducing the risk of adverse events",Operation and Maintenance,Both,6/1/2020,7/1/2020,3/1/2023,Developed with contracting resources., 72038620CA00006,No,No,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,"- Trust: There are two issues related to trust: a. the possibility of trusting model outputs over human judgement could lead to missing of some high risk cases b. Model interpretability poses a challenge, as the algorithms may hinder healthcare professionals' ability to trust or validate predictions. 
- Accuracy: The solution may produce inaccurate or inequitable predictions due to incomplete or unrepresentative data. This could lead to underperformance on certain sensitive cohorts, further affecting trust in AI-generated outcomes.
- These risks were identified and addressed through model evaluation, regular internal data audits and routine monitoring of pilot deployments.",Unknown,"Established Process of Machine Learning Operations: Alongside automated testing and drift detection, model re-training and re-deployments are supported by continuous integration pipelines that are managed by machine learning and data engineers on the platform, adapting work done by data science team into repeatable scripts for re-training and re-testing a model once deployed.",No - Some individual decisions or actions require direct human oversight.,Unknown,Unknown,Unknown,Both,0.6190476190476191,"The AI predicts the likelihood of adverse treatment outcomes in TB patients, such as treatment failure or mortality. By identifying high-risk patients early, It facilitates timely interventions, improves patient outcomes, optimizes resource allocation, and enhances the effectiveness of TB treatment programs. . AI model outputs a risk score for each patient that is converted to a HIGH-RISK or LOW-RISK label after comparing it with a district-wise threshold. This helps prioritize patients for timely interventions, improving treatment outcomes and reducing the risk of adverse events . - Trust: There are two issues related to trust: a. the possibility of trusting model outputs over human judgement could lead to missing of some high risk cases b. Model interpretability poses a challenge, as the algorithms may hinder healthcare professionals' ability to trust or validate predictions. 
- Accuracy: The solution may produce inaccurate or inequitable predictions due to incomplete or unrepresentative data. This could lead to underperformance on certain sensitive cohorts, further affecting trust in AI-generated outcomes.
- These risks were identified and addressed through model evaluation, regular internal data audits and routine monitoring of pilot deployments.","the ai predicts the likelihood of adverse treatment outcomes in tb patients, such as treatment failure or mortality. by identifying high-risk patients early, it facilitates timely interventions, improves patient outcomes, optimizes resource allocation, and enhances the effectiveness of tb treatment programs. . ai model outputs a risk score for each patient that is converted to a high-risk or low-risk label after comparing it with a district-wise threshold. this helps prioritize patients for timely interventions, improving treatment outcomes and reducing the risk of adverse events . - trust: there are two issues related to trust: a. the possibility of trusting model outputs over human judgement could lead to missing of some high risk cases b. model interpretability poses a challenge, as the algorithms may hinder healthcare professionals' ability to trust or validate predictions. - accuracy: the solution may produce inaccurate or inequitable predictions due to incomplete or unrepresentative data. this could lead to underperformance on certain sensitive cohorts, further affecting trust in ai-generated outcomes. - these risks were identified and addressed through model evaluation, regular internal data audits and routine monitoring of pilot deployments."
"Social listening to inform youth programming in Colombia (previously ""Communications Strategy"")",United States Agency for International Development,USAID,USAID/Colombia,Diplomacy & Trade,None of the above.,"Youth conversations are dynamic and are defined by geographic location, socioeconomic conditions, conflicts with the law, environment, level of vulnerability, gender and ethnicity. Youth develop their voices and identity and mobilize through the media, especially social networks. Active listening methods are a fundamental element to understand their interests, role models, and relevant issues in the youth agenda, in order to design campaigns and communication strategies that are relevant, segmented, and engaging. Consequently, the Youth Resilience Activity (Programa Jovenes Resilientes #EfectoColectivo), leverages Artificial Intelligence tools developed by Audience Insights (in partnership with Twitter and Facebook), using ethical data analysis guidelines (anonymization without personal identifying information). The tool collects and analyzes findings on defined variables (e.g., location, age, followers of an @ or #, interests), allowing us to understand socio-demographic trends and general correlations with greater precision. This allows the Program to creatively develop campaigns and strategies that are highly relevant to young people and their contexts, using distribution channels that they most often consult, thereby increasing the messages’ effectiveness, relevance and engagement with youth. Using the IBM Watson platform, the Audience Insights tool provides information on audience affinity, general interest trends, media following, rankings, and personality profiles. In addition, the Insights tool for Facebook provides general data on the number of people in a specific geography or segmentation, juxtapositions (for example, which other accounts they follow) and percentages of types of devices on which they connect.","Reports, visualizations",Retired,Neither,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"Youth conversations are dynamic and are defined by geographic location, socioeconomic conditions, conflicts with the law, environment, level of vulnerability, gender and ethnicity. Youth develop their voices and identity and mobilize through the media, especially social networks. Active listening methods are a fundamental element to understand their interests, role models, and relevant issues in the youth agenda, in order to design campaigns and communication strategies that are relevant, segmented, and engaging. Consequently, the Youth Resilience Activity (Programa Jovenes Resilientes #EfectoColectivo), leverages Artificial Intelligence tools developed by Audience Insights (in partnership with Twitter and Facebook), using ethical data analysis guidelines (anonymization without personal identifying information). The tool collects and analyzes findings on defined variables (e.g., location, age, followers of an @ or #, interests), allowing us to understand socio-demographic trends and general correlations with greater precision. This allows the Program to creatively develop campaigns and strategies that are highly relevant to young people and their contexts, using distribution channels that they most often consult, thereby increasing the messages’ effectiveness, relevance and engagement with youth. Using the IBM Watson platform, the Audience Insights tool provides information on audience affinity, general interest trends, media following, rankings, and personality profiles. In addition, the Insights tool for Facebook provides general data on the number of people in a specific geography or segmentation, juxtapositions (for example, which other accounts they follow) and percentages of types of devices on which they connect. . Reports, visualizations","youth conversations are dynamic and are defined by geographic location, socioeconomic conditions, conflicts with the law, environment, level of vulnerability, gender and ethnicity. youth develop their voices and identity and mobilize through the media, especially social networks. active listening methods are a fundamental element to understand their interests, role models, and relevant issues in the youth agenda, in order to design campaigns and communication strategies that are relevant, segmented, and engaging. consequently, the youth resilience activity (programa jovenes resilientes #efectocolectivo), leverages artificial intelligence tools developed by audience insights (in partnership with twitter and facebook), using ethical data analysis guidelines (anonymization without personal identifying information). the tool collects and analyzes findings on defined variables (e.g., location, age, followers of an @ or #, interests), allowing us to understand socio-demographic trends and general correlations with greater precision. this allows the program to creatively develop campaigns and strategies that are highly relevant to young people and their contexts, using distribution channels that they most often consult, thereby increasing the messages’ effectiveness, relevance and engagement with youth. using the ibm watson platform, the audience insights tool provides information on audience affinity, general interest trends, media following, rankings, and personality profiles. in addition, the insights tool for facebook provides general data on the number of people in a specific geography or segmentation, juxtapositions (for example, which other accounts they follow) and percentages of types of devices on which they connect. . reports, visualizations"
AI-enabled classification for Standardized Program Structure and Definitions,United States Agency for International Development,USAID,USAID/Bureau for the Middle East,Diplomacy & Trade,None of the above.,"Time-saving automated categorization of information into pre-established definitions. U.S. foreign assistance is categorized using a specific system known as the Standardized Program Structure and Definitions (SPSD). This is important for many reasons. First, the SPSD is comprised of broadly agreed-upon definitions for foreign assistance programs, providing a common language to describe programs. Second, by utilizing a common language, information for various types of programs can be aggregated within a country, regionally or globally allowing for the comparison and analysis of budget and performance data.",Indicators are sorted into the Standardized Program Structure and Definitions,Operation and Maintenance,Neither,1/31/2022,5/20/2022,7/15/2022,Developed with contracting resources.,GS00F280GA/7200AA19M00017,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Time-saving automated categorization of information into pre-established definitions. U.S. foreign assistance is categorized using a specific system known as the Standardized Program Structure and Definitions (SPSD). This is important for many reasons. First, the SPSD is comprised of broadly agreed-upon definitions for foreign assistance programs, providing a common language to describe programs. Second, by utilizing a common language, information for various types of programs can be aggregated within a country, regionally or globally allowing for the comparison and analysis of budget and performance data. . Indicators are sorted into the Standardized Program Structure and Definitions","time-saving automated categorization of information into pre-established definitions. u.s. foreign assistance is categorized using a specific system known as the standardized program structure and definitions (spsd). this is important for many reasons. first, the spsd is comprised of broadly agreed-upon definitions for foreign assistance programs, providing a common language to describe programs. second, by utilizing a common language, information for various types of programs can be aggregated within a country, regionally or globally allowing for the comparison and analysis of budget and performance data. . indicators are sorted into the standardized program structure and definitions"
Natural Language Processing to detect gender-based violence in Kenya,United States Agency for International Development,USAID,USAID/Kenya and East Africa,Diplomacy & Trade,None of the above.,"This tool will detect patterns of technology-facilitated gender based violence (TFGBV), provide early identification of potential threats, and facilitate automatic and anonymous reporting.  The grantee is planning to employ machine learning algorithms for pattern detection, natural language processing (NLP), computer vision, and sentiment analysis.","Reports, visualizations",Acquisition and/or Development,Neither,9/1/2023,4/1/2024,Unknown,Developed with contracting resources.,7200AA22RFA00002,No,Unknown,Unknown,Unknown,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"This tool will detect patterns of technology-facilitated gender based violence (TFGBV), provide early identification of potential threats, and facilitate automatic and anonymous reporting.  The grantee is planning to employ machine learning algorithms for pattern detection, natural language processing (NLP), computer vision, and sentiment analysis. . Reports, visualizations","this tool will detect patterns of technology-facilitated gender based violence (tfgbv), provide early identification of potential threats, and facilitate automatic and anonymous reporting. the grantee is planning to employ machine learning algorithms for pattern detection, natural language processing (nlp), computer vision, and sentiment analysis. . reports, visualizations"
Identifying harmful social media content in Sinhala and Tamil,United States Agency for International Development,USAID,USAID/Sri Lanka,Diplomacy & Trade,None of the above.,"The activity will develop AI tools that can identify hate speech online, and to design various counter-measures, ranging from streamlined mechanisms to report such content to crafting counter-narratives that can mitigate the negative effects on targeted communities",Reports on harmful content and content for counter narratives ,Acquisition and/or Development,Neither,7/1/2024,7/1/2024,Unknown,Developed with contracting resources.,Unknown,No,Unknown,Unknown,Unknown,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.38095238095238093,"The activity will develop AI tools that can identify hate speech online, and to design various counter-measures, ranging from streamlined mechanisms to report such content to crafting counter-narratives that can mitigate the negative effects on targeted communities . Reports on harmful content and content for counter narratives","the activity will develop ai tools that can identify hate speech online, and to design various counter-measures, ranging from streamlined mechanisms to report such content to crafting counter-narratives that can mitigate the negative effects on targeted communities . reports on harmful content and content for counter narratives"
Value4Her Connect Digital Platform,United States Agency for International Development,USAID,"USAID/Bureau for Resilience, Environment, and Food Security",Diplomacy & Trade,None of the above.,"The intended purpose of the AI is to provide personalized learning experiences, offering tutorials, courses, and resources tailored to the needs and interests of women in agriculture. The Value4Her Connect Digital Platform can connect women entrepreneurs, fostering collaboration and the exchange of ideas and best practices. AI will help women entrepreneurs access markets by providing insights into market trends, demand forecasts, and pricing strategies. The expected benefits of leveraging AI on the platform include empowering women with the skills and knowledge needed to innovate and lead in the agricultural sector.   This will improve efficiency and productivity and contribute to the economic growth of the women businesses and eventually to the agricultural sector.  AI will also be able to analyze vast amounts of data to provide actionable insights, helping women make informed decisions about their farming practices.","The outputs of the Value4Her Connect Digital Platform include:

* Personalized recommendations building on tailored educational content based on the user's interests and knowledge level.
* Insights into market demand and price trends to help women decide what to invest in
* Alerts and updates on the business performance identifying risks and opportunities.
* Networking and collaboration tools for discussion and exchange of ideas among young farmers.
* Connecting less experienced women with mentors who can provide guidance and support.
* Detailed reports on the profitability of various businesses and ventures.
* Calculations of the farm's carbon footprint and recommendations for reducing it.",Initiated,Neither,9/1/2024,Unknown,Unknown,Developed with contracting resources.,AID-OAA-A-17-00029 (PIATA),No,Unknown,Unknown,Unknown,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.38095238095238093,"The intended purpose of the AI is to provide personalized learning experiences, offering tutorials, courses, and resources tailored to the needs and interests of women in agriculture. The Value4Her Connect Digital Platform can connect women entrepreneurs, fostering collaboration and the exchange of ideas and best practices. AI will help women entrepreneurs access markets by providing insights into market trends, demand forecasts, and pricing strategies. The expected benefits of leveraging AI on the platform include empowering women with the skills and knowledge needed to innovate and lead in the agricultural sector.   This will improve efficiency and productivity and contribute to the economic growth of the women businesses and eventually to the agricultural sector.  AI will also be able to analyze vast amounts of data to provide actionable insights, helping women make informed decisions about their farming practices. . The outputs of the Value4Her Connect Digital Platform include:

* Personalized recommendations building on tailored educational content based on the user's interests and knowledge level.
* Insights into market demand and price trends to help women decide what to invest in
* Alerts and updates on the business performance identifying risks and opportunities.
* Networking and collaboration tools for discussion and exchange of ideas among young farmers.
* Connecting less experienced women with mentors who can provide guidance and support.
* Detailed reports on the profitability of various businesses and ventures.
* Calculations of the farm's carbon footprint and recommendations for reducing it.","the intended purpose of the ai is to provide personalized learning experiences, offering tutorials, courses, and resources tailored to the needs and interests of women in agriculture. the value4her connect digital platform can connect women entrepreneurs, fostering collaboration and the exchange of ideas and best practices. ai will help women entrepreneurs access markets by providing insights into market trends, demand forecasts, and pricing strategies. the expected benefits of leveraging ai on the platform include empowering women with the skills and knowledge needed to innovate and lead in the agricultural sector. this will improve efficiency and productivity and contribute to the economic growth of the women businesses and eventually to the agricultural sector. ai will also be able to analyze vast amounts of data to provide actionable insights, helping women make informed decisions about their farming practices. . the outputs of the value4her connect digital platform include: * personalized recommendations building on tailored educational content based on the user's interests and knowledge level. * insights into market demand and price trends to help women decide what to invest in * alerts and updates on the business performance identifying risks and opportunities. * networking and collaboration tools for discussion and exchange of ideas among young farmers. * connecting less experienced women with mentors who can provide guidance and support. * detailed reports on the profitability of various businesses and ventures. * calculations of the farm's carbon footprint and recommendations for reducing it."
"SERVIR Agriculture and Food Security: SERVIR is using machine learning techniques to interpret satellite data to improve crop area, crop type, crop condition, and yield estimates at the scale of 10s of meters, aggregated to country, district, province, or country.",United States Agency for International Development,USAID,"USAID/Bureau for Resilience, Environment, and Food Security",Science & Space,None of the above.,"Improve estimates of cropped areas, crop types, crop condition, and crop yield, to inform food security assessments.",geospatial information that can inform agricultural decision-making,Implementation and Assessment,Neither,7/14/1905,7/15/1905,7/16/1905,Developed in-house.,Unknown,No,Yes,No,No,Yes,Free-and-open satellite data from NASA and ESA,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,Yes – source code is publicly available.,No,Unknown,6-12 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Improve estimates of cropped areas, crop types, crop condition, and crop yield, to inform food security assessments. . geospatial information that can inform agricultural decision-making","improve estimates of cropped areas, crop types, crop condition, and crop yield, to inform food security assessments. . geospatial information that can inform agricultural decision-making"
"SERVIR Water Security: SERVIR is applying deep learning on in situ and global/regional hydrologic forecasts to fill gaps in historic and recent streamflow and river elevation estimates, and improve streamflow forecasts, to a vertical precision of meters to centimeters, potentially for thousands to millions or river reaches worldwide.",United States Agency for International Development,USAID,"USAID/Bureau for Resilience, Environment, and Food Security",Science & Space,None of the above.,Fill gaps in river elevation and streamflow estimates; to more accurately forecast streamflow in ungauged rivers.,estimates and forecasts that can inform water security decisions,Acquisition and/or Development,Neither,7/14/1905,7/14/1905,7/15/1905,Developed in-house.,Unknown,No,Unknown,Unknown,Unknown,Yes,Free-and-open satellite data from NASA and GEOGLOWS streamflow model,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,6-12 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,Fill gaps in river elevation and streamflow estimates; to more accurately forecast streamflow in ungauged rivers. . estimates and forecasts that can inform water security decisions,fill gaps in river elevation and streamflow estimates; to more accurately forecast streamflow in ungauged rivers. . estimates and forecasts that can inform water security decisions
"SERVIR Ecosystem and Carbon Management: Automation of land cover and change mapping, including forest disturbance due to roads and mining. SERVIR is applying machine learning and deep learning techniques on satellite data to automate the classification and identification of land surface activities at the scale of 10s of meters.",United States Agency for International Development,USAID,"USAID/Bureau for Resilience, Environment, and Food Security",Science & Space,None of the above.,"Improve detection of land cover changes to inform resource management, carbon and greenhouse gas emissions estimates.",geospatial information that can inform ecosystem and carbon management monitoring systems and decisions,Implementation and Assessment,Neither,7/12/1905,7/14/1905,7/15/1905,Developed in-house.,Unknown,No,Yes,No,No,Yes,Free-and-open satellite data from NASA and ESA,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,Yes – source code is publicly available.,No,Unknown,6-12 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Improve detection of land cover changes to inform resource management, carbon and greenhouse gas emissions estimates. . geospatial information that can inform ecosystem and carbon management monitoring systems and decisions","improve detection of land cover changes to inform resource management, carbon and greenhouse gas emissions estimates. . geospatial information that can inform ecosystem and carbon management monitoring systems and decisions"
"SERVIR Weather & Climate Resilience: Short-term and seasonal weather prediction and hazard mapping. SERVIR is mapping floods and landslides with machine learning classification techniques, on the order of 10m to 10km. SERVIR is exploring the performance of weather foundational models versus numerical weather predictions on the 4-10km scale.",United States Agency for International Development,USAID,"USAID/Bureau for Resilience, Environment, and Food Security",Science & Space,None of the above.,"Improve detection and mapping of hazards and more accurately predict severe weather, to ultimately reduce impacts of climate- and weather-related events.","estimates and forecasts that can inform short-term weather- and subseasonal climate decisions, and climate-related disaster risk reduction",Acquisition and/or Development,Neither,7/14/1905,7/14/1905,7/15/1905,Developed in-house.,Unknown,No,Unknown,Unknown,Unknown,Yes,Free-and-open satellite data from NASA and ECMWF weather predictions,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,6-12 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"Improve detection and mapping of hazards and more accurately predict severe weather, to ultimately reduce impacts of climate- and weather-related events. . estimates and forecasts that can inform short-term weather- and subseasonal climate decisions, and climate-related disaster risk reduction","improve detection and mapping of hazards and more accurately predict severe weather, to ultimately reduce impacts of climate- and weather-related events. . estimates and forecasts that can inform short-term weather- and subseasonal climate decisions, and climate-related disaster risk reduction"
"SERVIR Air Quality and Health: Processing modeled air quality forecasts and assimilating large volumes of data from satellite instruments. SERVIR is downscaling satellite- and model-derived air quality predictions with neural networks, down to 5km spatial resolution.",United States Agency for International Development,USAID,"USAID/Bureau for Resilience, Environment, and Food Security",Science & Space,None of the above.,"More accurately track and monitor air quality events, their sources, and their impact areas.",geospatial information that can inform air quality and related health monitoring and decisions,Implementation and Assessment,Neither,7/14/1905,7/15/1905,7/15/1905,Developed in-house.,Unknown,No,Yes,No,No,Yes,Free-and-open satellite data from NASA and ESA,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,6-12 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"More accurately track and monitor air quality events, their sources, and their impact areas. . geospatial information that can inform air quality and related health monitoring and decisions","more accurately track and monitor air quality events, their sources, and their impact areas. . geospatial information that can inform air quality and related health monitoring and decisions"
Amazonía Mia - AI for environmental law enforcement in Colombia,United States Agency for International Development,USAID,USAID/Colombia,Law & Justice,None of the above.,"Strengthen the capabilities of the Attorney General's Office for the investigation and prosecution of environmental crimes, with emphasis on deforestation in the Colombian Amazon, by improving technical, technological and human capabilities, facilitating an effective approach to criminal types and identification and prosecution of major drivers of these behaviors.","Reports, visualizations",Operation and Maintenance,"Rights-Impacting
",7/15/1905,7/15/1905,7/16/1905,Developed with contracting resources.,72051421D00005,No,No,No,Yes,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,,Unknown,Unknown,No - Some individual decisions or actions require direct human oversight.,Unknown,Unknown,Unknown,Rights-Impacting,0.5238095238095238,"Strengthen the capabilities of the Attorney General's Office for the investigation and prosecution of environmental crimes, with emphasis on deforestation in the Colombian Amazon, by improving technical, technological and human capabilities, facilitating an effective approach to criminal types and identification and prosecution of major drivers of these behaviors. . Reports, visualizations","strengthen the capabilities of the attorney general's office for the investigation and prosecution of environmental crimes, with emphasis on deforestation in the colombian amazon, by improving technical, technological and human capabilities, facilitating an effective approach to criminal types and identification and prosecution of major drivers of these behaviors. . reports, visualizations"
Knowledge management and situational awareness for civil society in Colombia,United States Agency for International Development,USAID,USAID/Colombia,Diplomacy & Trade,None of the above.,"Development of an application that allows social organizations to manage the knowledge they produce in their activities and projects. It seeks, on the one hand, to facilitate access to advanced digital solutions, which enhance the use of information through an intuitive natural language interface, to organizations that require information on security and violence in Colombia that are part of the Suma Social Program. On the other hand, to strengthen organizations that need to monitor security contexts and conditions to support their decision-making processes, with a new digital solution that not only provides, but also facilitates the efficient and effective use of crucial information.","Reports, visualizations, chat interface",Operation and Maintenance,Neither,7/16/1905,7/16/1905,7/16/1905,Developed with contracting resources.,72051421CA00006,No,No,No,Yes,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Development of an application that allows social organizations to manage the knowledge they produce in their activities and projects. It seeks, on the one hand, to facilitate access to advanced digital solutions, which enhance the use of information through an intuitive natural language interface, to organizations that require information on security and violence in Colombia that are part of the Suma Social Program. On the other hand, to strengthen organizations that need to monitor security contexts and conditions to support their decision-making processes, with a new digital solution that not only provides, but also facilitates the efficient and effective use of crucial information. . Reports, visualizations, chat interface","development of an application that allows social organizations to manage the knowledge they produce in their activities and projects. it seeks, on the one hand, to facilitate access to advanced digital solutions, which enhance the use of information through an intuitive natural language interface, to organizations that require information on security and violence in colombia that are part of the suma social program. on the other hand, to strengthen organizations that need to monitor security contexts and conditions to support their decision-making processes, with a new digital solution that not only provides, but also facilitates the efficient and effective use of crucial information. . reports, visualizations, chat interface"
Virtual host for attendees of the Invest for Climate activity,United States Agency for International Development,USAID,USAID/Colombia,Diplomacy & Trade,None of the above.,Present and interact with the attendees of the Invest for Climate Activity through a virtual host. Objectives include increasing the visibility of the project and facilitating interaction with the attendees.,"Interactive ""host"" avatar",Operation and Maintenance,Neither,7/16/1905,7/16/1905,7/16/1905,Developed with contracting resources.,72051424D00003,No,No,No,Yes,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Present and interact with the attendees of the Invest for Climate Activity through a virtual host. Objectives include increasing the visibility of the project and facilitating interaction with the attendees. . Interactive ""host"" avatar","present and interact with the attendees of the invest for climate activity through a virtual host. objectives include increasing the visibility of the project and facilitating interaction with the attendees. . interactive ""host"" avatar"
Farmer support chatbot in Colombia,United States Agency for International Development,USAID,USAID/Colombia,Diplomacy & Trade,None of the above.,Chatbot using AI to interpret agroclimatic information and share with producer families recommend practices based on climate information.,Reports,Operation and Maintenance,Neither,7/16/1905,7/16/1905,7/16/1905,Developed with contracting resources.,72051423D00002,No,No,No,Yes,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Chatbot using AI to interpret agroclimatic information and share with producer families recommend practices based on climate information. . Reports,chatbot using ai to interpret agroclimatic information and share with producer families recommend practices based on climate information. . reports
"LIDA: Instant Messaging System in the Family Commissioner's Office of Montería - Córdoba, Colombia",United States Agency for International Development,USAID,USAID/Colombia,Government Services (includes Benefits and Service Delivery),None of the above.,"The LIDA instant messaging system in the Family Commissioner´s Office of Montería,  seeks to strengthen communication channels with citizens and strengthen the promotion  of rights, based on knowledge of gender-based violence (GBV) referral pathways.",Reports,Operation and Maintenance,Neither,7/16/1905,7/16/1905,7/16/1905,Developed with contracting resources.,72051422D00001,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"The LIDA instant messaging system in the Family Commissioner´s Office of Montería,  seeks to strengthen communication channels with citizens and strengthen the promotion  of rights, based on knowledge of gender-based violence (GBV) referral pathways. . Reports","the lida instant messaging system in the family commissioner´s office of montería, seeks to strengthen communication channels with citizens and strengthen the promotion of rights, based on knowledge of gender-based violence (gbv) referral pathways. . reports"
AI assistant for Colombia Partners for Transparency,United States Agency for International Development,USAID,USAID/Colombia,Diplomacy & Trade,None of the above.,"This tool combines Large Language Models (LLM) with Retrieval-Augmented Generation (RAG) to enhance interaction with the PfT documentation. It allows users to upload documents in word and pdf and ask natural language questions, similar to ChatGPT, providing precise and contextually answers based on the specific content of the documents (https://www.evidencebase.ai/)",Chatbot interface,Operation and Maintenance,Neither,7/16/1905,7/16/1905,7/16/1905,Developed with contracting resources.,72051420CA00001,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"This tool combines Large Language Models (LLM) with Retrieval-Augmented Generation (RAG) to enhance interaction with the PfT documentation. It allows users to upload documents in word and pdf and ask natural language questions, similar to ChatGPT, providing precise and contextually answers based on the specific content of the documents (https://www.evidencebase.ai/) . Chatbot interface","this tool combines large language models (llm) with retrieval-augmented generation (rag) to enhance interaction with the pft documentation. it allows users to upload documents in word and pdf and ask natural language questions, similar to chatgpt, providing precise and contextually answers based on the specific content of the documents (https://www.evidencebase.ai/) . chatbot interface"
Twitter trend analysis in Colombia,United States Agency for International Development,USAID,USAID/Colombia,Diplomacy & Trade,None of the above.,"This system uses artificial intelligence through a control dashboard, presenting a counter of retweets in the last hour with gender classification, topic classifier, georeferencing, sentiment trends, and emotion analysis in tweets. This application is designed as a measure to counteract misinformation and promote transparency on social media. In the framework of promoting transparency, access to information, and open government as paths to combat corruption, the product will develop several Machine Learning algorithms for identifying trending posts, recognizing users' demographic, psychographic, and sociographic characteristics, and detecting fake news.","Reports, visualization",Operation and Maintenance,Neither,7/15/1905,7/15/1905,7/15/1905,Developed with contracting resources.,72051420CA00001,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"This system uses artificial intelligence through a control dashboard, presenting a counter of retweets in the last hour with gender classification, topic classifier, georeferencing, sentiment trends, and emotion analysis in tweets. This application is designed as a measure to counteract misinformation and promote transparency on social media. In the framework of promoting transparency, access to information, and open government as paths to combat corruption, the product will develop several Machine Learning algorithms for identifying trending posts, recognizing users' demographic, psychographic, and sociographic characteristics, and detecting fake news. . Reports, visualization","this system uses artificial intelligence through a control dashboard, presenting a counter of retweets in the last hour with gender classification, topic classifier, georeferencing, sentiment trends, and emotion analysis in tweets. this application is designed as a measure to counteract misinformation and promote transparency on social media. in the framework of promoting transparency, access to information, and open government as paths to combat corruption, the product will develop several machine learning algorithms for identifying trending posts, recognizing users' demographic, psychographic, and sociographic characteristics, and detecting fake news. . reports, visualization"
Project Data Machine: Open governance data in Colombia,United States Agency for International Development,USAID,USAID/Colombia,Diplomacy & Trade,None of the above.,"This is a technological tool focused on integrating information related to the Congress of the Republic into a homogeneous and easily downloadable format for data extraction or processing. The benefits of this project include (i) Providing data in open and standard formats that are suitable for reuse by the target audience, (ii) Using agreed-upon schemas and vocabularies and providing a defined data structure so that the information can be correctly interpreted when reused, (iii) Offering a consultation point that includes an inventory with descriptive and technical information about the exposed datasets, (iv) Providing accessible data from persistent and user-friendly web addresses, (v) The project serves as a fundamental catalyst for open data initiatives for public and private entities",Text,Operation and Maintenance,Neither,7/15/1905,7/15/1905,7/15/1905,Developed with contracting resources.,72051420CA00001,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"This is a technological tool focused on integrating information related to the Congress of the Republic into a homogeneous and easily downloadable format for data extraction or processing. The benefits of this project include (i) Providing data in open and standard formats that are suitable for reuse by the target audience, (ii) Using agreed-upon schemas and vocabularies and providing a defined data structure so that the information can be correctly interpreted when reused, (iii) Offering a consultation point that includes an inventory with descriptive and technical information about the exposed datasets, (iv) Providing accessible data from persistent and user-friendly web addresses, (v) The project serves as a fundamental catalyst for open data initiatives for public and private entities . Text","this is a technological tool focused on integrating information related to the congress of the republic into a homogeneous and easily downloadable format for data extraction or processing. the benefits of this project include (i) providing data in open and standard formats that are suitable for reuse by the target audience, (ii) using agreed-upon schemas and vocabularies and providing a defined data structure so that the information can be correctly interpreted when reused, (iii) offering a consultation point that includes an inventory with descriptive and technical information about the exposed datasets, (iv) providing accessible data from persistent and user-friendly web addresses, (v) the project serves as a fundamental catalyst for open data initiatives for public and private entities . text"
Mibanco: Equitable Finance Activity in Colombia,United States Agency for International Development,USAID,USAID/Colombia,Diplomacy & Trade,None of the above.,"MICOMUNIDAD Ecosystem: Create a relational ecosystem called Micomunidad, which generates impact for the development and growth of communities, involving different actors that adapt the collaborative work model, promoting financial inclusion, contributing to the reduction of poverty, in a sustainable and easily achievable way, weaving long-term relationships in vulnerable areas. One of the activities is the ChatBot for Financial Education, which is an innovative digital tool that seeks to improve the management of personal and family finances.",Chatbot interface,Operation and Maintenance,Neither,7/15/1905,7/15/1905,7/16/1905,Developed with contracting resources.,72051422D00003,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"MICOMUNIDAD Ecosystem: Create a relational ecosystem called Micomunidad, which generates impact for the development and growth of communities, involving different actors that adapt the collaborative work model, promoting financial inclusion, contributing to the reduction of poverty, in a sustainable and easily achievable way, weaving long-term relationships in vulnerable areas. One of the activities is the ChatBot for Financial Education, which is an innovative digital tool that seeks to improve the management of personal and family finances. . Chatbot interface","micomunidad ecosystem: create a relational ecosystem called micomunidad, which generates impact for the development and growth of communities, involving different actors that adapt the collaborative work model, promoting financial inclusion, contributing to the reduction of poverty, in a sustainable and easily achievable way, weaving long-term relationships in vulnerable areas. one of the activities is the chatbot for financial education, which is an innovative digital tool that seeks to improve the management of personal and family finances. . chatbot interface"
Remote sensing for land cover identificiation in Colombia,United States Agency for International Development,USAID,USAID/Colombia,Science & Space,None of the above.,Training dataset with water body identification capacbility using Landsat and Sentinel Imagery as input,Annotated maps,Operation and Maintenance,Neither,7/16/1905,7/16/1905,7/16/1905,Developed with contracting resources.,72051423D00003 ,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Training dataset with water body identification capacbility using Landsat and Sentinel Imagery as input . Annotated maps,training dataset with water body identification capacbility using landsat and sentinel imagery as input . annotated maps
AI to support road construction in rural Colombia,United States Agency for International Development,USAID,USAID/Colombia,Diplomacy & Trade,None of the above.,"An AI-based chatbot that supports the ""Caminos Comunitarios para la Paz Total"" Program seeks to bring to scale the Program's impact across Colombia. By deploying an AI-driven solution, the implementing partner can enhance its outreach and operational efficiency, enabling communities seeking to improve their road networks to access vital information regarding technical and legal requirements quickly through the chatbot solution.",Chatbot interface,Operation and Maintenance,Neither,7/16/1905,7/16/1905,7/16/1905,Developed with contracting resources.,72051425P00003,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"An AI-based chatbot that supports the ""Caminos Comunitarios para la Paz Total"" Program seeks to bring to scale the Program's impact across Colombia. By deploying an AI-driven solution, the implementing partner can enhance its outreach and operational efficiency, enabling communities seeking to improve their road networks to access vital information regarding technical and legal requirements quickly through the chatbot solution. . Chatbot interface","an ai-based chatbot that supports the ""caminos comunitarios para la paz total"" program seeks to bring to scale the program's impact across colombia. by deploying an ai-driven solution, the implementing partner can enhance its outreach and operational efficiency, enabling communities seeking to improve their road networks to access vital information regarding technical and legal requirements quickly through the chatbot solution. . chatbot interface"
Mobile application for reading rapid cryptococcosis tests,United States Agency for International Development,USAID,USAID/Guatemala,Health & Medical,None of the above.,"To reduce human error of reading of cryptococcosis tests, the app is used to confirm healthcare worker reading of test results",A positive or negative test result of a cryptoccosis test that a healthcare worker uses to confirm their original diagnosis,Implementation and Assessment,Both,11/1/2023,6/1/2024,12/1/2024,Developed with contracting resources.,72052023FA00001,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,Test-reading error (false positive or false negative),Unknown,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,N/A. Photos of test kits with results are the same for all demographics. Patient nor provider demographics are used in the system,Direct user testing,Yes,Both,0.6666666666666666,"To reduce human error of reading of cryptococcosis tests, the app is used to confirm healthcare worker reading of test results . A positive or negative test result of a cryptoccosis test that a healthcare worker uses to confirm their original diagnosis . Test-reading error (false positive or false negative)","to reduce human error of reading of cryptococcosis tests, the app is used to confirm healthcare worker reading of test results . a positive or negative test result of a cryptoccosis test that a healthcare worker uses to confirm their original diagnosis . test-reading error (false positive or false negative)"
Chatbot to respond to teachers' questions about how to prevent student dropout,United States Agency for International Development,USAID,USAID/Guatemala,Education & Workforce,None of the above.,To provide smoother interaction in a chatbot that guides teachers to options for interventions to reduce the likelihood of student dropout,Customized recommendations for teachers based on pre-established protocol of steps to follow to reduce the likelihood of student dropout,Acquisition and/or Development,"Rights-Impacting
",7/1/2024,8/1/2024,Unknown,Developed with contracting resources.,2052022CA00001,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Dropout Early Warning System Assistant,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,Minor risk of chatbot providing incorrect answers to teachers' questions. ,Unknown,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,No student or teacher infomation interacts with the proposed AI component of the larger app.,Direct user testing,Yes,Rights-Impacting,0.6507936507936508,To provide smoother interaction in a chatbot that guides teachers to options for interventions to reduce the likelihood of student dropout . Customized recommendations for teachers based on pre-established protocol of steps to follow to reduce the likelihood of student dropout . Minor risk of chatbot providing incorrect answers to teachers' questions.,to provide smoother interaction in a chatbot that guides teachers to options for interventions to reduce the likelihood of student dropout . customized recommendations for teachers based on pre-established protocol of steps to follow to reduce the likelihood of student dropout . minor risk of chatbot providing incorrect answers to teachers' questions.
Predictive analytics model to predict HIV Treatment Interruption Risk,United States Agency for International Development,USAID,USAID/Guatemala,Health & Medical,None of the above.,Predict HIV Treatment Interruption Risk ,Flagging of patients who may be at risk of dropping out of care,Acquisition and/or Development,Both,4/1/2024,10/1/2024,Unknown,Developed with contracting resources.,7200AA19CA00004,No,No,No,Unknown,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.4603174603174603,Predict HIV Treatment Interruption Risk . Flagging of patients who may be at risk of dropping out of care,predict hiv treatment interruption risk . flagging of patients who may be at risk of dropping out of care
Anti-Money Laundering and Shareholders Transparency IT System,United States Agency for International Development,USAID,USAID/Moldova,Diplomacy & Trade,None of the above.,"The implemented Anti-Money Laundering (AML) system currently relies on a rule-based detection framework with advanced networking capabilities. Building on this foundation, additional machine learning (ML) and artificial intelligence (AI) capabilities are being supported to enhance the accuracy of detection logic. This innovative system, the first of its kind in the region, equips employees of the National Bank of Moldova (NBM) with advanced tools to monitor all banking operations comprehensively. The system supports the NBM’s risk-based approach, enabling the identification of suspicious activities and raising of timely alerts. This supports early detection of money laundering and terrorist financing risks, as well as suspicious changes in the ownership structures of banks. Importantly, the NBM's role is supervisory rather than investigative; the system is not designed for law enforcement or prosecutorial purposes.


As part of the implementation parameter, a ML/AI proof of concept has been developed. This ML/AI use case specifically focuses on enhancing detection ratios by analyzing alert datasets and generating a segmentation scheme through a K-means clustering algorithm. This segmentation helps identify distinct subgroups (e.g., customers, accounts) with varying levels of money laundering risk, thereby refining the detection process and supporting risk-focused oversight.
","The primary output of the AI algorithm is a segmentation scheme, as described above. This scheme categorizes entities based on their risk levels, enabling improved detection logic and risk assessment within the AML framework.",Operation and Maintenance,Neither,4/1/2021,4/1/2021,8/1/2024,Developed with contracting resources.,72011719F00001,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The implemented Anti-Money Laundering (AML) system currently relies on a rule-based detection framework with advanced networking capabilities. Building on this foundation, additional machine learning (ML) and artificial intelligence (AI) capabilities are being supported to enhance the accuracy of detection logic. This innovative system, the first of its kind in the region, equips employees of the National Bank of Moldova (NBM) with advanced tools to monitor all banking operations comprehensively. The system supports the NBM’s risk-based approach, enabling the identification of suspicious activities and raising of timely alerts. This supports early detection of money laundering and terrorist financing risks, as well as suspicious changes in the ownership structures of banks. Importantly, the NBM's role is supervisory rather than investigative; the system is not designed for law enforcement or prosecutorial purposes.


As part of the implementation parameter, a ML/AI proof of concept has been developed. This ML/AI use case specifically focuses on enhancing detection ratios by analyzing alert datasets and generating a segmentation scheme through a K-means clustering algorithm. This segmentation helps identify distinct subgroups (e.g., customers, accounts) with varying levels of money laundering risk, thereby refining the detection process and supporting risk-focused oversight. . The primary output of the AI algorithm is a segmentation scheme, as described above. This scheme categorizes entities based on their risk levels, enabling improved detection logic and risk assessment within the AML framework.","the implemented anti-money laundering (aml) system currently relies on a rule-based detection framework with advanced networking capabilities. building on this foundation, additional machine learning (ml) and artificial intelligence (ai) capabilities are being supported to enhance the accuracy of detection logic. this innovative system, the first of its kind in the region, equips employees of the national bank of moldova (nbm) with advanced tools to monitor all banking operations comprehensively. the system supports the nbm’s risk-based approach, enabling the identification of suspicious activities and raising of timely alerts. this supports early detection of money laundering and terrorist financing risks, as well as suspicious changes in the ownership structures of banks. importantly, the nbm's role is supervisory rather than investigative; the system is not designed for law enforcement or prosecutorial purposes. as part of the implementation parameter, a ml/ai proof of concept has been developed. this ml/ai use case specifically focuses on enhancing detection ratios by analyzing alert datasets and generating a segmentation scheme through a k-means clustering algorithm. this segmentation helps identify distinct subgroups (e.g., customers, accounts) with varying levels of money laundering risk, thereby refining the detection process and supporting risk-focused oversight. . the primary output of the ai algorithm is a segmentation scheme, as described above. this scheme categorizes entities based on their risk levels, enabling improved detection logic and risk assessment within the aml framework."
AI analysis of Moldova's media content and social media landscape,United States Agency for International Development,USAID,USAID/Moldova,Diplomacy & Trade,None of the above.,"This project utilizes AI to provide analysis of Moldova's media landscape, focusing on narrative framing techniques in both Romanian and Russian-language media and social media. The goal of this application is situational awareness and strategic communications planning, not content removal or blocking.",Monthly Reports,Operation and Maintenance,Neither,10/1/2023,10/1/2023,6/1/2024,Developed with contracting resources.,AID-117-A-17-00001,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"This project utilizes AI to provide analysis of Moldova's media landscape, focusing on narrative framing techniques in both Romanian and Russian-language media and social media. The goal of this application is situational awareness and strategic communications planning, not content removal or blocking. . Monthly Reports","this project utilizes ai to provide analysis of moldova's media landscape, focusing on narrative framing techniques in both romanian and russian-language media and social media. the goal of this application is situational awareness and strategic communications planning, not content removal or blocking. . monthly reports"
"""Bu Mira"" financial inclusion chatbot",United States Agency for International Development,USAID,USAID/Indonesia,Diplomacy & Trade,None of the above.,"Bu Mira is designed to interface with women entrepreneurs as a business partner on WhatsApp. Through Bu Mira, women entrepreneurs will be able to access educational modules, financial tools, and loan applications.",WhatsApp messages,Operation and Maintenance,Neither,1/25/2022,1/25/2022,1/16/2024,Developed with contracting resources.,72049722FA00003,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Bu Mira is designed to interface with women entrepreneurs as a business partner on WhatsApp. Through Bu Mira, women entrepreneurs will be able to access educational modules, financial tools, and loan applications. . WhatsApp messages","bu mira is designed to interface with women entrepreneurs as a business partner on whatsapp. through bu mira, women entrepreneurs will be able to access educational modules, financial tools, and loan applications. . whatsapp messages"
Machine translation for communication with country teams,United States Agency for International Development,USAID,USAID/El Salvador,Diplomacy & Trade,None of the above.,Machine translation eases communication between the in-country team (which operates entirely in Spanish) and English-speaking colleagues.,Written text,Implementation and Assessment,Neither,6/20/2024,7/15/2024,Unknown,Developed with contracting resources.,"7200AA19D00015, 7200AA24F00002",No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Machine translation eases communication between the in-country team (which operates entirely in Spanish) and English-speaking colleagues. . Written text,machine translation eases communication between the in-country team (which operates entirely in spanish) and english-speaking colleagues. . written text
E-Audit system for the tax department in Jordan,United States Agency for International Development,USAID,USAID/Jordan,Diplomacy & Trade,None of the above.,"The system’s focus is on enhancing voluntary compliance, identifying potential high-risk cases for follow-up, and expediting refunds—ultimately supporting both fiscal responsibility and economic growth. By automating routine checks, the e-audit system saves time and resources, which helps the government reinforce public trust in the tax system. This approach also aligns with Jordan's macro fiscal policy objectives, helping generate revenues in a cost-effective way.","A list of scored taxpayers is generated from highest total score to lowest total score. Low scores would indicate low potential for change and thus will be at the bottom rank of risk and most likely will not be audited. Hence, this automated scoring system identifies the tax returns that appear more suspicious than their comparative returns in the same class or sector.",Operation and Maintenance,Neither,2/1/2022,6/1/2022,8/1/2022,Developed with contracting resources.,72027821C00001,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Planned or in-progress.,"Data clarity and accuracy.
PFMA planned for User Acceptance Testing with the counterpart to include all types of taxpayers' profile to ensure the consideration of all possible scenarios.",Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,"The taxpayers profling were idetified as part of the tool design, it refers only to taxpayers size and business type, with no reference to any demographic factors.",Direct user testing,Yes,Neither,0.7301587301587301,"The system’s focus is on enhancing voluntary compliance, identifying potential high-risk cases for follow-up, and expediting refunds—ultimately supporting both fiscal responsibility and economic growth. By automating routine checks, the e-audit system saves time and resources, which helps the government reinforce public trust in the tax system. This approach also aligns with Jordan's macro fiscal policy objectives, helping generate revenues in a cost-effective way. . A list of scored taxpayers is generated from highest total score to lowest total score. Low scores would indicate low potential for change and thus will be at the bottom rank of risk and most likely will not be audited. Hence, this automated scoring system identifies the tax returns that appear more suspicious than their comparative returns in the same class or sector. . Data clarity and accuracy.
PFMA planned for User Acceptance Testing with the counterpart to include all types of taxpayers' profile to ensure the consideration of all possible scenarios.","the system’s focus is on enhancing voluntary compliance, identifying potential high-risk cases for follow-up, and expediting refunds—ultimately supporting both fiscal responsibility and economic growth. by automating routine checks, the e-audit system saves time and resources, which helps the government reinforce public trust in the tax system. this approach also aligns with jordan's macro fiscal policy objectives, helping generate revenues in a cost-effective way. . a list of scored taxpayers is generated from highest total score to lowest total score. low scores would indicate low potential for change and thus will be at the bottom rank of risk and most likely will not be audited. hence, this automated scoring system identifies the tax returns that appear more suspicious than their comparative returns in the same class or sector. . data clarity and accuracy. pfma planned for user acceptance testing with the counterpart to include all types of taxpayers' profile to ensure the consideration of all possible scenarios."
Crop2Cash National Hotline for Agriculture,United States Agency for International Development,USAID,USAID/Nigeria,Diplomacy & Trade,None of the above.,"This  platform is an AI-powered advisory system accessible via phone call. It is available all the time (24/7) in four major Nigerian languages, this virtual extesnion agent provides personalized advice on crop management. pest control, soil health,and market trends. It also integrates real-time weather data and climate predictions, offering adaptive strategies to build rsilience against climate change impacts.",The telephone-based technology allows small holder farmrs access to extension services and information about market trends to impact them mitigate the impact of climate change.,Acquisition and/or Development,Neither,11/2/2024,11/2/2024,Unknown,Developed with contracting resources.,72062020C00001,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"This  platform is an AI-powered advisory system accessible via phone call. It is available all the time (24/7) in four major Nigerian languages, this virtual extesnion agent provides personalized advice on crop management. pest control, soil health,and market trends. It also integrates real-time weather data and climate predictions, offering adaptive strategies to build rsilience against climate change impacts. . The telephone-based technology allows small holder farmrs access to extension services and information about market trends to impact them mitigate the impact of climate change.","this platform is an ai-powered advisory system accessible via phone call. it is available all the time (24/7) in four major nigerian languages, this virtual extesnion agent provides personalized advice on crop management. pest control, soil health,and market trends. it also integrates real-time weather data and climate predictions, offering adaptive strategies to build rsilience against climate change impacts. . the telephone-based technology allows small holder farmrs access to extension services and information about market trends to impact them mitigate the impact of climate change."
FarmerChart,United States Agency for International Development,USAID,USAID/Nigeria,Diplomacy & Trade,None of the above.,FarmerChart is an AI-powered application that provides agricultural advisory services inclduing weather prediction to smallholder farmers.  This platform is accessible via android phones in two major Nigerian languages (Hausa and Yoruba). The system integrates real-time weather data and climate predictions to build farmers resilience against cimate change impacts.,The telephone-based technology allows small holder farmrs access to extension services and information about market trends to impact them mitigate the impact of climate change.,Operation and Maintenance,Neither,4/18/2023,4/18/2023,5/1/2023,Developed with contracting resources.,72062020C00001,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,FarmerChart is an AI-powered application that provides agricultural advisory services inclduing weather prediction to smallholder farmers.  This platform is accessible via android phones in two major Nigerian languages (Hausa and Yoruba). The system integrates real-time weather data and climate predictions to build farmers resilience against cimate change impacts. . The telephone-based technology allows small holder farmrs access to extension services and information about market trends to impact them mitigate the impact of climate change.,farmerchart is an ai-powered application that provides agricultural advisory services inclduing weather prediction to smallholder farmers. this platform is accessible via android phones in two major nigerian languages (hausa and yoruba). the system integrates real-time weather data and climate predictions to build farmers resilience against cimate change impacts. . the telephone-based technology allows small holder farmrs access to extension services and information about market trends to impact them mitigate the impact of climate change.
Automated due diligence for corruption risks,United States Agency for International Development,USAID,"USAID/Bureau for Democracy, Human Rights, and Governance",Diplomacy & Trade,None of the above.,"This platform will process and standardize data from published reports and make it publicly available on a screening platform. Compliance teams at financial institutions and corporations around the world will be able to screen their customers, transactions, and supply chains against the NGO data in the same way that they already screen sanctions and other watchlists.","Reports, visualizations",Initiated,Neither,11/1/2024,Unknown,Unknown,Developed with contracting resources.,7200-AA-21D-00024,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"This platform will process and standardize data from published reports and make it publicly available on a screening platform. Compliance teams at financial institutions and corporations around the world will be able to screen their customers, transactions, and supply chains against the NGO data in the same way that they already screen sanctions and other watchlists. . Reports, visualizations","this platform will process and standardize data from published reports and make it publicly available on a screening platform. compliance teams at financial institutions and corporations around the world will be able to screen their customers, transactions, and supply chains against the ngo data in the same way that they already screen sanctions and other watchlists. . reports, visualizations"
AI-powered portal for the Development Experience Clearinghouse,United States Agency for International Development,USAID,USAID/Bureau for Conflict Prevention and Stabilization,Diplomacy & Trade,None of the above.,Making information in the Development Experience Clearinghouse easier to access and use,Excel tables that feed into a Tableau dashboard with tables and other visualizations.,Operation and Maintenance,Neither,8/1/2023,9/1/2023,1/1/2024,Developed with contracting resources.,7200AA20D00013,No,No,No,No,Yes,Documents from the Development Experience Clearinghouse,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Unknown,No,No,Other,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,Making information in the Development Experience Clearinghouse easier to access and use . Excel tables that feed into a Tableau dashboard with tables and other visualizations.,making information in the development experience clearinghouse easier to access and use . excel tables that feed into a tableau dashboard with tables and other visualizations.
Crop pest identification and advisory,United States Agency for International Development,USAID,"USAID/Bureau for Resilience, Environment, and Food Security",Diplomacy & Trade,None of the above.,To more efficiently and precisely identify crop diseases and provide tailored recommendations on how to control the pest or disease,The AI system generates tailored plant health advisory in the form of text in local languages,Operation and Maintenance,Neither,7/12/1905,7/12/1905,7/15/1905,Developed with contracting resources.,7200AA21LE00005,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,To more efficiently and precisely identify crop diseases and provide tailored recommendations on how to control the pest or disease . The AI system generates tailored plant health advisory in the form of text in local languages,to more efficiently and precisely identify crop diseases and provide tailored recommendations on how to control the pest or disease . the ai system generates tailored plant health advisory in the form of text in local languages
AI-enabled agricultural logistics in Mozambique,United States Agency for International Development,USAID,"USAID/Bureau for Resilience, Environment, and Food Security",Diplomacy & Trade,None of the above.,"Appload is improving accessibility to logistics services for small agricultural aggregators and small transporters in Nampula and Zambezi provinces and the Beira Corridor via Appload's digital marketplace. Through the partnership, Appload will reach 1,588 shippers and 740 small transporters, facilitating the movement of over $675,000 in cargo through 860 unique trips on the platform. Additionally, in partnership with Hollard Insurance, the company is subsidizing goods in transit insurance for all shippers using the app.","App interface, supply chain predictions",Operation and Maintenance,Neither,10/15/2023,10/14/2025,10/30/2023,Developed with contracting resources.,7200AA20C00054,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Appload is improving accessibility to logistics services for small agricultural aggregators and small transporters in Nampula and Zambezi provinces and the Beira Corridor via Appload's digital marketplace. Through the partnership, Appload will reach 1,588 shippers and 740 small transporters, facilitating the movement of over $675,000 in cargo through 860 unique trips on the platform. Additionally, in partnership with Hollard Insurance, the company is subsidizing goods in transit insurance for all shippers using the app. . App interface, supply chain predictions","appload is improving accessibility to logistics services for small agricultural aggregators and small transporters in nampula and zambezi provinces and the beira corridor via appload's digital marketplace. through the partnership, appload will reach 1,588 shippers and 740 small transporters, facilitating the movement of over $675,000 in cargo through 860 unique trips on the platform. additionally, in partnership with hollard insurance, the company is subsidizing goods in transit insurance for all shippers using the app. . app interface, supply chain predictions"
Using AI to detect information manipulation,United States Agency for International Development,USAID,"USAID/Bureau for Democracy, Human Rights, and Governance",Diplomacy & Trade,None of the above.,To detect narratives and trends in social media alterations of images and video in order to identify information manipulation.,"Reports, visualizations",Operation and Maintenance,Neither,12/1/2020,12/1/2020,7/1/2023,Developed with contracting resources.,7200AA18CA00059,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"To detect narratives and trends in social media alterations of images and video in order to identify information manipulation. . Reports, visualizations","to detect narratives and trends in social media alterations of images and video in order to identify information manipulation. . reports, visualizations"
HelloTask: multichannel job platform for blue-collar workers,United States Agency for International Development,USAID,USAID/Bureau for Asia,Diplomacy & Trade,None of the above.,"Under the Market Systems & Partnerships mechanism, USAID/Asia is supporting a partnership with HelloTask, a multichannel job platform for blue-collar workers that connects clients to care service providers (e.g., cooking, cleaning, laundry) via a smartphone application. As part of this activity, USAID is providing funding to support the development of an AI-based interactive voice response (IVR) technology that allows users to speak into their phones to interact with the job placement functions of the app.","The telephone-based IVR technology will allow lower-income users to better interface with the online technology platform to accept jobs and provide feedback. Users often do not have smartphones, but instead feature phones that have limited capability to support more sophisticated mobile apps often used by gig workers in other countries. The IVR system currently allows domestic workers to speak directly into their featurephones with the message being transcribed into the online tracking system. The next phase is to allow domestic workers to verbally accept and decline jobs, file feedback, and otherwise interact with the company in a way that is similar to other apps with gig employment.",Acquisition and/or Development,Neither,8/1/2023,3/1/2024,Unknown,Developed with contracting resources.,7200AA20C00054,No,Unknown,Unknown,Unknown,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"Under the Market Systems & Partnerships mechanism, USAID/Asia is supporting a partnership with HelloTask, a multichannel job platform for blue-collar workers that connects clients to care service providers (e.g., cooking, cleaning, laundry) via a smartphone application. As part of this activity, USAID is providing funding to support the development of an AI-based interactive voice response (IVR) technology that allows users to speak into their phones to interact with the job placement functions of the app. . The telephone-based IVR technology will allow lower-income users to better interface with the online technology platform to accept jobs and provide feedback. Users often do not have smartphones, but instead feature phones that have limited capability to support more sophisticated mobile apps often used by gig workers in other countries. The IVR system currently allows domestic workers to speak directly into their featurephones with the message being transcribed into the online tracking system. The next phase is to allow domestic workers to verbally accept and decline jobs, file feedback, and otherwise interact with the company in a way that is similar to other apps with gig employment.","under the market systems & partnerships mechanism, usaid/asia is supporting a partnership with hellotask, a multichannel job platform for blue-collar workers that connects clients to care service providers (e.g., cooking, cleaning, laundry) via a smartphone application. as part of this activity, usaid is providing funding to support the development of an ai-based interactive voice response (ivr) technology that allows users to speak into their phones to interact with the job placement functions of the app. . the telephone-based ivr technology will allow lower-income users to better interface with the online technology platform to accept jobs and provide feedback. users often do not have smartphones, but instead feature phones that have limited capability to support more sophisticated mobile apps often used by gig workers in other countries. the ivr system currently allows domestic workers to speak directly into their featurephones with the message being transcribed into the online tracking system. the next phase is to allow domestic workers to verbally accept and decline jobs, file feedback, and otherwise interact with the company in a way that is similar to other apps with gig employment."
Georgian speech-to-text transcription,United States Agency for International Development,USAID,USAID/Georgia,Diplomacy & Trade,None of the above.,"The purpose of the tool is to transcribe spoken speech via automatic speech recognition (ASR). The expected benefit, based on use-cases, is to help users transcribe large amounts of audio data into text with enterprise accuracy and in a short amount of time.",Text (spoken audio content),Operation and Maintenance,Neither,9/22/2020,10/1/2021,12/30/2022,Developed with contracting resources.,72011420CA00002,No,No,No,No,Other,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"The purpose of the tool is to transcribe spoken speech via automatic speech recognition (ASR). The expected benefit, based on use-cases, is to help users transcribe large amounts of audio data into text with enterprise accuracy and in a short amount of time. . Text (spoken audio content)","the purpose of the tool is to transcribe spoken speech via automatic speech recognition (asr). the expected benefit, based on use-cases, is to help users transcribe large amounts of audio data into text with enterprise accuracy and in a short amount of time. . text (spoken audio content)"
Skhivi: AI tools for Georgian journalists,United States Agency for International Development,USAID,USAID/Georgia,Diplomacy & Trade,None of the above.,"Skhivi is designed for journalists, news editors, and media researchers and offers a news aggregator, trend analysis, a news distribution timeline across various media outlets, polarization metrics, and a temperature index. The team is now accepting subscription requests for more advanced features, and the tool is already being used by local Georgian media outlets like Tabula and Aprili.",Online dashboard with visualizations and reports,Implementation and Assessment,Neither,8/1/2022,4/1/2024,6/9/2024,Developed with contracting resources.,72011422CA00004,No,No,No,No,Unknown,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"Skhivi is designed for journalists, news editors, and media researchers and offers a news aggregator, trend analysis, a news distribution timeline across various media outlets, polarization metrics, and a temperature index. The team is now accepting subscription requests for more advanced features, and the tool is already being used by local Georgian media outlets like Tabula and Aprili. . Online dashboard with visualizations and reports","skhivi is designed for journalists, news editors, and media researchers and offers a news aggregator, trend analysis, a news distribution timeline across various media outlets, polarization metrics, and a temperature index. the team is now accepting subscription requests for more advanced features, and the tool is already being used by local georgian media outlets like tabula and aprili. . online dashboard with visualizations and reports"
Intelligent forecasting for Family Planning products (Cote d'Ivoire),United States Agency for International Development,USAID,USAID/Bureau for Global Health,Diplomacy & Trade,None of the above.,Optimizing health supply chains to reduce costs and improve quality of care by avoiding stock-outs.,Supply chain predictions,Implementation and Assessment,Neither,7/12/1905,7/13/1905,5/1/2023,Developed with contracting resources.,7200AA20CA00009,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Optimizing health supply chains to reduce costs and improve quality of care by avoiding stock-outs. . Supply chain predictions,optimizing health supply chains to reduce costs and improve quality of care by avoiding stock-outs. . supply chain predictions
Intelligent forecasting for Malaria products (Cote d'Ivoire),United States Agency for International Development,USAID,USAID/Bureau for Global Health,Diplomacy & Trade,None of the above.,Optimizing health supply chains to reduce costs and improve quality of care by avoiding stock-outs.,Supply chain predictions,Acquisition and/or Development,Neither,7/12/1905,7/14/1905,Unknown,Developed with contracting resources.,7200AA20CA00009,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,Optimizing health supply chains to reduce costs and improve quality of care by avoiding stock-outs. . Supply chain predictions,optimizing health supply chains to reduce costs and improve quality of care by avoiding stock-outs. . supply chain predictions
Intelligent forecasting for Routine Vaccines (Serbia),United States Agency for International Development,USAID,USAID/Bureau for Global Health,Diplomacy & Trade,None of the above.,Optimizing health supply chains to reduce costs and improve quality of care by avoiding stock-outs.,Supply chain predictions,Implementation and Assessment,Neither,7/15/1905,4/1/2024,8/1/2024,Developed with contracting resources.,7200AA20CA00009,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Optimizing health supply chains to reduce costs and improve quality of care by avoiding stock-outs. . Supply chain predictions,optimizing health supply chains to reduce costs and improve quality of care by avoiding stock-outs. . supply chain predictions
Intelligent forecasting for COVID-19 vaccines (Mali),United States Agency for International Development,USAID,USAID/Bureau for Global Health,Diplomacy & Trade,None of the above.,Optimizing health supply chains to reduce costs and improve quality of care by avoiding stock-outs.,Supply chain predictions,Implementation and Assessment,Neither,9/1/2022,10/1/2022,5/1/2023,Developed with contracting resources.,7200AA20CA00009,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Optimizing health supply chains to reduce costs and improve quality of care by avoiding stock-outs. . Supply chain predictions,optimizing health supply chains to reduce costs and improve quality of care by avoiding stock-outs. . supply chain predictions
Intelligent forecasting for TB products (Indonesia),United States Agency for International Development,USAID,USAID/Bureau for Global Health,Diplomacy & Trade,None of the above.,Optimizing health supply chains to reduce costs and improve quality of care by avoiding stock-outs.,Supply chain predictions,Acquisition and/or Development,Neither,8/1/2022,8/1/2022,Unknown,Developed with contracting resources.,7200AA20CA00009,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Unknown,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,Optimizing health supply chains to reduce costs and improve quality of care by avoiding stock-outs. . Supply chain predictions,optimizing health supply chains to reduce costs and improve quality of care by avoiding stock-outs. . supply chain predictions
Drip irrigation system with AI control,United States Agency for International Development,USAID,USAID/Bureau for the Middle East,Diplomacy & Trade,None of the above.,"GEAR Center's  smart irrigation system uses artificial intelligence to predict local weather, enabling the system to optimize water and energy use. This caters specifically to the unique challenges faced by smallholder farmers in water-stressed regions. By predicting solar exposure and tailoring irrigation needs for individual locations, this technology improves agricultural efficiency, addressing critical issues of soil degradation and water scarcity in regions that can most benefit from these gains.",The AI system outputs a hyper-local weather prediction for the next 24 hours based on historical data and inputs from local weather sensors.,Implementation and Assessment,Neither,7/1/2019,9/1/2019,9/1/2022,Developed with contracting resources.,7200AA21C00056,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"GEAR Center's  smart irrigation system uses artificial intelligence to predict local weather, enabling the system to optimize water and energy use. This caters specifically to the unique challenges faced by smallholder farmers in water-stressed regions. By predicting solar exposure and tailoring irrigation needs for individual locations, this technology improves agricultural efficiency, addressing critical issues of soil degradation and water scarcity in regions that can most benefit from these gains. . The AI system outputs a hyper-local weather prediction for the next 24 hours based on historical data and inputs from local weather sensors.","gear center's smart irrigation system uses artificial intelligence to predict local weather, enabling the system to optimize water and energy use. this caters specifically to the unique challenges faced by smallholder farmers in water-stressed regions. by predicting solar exposure and tailoring irrigation needs for individual locations, this technology improves agricultural efficiency, addressing critical issues of soil degradation and water scarcity in regions that can most benefit from these gains. . the ai system outputs a hyper-local weather prediction for the next 24 hours based on historical data and inputs from local weather sensors."
AI-based translation for key Global Health documents,United States Agency for International Development,USAID,USAID/Bureau for Global Health,Diplomacy & Trade,None of the above.,"USAID has missions in over 80 countries and programs in more than 100. THis means that there are likely hundreds if not thousands of languages spoken in the countries USAID supports. Translating key health documents and information into all the languages spoken where USAID has programming can be time consuming and expensive. The purpose of this use case is to see if we can make translation cheaper, faster, and eaiser so that more key USAID global health documents are available to partners and populations where we work. Note: no sensitive information is used in this use case.","Two word documents are produced for each translation, one containing just the translated material and one containing a translation report, highlighting any word choice differences or potential inconsistencies.",Acquisition and/or Development,Neither,7/1/2024,7/1/2024,Unknown,Developed in-house.,Unknown,No,Unknown,Unknown,Unknown,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.38095238095238093,"USAID has missions in over 80 countries and programs in more than 100. THis means that there are likely hundreds if not thousands of languages spoken in the countries USAID supports. Translating key health documents and information into all the languages spoken where USAID has programming can be time consuming and expensive. The purpose of this use case is to see if we can make translation cheaper, faster, and eaiser so that more key USAID global health documents are available to partners and populations where we work. Note: no sensitive information is used in this use case. . Two word documents are produced for each translation, one containing just the translated material and one containing a translation report, highlighting any word choice differences or potential inconsistencies.","usaid has missions in over 80 countries and programs in more than 100. this means that there are likely hundreds if not thousands of languages spoken in the countries usaid supports. translating key health documents and information into all the languages spoken where usaid has programming can be time consuming and expensive. the purpose of this use case is to see if we can make translation cheaper, faster, and eaiser so that more key usaid global health documents are available to partners and populations where we work. note: no sensitive information is used in this use case. . two word documents are produced for each translation, one containing just the translated material and one containing a translation report, highlighting any word choice differences or potential inconsistencies."
Retrieval-augmented generation for government-to-government due diligence,United States Agency for International Development,USAID,"USAID/Bureau for Inclusive Growth, Partnerships, and Innovation",Diplomacy & Trade,None of the above.,"Government-to-government (G2G) agreements provide a more efficient way for USAID to program funds, but require careful assessment of a partner country's fiscal management and budgetary systems. This application is using AI tools to expedite the initial desk review phase of the G2G Country Context Report by querying a set of authoritative documents with a standardized list of due-diligence questions.","Excel spreadsheet containing columns for questions, answers, and source citations.",Acquisition and/or Development,Neither,2/27/2024,7/11/2024,Unknown,Developed in-house.,Unknown,No,Unknown,Unknown,Unknown,Yes,Completed country context report workbooks from previous G2G assessments were used to benchmark the reliability of the vector search.,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,Less than 6 months,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"Government-to-government (G2G) agreements provide a more efficient way for USAID to program funds, but require careful assessment of a partner country's fiscal management and budgetary systems. This application is using AI tools to expedite the initial desk review phase of the G2G Country Context Report by querying a set of authoritative documents with a standardized list of due-diligence questions. . Excel spreadsheet containing columns for questions, answers, and source citations.","government-to-government (g2g) agreements provide a more efficient way for usaid to program funds, but require careful assessment of a partner country's fiscal management and budgetary systems. this application is using ai tools to expedite the initial desk review phase of the g2g country context report by querying a set of authoritative documents with a standardized list of due-diligence questions. . excel spreadsheet containing columns for questions, answers, and source citations."
Boomitra: Accelerating soil carbon removal on a planetary scale.,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Diplomacy & Trade,None of the above.,"Boomitra: Accelerating soil carbon removal on a planetary scale. Utilizes satellites and AI to measure soil moisture, nutrients and carbon. Boomitra aids farmers in reducing water and nutrient use by 30 percent while enhancing soil fertility. Funded through WFP Innovation Accelerator","Reports, visualizations",Implementation and Assessment,Neither,4/30/2024,4/30/2024,4/30/2024,Developed with contracting resources.,720BHA24IO00037,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Boomitra: Accelerating soil carbon removal on a planetary scale. Utilizes satellites and AI to measure soil moisture, nutrients and carbon. Boomitra aids farmers in reducing water and nutrient use by 30 percent while enhancing soil fertility. Funded through WFP Innovation Accelerator . Reports, visualizations","boomitra: accelerating soil carbon removal on a planetary scale. utilizes satellites and ai to measure soil moisture, nutrients and carbon. boomitra aids farmers in reducing water and nutrient use by 30 percent while enhancing soil fertility. funded through wfp innovation accelerator . reports, visualizations"
"DARTS: Accurate, accountable cash transfer data and reconciliation",United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Diplomacy & Trade,None of the above.,"DARTS: A web app that uses advanced machine learning to enable WFP country offices to ensure accurate, accountable cash transfer data and reconciliation. DARTS (Data Assurance and Reconciliation Tool Simplified) is a user-friendly web app. It enables WFP to implement controls on large cash transfer data and generate reconciliation reports, which is vital in cash assistance programs. This ensures accountability, evaluating efficiency, building trust and ensuring accurate fund distribution. Funded through WFP Innovation Accelerator",Web app interface,Implementation and Assessment,Neither,4/30/2024,4/30/2024,4/30/2024,Developed with contracting resources.,720BHA24IO00037,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"DARTS: A web app that uses advanced machine learning to enable WFP country offices to ensure accurate, accountable cash transfer data and reconciliation. DARTS (Data Assurance and Reconciliation Tool Simplified) is a user-friendly web app. It enables WFP to implement controls on large cash transfer data and generate reconciliation reports, which is vital in cash assistance programs. This ensures accountability, evaluating efficiency, building trust and ensuring accurate fund distribution. Funded through WFP Innovation Accelerator . Web app interface","darts: a web app that uses advanced machine learning to enable wfp country offices to ensure accurate, accountable cash transfer data and reconciliation. darts (data assurance and reconciliation tool simplified) is a user-friendly web app. it enables wfp to implement controls on large cash transfer data and generate reconciliation reports, which is vital in cash assistance programs. this ensures accountability, evaluating efficiency, building trust and ensuring accurate fund distribution. funded through wfp innovation accelerator . web app interface"
EYouth: Learning and training opportunities for refugees and host communities,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Diplomacy & Trade,None of the above.,EYouth: Provides learning and training opportunities for refugees and host communities through their AI-based tech learning platform. The venture offers different types of online learning paths through an interactive methodology that helps young people acquire the necessary skills and find the right job. Funded through WFP Innovation Accelerator,Customization of learning content,Implementation and Assessment,Neither,4/30/2024,4/30/2024,4/30/2024,Developed with contracting resources.,720BHA24IO00037,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,EYouth: Provides learning and training opportunities for refugees and host communities through their AI-based tech learning platform. The venture offers different types of online learning paths through an interactive methodology that helps young people acquire the necessary skills and find the right job. Funded through WFP Innovation Accelerator . Customization of learning content,eyouth: provides learning and training opportunities for refugees and host communities through their ai-based tech learning platform. the venture offers different types of online learning paths through an interactive methodology that helps young people acquire the necessary skills and find the right job. funded through wfp innovation accelerator . customization of learning content
Ignitia: AI-powered weather forceasts for smallholder farmers,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Diplomacy & Trade,None of the above.,"Ignitia: Uses a reliable and accessible SMS forecast and weather intelligence platform to provide smallholder farmers in the tropics with more efficient weather forecasts. Delivers localized forecasts to farmers through an advanced physics and AI predictive model optimized for the tropics that is twice as accurate as global models. To date, Ignitia has provided services to 2.7 million smallholder farmers, of which approximately 700,000 are recurrent. WFP supports Ingitia with grant funding to further its agribusiness models. This will help them improve agricultural yield. Funded through WFP Innovation Accelerator",Short text messages,Implementation and Assessment,Neither,4/30/2024,4/30/2024,4/30/2024,Developed with contracting resources.,720BHA24IO00037,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Ignitia: Uses a reliable and accessible SMS forecast and weather intelligence platform to provide smallholder farmers in the tropics with more efficient weather forecasts. Delivers localized forecasts to farmers through an advanced physics and AI predictive model optimized for the tropics that is twice as accurate as global models. To date, Ignitia has provided services to 2.7 million smallholder farmers, of which approximately 700,000 are recurrent. WFP supports Ingitia with grant funding to further its agribusiness models. This will help them improve agricultural yield. Funded through WFP Innovation Accelerator . Short text messages","ignitia: uses a reliable and accessible sms forecast and weather intelligence platform to provide smallholder farmers in the tropics with more efficient weather forecasts. delivers localized forecasts to farmers through an advanced physics and ai predictive model optimized for the tropics that is twice as accurate as global models. to date, ignitia has provided services to 2.7 million smallholder farmers, of which approximately 700,000 are recurrent. wfp supports ingitia with grant funding to further its agribusiness models. this will help them improve agricultural yield. funded through wfp innovation accelerator . short text messages"
SKAI: Large-scale assessment of building damage,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Emergency Management,None of the above.,"SKAI: An open-sourced tool that uses satellite imagery and AI to automatically assess building damage on a large scale. Enables real-time insights and actionable intelligence for effective decision-making during disaster response. Fuses cutting-edge machine learning algorithms and vast satellite data. Helps organizations make data-driven decisions with unprecedented precision and speed, therefore speeding up humanitarian assistance to affected households after disasters. Funded through WFP Innovation Accelerator","Maps, reports",Implementation and Assessment,Neither,4/30/2024,4/30/2024,4/30/2024,Developed with contracting resources.,720BHA24IO00037,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"SKAI: An open-sourced tool that uses satellite imagery and AI to automatically assess building damage on a large scale. Enables real-time insights and actionable intelligence for effective decision-making during disaster response. Fuses cutting-edge machine learning algorithms and vast satellite data. Helps organizations make data-driven decisions with unprecedented precision and speed, therefore speeding up humanitarian assistance to affected households after disasters. Funded through WFP Innovation Accelerator . Maps, reports","skai: an open-sourced tool that uses satellite imagery and ai to automatically assess building damage on a large scale. enables real-time insights and actionable intelligence for effective decision-making during disaster response. fuses cutting-edge machine learning algorithms and vast satellite data. helps organizations make data-driven decisions with unprecedented precision and speed, therefore speeding up humanitarian assistance to affected households after disasters. funded through wfp innovation accelerator . maps, reports"
NEMO: Real-time categorization of hotline calls,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Emergency Management,None of the above.,"NEMO: Aims to streamline and improve the quality of collected feedback and complaints by using AI to provide real-time categorization recommendations of hotline calls. Employs customized open-source AI algorithms and off-the-shelf AI services. This AI-natural language processing combination improves data collection efficiency through automated speech-to-text for feedback call transcriptions, real-time call categorization assisting hotline operators, and error identification. Funded through WFP Innovation Accelerator",Categorization of calls,Implementation and Assessment,Neither,4/30/2024,4/30/2024,4/30/2024,Developed with contracting resources.,720BHA24IO00037,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"NEMO: Aims to streamline and improve the quality of collected feedback and complaints by using AI to provide real-time categorization recommendations of hotline calls. Employs customized open-source AI algorithms and off-the-shelf AI services. This AI-natural language processing combination improves data collection efficiency through automated speech-to-text for feedback call transcriptions, real-time call categorization assisting hotline operators, and error identification. Funded through WFP Innovation Accelerator . Categorization of calls","nemo: aims to streamline and improve the quality of collected feedback and complaints by using ai to provide real-time categorization recommendations of hotline calls. employs customized open-source ai algorithms and off-the-shelf ai services. this ai-natural language processing combination improves data collection efficiency through automated speech-to-text for feedback call transcriptions, real-time call categorization assisting hotline operators, and error identification. funded through wfp innovation accelerator . categorization of calls"
"WFP AI Sandbox: Platform to innovate, pilot, and scale AI models and use cases",United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Diplomacy & Trade,None of the above.,"AI Sandbox: Aims to create a collaborative environment that provides WFP colleagues and partners wtih a platform to experiment, innovate, pilot and scale AI models and use cases. Empowers WFP teams to work efficiently and impactfully at scale. Enables the scaling of AI within WFP, piloting impactful use cases with scalability potential. Invests in talent, builds an ethical governance framework, and establishes a responsible-use roadmap. Funded through WFP Innovation Accelerator",Varies by specific use case,Implementation and Assessment,Neither,4/30/2024,4/30/2024,4/30/2024,Developed with contracting resources.,720BHA24IO00037,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"AI Sandbox: Aims to create a collaborative environment that provides WFP colleagues and partners wtih a platform to experiment, innovate, pilot and scale AI models and use cases. Empowers WFP teams to work efficiently and impactfully at scale. Enables the scaling of AI within WFP, piloting impactful use cases with scalability potential. Invests in talent, builds an ethical governance framework, and establishes a responsible-use roadmap. Funded through WFP Innovation Accelerator . Varies by specific use case","ai sandbox: aims to create a collaborative environment that provides wfp colleagues and partners wtih a platform to experiment, innovate, pilot and scale ai models and use cases. empowers wfp teams to work efficiently and impactfully at scale. enables the scaling of ai within wfp, piloting impactful use cases with scalability potential. invests in talent, builds an ethical governance framework, and establishes a responsible-use roadmap. funded through wfp innovation accelerator . varies by specific use case"
Enhancing Smart Agriculture in Rural Africa,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Diplomacy & Trade,None of the above.,"Enhancing Smart Agriculture in Rural Africa: Eliminating post-harvest loss Rooted in real-time data collection, it leverages TinyML and AI to provide actionable insights for farmers. Their system emphasizes early pest and disease detection, ensuring reduced crop losses. Funded through WFP Innovation Accelerator East Africa Regional Hub","Reports, visualizations",Implementation and Assessment,Neither,4/30/2024,4/30/2024,4/30/2024,Developed with contracting resources.,720BHA24IO00037,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Enhancing Smart Agriculture in Rural Africa: Eliminating post-harvest loss Rooted in real-time data collection, it leverages TinyML and AI to provide actionable insights for farmers. Their system emphasizes early pest and disease detection, ensuring reduced crop losses. Funded through WFP Innovation Accelerator East Africa Regional Hub . Reports, visualizations","enhancing smart agriculture in rural africa: eliminating post-harvest loss rooted in real-time data collection, it leverages tinyml and ai to provide actionable insights for farmers. their system emphasizes early pest and disease detection, ensuring reduced crop losses. funded through wfp innovation accelerator east africa regional hub . reports, visualizations"
Emata: Affordable digital finance for farmers,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Diplomacy & Trade,None of the above.,"Emata: Affordable digital finance for farmers Emata is a Ugandan fintech and licensed microfinance institution, leveraging technology and partnerships with agricultural cooperatives to offer farmers affordable, digital financing. In place of collateral requirements, the company creates AI-powered alternative credit scores based on data points like a farmer’s delivery history. Emata encourages farmers to grow their farms into businesses and invest in a productive future. Funded through WFP Innovation Accelerator East Africa Regional Hub",Credit scores,Implementation and Assessment,Neither,4/30/2024,4/30/2024,4/30/2024,Developed with contracting resources.,720BHA24IO00037,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Emata: Affordable digital finance for farmers Emata is a Ugandan fintech and licensed microfinance institution, leveraging technology and partnerships with agricultural cooperatives to offer farmers affordable, digital financing. In place of collateral requirements, the company creates AI-powered alternative credit scores based on data points like a farmer’s delivery history. Emata encourages farmers to grow their farms into businesses and invest in a productive future. Funded through WFP Innovation Accelerator East Africa Regional Hub . Credit scores","emata: affordable digital finance for farmers emata is a ugandan fintech and licensed microfinance institution, leveraging technology and partnerships with agricultural cooperatives to offer farmers affordable, digital financing. in place of collateral requirements, the company creates ai-powered alternative credit scores based on data points like a farmer’s delivery history. emata encourages farmers to grow their farms into businesses and invest in a productive future. funded through wfp innovation accelerator east africa regional hub . credit scores"
SoilPro 2.0: Advancing impact monitoring & monetization,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Diplomacy & Trade,None of the above.,"SoilPRO 2.0, a collaboration with SoilWatch, pioneers in science-based ecosystem and food system restoration and transformation, aims to integrate further innovative environmental monitoring into AIMS by using satellite imagery, peer-reviewed models, machine learning, and in-field testing-based validation. Utilizing the robust data and analysis provided by AIMS for evaluating the impact of our NBS, the project can advance every project activity in the region through a funding and carbon financing toolkit. Funded through WFP Innovation Accelerator East Africa Regional Hub","Reports, visualizations",Implementation and Assessment,Neither,4/30/2024,4/30/2024,4/30/2024,Developed with contracting resources.,720BHA24IO00037,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"SoilPRO 2.0, a collaboration with SoilWatch, pioneers in science-based ecosystem and food system restoration and transformation, aims to integrate further innovative environmental monitoring into AIMS by using satellite imagery, peer-reviewed models, machine learning, and in-field testing-based validation. Utilizing the robust data and analysis provided by AIMS for evaluating the impact of our NBS, the project can advance every project activity in the region through a funding and carbon financing toolkit. Funded through WFP Innovation Accelerator East Africa Regional Hub . Reports, visualizations","soilpro 2.0, a collaboration with soilwatch, pioneers in science-based ecosystem and food system restoration and transformation, aims to integrate further innovative environmental monitoring into aims by using satellite imagery, peer-reviewed models, machine learning, and in-field testing-based validation. utilizing the robust data and analysis provided by aims for evaluating the impact of our nbs, the project can advance every project activity in the region through a funding and carbon financing toolkit. funded through wfp innovation accelerator east africa regional hub . reports, visualizations"
RAG system for FEWS NET food security reports,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Mission-Enabling,None of the above.,"The FEWS NET Data, Learning and Communications Hub is developing an AWS SageMaker backed, Retrieval-Augmented Generation (RAG) model utilizing 40 years of historical FEWS NET food security reports. This allows analysts to use natural language to quickly search and retrieve relevant information, with links to the original reports. This model ensures that the wealth of historical FEWS NET food security analysis is accessible. The result is a more informed, data-driven foundation for future project decisions.",Text responses to queries,Acquisition and/or Development,Neither,10/1/2024,10/1/2024,Unknown,Developed with contracting resources.,7200AA24F00008,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"The FEWS NET Data, Learning and Communications Hub is developing an AWS SageMaker backed, Retrieval-Augmented Generation (RAG) model utilizing 40 years of historical FEWS NET food security reports. This allows analysts to use natural language to quickly search and retrieve relevant information, with links to the original reports. This model ensures that the wealth of historical FEWS NET food security analysis is accessible. The result is a more informed, data-driven foundation for future project decisions. . Text responses to queries","the fews net data, learning and communications hub is developing an aws sagemaker backed, retrieval-augmented generation (rag) model utilizing 40 years of historical fews net food security reports. this allows analysts to use natural language to quickly search and retrieve relevant information, with links to the original reports. this model ensures that the wealth of historical fews net food security analysis is accessible. the result is a more informed, data-driven foundation for future project decisions. . text responses to queries"
Generating SQL queries from natural language,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Mission-Enabling,None of the above.,"The FEWS NET Data, Learning and Communications Hub is developing an application that  allows select users to generate SQL queries using natural language, based on a modified and secure database schema. By allowing analysts without SQL expertise to efficiently extract data from the FEWS NET Data Warehouse, this tool enhances accessibility and streamlines data retrieval, making critical insights more readily available for decision-making.",SQL queries,Acquisition and/or Development,Neither,10/1/2024,10/1/2024,Unknown,Developed with contracting resources.,7200AA24F00008,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"The FEWS NET Data, Learning and Communications Hub is developing an application that  allows select users to generate SQL queries using natural language, based on a modified and secure database schema. By allowing analysts without SQL expertise to efficiently extract data from the FEWS NET Data Warehouse, this tool enhances accessibility and streamlines data retrieval, making critical insights more readily available for decision-making. . SQL queries","the fews net data, learning and communications hub is developing an application that allows select users to generate sql queries using natural language, based on a modified and secure database schema. by allowing analysts without sql expertise to efficiently extract data from the fews net data warehouse, this tool enhances accessibility and streamlines data retrieval, making critical insights more readily available for decision-making. . sql queries"
API queries from natural language prompts,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Mission-Enabling,None of the above.,"The FEWS NET Data, Learning and Communications Hub is developing an application that enables a large language model (LLM) to interpret the FEWS NET Data Warehouse OpenAPI specifications, allowing it to generate API queries from user-provided natural language prompts. The LLM can then either extract data from the API or answer questions based on the API responses, facilitating seamless data access and interaction for users without in-depth API knowledge.",Formatted API outputs,Acquisition and/or Development,Neither,10/1/2024,10/1/2024,Unknown,Developed with contracting resources.,7200AA24F00008,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"The FEWS NET Data, Learning and Communications Hub is developing an application that enables a large language model (LLM) to interpret the FEWS NET Data Warehouse OpenAPI specifications, allowing it to generate API queries from user-provided natural language prompts. The LLM can then either extract data from the API or answer questions based on the API responses, facilitating seamless data access and interaction for users without in-depth API knowledge. . Formatted API outputs","the fews net data, learning and communications hub is developing an application that enables a large language model (llm) to interpret the fews net data warehouse openapi specifications, allowing it to generate api queries from user-provided natural language prompts. the llm can then either extract data from the api or answer questions based on the api responses, facilitating seamless data access and interaction for users without in-depth api knowledge. . formatted api outputs"
Interactive exploration of FEWS NET food security reports,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Mission-Enabling,None of the above.,"The FEWS NET Data, Learning and Communications Hub is implementing a dashboard application developed by the International Center for Tropical Agriculture (CIAT) that uses natural language processing (NLP) to allow uses to interactively explore FEWS NET's 40 years of historic food security reporting to understand the evolution of drivers and trends in food security in the countries that FEWS NET covers.","Reports, visualizations",Acquisition and/or Development,Neither,10/1/2024,10/1/2024,Unknown,Developed with contracting resources.,7200AA24F00008,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"The FEWS NET Data, Learning and Communications Hub is implementing a dashboard application developed by the International Center for Tropical Agriculture (CIAT) that uses natural language processing (NLP) to allow uses to interactively explore FEWS NET's 40 years of historic food security reporting to understand the evolution of drivers and trends in food security in the countries that FEWS NET covers. . Reports, visualizations","the fews net data, learning and communications hub is implementing a dashboard application developed by the international center for tropical agriculture (ciat) that uses natural language processing (nlp) to allow uses to interactively explore fews net's 40 years of historic food security reporting to understand the evolution of drivers and trends in food security in the countries that fews net covers. . reports, visualizations"
Media synthesis for food security,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Diplomacy & Trade,None of the above.,"The FEWS NET Data, Learning and Communications Hub is implementing a dashboard application developed by the International Center for Tropical Agriculture (CIAT) that uses natural language processing (NLP) and machine learning to identify to report on relevant topics is both social media (currently Telegram) and formal media (using MedlaCloud) in relevant countries. The media content is evaluated for relvance using a comprehensive taxonomy covering drivers of food insecurity.","Reports, visualizations",Acquisition and/or Development,Neither,10/1/2024,10/1/2024,Unknown,Developed with contracting resources.,7200AA24F00008,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"The FEWS NET Data, Learning and Communications Hub is implementing a dashboard application developed by the International Center for Tropical Agriculture (CIAT) that uses natural language processing (NLP) and machine learning to identify to report on relevant topics is both social media (currently Telegram) and formal media (using MedlaCloud) in relevant countries. The media content is evaluated for relvance using a comprehensive taxonomy covering drivers of food insecurity. . Reports, visualizations","the fews net data, learning and communications hub is implementing a dashboard application developed by the international center for tropical agriculture (ciat) that uses natural language processing (nlp) and machine learning to identify to report on relevant topics is both social media (currently telegram) and formal media (using medlacloud) in relevant countries. the media content is evaluated for relvance using a comprehensive taxonomy covering drivers of food insecurity. . reports, visualizations"
Labeling and classifying food security activities,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Mission-Enabling,None of the above.,"Through a buy-in to IPI's Technical Assistance Project for Economic Growth (TAP-EG) award, ALT funded a third Food Assistance and Food Security Assessment (FAFSA-3), which included AI and machine learning models to discover and label each occurence of three project-specific approaches within 3,000 BHA files from resilience food security activities (since 2015) against a taxonomy developed within this project. ",Project classifications,Retired,Neither,5/1/2023,6/25/2023,4/1/2024,Developed with contracting resources.,47QRAD20DD1072 (TO - 7200AAN00006),No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Through a buy-in to IPI's Technical Assistance Project for Economic Growth (TAP-EG) award, ALT funded a third Food Assistance and Food Security Assessment (FAFSA-3), which included AI and machine learning models to discover and label each occurence of three project-specific approaches within 3,000 BHA files from resilience food security activities (since 2015) against a taxonomy developed within this project. . Project classifications","through a buy-in to ipi's technical assistance project for economic growth (tap-eg) award, alt funded a third food assistance and food security assessment (fafsa-3), which included ai and machine learning models to discover and label each occurence of three project-specific approaches within 3,000 bha files from resilience food security activities (since 2015) against a taxonomy developed within this project. . project classifications"
SOPHIA: Solutions Platform for Humanitarian analysis,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Diplomacy & Trade,Searching for information using AI.,"ACAPS, part of the Norwegian Refugee Council (NRC), is a humanitarian information provider that has developed a tool called the Solutions Platform for Humanitarian Analysis (SOPHIA), which uses NLP to filter through hundreds of humanitarian data sources in more than 70 languages, highlighting where to find specific indicators upon request by a user.",Report excerpts,Operation and Maintenance,Neither,1/18/2023,9/26/2023,5/30/2024,Developed with contracting resources.,720BHA23CA00034,No,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.2698412698412698,"ACAPS, part of the Norwegian Refugee Council (NRC), is a humanitarian information provider that has developed a tool called the Solutions Platform for Humanitarian Analysis (SOPHIA), which uses NLP to filter through hundreds of humanitarian data sources in more than 70 languages, highlighting where to find specific indicators upon request by a user. . Report excerpts","acaps, part of the norwegian refugee council (nrc), is a humanitarian information provider that has developed a tool called the solutions platform for humanitarian analysis (sophia), which uses nlp to filter through hundreds of humanitarian data sources in more than 70 languages, highlighting where to find specific indicators upon request by a user. . report excerpts"
DEEP: Data Entry and Exploration Platform,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Mission-Enabling,Summarizing the key points of a lengthy report using AI.,"The Data Entry and Exploration Platform (DEEP), an online platform for storing, sharing, and analyzing humanitarian data, especially unstructured and qualitative data. This includes an AI component of the platform that uses NLP to filter, annotate, and categorize content much faster than a human can. This “tagging” feature is offered to the humanitarian community at large, decreasing the time and effort required to summarize data for decision-makers.","Reports, visualizations",Implementation and Assessment,Neither,6/8/2020,10/1/2021,4/1/2022,Developed with contracting resources.,720FDA20CA00076,No,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.2698412698412698,"The Data Entry and Exploration Platform (DEEP), an online platform for storing, sharing, and analyzing humanitarian data, especially unstructured and qualitative data. This includes an AI component of the platform that uses NLP to filter, annotate, and categorize content much faster than a human can. This “tagging” feature is offered to the humanitarian community at large, decreasing the time and effort required to summarize data for decision-makers. . Reports, visualizations","the data entry and exploration platform (deep), an online platform for storing, sharing, and analyzing humanitarian data, especially unstructured and qualitative data. this includes an ai component of the platform that uses nlp to filter, annotate, and categorize content much faster than a human can. this “tagging” feature is offered to the humanitarian community at large, decreasing the time and effort required to summarize data for decision-makers. . reports, visualizations"
AI for humanitarian logistics,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Emergency Management,None of the above.,"Handicap International: Uses AI to create an operational decision support tool based on the automatic analysis of field data. This tool will enable aid providers to develop optimized strategies in the Cabo Delgado and Nampula regions. Models a logistical environment (warehouses, airports, hospitals, schools, etc.) through their interactions, thus forming a complex network. Enables a better understanding of the complex interactions between all the network's constituents. Funded through Humanitarian Grand Challenge","Reports, visualizations",Retired,Neither,6/26/2023,9/24/2024,Unknown,Developed with contracting resources.,720BHA23GR00104,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Handicap International: Uses AI to create an operational decision support tool based on the automatic analysis of field data. This tool will enable aid providers to develop optimized strategies in the Cabo Delgado and Nampula regions. Models a logistical environment (warehouses, airports, hospitals, schools, etc.) through their interactions, thus forming a complex network. Enables a better understanding of the complex interactions between all the network's constituents. Funded through Humanitarian Grand Challenge . Reports, visualizations","handicap international: uses ai to create an operational decision support tool based on the automatic analysis of field data. this tool will enable aid providers to develop optimized strategies in the cabo delgado and nampula regions. models a logistical environment (warehouses, airports, hospitals, schools, etc.) through their interactions, thus forming a complex network. enables a better understanding of the complex interactions between all the network's constituents. funded through humanitarian grand challenge . reports, visualizations"
"SMARTplus: digital system for fast, reliable malnutrition data tackling the health-crisis in Somalia",United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Health & Medical,None of the above.,"SMARTplus: digital system for fast, reliable malnutrition data tackling the health-crisis in Somalia, Action Against Hunger ACF (Canada): SMARTplus uses 3D scans to streamline nutrition and mortality data collection. It analyzes and verifies data, aggregating it for visualization on a public dashboard. This prevents delays and reliance on potentially false information. Funded through Humanitarian Grand Challenge","Reports, visualizations",Implementation and Assessment,Neither,6/26/2023,3/14/2024,3/14/2024,Developed with contracting resources.,720BHA23GR00104,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"SMARTplus: digital system for fast, reliable malnutrition data tackling the health-crisis in Somalia, Action Against Hunger ACF (Canada): SMARTplus uses 3D scans to streamline nutrition and mortality data collection. It analyzes and verifies data, aggregating it for visualization on a public dashboard. This prevents delays and reliance on potentially false information. Funded through Humanitarian Grand Challenge . Reports, visualizations","smartplus: digital system for fast, reliable malnutrition data tackling the health-crisis in somalia, action against hunger acf (canada): smartplus uses 3d scans to streamline nutrition and mortality data collection. it analyzes and verifies data, aggregating it for visualization on a public dashboard. this prevents delays and reliance on potentially false information. funded through humanitarian grand challenge . reports, visualizations"
HOPE and HELP: Modeling for complex humanitarian engagement,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Emergency Management,None of the above.,"HOPE and HELP: Modeling for complex humanitarian engagement, Humanity Data Systems: HOPE and HELP, created by Lensinsight and AllSource Analytics, use advanced technology like Machine Learning and Complex Event Processing to simplify gathering community feedback in humanitarian efforts. They also aid in project and stakeholder management. Funded through Humanitarian Grand Challenge","Reports, visualizations",Implementation and Assessment,Neither,6/26/2023,3/14/2024,3/14/2024,Developed with contracting resources.,720BHA23GR00104,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"HOPE and HELP: Modeling for complex humanitarian engagement, Humanity Data Systems: HOPE and HELP, created by Lensinsight and AllSource Analytics, use advanced technology like Machine Learning and Complex Event Processing to simplify gathering community feedback in humanitarian efforts. They also aid in project and stakeholder management. Funded through Humanitarian Grand Challenge . Reports, visualizations","hope and help: modeling for complex humanitarian engagement, humanity data systems: hope and help, created by lensinsight and allsource analytics, use advanced technology like machine learning and complex event processing to simplify gathering community feedback in humanitarian efforts. they also aid in project and stakeholder management. funded through humanitarian grand challenge . reports, visualizations"
Advanced language technology in marginalized languages for efficient information gathering,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Mission-Enabling,None of the above.,"Language technology to improve data quality and engagement, Translators without Borders US Inc. aims to create advanced language technology in marginalized languages for efficient information gathering.This pioneering effort could revolutionize understanding of humanitarian needs and priorities in remote regions. Funded through Humanitarian Grand Challenge",App interface,Implementation and Assessment,Neither,6/26/2023,3/14/2024,3/14/2024,Developed with contracting resources.,720BHA23GR00104,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Language technology to improve data quality and engagement, Translators without Borders US Inc. aims to create advanced language technology in marginalized languages for efficient information gathering.This pioneering effort could revolutionize understanding of humanitarian needs and priorities in remote regions. Funded through Humanitarian Grand Challenge . App interface","language technology to improve data quality and engagement, translators without borders us inc. aims to create advanced language technology in marginalized languages for efficient information gathering.this pioneering effort could revolutionize understanding of humanitarian needs and priorities in remote regions. funded through humanitarian grand challenge . app interface"
Camp forecast: AI-driven supply chain forecasting to reach 7% more refugees without spending more,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Diplomacy & Trade,None of the above.,"Camp forecast: AI-driven supply chain forecasting to reach 7% more refugees without spending more. The ""Camp Forecast"" (CF) tool employs AI for camp managers to make precise demand forecasts, potentially saving 7% of operational costs. It predicts resident numbers and required supplies in humanitarian camps, pioneering AI-driven forecasting in this context. Funded through Humanitarian Grand Challenge","Reports, visualizations",Implementation and Assessment,Neither,6/26/2023,3/14/2024,3/14/2024,Developed with contracting resources.,720BHA23GR00104,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Camp forecast: AI-driven supply chain forecasting to reach 7% more refugees without spending more. The ""Camp Forecast"" (CF) tool employs AI for camp managers to make precise demand forecasts, potentially saving 7% of operational costs. It predicts resident numbers and required supplies in humanitarian camps, pioneering AI-driven forecasting in this context. Funded through Humanitarian Grand Challenge . Reports, visualizations","camp forecast: ai-driven supply chain forecasting to reach 7% more refugees without spending more. the ""camp forecast"" (cf) tool employs ai for camp managers to make precise demand forecasts, potentially saving 7% of operational costs. it predicts resident numbers and required supplies in humanitarian camps, pioneering ai-driven forecasting in this context. funded through humanitarian grand challenge . reports, visualizations"
Humanitarian language toolkit: AI-enabled qualitative engagement with conflict populations,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Diplomacy & Trade,None of the above.,"Humanitarian language toolkit: AI-enabled qualitative engagement with conflict populations, Kobo, Inc.: This project aims to create a toolkit using NLP to record, transcribe, and translate qualitative interviews in humanitarian settings. Field workers can use standard data collection platforms for automatic recording and correction of transcriptions and translations. Funded through Humanitarian Grand Challenge",Text ,Implementation and Assessment,Neither,6/26/2023,3/14/2024,3/14/2024,Developed with contracting resources.,720BHA23GR00104,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Humanitarian language toolkit: AI-enabled qualitative engagement with conflict populations, Kobo, Inc.: This project aims to create a toolkit using NLP to record, transcribe, and translate qualitative interviews in humanitarian settings. Field workers can use standard data collection platforms for automatic recording and correction of transcriptions and translations. Funded through Humanitarian Grand Challenge . Text","humanitarian language toolkit: ai-enabled qualitative engagement with conflict populations, kobo, inc.: this project aims to create a toolkit using nlp to record, transcribe, and translate qualitative interviews in humanitarian settings. field workers can use standard data collection platforms for automatic recording and correction of transcriptions and translations. funded through humanitarian grand challenge . text"
Chatbot photo diagnosis for malnutrition,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Health & Medical,None of the above.,"Smart and effective communication in nutrition; ""Chatbot photo diagnosis platform"", Fundación Acción Contra el Hambre (Spain): This project establishes a smart communication channel for effective malnutrition diagnosis and monitoring, a joint effort between the World Food Programme (WFP) and Action Against Hunger (ACF). Funded through Humanitarian Grand Challenge",Chatbot interface,Implementation and Assessment,Neither,6/26/2023,3/14/2024,3/14/2024,Developed with contracting resources.,720BHA23GR00104,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Smart and effective communication in nutrition; ""Chatbot photo diagnosis platform"", Fundación Acción Contra el Hambre (Spain): This project establishes a smart communication channel for effective malnutrition diagnosis and monitoring, a joint effort between the World Food Programme (WFP) and Action Against Hunger (ACF). Funded through Humanitarian Grand Challenge . Chatbot interface","smart and effective communication in nutrition; ""chatbot photo diagnosis platform"", fundación acción contra el hambre (spain): this project establishes a smart communication channel for effective malnutrition diagnosis and monitoring, a joint effort between the world food programme (wfp) and action against hunger (acf). funded through humanitarian grand challenge . chatbot interface"
PathVis: A water monitoring device for Vibrio cholerae detection,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Diplomacy & Trade,None of the above.,"PathVis: A water monitoring device for Vibrio cholerae detection, OmniVis Inc: A groundbreaking smartphone diagnostic tool for rapid V. cholerae detection, providing results in under 30 minutes. Data is instantly sent to a cloud-based server for automated logging. In remote areas, a wifi hotspot enables data upload.This is the first water-based V. cholerae detection device, offering automated, time-stamped, and location-specific data logging. Funded through Humanitarian Grand Challenge",Prediction of V. Cholerae contamination,Implementation and Assessment,Neither,6/26/2023,3/14/2024,3/14/2024,Developed with contracting resources.,720BHA23GR00104,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"PathVis: A water monitoring device for Vibrio cholerae detection, OmniVis Inc: A groundbreaking smartphone diagnostic tool for rapid V. cholerae detection, providing results in under 30 minutes. Data is instantly sent to a cloud-based server for automated logging. In remote areas, a wifi hotspot enables data upload.This is the first water-based V. cholerae detection device, offering automated, time-stamped, and location-specific data logging. Funded through Humanitarian Grand Challenge . Prediction of V. Cholerae contamination","pathvis: a water monitoring device for vibrio cholerae detection, omnivis inc: a groundbreaking smartphone diagnostic tool for rapid v. cholerae detection, providing results in under 30 minutes. data is instantly sent to a cloud-based server for automated logging. in remote areas, a wifi hotspot enables data upload.this is the first water-based v. cholerae detection device, offering automated, time-stamped, and location-specific data logging. funded through humanitarian grand challenge . prediction of v. cholerae contamination"
NeedsBot - Aggregating real-time needs from anywhere via chatbot,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Emergency Management,None of the above.,"NeedsBot - Aggregating real-time needs from anywhere via chatbot, Needslist: NeedsBot enables vetted frontline responders to text urgent information, supply, and resource needs into the NeedsList platform.This data is then securely aggregated and accessible to multiple stakeholders, facilitating real-time response from individuals, corporations, and governments in the humanitarian aid sector. Funded through Humanitarian Grand Challenge","Reports, visualizations",Implementation and Assessment,Neither,6/26/2023,3/14/2024,3/14/2024,Developed with contracting resources.,720BHA23GR00104,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"NeedsBot - Aggregating real-time needs from anywhere via chatbot, Needslist: NeedsBot enables vetted frontline responders to text urgent information, supply, and resource needs into the NeedsList platform.This data is then securely aggregated and accessible to multiple stakeholders, facilitating real-time response from individuals, corporations, and governments in the humanitarian aid sector. Funded through Humanitarian Grand Challenge . Reports, visualizations","needsbot - aggregating real-time needs from anywhere via chatbot, needslist: needsbot enables vetted frontline responders to text urgent information, supply, and resource needs into the needslist platform.this data is then securely aggregated and accessible to multiple stakeholders, facilitating real-time response from individuals, corporations, and governments in the humanitarian aid sector. funded through humanitarian grand challenge . reports, visualizations"
Using technology to combat violence against female refugees,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Diplomacy & Trade,None of the above.,"Using technology to combat violence against female refugees, ActionAid in London (UK): A low-cost mobile platform for at-risk women and girls to offer essential information on rights, services, safe spaces, and GBV referrals. It enables incident reporting and risk mapping in urban areas. Pilot 1 will launch in collaboration with a Mobile Network Operator and Women's Protection Action Groups. Funded through Humanitarian Grand Challenge","Reports, maps",Implementation and Assessment,Neither,6/26/2023,3/14/2024,3/14/2024,Developed with contracting resources.,720BHA23GR00104,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Using technology to combat violence against female refugees, ActionAid in London (UK): A low-cost mobile platform for at-risk women and girls to offer essential information on rights, services, safe spaces, and GBV referrals. It enables incident reporting and risk mapping in urban areas. Pilot 1 will launch in collaboration with a Mobile Network Operator and Women's Protection Action Groups. Funded through Humanitarian Grand Challenge . Reports, maps","using technology to combat violence against female refugees, actionaid in london (uk): a low-cost mobile platform for at-risk women and girls to offer essential information on rights, services, safe spaces, and gbv referrals. it enables incident reporting and risk mapping in urban areas. pilot 1 will launch in collaboration with a mobile network operator and women's protection action groups. funded through humanitarian grand challenge . reports, maps"
AutoAnthro: 3D scanning for improved malnutrition assessment in conflict areas,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Health & Medical,None of the above.,"AutoAnthro: 3D scanning for improved malnutrition assessment in conflict areas, Body Surface Translations, Inc.: Body Surface Translations' AutoAnthro is a cutting-edge 3D scanning system revolutionizing child anthropometry. It swiftly captures precise digital measurements, enhancing outcomes for malnutrition at both individual and population levels. AutoAnthro extracts crucial data, measuring length/height, mid-upper arm circumference (MUAC), and head circumference for children aged 0-5 years. Funded through Humanitarian Grand Challenge",Body measurements of children,Implementation and Assessment,Neither,6/26/2023,3/14/2024,3/14/2024,Developed with contracting resources.,720BHA23GR00104,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"AutoAnthro: 3D scanning for improved malnutrition assessment in conflict areas, Body Surface Translations, Inc.: Body Surface Translations' AutoAnthro is a cutting-edge 3D scanning system revolutionizing child anthropometry. It swiftly captures precise digital measurements, enhancing outcomes for malnutrition at both individual and population levels. AutoAnthro extracts crucial data, measuring length/height, mid-upper arm circumference (MUAC), and head circumference for children aged 0-5 years. Funded through Humanitarian Grand Challenge . Body measurements of children","autoanthro: 3d scanning for improved malnutrition assessment in conflict areas, body surface translations, inc.: body surface translations' autoanthro is a cutting-edge 3d scanning system revolutionizing child anthropometry. it swiftly captures precise digital measurements, enhancing outcomes for malnutrition at both individual and population levels. autoanthro extracts crucial data, measuring length/height, mid-upper arm circumference (muac), and head circumference for children aged 0-5 years. funded through humanitarian grand challenge . body measurements of children"
Machine learning enabled safe water optimization tool for humanitarian response,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Emergency Management,None of the above.,"Machine learning enabled safe water optimization tool for humanitarian response, York University: This project aims to create a cloud-based tool using AI to optimize safe water practices. By analyzing existing water quality data, it generates customized chlorination instructions for specific field sites worldwide, ensuring safe drinking water based on evidence and site conditions. Funded through Humanitarian Grand Challenge",Text,Implementation and Assessment,Neither,6/26/2023,9/24/2024,9/24/2024,Developed with contracting resources.,720BHA23GR00104,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Machine learning enabled safe water optimization tool for humanitarian response, York University: This project aims to create a cloud-based tool using AI to optimize safe water practices. By analyzing existing water quality data, it generates customized chlorination instructions for specific field sites worldwide, ensuring safe drinking water based on evidence and site conditions. Funded through Humanitarian Grand Challenge . Text","machine learning enabled safe water optimization tool for humanitarian response, york university: this project aims to create a cloud-based tool using ai to optimize safe water practices. by analyzing existing water quality data, it generates customized chlorination instructions for specific field sites worldwide, ensuring safe drinking water based on evidence and site conditions. funded through humanitarian grand challenge . text"
Combatting health misinformation in Yemen,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Diplomacy & Trade,None of the above.,"Combatting disinformation in conflict zones, The SecDev Foundation: SecDev Foundation collaborates with local partners in Yemen to empower conflict-affected communities in combating harmful health misinformation.They provide guidance through trusted local networks via various channels like emergency alerts, online campaigns, and radio bulletins to ensure accurate information reaches those in need. Funded through Humanitarian Grand Challenge",Text,Implementation and Assessment,Neither,6/26/2023,3/14/2024,3/14/2024,Developed with contracting resources.,720BHA23GR00104,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Combatting disinformation in conflict zones, The SecDev Foundation: SecDev Foundation collaborates with local partners in Yemen to empower conflict-affected communities in combating harmful health misinformation.They provide guidance through trusted local networks via various channels like emergency alerts, online campaigns, and radio bulletins to ensure accurate information reaches those in need. Funded through Humanitarian Grand Challenge . Text","combatting disinformation in conflict zones, the secdev foundation: secdev foundation collaborates with local partners in yemen to empower conflict-affected communities in combating harmful health misinformation.they provide guidance through trusted local networks via various channels like emergency alerts, online campaigns, and radio bulletins to ensure accurate information reaches those in need. funded through humanitarian grand challenge . text"
AI-enabled air raid sirens and war crime documentation,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Emergency Management,None of the above.,"Hala Systems Inc.: Hala will utilize a network of air-raid sirens administered by civil society, combined with predictive algorithms, training and community engagement efforts to monitor other forms of violence (e.g. artillery, mortars), It will employ blockchain to generate immutable evidence of war crimes. Funded through Humanitarian Grand Challenge",Structured data,Implementation and Assessment,Neither,6/26/2023,3/14/2024,3/14/2024,Developed with contracting resources.,720BHA23GR00104,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Hala Systems Inc.: Hala will utilize a network of air-raid sirens administered by civil society, combined with predictive algorithms, training and community engagement efforts to monitor other forms of violence (e.g. artillery, mortars), It will employ blockchain to generate immutable evidence of war crimes. Funded through Humanitarian Grand Challenge . Structured data","hala systems inc.: hala will utilize a network of air-raid sirens administered by civil society, combined with predictive algorithms, training and community engagement efforts to monitor other forms of violence (e.g. artillery, mortars), it will employ blockchain to generate immutable evidence of war crimes. funded through humanitarian grand challenge . structured data"
AI tools for volunteer mapping,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Diplomacy & Trade,None of the above.,"Missing maps project, Humanitarian OpenStreetMap Team United States: Missing Maps, a global collaboration, mobilizes volunteers to map uncharted areas through user- friendly tools and open-source apps. Remote volunteers use satellite imagery to create base maps, then local volunteers and agencies refine and verify the details. This hyper-local data supports life-saving crisis response programs. Funded through Humanitarian Grand Challenge",Mapping tools,Implementation and Assessment,Neither,6/26/2023,9/24/2024,9/24/2024,Developed with contracting resources.,720BHA23GR00104,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Missing maps project, Humanitarian OpenStreetMap Team United States: Missing Maps, a global collaboration, mobilizes volunteers to map uncharted areas through user- friendly tools and open-source apps. Remote volunteers use satellite imagery to create base maps, then local volunteers and agencies refine and verify the details. This hyper-local data supports life-saving crisis response programs. Funded through Humanitarian Grand Challenge . Mapping tools","missing maps project, humanitarian openstreetmap team united states: missing maps, a global collaboration, mobilizes volunteers to map uncharted areas through user- friendly tools and open-source apps. remote volunteers use satellite imagery to create base maps, then local volunteers and agencies refine and verify the details. this hyper-local data supports life-saving crisis response programs. funded through humanitarian grand challenge . mapping tools"
Chatbot to provide accurate COVID-19 information,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Diplomacy & Trade,None of the above.,"Fighting COVID-19 misinformation with language technology, Translators without Borders US Inc.: The chatbot engages people in their native language, providing accurate pandemic information and informing targeted responses to misinformation. Currently in the Democratic Republic of the Congo,TWB plans to expand to more languages and countries. This innovative solution, accessible via platforms like WhatsApp and Messenger, ensures reliable COVID-19 information reaches vulnerable communities in a language they comprehend. Funded through Humanitarian Grand Challenge",Chatbot interface,Implementation and Assessment,Neither,6/26/2023,3/14/2024,3/14/2024,Developed with contracting resources.,720BHA23GR00104,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Fighting COVID-19 misinformation with language technology, Translators without Borders US Inc.: The chatbot engages people in their native language, providing accurate pandemic information and informing targeted responses to misinformation. Currently in the Democratic Republic of the Congo,TWB plans to expand to more languages and countries. This innovative solution, accessible via platforms like WhatsApp and Messenger, ensures reliable COVID-19 information reaches vulnerable communities in a language they comprehend. Funded through Humanitarian Grand Challenge . Chatbot interface","fighting covid-19 misinformation with language technology, translators without borders us inc.: the chatbot engages people in their native language, providing accurate pandemic information and informing targeted responses to misinformation. currently in the democratic republic of the congo,twb plans to expand to more languages and countries. this innovative solution, accessible via platforms like whatsapp and messenger, ensures reliable covid-19 information reaches vulnerable communities in a language they comprehend. funded through humanitarian grand challenge . chatbot interface"
Imagery authentication from conflict zones,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Diplomacy & Trade,None of the above.,"Snap truth to power, Sealr: Sealr, a mobile app, combines AI and blockchain to authenticate imagery from conflict zones.This
 empowers vulnerable populations to report needs securely. As the first verified image-capture app for humanitarian crises, Sealr has the potential to revolutionize a sector often hindered by inefficient feedback systems. Funded through Humanitarian Grand Challenge",Image metadata,Implementation and Assessment,Neither,6/26/2023,3/14/2024,3/14/2024,Developed with contracting resources.,720BHA23GR00104,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Snap truth to power, Sealr: Sealr, a mobile app, combines AI and blockchain to authenticate imagery from conflict zones.This
 empowers vulnerable populations to report needs securely. As the first verified image-capture app for humanitarian crises, Sealr has the potential to revolutionize a sector often hindered by inefficient feedback systems. Funded through Humanitarian Grand Challenge . Image metadata","snap truth to power, sealr: sealr, a mobile app, combines ai and blockchain to authenticate imagery from conflict zones.this empowers vulnerable populations to report needs securely. as the first verified image-capture app for humanitarian crises, sealr has the potential to revolutionize a sector often hindered by inefficient feedback systems. funded through humanitarian grand challenge . image metadata"
Tracking COVID-19 misinformation on social media,United States Agency for International Development,USAID,USAID/Bureau for Humanitarian Assistance,Diplomacy & Trade,None of the above.,"Countering COVID-19 misinformation in areas affected by conflict, Murmurate: Murmurate tracks how misinformation spreads in communities via social media. Using Computer Vision and Natural Language Processing, Murmurate maps COVID-19 conversations online, identifying misinformation that delivers false and damaging public health statements.These insights drive the production and distribution of OVID-19-related counter-messaging. Funded through Humanitarian Grand Challenge","Reports, visualizations",Implementation and Assessment,Neither,6/26/2023,3/14/2024,3/14/2024,Developed with contracting resources.,720BHA23GR00104,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Countering COVID-19 misinformation in areas affected by conflict, Murmurate: Murmurate tracks how misinformation spreads in communities via social media. Using Computer Vision and Natural Language Processing, Murmurate maps COVID-19 conversations online, identifying misinformation that delivers false and damaging public health statements.These insights drive the production and distribution of OVID-19-related counter-messaging. Funded through Humanitarian Grand Challenge . Reports, visualizations","countering covid-19 misinformation in areas affected by conflict, murmurate: murmurate tracks how misinformation spreads in communities via social media. using computer vision and natural language processing, murmurate maps covid-19 conversations online, identifying misinformation that delivers false and damaging public health statements.these insights drive the production and distribution of ovid-19-related counter-messaging. funded through humanitarian grand challenge . reports, visualizations"
Bambara language processing tool,United States Agency for International Development,USAID,USAID/Mali,Diplomacy & Trade,None of the above.,"The Bambara language processing tool is being designed to fill a gap that current technology lacks in tracking narratives, usually originating in French here in Mali. Through various platforms, messages are replicated and shared without verification, and at the moment, we're only able to gauge how far French narratives are spreading. This will enable us to track social media narratives in Bambara, a language spoken by 90% of Mali's population.","Reports, visualizations",Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.31746031746031744,"The Bambara language processing tool is being designed to fill a gap that current technology lacks in tracking narratives, usually originating in French here in Mali. Through various platforms, messages are replicated and shared without verification, and at the moment, we're only able to gauge how far French narratives are spreading. This will enable us to track social media narratives in Bambara, a language spoken by 90% of Mali's population. . Reports, visualizations","the bambara language processing tool is being designed to fill a gap that current technology lacks in tracking narratives, usually originating in french here in mali. through various platforms, messages are replicated and shared without verification, and at the moment, we're only able to gauge how far french narratives are spreading. this will enable us to track social media narratives in bambara, a language spoken by 90% of mali's population. . reports, visualizations"
Repair Spend,Department of Agriculture,USDA,ARS: Agricultural Research Service,Mission-Enabling,None of the above.,"The intended purpose of this model is to review financial documents and then classify each expense as money spent on ""facility repairs"" or ""not facility repairs"". The expected benefits include reduction of manual hours identifying the types of transactions.","The output of the model is a recommendation of which financial transactions should be identified as ""repair"" expenses.",Operation and Maintenance,Neither,10/1/2019,10/1/2019,6/9/2020,Developed with both contracting and in-house resources.,Developed by REE-Analytics within EDAPT to refresh Impala tables and visualized on the Tableau Server,No,No,No,No,Yes,"Approximately 14,000 financial transactions were used to train the model and finetune its parameters. Approximately 3,500 financial transactions were used to test the performance of the final model.","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,"Enterprise Data Analytics Platform & Toolset (EDAPT, Impala, Tableau)",Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"The intended purpose of this model is to review financial documents and then classify each expense as money spent on ""facility repairs"" or ""not facility repairs"". The expected benefits include reduction of manual hours identifying the types of transactions. . The output of the model is a recommendation of which financial transactions should be identified as ""repair"" expenses.","the intended purpose of this model is to review financial documents and then classify each expense as money spent on ""facility repairs"" or ""not facility repairs"". the expected benefits include reduction of manual hours identifying the types of transactions. . the output of the model is a recommendation of which financial transactions should be identified as ""repair"" expenses."
ARS Project Mapping,Department of Agriculture,USDA,ARS: Agricultural Research Service,Science & Space,None of the above.,"The intended purpose of this model is to process research plans from various research program portfolios in the Agricultural Research Service (ARS) to find patterns and opportunities between projects. The expected benefits include decreasing the time that humans would spend to manually read, pull out key terms, and group the projects by topic. The model may also find patterns that a human might miss.","The model outputs groups of similar projects and project terms. The output includes metrics (silhouette scores, term rank, importance scores) that show how well the projects and terms in a group match.",Operation and Maintenance,Neither,1/1/2020,1/1/2021,5/1/2022,Developed with contracting resources.,1232SA21F0067,No,No,No,No,Yes,"The data is a collection of project plans written by scientists, roughly 600 text documents. The texts are related to publicly available 5-year action plans. ","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Enterprise Data Analytics Platform & Toolset (EDAPT),Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"The intended purpose of this model is to process research plans from various research program portfolios in the Agricultural Research Service (ARS) to find patterns and opportunities between projects. The expected benefits include decreasing the time that humans would spend to manually read, pull out key terms, and group the projects by topic. The model may also find patterns that a human might miss. . The model outputs groups of similar projects and project terms. The output includes metrics (silhouette scores, term rank, importance scores) that show how well the projects and terms in a group match.","the intended purpose of this model is to process research plans from various research program portfolios in the agricultural research service (ars) to find patterns and opportunities between projects. the expected benefits include decreasing the time that humans would spend to manually read, pull out key terms, and group the projects by topic. the model may also find patterns that a human might miss. . the model outputs groups of similar projects and project terms. the output includes metrics (silhouette scores, term rank, importance scores) that show how well the projects and terms in a group match."
NAL Automated Indexing,Department of Agriculture,USDA,ARS: Agricultural Research Service,Science & Space,None of the above.,This system automatically assigns word tags to agricultural research articles from a controlled list of terms provided by the National Agricultural Library Thesaurus (NALT). The tags can be used to look up and retrieve articles. Using these tags benefits users by making it easier to find the content they are looking for.,The model outputs terms to use as search tags that are specific to the article that the model analyzed.,Operation and Maintenance,Neither,6/1/2011,1/1/2012,6/1/2012,Developed with both contracting and in-house resources.,1232SSA1C0014,No,Yes,No,No,Yes,The data is a collection of text scripts from publishers that have undergone quality assurance and quality control processing.,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,No,Unknown,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,This system automatically assigns word tags to agricultural research articles from a controlled list of terms provided by the National Agricultural Library Thesaurus (NALT). The tags can be used to look up and retrieve articles. Using these tags benefits users by making it easier to find the content they are looking for. . The model outputs terms to use as search tags that are specific to the article that the model analyzed.,this system automatically assigns word tags to agricultural research articles from a controlled list of terms provided by the national agricultural library thesaurus (nalt). the tags can be used to look up and retrieve articles. using these tags benefits users by making it easier to find the content they are looking for. . the model outputs terms to use as search tags that are specific to the article that the model analyzed.
Predictive Modeling of Invasive Pest Species,Department of Agriculture,USDA,APHIS: Animal and Plant Health Inspection Service,Mission-Enabling,None of the above.,"The purpose of the model is to check how likely it is for imported agricultural products from other countries to have pests. Benefits include more reliable discovery and quarantine of invasive pests, preventing pest invasion and making trade safer.",The model outputs are a prediction of whether a product carries an invasive species and what invasive species category the pest belongs to.,Operation and Maintenance,Neither,7/1/2015,7/1/2015,5/1/2018,Developed in-house.,Unknown,No,No,No,No,Yes,Inspection data was collected from Plant Protection and Quarantine (PPQ) and Customs and Border Protection (CBP). Quality control of the data was conducted by data analysts. There was no data augmention performed. The data is structured and has several million records with more than 50 numerical and categorical variables. This data is not publicly available. ,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"The purpose of the model is to check how likely it is for imported agricultural products from other countries to have pests. Benefits include more reliable discovery and quarantine of invasive pests, preventing pest invasion and making trade safer. . The model outputs are a prediction of whether a product carries an invasive species and what invasive species category the pest belongs to.","the purpose of the model is to check how likely it is for imported agricultural products from other countries to have pests. benefits include more reliable discovery and quarantine of invasive pests, preventing pest invasion and making trade safer. . the model outputs are a prediction of whether a product carries an invasive species and what invasive species category the pest belongs to."
Democratizing Data,Department of Agriculture,USDA,NASS: National Agricultural Statistics Service,Mission-Enabling,None of the above.,"This system scans collections of published documents to find how publicly-funded data and evidence are used to serve science and society. This helps the National Agricultural Statistics Service and the Economic Research Service understand who is using their data and why. This improves customer service, helps evaluate programs, and answers important questions for planning and learning.",The model outputs text containing the identified dataset reference information.,Operation and Maintenance,Neither,3/8/2021,6/3/2021,7/30/2021,Developed with contracting resources.,58-3AEU-3-0003,No,Yes,No,No,Yes,The model is trained on publicly available peer reviewed research (text data) and known usages of datasets.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,Yes – source code is publicly available.,No,Unknown,Less than 6 months,No,Other,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"This system scans collections of published documents to find how publicly-funded data and evidence are used to serve science and society. This helps the National Agricultural Statistics Service and the Economic Research Service understand who is using their data and why. This improves customer service, helps evaluate programs, and answers important questions for planning and learning. . The model outputs text containing the identified dataset reference information.","this system scans collections of published documents to find how publicly-funded data and evidence are used to serve science and society. this helps the national agricultural statistics service and the economic research service understand who is using their data and why. this improves customer service, helps evaluate programs, and answers important questions for planning and learning. . the model outputs text containing the identified dataset reference information."
Land Change Analysis Tool (LCAT),Department of Agriculture,USDA,FPAC-BC: FPAC Business Center,Mission-Enabling,None of the above.,"The Land Change Analysis Tool (LCAT) creates high resolution maps to help make land use decisions. For example, it has been used to monitor eastern redcedar for about 40 years in South Dakota and to support wildlife hazard assessments at airports with various organizations. This tool reduced the labor hours needed by the Farm Service Agency (FSA) to review land data accuracy in Georgia by 100 times.",The model outputs land cover maps.,Implementation and Assessment,Neither,10/1/2018,10/1/2018,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,"The dataset is created from the National Agriculture Imagery Program, which contains images of land taken from aircrafts.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,6-12 months,Yes,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The Land Change Analysis Tool (LCAT) creates high resolution maps to help make land use decisions. For example, it has been used to monitor eastern redcedar for about 40 years in South Dakota and to support wildlife hazard assessments at airports with various organizations. This tool reduced the labor hours needed by the Farm Service Agency (FSA) to review land data accuracy in Georgia by 100 times. . The model outputs land cover maps.","the land change analysis tool (lcat) creates high resolution maps to help make land use decisions. for example, it has been used to monitor eastern redcedar for about 40 years in south dakota and to support wildlife hazard assessments at airports with various organizations. this tool reduced the labor hours needed by the farm service agency (fsa) to review land data accuracy in georgia by 100 times. . the model outputs land cover maps."
OCIO/CDO Council Comment Analysis Tool,Department of Agriculture,USDA,OCIO: Office of the Chief Information Officer,Mission-Enabling,None of the above.,"This prototype helps reviewers identify the main topics and themes of comments, and then group similar comments together. This makes the comment review process more efficient by providing new insights and speeding up comment processing. Benefits include reducing repeated development efforts across the government and saving costs.",The model outputs groups of comments categorized by topic and similarity.,Acquisition and/or Development,Neither,12/1/2020,1/1/2021,Unknown,Developed with contracting resources.,12314418F0020/P00075,No,Unknown,Unknown,No,Yes,The training data contains publicly available comment data exported from Regulations.gov. The data largely contains text; some data samples could contain images or data tables. ,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Unknown,Unknown,No,Unknown,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"This prototype helps reviewers identify the main topics and themes of comments, and then group similar comments together. This makes the comment review process more efficient by providing new insights and speeding up comment processing. Benefits include reducing repeated development efforts across the government and saving costs. . The model outputs groups of comments categorized by topic and similarity.","this prototype helps reviewers identify the main topics and themes of comments, and then group similar comments together. this makes the comment review process more efficient by providing new insights and speeding up comment processing. benefits include reducing repeated development efforts across the government and saving costs. . the model outputs groups of comments categorized by topic and similarity."
Retailer Receipt Analysis,Department of Agriculture,USDA,FNS: Food and Nutrition Service,Government Services (includes Benefits and Service Delivery),Inputting large amounts of data from paper forms into a digital system using AI.,This system uses optical character recognition (OCR) to convert physical inventory documentation into digital text. This makes the review of inventory documents more efficient and consistent.,The model outputs digital text of inventory documentation and distinguishes food items and categories.,Acquisition and/or Development,Neither,10/1/2021,10/1/2021,Unknown,Developed with contracting resources.,Contract number: 47QTCA18D0081,No,Unknown,Unknown,No,Yes,The dataset is a collection of Supplemental Nutrition Assistance Program (SNAP) retailer inventory documentation submitted in response to charges of program violations. ,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,More than 12 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,This system uses optical character recognition (OCR) to convert physical inventory documentation into digital text. This makes the review of inventory documents more efficient and consistent. . The model outputs digital text of inventory documentation and distinguishes food items and categories.,this system uses optical character recognition (ocr) to convert physical inventory documentation into digital text. this makes the review of inventory documents more efficient and consistent. . the model outputs digital text of inventory documentation and distinguishes food items and categories.
Ecosystem Management Decision Support System (EMDS),Department of Agriculture,USDA,FS: Forest Service,Energy & the Environment,None of the above.,This system provides decision support for environmental analysis and planning by using AI-powered tools in ArcGIS and QGIS. Use of this system empowers stakeholders to make more informed and effective decisions about natural resource management.,"Outputs from this system include the identification of landscapes in need of management/maintenance, along with suggested management actions based on considerations such as cost, efficacy, and policy.",Operation and Maintenance,Neither,1/1/1994,6/1/1995,2/1/1997,Developed with contracting resources.,Many contracts dating from 1995,No,No,No,No,Yes,"Primary data sources included observed insect and pathogen-induced mortality, key critical loads for soil and the atmosphere, long term seasonal departures in temperature and precipitation, road densities, uncharacteristic wildfires, historical fire regime departure, wildfire potential, insect and pathogen risk, and vegetation departure from natural range of variability.","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,No – agency does not have access to source code.,Yes,Ecosystem Management Decision Support System,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"This system provides decision support for environmental analysis and planning by using AI-powered tools in ArcGIS and QGIS. Use of this system empowers stakeholders to make more informed and effective decisions about natural resource management. . Outputs from this system include the identification of landscapes in need of management/maintenance, along with suggested management actions based on considerations such as cost, efficacy, and policy.","this system provides decision support for environmental analysis and planning by using ai-powered tools in arcgis and qgis. use of this system empowers stakeholders to make more informed and effective decisions about natural resource management. . outputs from this system include the identification of landscapes in need of management/maintenance, along with suggested management actions based on considerations such as cost, efficacy, and policy."
Cross-Laminated Timber (CLT) Knowledge Database,Department of Agriculture,USDA,FS: Forest Service,Energy & the Environment,None of the above.,"This system enables researchers, practitioners, and the public to find specialized information about timber products. Benefits include faster information sharing and less time spent on manual searches.",System outputs are webpage links from the timber knowledge database.,Operation and Maintenance,Neither,12/1/2017,12/1/2017,6/1/2018,Developed with contracting resources.,"This was done through a Partnership with the University of Minnesota. Question 35 does not include this option, no contracts were used.",No,Yes,No,No,Yes,"Public domain agency publications were used for the dataset. This includes station papers, research, and journal papers by US Forest Service and other scientists.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,Other,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"This system enables researchers, practitioners, and the public to find specialized information about timber products. Benefits include faster information sharing and less time spent on manual searches. . System outputs are webpage links from the timber knowledge database.","this system enables researchers, practitioners, and the public to find specialized information about timber products. benefits include faster information sharing and less time spent on manual searches. . system outputs are webpage links from the timber knowledge database."
Raster Tools,Department of Agriculture,USDA,FS: Forest Service,Science & Space,None of the above.,"This system will make machine learning techniques available for geospatial applications. Benefits include standardization of methods, improved work quality, and increased user productivity.","The system API (Application Programming Interface) provides various AI outputs, usually in the form of raster images and data tables.",Implementation and Assessment,Neither,8/1/2021,8/1/2021,Unknown,Developed in-house.,Unknown,No,No,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,Yes – source code is publicly available.,No,Unknown,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"This system will make machine learning techniques available for geospatial applications. Benefits include standardization of methods, improved work quality, and increased user productivity. . The system API (Application Programming Interface) provides various AI outputs, usually in the form of raster images and data tables.","this system will make machine learning techniques available for geospatial applications. benefits include standardization of methods, improved work quality, and increased user productivity. . the system api (application programming interface) provides various ai outputs, usually in the form of raster images and data tables."
TreeMap and FuelMap (all versions),Department of Agriculture,USDA,FS: Forest Service; Forest Service Research & Development,Energy & the Environment,None of the above.,"TreeMap provides a detailed model of the forests in the US. It is used for measuring carbon, planning fuel treatments, starting landscape vegetation models, assessing fire effects, and more. Users include the US Forest Service, private companies, and state governments.",TreeMap produces a detailed map of a plot of forest and a database table listing individual tree records or fuel characteristics for each plot.,Operation and Maintenance,Neither,1/1/2010,10/12/2016,10/1/2018,Developed in-house.,Unknown,No,Yes,No,No,Yes,"TreeMap is validated using Forest Inventory and Analysis field plot data, consisting of percent forest cover, height, vegetation type, topography (slope, elevation, and aspect), location (latitude and longitude), biophysical variables (photosynthetically active radiation, precipitation, maximum temperature, minimum temperature, relative humidity, and vapour pressure deficit), and disturbance history (time since disturbance and disturbance type) for the landscape around 2016.","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,Yes – source code is publicly available.,No,Unknown,6-12 months,No,Yes,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"TreeMap provides a detailed model of the forests in the US. It is used for measuring carbon, planning fuel treatments, starting landscape vegetation models, assessing fire effects, and more. Users include the US Forest Service, private companies, and state governments. . TreeMap produces a detailed map of a plot of forest and a database table listing individual tree records or fuel characteristics for each plot.","treemap provides a detailed model of the forests in the us. it is used for measuring carbon, planning fuel treatments, starting landscape vegetation models, assessing fire effects, and more. users include the us forest service, private companies, and state governments. . treemap produces a detailed map of a plot of forest and a database table listing individual tree records or fuel characteristics for each plot."
Landscape Change Monitoring System (LCMS),Department of Agriculture,USDA,FS: Forest Service,Science & Space,None of the above.,This project monitors large areas for changes in land cover and land use over time. The benefits include creating a consistent method for tracking changes in the landscape.,"The model outputs predictions of vegetation gain, vegetation loss, land cover, and land uses.",Operation and Maintenance,Neither,1/15/2016,1/1/2017,3/1/2021,Developed in-house.,Unknown,No,Yes,No,No,Yes,"Training and evaluation data used includes 15,000 sample plots spanning from 1985 - 2019. Features of the dataset are land cover, land use, and change process categories. Data is available upon request.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Google Cloud Platform contract,6-12 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"This project monitors large areas for changes in land cover and land use over time. The benefits include creating a consistent method for tracking changes in the landscape. . The model outputs predictions of vegetation gain, vegetation loss, land cover, and land uses.","this project monitors large areas for changes in land cover and land use over time. the benefits include creating a consistent method for tracking changes in the landscape. . the model outputs predictions of vegetation gain, vegetation loss, land cover, and land uses."
Forest Health Detection Monitoring,Department of Agriculture,USDA,FS: Forest Service,Energy & the Environment,None of the above.,This project monitors forest health by detecting tree damage through changes in light patterns collected by satellites. This detection method helps the Forest Health Protection program monitor areas that can't be checked on the ground or with aerial surveys.,"The model outputs the stage of forest health based on the image, along with a map (polygons) of the area for monitoring.",Implementation and Assessment,Neither,6/29/2021,11/18/2021,Unknown,Developed with both contracting and in-house resources.,1284JC18D005/1284JC20F0036,No,No,No,No,Yes,"Training data was collected from the National Agriculture Imagery Program (NAIP) and WorldView imagery. Similar high resolution imagery has been used in evaluation and validation. There are approximately 1000 labeled images in the training dataset, each containing spectral data. The data are not publicly available.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"This project monitors forest health by detecting tree damage through changes in light patterns collected by satellites. This detection method helps the Forest Health Protection program monitor areas that can't be checked on the ground or with aerial surveys. . The model outputs the stage of forest health based on the image, along with a map (polygons) of the area for monitoring.","this project monitors forest health by detecting tree damage through changes in light patterns collected by satellites. this detection method helps the forest health protection program monitor areas that can't be checked on the ground or with aerial surveys. . the model outputs the stage of forest health based on the image, along with a map (polygons) of the area for monitoring."
Cropland Data Layer,Department of Agriculture,USDA,NASS: National Agricultural Statistics Service,Other,None of the above.,This project produces supplemental estimates of crop acreage and releases geospatial data products to the user community.,The system outputs are an acreage estimate and agrigulture-specific land cover product.,Operation and Maintenance,Neither,1/1/2008,1/1/2008,1/1/2008,Developed in-house.,Unknown,No,Yes,No,No,Yes,"USDA/Farm Service Agency Common Land Unit data, consisting of Geographic Information System (GIS) shapefiles and associated attribute data, was used for training and testing.  ","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,See5,Less than 6 months,No,No,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,This project produces supplemental estimates of crop acreage and releases geospatial data products to the user community. . The system outputs are an acreage estimate and agrigulture-specific land cover product.,this project produces supplemental estimates of crop acreage and releases geospatial data products to the user community. . the system outputs are an acreage estimate and agrigulture-specific land cover product.
List Frame Deadwood Identification,Department of Agriculture,USDA,NASS: National Agricultural Statistics Service,Other,None of the above.,"This model helps identify farms that may be out of business on the National Agricultural Statistics Service list. Parts of the model were used to create clear rules to identify these farms. The resulting list is more accurate and allows for smaller sample sizes, reducing the burden on respondents.",The output of the model was a probability score that a farm is out of business. ,Operation and Maintenance,Neither,2/4/2014,2/3/2016,1/24/2018,Developed in-house.,Unknown,No,No,Yes,No,Yes,"National Agricultural Statistics Service List frame and survey data, consisting of features put together to form profiles of farming operations, was used to train and validate the model.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,"Research, Education, and Economics (REE) - National Agricultural Statistics Service (NASS) - Citrix Environment",Less than 6 months,No,Yes,No,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"This model helps identify farms that may be out of business on the National Agricultural Statistics Service list. Parts of the model were used to create clear rules to identify these farms. The resulting list is more accurate and allows for smaller sample sizes, reducing the burden on respondents. . The output of the model was a probability score that a farm is out of business.","this model helps identify farms that may be out of business on the national agricultural statistics service list. parts of the model were used to create clear rules to identify these farms. the resulting list is more accurate and allows for smaller sample sizes, reducing the burden on respondents. . the output of the model was a probability score that a farm is out of business."
Climate Change Classification NLP,Department of Agriculture,USDA,NIFA: National Institute of Food and Agriculture,Mission-Enabling,None of the above.,The Climate Change Classification Natural Language Processing (NLP) model identifies likely climate-related projects within National Institute of Food and Agriculture's (NIFA) large and diverse funding portfolio. Expected benefits include reduced labor hours for reporting and increased repeatability and accuracy of reporting.,"Model output is a list of climate change projects classified as ""climate change related"" or ""not climate change related"" for National Institute of Food and Agriculture (NIFA) internal project review/adjudication and reporting.",Acquisition and/or Development,Neither,7/1/2021,7/1/2024,Unknown,Developed with both contracting and in-house resources.,HHSN316201200002W,No,Unknown,Unknown,No,Yes,"This model is trained on publicly available project information provided by the project directors via National Institute of Food and Agriculture's (NIFA) REEport reporting portal.  This includes text fields containing the project's title, summary, objectives, and keywords.  The training data is catagorized by the project team. ","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,More than 12 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"The Climate Change Classification Natural Language Processing (NLP) model identifies likely climate-related projects within National Institute of Food and Agriculture's (NIFA) large and diverse funding portfolio. Expected benefits include reduced labor hours for reporting and increased repeatability and accuracy of reporting. . Model output is a list of climate change projects classified as ""climate change related"" or ""not climate change related"" for National Institute of Food and Agriculture (NIFA) internal project review/adjudication and reporting.","the climate change classification natural language processing (nlp) model identifies likely climate-related projects within national institute of food and agriculture's (nifa) large and diverse funding portfolio. expected benefits include reduced labor hours for reporting and increased repeatability and accuracy of reporting. . model output is a list of climate change projects classified as ""climate change related"" or ""not climate change related"" for national institute of food and agriculture (nifa) internal project review/adjudication and reporting."
Video Surveillance System,Department of Agriculture,USDA,"OSSP: Office of Safety, Security, and Protection",Mission-Enabling,None of the above.,The purpose of this system is to conduct facial recognition video surveillance to provide enhanced security. Benefits include reduced labor hours for technicians and augmented surveillance capability.    ,"The system outputs a positive match to the security control center, indicating identification of the selected individual. An alarm notification is sent to alert security personnel.    ",Operation and Maintenance,"Rights-Impacting
",2/14/2020,2/14/2020,2/14/2021,Developed with both contracting and in-house resources.,12314420A0001,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,No,The key risk of this system is the misidentification of an individual.,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,Yes,Rights-Impacting,0.6984126984126984,"The purpose of this system is to conduct facial recognition video surveillance to provide enhanced security. Benefits include reduced labor hours for technicians and augmented surveillance capability. . The system outputs a positive match to the security control center, indicating identification of the selected individual. An alarm notification is sent to alert security personnel. . The key risk of this system is the misidentification of an individual.","the purpose of this system is to conduct facial recognition video surveillance to provide enhanced security. benefits include reduced labor hours for technicians and augmented surveillance capability. . the system outputs a positive match to the security control center, indicating identification of the selected individual. an alarm notification is sent to alert security personnel. . the key risk of this system is the misidentification of an individual."
Aquisition Approval Request Compliance Tool,Department of Agriculture,USDA,OCIO: Office of the Chief Information Officer,Mission-Enabling,None of the above.,This project was developed to help identify likely Information Technology (IT) purchases that do not have an associated Acquisition Approval Request. The benefits are reducing unauthorized IT purchases and increasing compliance with IT procurement procedures and approvals.,The output is a score indicating how likely it is that the purchase is an Information Technology (IT) purchase.,Acquisition and/or Development,Neither,10/31/2019,4/6/2020,Unknown,Developed with both contracting and in-house resources.,12314419D0006,No,Unknown,Unknown,No,Yes,The model was developed using text entries entered into USDA's Integrated Acquisition System (IAS).,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,Less than 6 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,This project was developed to help identify likely Information Technology (IT) purchases that do not have an associated Acquisition Approval Request. The benefits are reducing unauthorized IT purchases and increasing compliance with IT procurement procedures and approvals. . The output is a score indicating how likely it is that the purchase is an Information Technology (IT) purchase.,this project was developed to help identify likely information technology (it) purchases that do not have an associated acquisition approval request. the benefits are reducing unauthorized it purchases and increasing compliance with it procurement procedures and approvals. . the output is a score indicating how likely it is that the purchase is an information technology (it) purchase.
Operational Water Supply Forecasting for Western US Rivers,Department of Agriculture,USDA,NRCS: Natural Resources Conservation Service,Mission-Enabling,None of the above.,"The National Water and Climate Center has a multi-model machine-learning metasystem (M4) for generating water supply forecasts. This model uses AI and other data-science technologies to reduce forecast errors, helping stakeholders make better decisions about water supply availability.",The model outputs water supply forecasts.,Operation and Maintenance,Neither,12/1/2019,12/1/2019,1/1/2024,Developed in-house.,Unknown,Yes,Yes,No,No,Yes,The data contains snow and precipitation metrics from the Natural Resources Conservation Service (NRCS) Snow Survey and Water Supply Forecast program Snow Telemetry (SNOTEL) monitoring network. ,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"The National Water and Climate Center has a multi-model machine-learning metasystem (M4) for generating water supply forecasts. This model uses AI and other data-science technologies to reduce forecast errors, helping stakeholders make better decisions about water supply availability. . The model outputs water supply forecasts.","the national water and climate center has a multi-model machine-learning metasystem (m4) for generating water supply forecasts. this model uses ai and other data-science technologies to reduce forecast errors, helping stakeholders make better decisions about water supply availability. . the model outputs water supply forecasts."
Intelligent Ticket Routing,Department of Agriculture,USDA,OCIO: Office of the Chief Information Officer,Mission-Enabling,None of the above.,"Help desk tickets are often sent to the wrong group and must be manually re-routed to the correct group, which can take time, resources, and may delay issue resolution. The Intelligent Ticket Routing system helps to send the ticket to the correct group, increasing customer satisfaction by reducing the number of times a customer is transferred or placed on hold, and decreasing the average handle time (AHT). In our specific use case, we reduce the time taken to route a ticket to the appropriate group, shortening the time required to resolve an issue.",The system outputs a prediction of the appropriate group for ticket management.,Operation and Maintenance,Neither,1/1/2022,1/1/2022,1/1/2022,Developed with both contracting and in-house resources.,ITSS Contract,No,No,No,No,Yes,"The data is a collection of scripts from Remedy reporting database, a structured database containing freeform text fields. More than 100,000 samples and 526 features comprise the data, including numerical, categorical, text, and date types. Data is not public.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Midrange + Moving to Sagemaker (ATO in progress),Less than 6 months,Other,Other,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"Help desk tickets are often sent to the wrong group and must be manually re-routed to the correct group, which can take time, resources, and may delay issue resolution. The Intelligent Ticket Routing system helps to send the ticket to the correct group, increasing customer satisfaction by reducing the number of times a customer is transferred or placed on hold, and decreasing the average handle time (AHT). In our specific use case, we reduce the time taken to route a ticket to the appropriate group, shortening the time required to resolve an issue. . The system outputs a prediction of the appropriate group for ticket management.","help desk tickets are often sent to the wrong group and must be manually re-routed to the correct group, which can take time, resources, and may delay issue resolution. the intelligent ticket routing system helps to send the ticket to the correct group, increasing customer satisfaction by reducing the number of times a customer is transferred or placed on hold, and decreasing the average handle time (aht). in our specific use case, we reduce the time taken to route a ticket to the appropriate group, shortening the time required to resolve an issue. . the system outputs a prediction of the appropriate group for ticket management."
Predictive Maintenance Impacts,Department of Agriculture,USDA,OCIO: Office of the Chief Information Officer,Mission-Enabling,None of the above.,"A natural language processing (NLP) model classifies whether infrastructure maintenance changes will or will not cause an incident at the Digital Infrastructure Services Center (DISC). Using this system, the business can improve the review process or address specific needs within groups. This will lead to process improvements, increased productivity, higher performance and job satisfaction, higher client satisfaction, and better achievement of key performance indicators (KPIs).",The model outputs a score between 0 to 1. Closer to 1 indicates higher likelihood of an incident created by the proposed change.,Operation and Maintenance,Neither,3/1/2020,3/1/2020,3/1/2020,Developed with both contracting and in-house resources.,ITSS Contracts,No,No,No,No,Yes,"The dataset is composed of Remedy data, containing inventory data of all of assets at DISC: hardware, software, incident tickets, and change tickets. Data contains numerical, categorical, text, and date data types.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Midrange (current) + Sagemaker (ATO in process now - future),Less than 6 months,Other,Other,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"A natural language processing (NLP) model classifies whether infrastructure maintenance changes will or will not cause an incident at the Digital Infrastructure Services Center (DISC). Using this system, the business can improve the review process or address specific needs within groups. This will lead to process improvements, increased productivity, higher performance and job satisfaction, higher client satisfaction, and better achievement of key performance indicators (KPIs). . The model outputs a score between 0 to 1. Closer to 1 indicates higher likelihood of an incident created by the proposed change.","a natural language processing (nlp) model classifies whether infrastructure maintenance changes will or will not cause an incident at the digital infrastructure services center (disc). using this system, the business can improve the review process or address specific needs within groups. this will lead to process improvements, increased productivity, higher performance and job satisfaction, higher client satisfaction, and better achievement of key performance indicators (kpis). . the model outputs a score between 0 to 1. closer to 1 indicates higher likelihood of an incident created by the proposed change."
Approximate String Matching (aka fuzzy matching) to Standardize Data,Department of Agriculture,USDA,APHIS: Animal and Plant Health Inspection Service,Mission-Enabling,None of the above.,"A model is used to replace typos in Plant Protection and Quarantine (PPQ) program data using a list of standardized producer and commodity names. This results in clean, standardized data through an automated workflow. Benefits include reducing labor hours compared to manual data cleaning, makes near-real-time reporting possible, and accurate data enables program managers to conduct efficient policy enforcement and program monitoring.",The model outputs corrected text data.,Operation and Maintenance,Neither,2/1/2023,2/23/2023,5/16/2023,Developed in-house.,Unknown,No,No,No,No,Yes,"The data used includes data tables from the Agriculture Risk Management (ARM) data system, Agricultural Commodity Import Requirements (ACIR), and the Participating Government Agencies (PGA) Message Set. The cleaned data is displayed in a dashboard for program monitoring. Quality control data are exported from each matching routine and used to validate results as needed.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"A model is used to replace typos in Plant Protection and Quarantine (PPQ) program data using a list of standardized producer and commodity names. This results in clean, standardized data through an automated workflow. Benefits include reducing labor hours compared to manual data cleaning, makes near-real-time reporting possible, and accurate data enables program managers to conduct efficient policy enforcement and program monitoring. . The model outputs corrected text data.","a model is used to replace typos in plant protection and quarantine (ppq) program data using a list of standardized producer and commodity names. this results in clean, standardized data through an automated workflow. benefits include reducing labor hours compared to manual data cleaning, makes near-real-time reporting possible, and accurate data enables program managers to conduct efficient policy enforcement and program monitoring. . the model outputs corrected text data."
Automated PDF Document Processing and Information Extraction,Department of Agriculture,USDA,APHIS: Animal and Plant Health Inspection Service,Mission-Enabling,None of the above.,This use case takes program and workforce related information stored in thousands of PDFs and converts the information into data tables that can be used for analytics and dashboards. This makes information that is difficult to find available in real-time to support decision making and saves large amounts of time compared to previous methods used.,The model outputs structured database tables.,Operation and Maintenance,Neither,10/15/2022,10/24/2022,1/15/2023,Developed in-house.,Unknown,No,No,No,No,Yes,"The data used is from Agriculture Vessel Inspection forms concerning flighted spongy moth complex, sent from Customs and Border Protection to Plant Protection and Quarantine.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",No,No – agency does not have access to source code.,Yes,Microsoft Power Automate,Less than 6 months,No,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,This use case takes program and workforce related information stored in thousands of PDFs and converts the information into data tables that can be used for analytics and dashboards. This makes information that is difficult to find available in real-time to support decision making and saves large amounts of time compared to previous methods used. . The model outputs structured database tables.,this use case takes program and workforce related information stored in thousands of pdfs and converts the information into data tables that can be used for analytics and dashboards. this makes information that is difficult to find available in real-time to support decision making and saves large amounts of time compared to previous methods used. . the model outputs structured database tables.
Census Propensity Scores via ML,Department of Agriculture,USDA,NASS: National Agricultural Statistics Service,Other,None of the above.,This model predicts how likely individuals or operations are to complete the Census of Agriculture. The predictions can help data collectors decide where they need to focus their efforts in order to get more complete census responses.,The model outputs a probability score (all values from and including 0 to 1).,Operation and Maintenance,Neither,10/1/2022,10/1/2022,11/1/2022,Developed in-house.,Unknown,No,No,No,No,Yes,"The data is collected from the 2017 Census of Agriculture, which includes number of farms by size and type, inventory and values for crops and livestock, producer characteristics, and much more.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,This model predicts how likely individuals or operations are to complete the Census of Agriculture. The predictions can help data collectors decide where they need to focus their efforts in order to get more complete census responses. . The model outputs a probability score (all values from and including 0 to 1).,this model predicts how likely individuals or operations are to complete the census of agriculture. the predictions can help data collectors decide where they need to focus their efforts in order to get more complete census responses. . the model outputs a probability score (all values from and including 0 to 1).
Conservation Effects Assessment Project,Department of Agriculture,USDA,NRCS: Natural Resources Conservation Service,Mission-Enabling,None of the above.,"The purpose of the use case is to predict the conservation effects of cropland practices in real time, with no technical skill required. Such models would allow field conservation planners to have real-time conservation effects on sediment and nutrients.",The model outputs predictions of sediment and nutrient change values based on conservation methods.,Acquisition and/or Development,Neither,11/1/2021,11/1/2021,Unknown,Developed in-house.,Unknown,No,Unknown,Unknown,No,Yes,"The data is collected from the Conservation Effects Assessment Project Survey cropland assessment, which measures trends in cropland conservation practices and their outcomes over time.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,Yes,R Studio,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"The purpose of the use case is to predict the conservation effects of cropland practices in real time, with no technical skill required. Such models would allow field conservation planners to have real-time conservation effects on sediment and nutrients. . The model outputs predictions of sediment and nutrient change values based on conservation methods.","the purpose of the use case is to predict the conservation effects of cropland practices in real time, with no technical skill required. such models would allow field conservation planners to have real-time conservation effects on sediment and nutrients. . the model outputs predictions of sediment and nutrient change values based on conservation methods."
Nutrition Education & Local Access Dashboard,Department of Agriculture,USDA,FNS: Food and Nutrition Service,Government Services (includes Benefits and Service Delivery),None of the above.,"The goal of this dashboard is to provide county-level information on nutrition education and local food access, alongside other metrics related to hunger and nutritional health. This interactive dashboard can provide specific details based on the properties of farm to school intensity and size, program activity intensity, ethnicity and race, fresh food access, school size, and program participation. These properties allow users to find similar states based on any of these characteristics, opening up opportunities for partnerships with states they may not have considered. Benefits include increasing stakeholder awareness and empowering more informed decision-making and collaboration.",The model outputs groups of similar counties/states based on the different combinations of properties available for states. ,Operation and Maintenance,Neither,11/9/2022,1/20/2023,6/23/2023,Developed with both contracting and in-house resources.,Unknown,No,Yes,No,No,Yes,"The data is composed of numerical and categorical data describing farm to school intensity and size, program activity intensity, ethnicity and race, fresh food access, school size, and program participation.","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The goal of this dashboard is to provide county-level information on nutrition education and local food access, alongside other metrics related to hunger and nutritional health. This interactive dashboard can provide specific details based on the properties of farm to school intensity and size, program activity intensity, ethnicity and race, fresh food access, school size, and program participation. These properties allow users to find similar states based on any of these characteristics, opening up opportunities for partnerships with states they may not have considered. Benefits include increasing stakeholder awareness and empowering more informed decision-making and collaboration. . The model outputs groups of similar counties/states based on the different combinations of properties available for states.","the goal of this dashboard is to provide county-level information on nutrition education and local food access, alongside other metrics related to hunger and nutritional health. this interactive dashboard can provide specific details based on the properties of farm to school intensity and size, program activity intensity, ethnicity and race, fresh food access, school size, and program participation. these properties allow users to find similar states based on any of these characteristics, opening up opportunities for partnerships with states they may not have considered. benefits include increasing stakeholder awareness and empowering more informed decision-making and collaboration. . the model outputs groups of similar counties/states based on the different combinations of properties available for states."
Survey Text Remarks Value Scoring,Department of Agriculture,USDA,NASS: National Agricultural Statistics Service,Mission-Enabling,None of the above.,The purpose of this use case is to analyze a large amout of text in survey responses and score all comments with a priority value. The highly scored blocks of text then get prioritized for review by a human and are responded to more quickly than if they were to be retained in a queue.  ,"The model outputs a value score on each snippit of text, highly scored snippits of text are placed at the front of the queue before lower scored blocks to capture text of value more quickly.",Operation and Maintenance,Neither,9/24/2021,12/27/2021,4/20/2022,Developed in-house.,Unknown,No,No,Yes,No,Yes,The training data is created from historically collected text and survey remarks. ,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,ELMO,Less than 6 months,No,Other,Unknown,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The purpose of this use case is to analyze a large amout of text in survey responses and score all comments with a priority value. The highly scored blocks of text then get prioritized for review by a human and are responded to more quickly than if they were to be retained in a queue. . The model outputs a value score on each snippit of text, highly scored snippits of text are placed at the front of the queue before lower scored blocks to capture text of value more quickly.","the purpose of this use case is to analyze a large amout of text in survey responses and score all comments with a priority value. the highly scored blocks of text then get prioritized for review by a human and are responded to more quickly than if they were to be retained in a queue. . the model outputs a value score on each snippit of text, highly scored snippits of text are placed at the front of the queue before lower scored blocks to capture text of value more quickly."
Survey Outlier Detection Model,Department of Agriculture,USDA,NASS: National Agricultural Statistics Service,Other,None of the above.,The purpose of this use case is to identify abnormal values to edit in surveys. This reduces manual labor and improves data quality.,The model outputs a recommendation of which values in a dataset should be changed.,Operation and Maintenance,Neither,5/1/2022,5/1/2022,5/27/2022,Developed in-house.,Unknown,No,No,No,No,Yes,"The data is collected from National Agricultural Statistics Service Survey Data, which is structured data containing categorical and numerical values.","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,Yes – source code is publicly available.,No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,The purpose of this use case is to identify abnormal values to edit in surveys. This reduces manual labor and improves data quality. . The model outputs a recommendation of which values in a dataset should be changed.,the purpose of this use case is to identify abnormal values to edit in surveys. this reduces manual labor and improves data quality. . the model outputs a recommendation of which values in a dataset should be changed.
Multilingual Translation of Recalls and Public Health Alerts,Department of Agriculture,USDA,FSIS: Food Safety and Inspection Service,Government Services (includes Benefits and Service Delivery),None of the above.,"The purpose of this system is to expand the multilingual outreach of food safety information like recalls and public health alerts. Benefits include cost savings on vendor translation services, faster messaging circulation, and increased number of available languages to the general public.",The model outputs multilingual translations created from the original english text.,Acquisition and/or Development,Neither,6/15/2023,6/15/2023,Unknown,Developed with both contracting and in-house resources.,Leveraging Azure's COTS product service,No,Unknown,Unknown,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"The purpose of this system is to expand the multilingual outreach of food safety information like recalls and public health alerts. Benefits include cost savings on vendor translation services, faster messaging circulation, and increased number of available languages to the general public. . The model outputs multilingual translations created from the original english text.","the purpose of this system is to expand the multilingual outreach of food safety information like recalls and public health alerts. benefits include cost savings on vendor translation services, faster messaging circulation, and increased number of available languages to the general public. . the model outputs multilingual translations created from the original english text."
Genomic Analyses of Pathogen Subtypes,Department of Agriculture,USDA,FSIS: Food Safety and Inspection Service,Science & Space,None of the above.,"The purpose of this use case is to use machine learning (ML) methods to group foodborne germs based on patterns in their genes, then connect this information with available health data to evaluate foodborne germ risk to public health. Expected benefits include improving our understanding of important foodborne germ genes, assessing key genes and new trends, and identifying and ranking germs that are important for public health.","The model outputs predictions of high risk foodborne germ subtypes, key genetic markers by importance, and emerging trends.",Acquisition and/or Development,Neither,8/1/2022,2/1/2023,Unknown,Developed in-house.,Unknown,No,Unknown,Unknown,No,Yes,"The data is composed of agency-owned data, including whole genome sequencing (WGS) data from Food Safety and Inspection Service (FSIS). Sampling programs are publicly available and hosted on the National Center for Biotechnology Information (NCBI) database.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"The purpose of this use case is to use machine learning (ML) methods to group foodborne germs based on patterns in their genes, then connect this information with available health data to evaluate foodborne germ risk to public health. Expected benefits include improving our understanding of important foodborne germ genes, assessing key genes and new trends, and identifying and ranking germs that are important for public health. . The model outputs predictions of high risk foodborne germ subtypes, key genetic markers by importance, and emerging trends.","the purpose of this use case is to use machine learning (ml) methods to group foodborne germs based on patterns in their genes, then connect this information with available health data to evaluate foodborne germ risk to public health. expected benefits include improving our understanding of important foodborne germ genes, assessing key genes and new trends, and identifying and ranking germs that are important for public health. . the model outputs predictions of high risk foodborne germ subtypes, key genetic markers by importance, and emerging trends."
Foodborne Illness Source Attribution,Department of Agriculture,USDA,FSIS: Food Safety and Inspection Service,Science & Space,None of the above.,"The Interagency Food Safety Analytics Collaboration (IFSAC) - a partnership between the Centers for Disease Control and Prevention (CDC), the U.S. Food and Drug Administration (FDA), and the Food Safety and Inspection Service (FSIS) - has used computer-based methods to predict the likely sources of foodborne illnesses in humans caused by various germs (e.g., Salmonella, Campylobacter). Expected benefits include improving our understanding of where these germs come from and how they spread, which can help in creating measures and policies to prevent or reduce illnesses and the overall impact of these diseases.","The model outputs predictions of likely sources of foodborne human illness cases, along with a confidence score of how probable it is that the illness came from the predicted source.",Acquisition and/or Development,Neither,8/2/2021,2/1/2023,Unknown,Developed in-house.,Unknown,No,Unknown,Unknown,No,Yes,"The data is composed of agency-owned data, including whole genome sequencing (WGS) data from Food Safety and Inspection Service (FSIS). Sampling programs are publicly available and hosted on the National Center for Biotechnology Information (NCBI) database.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"The Interagency Food Safety Analytics Collaboration (IFSAC) - a partnership between the Centers for Disease Control and Prevention (CDC), the U.S. Food and Drug Administration (FDA), and the Food Safety and Inspection Service (FSIS) - has used computer-based methods to predict the likely sources of foodborne illnesses in humans caused by various germs (e.g., Salmonella, Campylobacter). Expected benefits include improving our understanding of where these germs come from and how they spread, which can help in creating measures and policies to prevent or reduce illnesses and the overall impact of these diseases. . The model outputs predictions of likely sources of foodborne human illness cases, along with a confidence score of how probable it is that the illness came from the predicted source.","the interagency food safety analytics collaboration (ifsac) - a partnership between the centers for disease control and prevention (cdc), the u.s. food and drug administration (fda), and the food safety and inspection service (fsis) - has used computer-based methods to predict the likely sources of foodborne illnesses in humans caused by various germs (e.g., salmonella, campylobacter). expected benefits include improving our understanding of where these germs come from and how they spread, which can help in creating measures and policies to prevent or reduce illnesses and the overall impact of these diseases. . the model outputs predictions of likely sources of foodborne human illness cases, along with a confidence score of how probable it is that the illness came from the predicted source."
Public Comments Analysis,Department of Agriculture,USDA,APHIS: Animal and Plant Health Inspection Service; AMS: Agricultural Marketing Service,Mission-Enabling,None of the above.,The purpose of the model is to automate the analysis of comments from regulations.gov to help personnel in their review and response tasks. Benefits include a reduction in the number of labor hours needed for review and response.,The model outputs text analysis and categorization of the public comments. ,Acquisition and/or Development,Neither,11/1/2023,11/1/2023,Unknown,Developed in-house.,Unknown,No,Unknown,Unknown,No,Yes,Unknown,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,Yes,Marketing and Regulatory Programs Azure GSS; Marketplace for Data & Analytics (MDA),6-12 months,No,Other,No,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,The purpose of the model is to automate the analysis of comments from regulations.gov to help personnel in their review and response tasks. Benefits include a reduction in the number of labor hours needed for review and response. . The model outputs text analysis and categorization of the public comments.,the purpose of the model is to automate the analysis of comments from regulations.gov to help personnel in their review and response tasks. benefits include a reduction in the number of labor hours needed for review and response. . the model outputs text analysis and categorization of the public comments.
Rangeland Analysis Platform,Department of Agriculture,USDA,ARS: Agricultural Research Service,Energy & the Environment,None of the above.,"The Rangeland Analysis Platform (RAP) allows users to track changes in plant growth and coverage over time. By monitoring the condition of agricultural ecosystems and the impact of conservation efforts, it can guide conservation practices for wildlife habitats, carbon assessments, and tax assessments.",The system outputs estimated fractional plant cover and net primary productivity estimates.,Operation and Maintenance,Neither,4/1/2022,3/1/2023,1/1/2017,Developed in-house.,Unknown,No,Yes,No,No,Yes,"The data is collected from over 75,000 monitoring locations collected by Natural Resources Conservation Service (NRCS), Bureau of Land Management, and other agencies. These data were combined and stored in the Landscape Data Commons (www.landscapedatacommons.org).","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,"Google Earth Engine, Google Cloud",Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The Rangeland Analysis Platform (RAP) allows users to track changes in plant growth and coverage over time. By monitoring the condition of agricultural ecosystems and the impact of conservation efforts, it can guide conservation practices for wildlife habitats, carbon assessments, and tax assessments. . The system outputs estimated fractional plant cover and net primary productivity estimates.","the rangeland analysis platform (rap) allows users to track changes in plant growth and coverage over time. by monitoring the condition of agricultural ecosystems and the impact of conservation efforts, it can guide conservation practices for wildlife habitats, carbon assessments, and tax assessments. . the system outputs estimated fractional plant cover and net primary productivity estimates."
Predictive Cropland Data Layer,Department of Agriculture,USDA,NASS: National Agricultural Statistics Service,Other,None of the above.,The purpose of this system is to predict crop rotations. Benefits include improving data quality of area-based surveys.,The system outputs predictions of the types of crops in specific locations within the Conterminous United States (CONUS).,Operation and Maintenance,Neither,1/1/2021,1/1/2021,2/15/2021,Developed in-house.,Unknown,No,No,No,No,Yes,The data is derived from Cropland Data Layer (CDL) and Farm Service Agency (FSA) administrative data. CDL is produced using satellite images and extensive agricultural ground reference data.,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,6-12 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,The purpose of this system is to predict crop rotations. Benefits include improving data quality of area-based surveys. . The system outputs predictions of the types of crops in specific locations within the Conterminous United States (CONUS).,the purpose of this system is to predict crop rotations. benefits include improving data quality of area-based surveys. . the system outputs predictions of the types of crops in specific locations within the conterminous united states (conus).
Dam Inspection Report Document Processing,Department of Agriculture,USDA,NRCS: Natural Resources Conservation Service,Mission-Enabling,None of the above.,"The purpose of this AI is to pull out and organize data from thousands of dam inspection documents so that we can use Microsoft Power BI to understand the condition of thousands of USDA Watershed program dams. This allows us to identify the biggest issues and trends across our collection of over 2,100 dams in Oklahoma while reducing labor hours required to complete the task manually.","The model outputs text and checkbox responses, including dam metadata, inspection issue tracking (yes and no checkboxes), and further remarks on the issue or what has been/needs to be done on the dam.",Operation and Maintenance,Neither,5/1/2023,9/1/2023,12/1/2023,Developed in-house.,Unknown,No,No,No,No,Yes,"The data is made up of dam inspection forms collected by dam owners (ex. conservation districts, cities) or Natural Resources Conservation Service (NRCS) staff. The data was validated through manual checks and is composed of 30+ dam inspection reports with different handwriting. The data contains 120+ variables that are read from each two-page inspection report. The data types are date, text, table (text variable types in the table), and checkbox binary outputs (checked: yes or no). The data is not publicly available.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,Yes,AI Builder,Less than 6 months,Other,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"The purpose of this AI is to pull out and organize data from thousands of dam inspection documents so that we can use Microsoft Power BI to understand the condition of thousands of USDA Watershed program dams. This allows us to identify the biggest issues and trends across our collection of over 2,100 dams in Oklahoma while reducing labor hours required to complete the task manually. . The model outputs text and checkbox responses, including dam metadata, inspection issue tracking (yes and no checkboxes), and further remarks on the issue or what has been/needs to be done on the dam.","the purpose of this ai is to pull out and organize data from thousands of dam inspection documents so that we can use microsoft power bi to understand the condition of thousands of usda watershed program dams. this allows us to identify the biggest issues and trends across our collection of over 2,100 dams in oklahoma while reducing labor hours required to complete the task manually. . the model outputs text and checkbox responses, including dam metadata, inspection issue tracking (yes and no checkboxes), and further remarks on the issue or what has been/needs to be done on the dam."
GIS Invasive Tree Extraction for Field Level Users,Department of Agriculture,USDA,NRCS: Natural Resources Conservation Service,Energy & the Environment,None of the above.,"The puspose of the model is to estimate of the spread of invasive tree infestation, specifically Eastern redcedar. This helps to avoid poor or inaccurate estimates caused by time constraints and heavy workloads when manually collecting the data.",The model outputs polygons representing the extent of trees present in landscape.,Operation and Maintenance,Neither,7/15/2019,7/15/2019,9/30/2019,Developed in-house.,Unknown,No,No,No,No,Yes,"The data is created from the National Agriculture Imagery Program (NAIP), aerial images of land taken from aircrafts. The data is not limited to NAIP, but the 2020 NAIP has proven to be the most reliable source data.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The puspose of the model is to estimate of the spread of invasive tree infestation, specifically Eastern redcedar. This helps to avoid poor or inaccurate estimates caused by time constraints and heavy workloads when manually collecting the data. . The model outputs polygons representing the extent of trees present in landscape.","the puspose of the model is to estimate of the spread of invasive tree infestation, specifically eastern redcedar. this helps to avoid poor or inaccurate estimates caused by time constraints and heavy workloads when manually collecting the data. . the model outputs polygons representing the extent of trees present in landscape."
DISTRIB-II: Habitat Suitability of Eastern United States Tree,Department of Agriculture,USDA,FS: Forest Service,Science & Space,None of the above.,"The purpose and expected benefits of the Climate Change Atlas are to give forest resource managers, forest landowners, and the general public information on the current and potential future of habitats for various tree species in the eastern United States. This information can contribute to forest management decisions when considering how climate change may affect the trees currently present and how likely it is that other tree species not currently in an area might find new habitats under different climate change scenarios.","The system outputs predictions of how well a tree species can live in a certain habitat based on climate change scenarios. Maps, graphs, and reports are generated from the modeled geographic information systems (GIS) data.",Operation and Maintenance,Neither,4/10/1998,5/16/2016,9/12/2019,Developed in-house.,Unknown,No,Yes,No,No,Yes,The data is created from publicly available Forest Inventory and Analysis data.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The purpose and expected benefits of the Climate Change Atlas are to give forest resource managers, forest landowners, and the general public information on the current and potential future of habitats for various tree species in the eastern United States. This information can contribute to forest management decisions when considering how climate change may affect the trees currently present and how likely it is that other tree species not currently in an area might find new habitats under different climate change scenarios. . The system outputs predictions of how well a tree species can live in a certain habitat based on climate change scenarios. Maps, graphs, and reports are generated from the modeled geographic information systems (GIS) data.","the purpose and expected benefits of the climate change atlas are to give forest resource managers, forest landowners, and the general public information on the current and potential future of habitats for various tree species in the eastern united states. this information can contribute to forest management decisions when considering how climate change may affect the trees currently present and how likely it is that other tree species not currently in an area might find new habitats under different climate change scenarios. . the system outputs predictions of how well a tree species can live in a certain habitat based on climate change scenarios. maps, graphs, and reports are generated from the modeled geographic information systems (gis) data."
FSA FLP Chatbot,Department of Agriculture,USDA,FSA: Farm Service Agency,Mission-Enabling,None of the above.,The purpose of this use case is to solve the problem of searching Loan handbooks to provide better customer service. The expected benefit is to help employees provide better service. A second benefit that is being explored is providing Veteran specific answers to services.,The expected output is text answers to prompt questions. ,Acquisition and/or Development,Neither,12/14/2023,3/29/2024,Unknown,Developed in-house.,Unknown,No,Unknown,Unknown,No,Yes,"The data is collected from the 1 and 3 Farm Loan handbooks that are available on USDA intranet. The handbooks contain text, tables, and charts. There are also images such as screen shots in the handbooks.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,6-12 months,Other,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,The purpose of this use case is to solve the problem of searching Loan handbooks to provide better customer service. The expected benefit is to help employees provide better service. A second benefit that is being explored is providing Veteran specific answers to services. . The expected output is text answers to prompt questions.,the purpose of this use case is to solve the problem of searching loan handbooks to provide better customer service. the expected benefit is to help employees provide better service. a second benefit that is being explored is providing veteran specific answers to services. . the expected output is text answers to prompt questions.
ROE Document Recognition - RoeDR,Department of Agriculture,USDA,RMA: Risk Management Agency,Mission-Enabling,None of the above.,"The purpose of this model is to analyze documents from producers and Authorized Insurance Providers (AIPs), pick the appropriate page from the documents, read the signature date and producer signature name, convert the date and name to text, and load it into an application. This feature saves us time from having to input the data manually.  We can then use the data for reporting purposes.",The model outputs the producer signature and signature date within document as text.,Operation and Maintenance,Neither,5/15/2024,5/23/2024,6/12/2024,Developed in-house.,Unknown,No,No,No,No,Yes,The data is created from actuarial change forms and determined yield requests received from authorized insurance providers. The data contained 150 documents. ,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,No,Unknown,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The purpose of this model is to analyze documents from producers and Authorized Insurance Providers (AIPs), pick the appropriate page from the documents, read the signature date and producer signature name, convert the date and name to text, and load it into an application. This feature saves us time from having to input the data manually.  We can then use the data for reporting purposes. . The model outputs the producer signature and signature date within document as text.","the purpose of this model is to analyze documents from producers and authorized insurance providers (aips), pick the appropriate page from the documents, read the signature date and producer signature name, convert the date and name to text, and load it into an application. this feature saves us time from having to input the data manually. we can then use the data for reporting purposes. . the model outputs the producer signature and signature date within document as text."
IRIS,Department of Agriculture,USDA,APHIS: Animal and Plant Health Inspection Service,Science & Space,None of the above.,The purpose of this system is to make literature searches more effective for Biotechnology Regulatory Services. This increases work efficiency with our regulatory tasks.,The model outputs recommended literature list for scientists.,Operation and Maintenance,Neither,1/9/2023,2/5/2024,5/15/2024,Developed with contracting resources.,12639524P0188  ,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,The purpose of this system is to make literature searches more effective for Biotechnology Regulatory Services. This increases work efficiency with our regulatory tasks. . The model outputs recommended literature list for scientists.,the purpose of this system is to make literature searches more effective for biotechnology regulatory services. this increases work efficiency with our regulatory tasks. . the model outputs recommended literature list for scientists.
Ticket Resolution Categorization (Incident/Change),Department of Agriculture,USDA,OCIO: Office of the Chief Information Officer,Mission-Enabling,None of the above.,"The purpose of this model is to classify the resolution type and tier of all support desk tickets after they have been closed. This model allows the support team to spend more time identifying process inefficiencies and plan solutions rather than categorizing tickets. This will lead to process improvement, automation of repetitive tasks, increased productivity, and higher performance.",The model outputs the classification category of support ticket resolutions.,Operation and Maintenance,Neither,6/1/2023,6/1/2023,6/1/2023,Developed with both contracting and in-house resources.,ITSS Contracts,No,No,No,No,Yes,"The dataset is composed of Remedy data, containing inventory data of all of assets at DISC: hardware, software, incident tickets, and change tickets. Data contains numerical, categorical, text, and date data types.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Other,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"The purpose of this model is to classify the resolution type and tier of all support desk tickets after they have been closed. This model allows the support team to spend more time identifying process inefficiencies and plan solutions rather than categorizing tickets. This will lead to process improvement, automation of repetitive tasks, increased productivity, and higher performance. . The model outputs the classification category of support ticket resolutions.","the purpose of this model is to classify the resolution type and tier of all support desk tickets after they have been closed. this model allows the support team to spend more time identifying process inefficiencies and plan solutions rather than categorizing tickets. this will lead to process improvement, automation of repetitive tasks, increased productivity, and higher performance. . the model outputs the classification category of support ticket resolutions."
Ticket Templatization,Department of Agriculture,USDA,OCIO: Office of the Chief Information Officer,Mission-Enabling,None of the above.,"This model is a non-production exploratory model, meaning it does not make predictions but is used to explore trends within data to gain insights that can help in making data-driven decisions. It is designed to explore and analyze service and change requests submitted through the 105 general form or without templates. This model helps to identify subcategories within the larger dataset that could be candidates for standardization and automation, potentially leading to improved operational efficiency, cost savings, and customer satisfaction.",The model outputs trends within data to assist in decision making.,Acquisition and/or Development,Neither,1/1/2024,1/1/2024,Unknown,Developed with contracting resources.,ITSS Contracts,No,Unknown,Unknown,No,Yes,"The dataset is composed of Remedy data, containing inventory data of all of assets at DISC: hardware, software, incident tickets, and change tickets. Data contains numerical, categorical, text, and date data types.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"This model is a non-production exploratory model, meaning it does not make predictions but is used to explore trends within data to gain insights that can help in making data-driven decisions. It is designed to explore and analyze service and change requests submitted through the 105 general form or without templates. This model helps to identify subcategories within the larger dataset that could be candidates for standardization and automation, potentially leading to improved operational efficiency, cost savings, and customer satisfaction. . The model outputs trends within data to assist in decision making.","this model is a non-production exploratory model, meaning it does not make predictions but is used to explore trends within data to gain insights that can help in making data-driven decisions. it is designed to explore and analyze service and change requests submitted through the 105 general form or without templates. this model helps to identify subcategories within the larger dataset that could be candidates for standardization and automation, potentially leading to improved operational efficiency, cost savings, and customer satisfaction. . the model outputs trends within data to assist in decision making."
File Rename Automation,Department of Agriculture,USDA,FSA: Farm Service Agency,Mission-Enabling,None of the above.,The purpose of this tool is to rename thousands of documents converted from physical to digital records that were given a generic file name. This tool can grab text from page 1 of each document and apply a correct file rename instead of employees having to spend hours manually renaming documents.,The model outputs renamed files.,Implementation and Assessment,Neither,11/6/2023,11/6/2023,Unknown,Developed in-house.,Unknown,No,No,No,No,Yes,The data is created from agency records and were used to train Microsoft Power Automate to extract data from records and use in the file renaming process.,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,"Yes – agency has access to source code, but it is not public.",No,Unknown,6-12 months,Yes,No,Other,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,The purpose of this tool is to rename thousands of documents converted from physical to digital records that were given a generic file name. This tool can grab text from page 1 of each document and apply a correct file rename instead of employees having to spend hours manually renaming documents. . The model outputs renamed files.,the purpose of this tool is to rename thousands of documents converted from physical to digital records that were given a generic file name. this tool can grab text from page 1 of each document and apply a correct file rename instead of employees having to spend hours manually renaming documents. . the model outputs renamed files.
Dynamic Soils Hub ,Department of Agriculture,USDA,NRCS: Natural Resources Conservation Service,Science & Space,None of the above.,"The Dynamic Soils Hub (DS Hub) under the Natural Resources Conservation Service (NRCS) is a tool designed to help both government workers and the public understand and analyze soil information. The DS Hub links different soil and conservation databases, making it easier to evaluate the environmental benefits of conservation programs by accessing previously separate data and models. This enhances the USDA’s ability to study and report on how soil properties change with conservation efforts over time.",The system outputs the class of soil based on the supplied soil information.,Implementation and Assessment,Neither,11/11/2020,11/11/2020,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Yes,Unknown,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,"Yes – agency has access to source code, but it is not public.",Yes,Dynamic Soils Hub ,6-12 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The Dynamic Soils Hub (DS Hub) under the Natural Resources Conservation Service (NRCS) is a tool designed to help both government workers and the public understand and analyze soil information. The DS Hub links different soil and conservation databases, making it easier to evaluate the environmental benefits of conservation programs by accessing previously separate data and models. This enhances the USDA’s ability to study and report on how soil properties change with conservation efforts over time. . The system outputs the class of soil based on the supplied soil information.","the dynamic soils hub (ds hub) under the natural resources conservation service (nrcs) is a tool designed to help both government workers and the public understand and analyze soil information. the ds hub links different soil and conservation databases, making it easier to evaluate the environmental benefits of conservation programs by accessing previously separate data and models. this enhances the usda’s ability to study and report on how soil properties change with conservation efforts over time. . the system outputs the class of soil based on the supplied soil information."
Cover Crop Mapping,Department of Agriculture,USDA,RMA: Risk Management Agency,Law & Justice,None of the above.,"This project aims to measure the use of cover crops on farms in the U.S. Midwest by creating yearly maps showing cover crops in the fall and spring. These maps are made using satellite images and models of plant growth. This data helps the agency independently check and track how many farmers are using cover crops, reducing the need for on-site visits to determine if cover crops are present on a field.","The output is a state-level map of detected cover crops by year, classified by planting date (fall, spring).",Acquisition and/or Development,"Rights-Impacting
",9/2/2022,9/2/2022,Unknown,Developed with both contracting and in-house resources.,This work is supported under a Research Support Agreement with the University of Illinois (RMA22CPT0012747),No,Unknown,Unknown,No,Yes,The data is created from satellite imagery that uses crop insurance cover crop subsidy data as an independent data validation source. Data is not publicly available.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,Less than 6 months,No,No,Other,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Planned or in-progress.,"The current key risks for the Cover Crop detection are Type I and Type II errors.  For this particular use case, a False Positive (Type I error) would be where cover crop was detected by the model, when in fact none was planted by the producer and False Negatives (Type II errors) where cover crop was planted and emerged, but the model did not detect cover crop.  Model results will not be used without independent review before a potential compliance action takes place.",Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,No analysis specific to this use case has been conducted as the model is focused on the detection of a particular land use-land cover class.,Direct user testing,Yes,Rights-Impacting,0.6349206349206349,"This project aims to measure the use of cover crops on farms in the U.S. Midwest by creating yearly maps showing cover crops in the fall and spring. These maps are made using satellite images and models of plant growth. This data helps the agency independently check and track how many farmers are using cover crops, reducing the need for on-site visits to determine if cover crops are present on a field. . The output is a state-level map of detected cover crops by year, classified by planting date (fall, spring). . The current key risks for the Cover Crop detection are Type I and Type II errors.  For this particular use case, a False Positive (Type I error) would be where cover crop was detected by the model, when in fact none was planted by the producer and False Negatives (Type II errors) where cover crop was planted and emerged, but the model did not detect cover crop.  Model results will not be used without independent review before a potential compliance action takes place.","this project aims to measure the use of cover crops on farms in the u.s. midwest by creating yearly maps showing cover crops in the fall and spring. these maps are made using satellite images and models of plant growth. this data helps the agency independently check and track how many farmers are using cover crops, reducing the need for on-site visits to determine if cover crops are present on a field. . the output is a state-level map of detected cover crops by year, classified by planting date (fall, spring). . the current key risks for the cover crop detection are type i and type ii errors. for this particular use case, a false positive (type i error) would be where cover crop was detected by the model, when in fact none was planted by the producer and false negatives (type ii errors) where cover crop was planted and emerged, but the model did not detect cover crop. model results will not be used without independent review before a potential compliance action takes place."
Planting Date Detection,Department of Agriculture,USDA,RMA: Risk Management Agency,Law & Justice,None of the above.,"This project aims to find out the planting dates for corn, soybean, and winter wheat on farms in the U.S. Midwest. Maps containing planting dates are made using satellite images and models of plant growth. This data helps the agency independently check and confirm the planting dates reported by farmers to ensure the integrity of their programs, reducing the need for on-site visits to determine when a field was planted.","The output is an annual map of planting dates for corn, soybean, and winter wheat for crop years 2016-2023.",Acquisition and/or Development,"Rights-Impacting
",9/7/2022,9/1/2023,Unknown,Developed with both contracting and in-house resources.,This work was developed under a Research Support Agreement with the University of Illinois (RMA22CPT0012747),No,Unknown,Unknown,No,Yes,The data is created from satellite imagery that uses crop insurance unit-level planting dates at the crop/type/practice level as an independent data validation source. Data is not publicly available.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,Less than 6 months,Unknown,Unknown,Unknown,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Planned or in-progress.,"The current key risks for the planting date model results are Type I and Type II errors.  For this particular use case, a False Positive (Type I error) would be where the planting date detected by the model was outside the margin of error, when in fact the planting date occurred within the margin of error and False Negatives (Type II errors) where the planting date detected by the model was inside the margin of error, when in fact the field was planted either earlier or later.  Model results will not be used without independent review before a potential compliance action takes place. The model produces a crop-field specific observation (planting date) and is not tied to a demographic group.",Planned or in-progress,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,The model produces a crop-field specific observation (planting date) and is not tied to a demographic group.,Direct user testing,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Rights-Impacting,0.6031746031746031,"This project aims to find out the planting dates for corn, soybean, and winter wheat on farms in the U.S. Midwest. Maps containing planting dates are made using satellite images and models of plant growth. This data helps the agency independently check and confirm the planting dates reported by farmers to ensure the integrity of their programs, reducing the need for on-site visits to determine when a field was planted. . The output is an annual map of planting dates for corn, soybean, and winter wheat for crop years 2016-2023. . The current key risks for the planting date model results are Type I and Type II errors.  For this particular use case, a False Positive (Type I error) would be where the planting date detected by the model was outside the margin of error, when in fact the planting date occurred within the margin of error and False Negatives (Type II errors) where the planting date detected by the model was inside the margin of error, when in fact the field was planted either earlier or later.  Model results will not be used without independent review before a potential compliance action takes place. The model produces a crop-field specific observation (planting date) and is not tied to a demographic group.","this project aims to find out the planting dates for corn, soybean, and winter wheat on farms in the u.s. midwest. maps containing planting dates are made using satellite images and models of plant growth. this data helps the agency independently check and confirm the planting dates reported by farmers to ensure the integrity of their programs, reducing the need for on-site visits to determine when a field was planted. . the output is an annual map of planting dates for corn, soybean, and winter wheat for crop years 2016-2023. . the current key risks for the planting date model results are type i and type ii errors. for this particular use case, a false positive (type i error) would be where the planting date detected by the model was outside the margin of error, when in fact the planting date occurred within the margin of error and false negatives (type ii errors) where the planting date detected by the model was inside the margin of error, when in fact the field was planted either earlier or later. model results will not be used without independent review before a potential compliance action takes place. the model produces a crop-field specific observation (planting date) and is not tied to a demographic group."
Acreage and Crop Type Validation,Department of Agriculture,USDA,RMA: Risk Management Agency,Law & Justice,None of the above.,"This project uses satellite images and plant growth models to check if the reported sizes of farm fields and the types of crops grown match what is actually seen on the ground in the U.S. Midwest. This data helps the agency independently verify the accuracy of reported field sizes and crop types, supporting efforts to ensure the integrity of their programs.","The output is a validation of reported acreage and validation of reported crop type for corn, soybean, and winter wheat on farm fields.",Acquisition and/or Development,"Rights-Impacting
",9/1/2022,9/1/2022,Unknown,Developed with both contracting and in-house resources.,This work was developed under a Research Support Agreement with the University of Illinois (RMA22CPT0012747),No,Unknown,Unknown,No,Yes,"The data is created from satellite imagery that uses planted acreage and crop type (corn, soybean, and winter wheat) features as an independent data validation source. Data is not publicly available.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,Less than 6 months,Unknown,Unknown,Unknown,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Planned or in-progress.,"The current key risks for the planting date model results are Type I and Type II errors.  For this particular use case, a False Positive (Type I error) would be where the acreage detected by the model was outside the margin of error, when in fact the acreage reported was within the margin of error and False Negatives (Type II errors) where the acreage detected by the model was inside the margin of error, when in fact the acreage was outside the margin of error.  Similar logic would be applied to the crop type detection. Model results will not be used without independent review before a potential compliance action takes place. The model produces a field specific observation and is not tied to a demographic group.",Planned or in-progress,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,No analysis specific to this use case has been conducted as the model is focused on the detection of a particular land use-land cover class and agronomic variable.,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Rights-Impacting,0.6031746031746031,"This project uses satellite images and plant growth models to check if the reported sizes of farm fields and the types of crops grown match what is actually seen on the ground in the U.S. Midwest. This data helps the agency independently verify the accuracy of reported field sizes and crop types, supporting efforts to ensure the integrity of their programs. . The output is a validation of reported acreage and validation of reported crop type for corn, soybean, and winter wheat on farm fields. . The current key risks for the planting date model results are Type I and Type II errors.  For this particular use case, a False Positive (Type I error) would be where the acreage detected by the model was outside the margin of error, when in fact the acreage reported was within the margin of error and False Negatives (Type II errors) where the acreage detected by the model was inside the margin of error, when in fact the acreage was outside the margin of error.  Similar logic would be applied to the crop type detection. Model results will not be used without independent review before a potential compliance action takes place. The model produces a field specific observation and is not tied to a demographic group.","this project uses satellite images and plant growth models to check if the reported sizes of farm fields and the types of crops grown match what is actually seen on the ground in the u.s. midwest. this data helps the agency independently verify the accuracy of reported field sizes and crop types, supporting efforts to ensure the integrity of their programs. . the output is a validation of reported acreage and validation of reported crop type for corn, soybean, and winter wheat on farm fields. . the current key risks for the planting date model results are type i and type ii errors. for this particular use case, a false positive (type i error) would be where the acreage detected by the model was outside the margin of error, when in fact the acreage reported was within the margin of error and false negatives (type ii errors) where the acreage detected by the model was inside the margin of error, when in fact the acreage was outside the margin of error. similar logic would be applied to the crop type detection. model results will not be used without independent review before a potential compliance action takes place. the model produces a field specific observation and is not tied to a demographic group."
U.S. Poultry Operations and Populations Dataset,Department of Agriculture,USDA,APHIS: Animal and Plant Health Inspection Service,Emergency Management,None of the above.,"The purpose of this case is to develop a dataset that addresses the problem of not having complete information about where poultry farms are located and how many birds they have. Filling this gap provides detailed data on poultry farm locations and populations, which is essential for planning animal health emergencies and predicting the spread of diseases.",Output is a national-level dataset of domestic poultry operations and estimated populations. ,Operation and Maintenance,Neither,5/15/2016,5/15/2016,6/3/2019,Developed in-house.,Unknown,No,No,No,No,Yes,The data consists of a subset of farm locations that were validated using manually digitized poultry operations in 41 counties. Data is not publicly available.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,Other,Unknown,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The purpose of this case is to develop a dataset that addresses the problem of not having complete information about where poultry farms are located and how many birds they have. Filling this gap provides detailed data on poultry farm locations and populations, which is essential for planning animal health emergencies and predicting the spread of diseases. . Output is a national-level dataset of domestic poultry operations and estimated populations.","the purpose of this case is to develop a dataset that addresses the problem of not having complete information about where poultry farms are located and how many birds they have. filling this gap provides detailed data on poultry farm locations and populations, which is essential for planning animal health emergencies and predicting the spread of diseases. . output is a national-level dataset of domestic poultry operations and estimated populations."
Equine Operations and Populations Dataset for the U.S.,Department of Agriculture,USDA,APHIS: Animal and Plant Health Inspection Service,Emergency Management,None of the above.,"The purpose of this case is to develop a dataset that addresses the problem of not having complete information about where horse farms are located and how many horses they have. Filling this gap provides detailed data on horse farm locations and populations, which is essential for planning emergencies and predicting the spread of diseases.",Output is a national-level dataset of domestic horse operations and estimated populations.,Acquisition and/or Development,Neither,1/2/2023,9/1/2024,Unknown,Developed with both contracting and in-house resources.,"NADPRP / Farm Bill funds , cooperative agreement with Colorado State University",No,Unknown,Unknown,No,Yes,"The data is created from National Agriculture Imagery Program (NAIP) images and the National Agricultural Statistics Service Census of Agriculture, which includes number of farms by size and type, inventory and values for crops and livestock, producer characteristics, and much more. Data is not publicly available.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,Less than 6 months,No,Other,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"The purpose of this case is to develop a dataset that addresses the problem of not having complete information about where horse farms are located and how many horses they have. Filling this gap provides detailed data on horse farm locations and populations, which is essential for planning emergencies and predicting the spread of diseases. . Output is a national-level dataset of domestic horse operations and estimated populations.","the purpose of this case is to develop a dataset that addresses the problem of not having complete information about where horse farms are located and how many horses they have. filling this gap provides detailed data on horse farm locations and populations, which is essential for planning emergencies and predicting the spread of diseases. . output is a national-level dataset of domestic horse operations and estimated populations."
County-level remotely-sensed corn and soybean yield estimation,Department of Agriculture,USDA,NASS: National Agricultural Statistics Service,Other,None of the above.,The purpose of this tool is to estimate yearly corn and soybean yields for each county using satellite images. More details can be found in the paper titled “An assessment of pre- and within-season remotely sensed variables for forecasting corn and soybean yields in the United States” (https://doi.org/10.1016/j.rse.2013.10.027). Benefits of providing county-level crop yield statistics allow stakeholders to make more informed planning and decisions.,The model outputs county-level crop yield estimates for corn and soybeans in the amount of bushels per acre.,Operation and Maintenance,Neither,1/1/2007,1/1/2009,1/1/2014,Developed in-house.,Unknown,No,No,No,No,Yes,Historical National Agricultural Statistics Service county-level crop yields are used as the dataset. The foundation of the data is images from NASA Moderate Resolution Imaging Spectroradiometer (MODIS) satellites.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",No,"Yes – agency has access to source code, but it is not public.",No,Unknown,More than 12 months,Yes,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,The purpose of this tool is to estimate yearly corn and soybean yields for each county using satellite images. More details can be found in the paper titled “An assessment of pre- and within-season remotely sensed variables for forecasting corn and soybean yields in the United States” (https://doi.org/10.1016/j.rse.2013.10.027). Benefits of providing county-level crop yield statistics allow stakeholders to make more informed planning and decisions. . The model outputs county-level crop yield estimates for corn and soybeans in the amount of bushels per acre.,the purpose of this tool is to estimate yearly corn and soybean yields for each county using satellite images. more details can be found in the paper titled “an assessment of pre- and within-season remotely sensed variables for forecasting corn and soybean yields in the united states” (https://doi.org/10.1016/j.rse.2013.10.027). benefits of providing county-level crop yield statistics allow stakeholders to make more informed planning and decisions. . the model outputs county-level crop yield estimates for corn and soybeans in the amount of bushels per acre.
XyloTron/XyloPhone Wood Identification System,Department of Agriculture,USDA,FS: Forest Service,Law & Justice,None of the above.,The purpose of these tools is to identify different types of wood based on their cross-section. These tools will help industries follow laws and support law enforcement in meeting national (e.g. Lacey Act) and international (e.g. CITES) regulations.,The tools will output a prediction of the type wood.,Implementation and Assessment,Neither,1/1/2016,1/1/2016,Unknown,Developed in-house.,Unknown,No,No,No,No,Yes,"The data contains images of the cross-section of polished wood surfaces. As of July 2024, there are 139,639 images of 10,888 wood specimens.","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,No,Unknown,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,The purpose of these tools is to identify different types of wood based on their cross-section. These tools will help industries follow laws and support law enforcement in meeting national (e.g. Lacey Act) and international (e.g. CITES) regulations. . The tools will output a prediction of the type wood.,the purpose of these tools is to identify different types of wood based on their cross-section. these tools will help industries follow laws and support law enforcement in meeting national (e.g. lacey act) and international (e.g. cites) regulations. . the tools will output a prediction of the type wood.
Incident Invoice Document Understanding,Department of Agriculture,USDA,FS: Forest Service,Mission-Enabling,None of the above.,The purpose of this tool is to analyze incident invoices and return the values that need to be entered into a database. This new approach leads to faster invoice processing and reduces data entry mistakes for more accurate data.,The tool outputs an Excel document containing required values identified from incident invoices.,Acquisition and/or Development,Neither,1/21/2021,1/26/2021,Unknown,Developed with contracting resources.,12314419F0612 and another USDA DISC contract that I don't have access to,No,Unknown,Unknown,No,Yes,"The dataset is created with PDF files of OF-286 documents, which are paid invoices of varying quality and structure.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,Yes,UiPath,More than 12 months,Other,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,The purpose of this tool is to analyze incident invoices and return the values that need to be entered into a database. This new approach leads to faster invoice processing and reduces data entry mistakes for more accurate data. . The tool outputs an Excel document containing required values identified from incident invoices.,the purpose of this tool is to analyze incident invoices and return the values that need to be entered into a database. this new approach leads to faster invoice processing and reduces data entry mistakes for more accurate data. . the tool outputs an excel document containing required values identified from incident invoices.
Forest disease detection and screening,Department of Agriculture,USDA,FS: Forest Service,Science & Space,None of the above.,"The purpose of this project is to improve tree disease diagnosis and screening, thereby facilitating ongoing efforts within and outside the Forest Service to manage diseases of forest trees. ","The model will output a prediction indicating whether a tree is diseased or not, and if a tree is resistant or susceptible to a disease.",Acquisition and/or Development,Neither,8/3/2020,8/3/2020,Unknown,Developed in-house.,Unknown,No,Unknown,Unknown,No,Yes,"Data has and is currently being collected using commercially available and custom sensors and images from trees in natural forest landscape, trees in planted settings, and trees that are being screened for disease resistance in controlled environments. Data is collected for both model calibration and validation. ","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,More than 12 months,No,Other,Unknown,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"The purpose of this project is to improve tree disease diagnosis and screening, thereby facilitating ongoing efforts within and outside the Forest Service to manage diseases of forest trees. . The model will output a prediction indicating whether a tree is diseased or not, and if a tree is resistant or susceptible to a disease.","the purpose of this project is to improve tree disease diagnosis and screening, thereby facilitating ongoing efforts within and outside the forest service to manage diseases of forest trees. . the model will output a prediction indicating whether a tree is diseased or not, and if a tree is resistant or susceptible to a disease."
IPWG Application Survey Analysis,Department of Agriculture,USDA,FS: Forest Service,Mission-Enabling,None of the above.,"The purpose of this tool is to analyze over 5,000 responses from an internal employee survey on IT applications, then give a summary of the employee feedback regarding each IT application. This decreases the time required to go through each response manually, helping the team make informed investment decisions more quickly.","The model outputs a text summarization for each IT application in the survey data, and potentially includes text summaries of responses and sentiment analysis. The project will also produce a dashboard that allows users to see similar attributes at the agency, office, application, and individual response level.",Acquisition and/or Development,Neither,10/3/2024,10/3/2024,Unknown,Developed in-house.,Unknown,No,Unknown,Unknown,No,Yes,"The data has been collected from a Microsoft Forms survey sent to Forest Service employees and downloaded into a CSV (comma-separated values) file. The survey responses are not anonymous and contain personnel information such as region, office, and job title. There are approximately 5000 responses about hundreds of applications. The data is comprised of approximately 54 attributes, with a mix of numerical ratings, categories, and free text comments. The data is restricted and encrypted due to personnel information. ","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,Yes,"Excel, Python, Google Cloud Platform ",Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"The purpose of this tool is to analyze over 5,000 responses from an internal employee survey on IT applications, then give a summary of the employee feedback regarding each IT application. This decreases the time required to go through each response manually, helping the team make informed investment decisions more quickly. . The model outputs a text summarization for each IT application in the survey data, and potentially includes text summaries of responses and sentiment analysis. The project will also produce a dashboard that allows users to see similar attributes at the agency, office, application, and individual response level.","the purpose of this tool is to analyze over 5,000 responses from an internal employee survey on it applications, then give a summary of the employee feedback regarding each it application. this decreases the time required to go through each response manually, helping the team make informed investment decisions more quickly. . the model outputs a text summarization for each it application in the survey data, and potentially includes text summaries of responses and sentiment analysis. the project will also produce a dashboard that allows users to see similar attributes at the agency, office, application, and individual response level."
Fire Resilent Landscapes,Department of Agriculture,USDA,FS: Forest Service,Science & Space,None of the above.,The goal of this tool is to quantify the cost of forest treatments. Benefits include providing the ability to accurately map treatment costs for users to make more informed decisions.,The tool outputs predictions in the form of raster surfaces/maps.,Implementation and Assessment,Neither,8/1/2021,8/1/2021,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,No agency-owned data used in this project.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,Yes – source code is publicly available.,No,Unknown,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,The goal of this tool is to quantify the cost of forest treatments. Benefits include providing the ability to accurately map treatment costs for users to make more informed decisions. . The tool outputs predictions in the form of raster surfaces/maps.,the goal of this tool is to quantify the cost of forest treatments. benefits include providing the ability to accurately map treatment costs for users to make more informed decisions. . the tool outputs predictions in the form of raster surfaces/maps.
PC Rasterize,Department of Agriculture,USDA,FS: Forest Service,Science & Space,None of the above.,The purpose of this tool is to be able to process point cloud data more efficiently. This will reduce costs associated with processing point cloud data.,The tool outputs point clouds and raster surfaces/maps.,Implementation and Assessment,Neither,8/1/2024,8/1/2024,Unknown,Developed in-house.,Unknown,No,No,No,No,Yes,No agency-owned data used in this project.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,Yes – source code is publicly available.,No,Unknown,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,The purpose of this tool is to be able to process point cloud data more efficiently. This will reduce costs associated with processing point cloud data. . The tool outputs point clouds and raster surfaces/maps.,the purpose of this tool is to be able to process point cloud data more efficiently. this will reduce costs associated with processing point cloud data. . the tool outputs point clouds and raster surfaces/maps.
Spread and Balance Sample Design,Department of Agriculture,USDA,FS: Forest Service,Science & Space,None of the above.,The purpose of this tool is to produce samples that are well spread and balanced. This sample design will reduce the quantity of samples needed and further reduce costs associated with collecting field data.,The tool outputs data frames and geospatial-data-frames.,Implementation and Assessment,Neither,5/1/2024,5/1/2024,Unknown,Developed in-house.,Unknown,No,Yes,No,No,Yes,No agency-owned data used in this project.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,Yes – source code is publicly available.,No,Unknown,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,The purpose of this tool is to produce samples that are well spread and balanced. This sample design will reduce the quantity of samples needed and further reduce costs associated with collecting field data. . The tool outputs data frames and geospatial-data-frames.,the purpose of this tool is to produce samples that are well spread and balanced. this sample design will reduce the quantity of samples needed and further reduce costs associated with collecting field data. . the tool outputs data frames and geospatial-data-frames.
"Regression, Classification, Clustering with Hilbert Curves",Department of Agriculture,USDA,FS: Forest Service,Science & Space,None of the above.,"The purpose of this tool is to perform better regression, classification, and clustering. This will create a new and better ways to produce various estimates, reducing cost and error.  ",The tools will output Data frame and Raster Surfaces/maps. ,Implementation and Assessment,Neither,6/1/2024,6/1/2024,Unknown,Developed in-house.,Unknown,No,No,No,No,Yes,No agency-owned data used in this project.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,Yes – source code is publicly available.,No,Unknown,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The purpose of this tool is to perform better regression, classification, and clustering. This will create a new and better ways to produce various estimates, reducing cost and error. . The tools will output Data frame and Raster Surfaces/maps.","the purpose of this tool is to perform better regression, classification, and clustering. this will create a new and better ways to produce various estimates, reducing cost and error. . the tools will output data frame and raster surfaces/maps."
"The Big Data, Mapping, and Analytics Platform (BIGMAP) Project",Department of Agriculture,USDA,FS: Forest Service,Science & Space,None of the above.,The purpose of this project is to use geospatial predictions from Forest Inventory and Analysis samples to make more accurate estimates of different forest characteristics. Greater precision in estimates leads to more informed decisions about the forest resources in the US.,The model outputs predictions in the form of raster maps.,Operation and Maintenance,Neither,1/1/2019,1/7/2019,7/5/2021,Developed with both contracting and in-house resources.,Esri FS GeoPlatform,No,Yes,Yes,No,Yes,"Data from the Forest Inventory and Analysis program and the Forest Inventory and Analysis Database were used to finetune and validate the models used. While the data are publicly available, the actual coordinates of the plot locations used are not, in order to protect the confidentiality of the land owner.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Esri Managed Cloud Services; Amazon Web Services,More than 12 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,The purpose of this project is to use geospatial predictions from Forest Inventory and Analysis samples to make more accurate estimates of different forest characteristics. Greater precision in estimates leads to more informed decisions about the forest resources in the US. . The model outputs predictions in the form of raster maps.,the purpose of this project is to use geospatial predictions from forest inventory and analysis samples to make more accurate estimates of different forest characteristics. greater precision in estimates leads to more informed decisions about the forest resources in the us. . the model outputs predictions in the form of raster maps.
BirdNET to detect bird vocalizations for research and species monitoring,Department of Agriculture,USDA,FS: Forest Service,Science & Space,None of the above.,"BirdNET quickly scans thousands of hours of forest audio recordings to detect bird calls from species that are important for forest monitoring, like spotted owls, black-backed woodpeckers, and willow flycatchers. This decreases the time and cost associated with manually listening to recordings to identify bird calls.","The model outputs text files of bird calls, which include the bird species and time that the call was recorded.",Operation and Maintenance,Neither,6/1/2021,6/1/2021,2/13/2023,Developed in-house.,Unknown,No,No,No,No,Yes,BirdNET was developed using data from the MacAulay Library of Natural Sounds at Cornell University. Forest Service did not provide any contract or data. BirdNET is publicly available.,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,Yes – source code is publicly available.,No,Unknown,More than 12 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"BirdNET quickly scans thousands of hours of forest audio recordings to detect bird calls from species that are important for forest monitoring, like spotted owls, black-backed woodpeckers, and willow flycatchers. This decreases the time and cost associated with manually listening to recordings to identify bird calls. . The model outputs text files of bird calls, which include the bird species and time that the call was recorded.","birdnet quickly scans thousands of hours of forest audio recordings to detect bird calls from species that are important for forest monitoring, like spotted owls, black-backed woodpeckers, and willow flycatchers. this decreases the time and cost associated with manually listening to recordings to identify bird calls. . the model outputs text files of bird calls, which include the bird species and time that the call was recorded."
Hurricane impact descriptions,Department of Agriculture,USDA,FS: Forest Service,Mission-Enabling,None of the above.,"The purpose of the AI model is to convert a table of data about a tropical cyclone's path and estimated impact on forests into a clear and understandable story. This is part of a rapid assessment given to stakeholders after a cyclone hits, so it needs to be done fast. We are creating a tool to automate this process and the AI helps to make better quality reports.",The model outputs a few paragraphs of easy-to-read text that explains the effects of a cyclone. ,Acquisition and/or Development,Neither,7/1/2024,7/1/2024,Unknown,Developed in-house.,Unknown,No,Unknown,Unknown,No,Yes,"No agency owned data is used to train, fine-tune, or evaluate the performance. ","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,Less than 6 months,Unknown,Other,Other,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"The purpose of the AI model is to convert a table of data about a tropical cyclone's path and estimated impact on forests into a clear and understandable story. This is part of a rapid assessment given to stakeholders after a cyclone hits, so it needs to be done fast. We are creating a tool to automate this process and the AI helps to make better quality reports. . The model outputs a few paragraphs of easy-to-read text that explains the effects of a cyclone.","the purpose of the ai model is to convert a table of data about a tropical cyclone's path and estimated impact on forests into a clear and understandable story. this is part of a rapid assessment given to stakeholders after a cyclone hits, so it needs to be done fast. we are creating a tool to automate this process and the ai helps to make better quality reports. . the model outputs a few paragraphs of easy-to-read text that explains the effects of a cyclone."
FuelCast,Department of Agriculture,USDA,FS: Forest Service,Emergency Management,None of the above.,"This project predicts future fuel conditions and gives early warnings to help plan fuel management. The benefits include better preparation of the US firefighting teams for potential increases in large wildfires. This system also reduces the workload for fire behavior analysts because it provides fuel estimates, so they don't have to spend as much time figuring out fire behavior patterns through trial and error.",The model outputs predictions of the future quantity of wood and plants that could be present and contribute to wildfires. ,Operation and Maintenance,Neither,10/17/2019,10/17/2020,10/18/2020,Developed with both contracting and in-house resources.,I have no idea what PIID is,No,Yes,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,More than 12 months,Unknown,Other,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"This project predicts future fuel conditions and gives early warnings to help plan fuel management. The benefits include better preparation of the US firefighting teams for potential increases in large wildfires. This system also reduces the workload for fire behavior analysts because it provides fuel estimates, so they don't have to spend as much time figuring out fire behavior patterns through trial and error. . The model outputs predictions of the future quantity of wood and plants that could be present and contribute to wildfires.","this project predicts future fuel conditions and gives early warnings to help plan fuel management. the benefits include better preparation of the us firefighting teams for potential increases in large wildfires. this system also reduces the workload for fire behavior analysts because it provides fuel estimates, so they don't have to spend as much time figuring out fire behavior patterns through trial and error. . the model outputs predictions of the future quantity of wood and plants that could be present and contribute to wildfires."
Wildlife deterrent system,Department of Agriculture,USDA,FS: Forest Service; Forest Service Research & Development,Science & Space,None of the above.,"The purpose of the AI device is to keep coyotes out of a fenced area by blocking their entry through a gap in the fence, while still allowing other wildlife to pass through. The benefits include making ecological research possible that couldn't be done otherwise, and saving time by reducing the need to watch camera footage and manually control the fence.","The model performs video object detection of a coyotes, and arms an electrical barrier to prevent passage of the coyote.",Implementation and Assessment,Neither,8/26/2021,8/26/2021,Unknown,Developed with contracting resources.,GS-35F-0207T,No,No,No,No,Yes,The data is composed of a few thousand photographs of coyotes collected from trail cameras. Images were used by the contractor to train the system to detect coyotes.,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,No,Unknown,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The purpose of the AI device is to keep coyotes out of a fenced area by blocking their entry through a gap in the fence, while still allowing other wildlife to pass through. The benefits include making ecological research possible that couldn't be done otherwise, and saving time by reducing the need to watch camera footage and manually control the fence. . The model performs video object detection of a coyotes, and arms an electrical barrier to prevent passage of the coyote.","the purpose of the ai device is to keep coyotes out of a fenced area by blocking their entry through a gap in the fence, while still allowing other wildlife to pass through. the benefits include making ecological research possible that couldn't be done otherwise, and saving time by reducing the need to watch camera footage and manually control the fence. . the model performs video object detection of a coyotes, and arms an electrical barrier to prevent passage of the coyote."
The Lost Meadows Model,Department of Agriculture,USDA,FS: Forest Service,Energy & the Environment,None of the above.,"The purpose of this model is to find out where meadows used to be and how often they appeared in order to understand their original state and their potential for restoration. The discovery of these areas increases the potential for meadow restoration, which can benefit biodiversity, wildfire management, carbon storage, and water storage.","The model outputs predictions of areas with meadow-like environmental conditions. The predicted areas include a mixture of existing but undocumented meadows, non-meadow lands that may have once been meadows, and meadow-like areas that may never have been a meadow. ",Operation and Maintenance,Neither,10/10/2022,10/10/2022,7/1/2023,Developed in-house.,Unknown,No,Yes,No,No,Yes,"The data is a collection of 11,127 publicly available meadow polygons from the Sierra Nevada MultiSource Meadow Polygons Compilation, Version 2 (UC Davis and USDA Forest Service 2017). ","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,No,Other,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The purpose of this model is to find out where meadows used to be and how often they appeared in order to understand their original state and their potential for restoration. The discovery of these areas increases the potential for meadow restoration, which can benefit biodiversity, wildfire management, carbon storage, and water storage. . The model outputs predictions of areas with meadow-like environmental conditions. The predicted areas include a mixture of existing but undocumented meadows, non-meadow lands that may have once been meadows, and meadow-like areas that may never have been a meadow.","the purpose of this model is to find out where meadows used to be and how often they appeared in order to understand their original state and their potential for restoration. the discovery of these areas increases the potential for meadow restoration, which can benefit biodiversity, wildfire management, carbon storage, and water storage. . the model outputs predictions of areas with meadow-like environmental conditions. the predicted areas include a mixture of existing but undocumented meadows, non-meadow lands that may have once been meadows, and meadow-like areas that may never have been a meadow."
Markov random fields,Department of Agriculture,USDA,FS: Forest Service,Science & Space,None of the above.,The purpose of this tool is to improve the accuracy of estimates in machine learning models. The benefits include helping stakeholders make more informed and effective decisions for managing mixed forests.,"The model outputs predicted counts of tree species in a location, and the degree of competition between different tree species in the same location.",Acquisition and/or Development,Neither,10/1/2022,10/1/2022,Unknown,Developed in-house.,Unknown,No,Unknown,Unknown,No,Yes,The data is comprised of ForestGEO tree count data from the Republic of Palau.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,No,Unknown,More than 12 months,Yes,Yes,No,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"The purpose of this tool is to improve the accuracy of estimates in machine learning models. The benefits include helping stakeholders make more informed and effective decisions for managing mixed forests. . The model outputs predicted counts of tree species in a location, and the degree of competition between different tree species in the same location.","the purpose of this tool is to improve the accuracy of estimates in machine learning models. the benefits include helping stakeholders make more informed and effective decisions for managing mixed forests. . the model outputs predicted counts of tree species in a location, and the degree of competition between different tree species in the same location."
AI for regional forest mapping and monitoring,Department of Agriculture,USDA,FS: Forest Service,Energy & the Environment,None of the above.,The purpose of this model is to use existing satellite images and forest survey data from the USDA Forest Service to create detailed maps of forest structures. This information will help land managers be more effective and efficient with their planning.,"The model outputs GeoTiffs (raster maps of forest attributes, such as tree density and tree species data).",Operation and Maintenance,Neither,1/1/2000,1/1/2000,10/1/2002,Developed with contracting resources.,22-JV-11261959-067 (with Oregon State University; other past JVAs as well),No,Yes,No,No,Yes,"Forest Inventory and Analysis data collected by the USDA Forest Service (Research & Development). It contains 200,000 individual plot measurements, and hundreds of Geospatial Information System (GIS) features (climate data, satellite images, topography, etc.) stored as 30-meter raster maps (GeoTiffs).","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,Yes – source code is publicly available.,No,Unknown,Less than 6 months,Other,Other,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"The purpose of this model is to use existing satellite images and forest survey data from the USDA Forest Service to create detailed maps of forest structures. This information will help land managers be more effective and efficient with their planning. . The model outputs GeoTiffs (raster maps of forest attributes, such as tree density and tree species data).","the purpose of this model is to use existing satellite images and forest survey data from the usda forest service to create detailed maps of forest structures. this information will help land managers be more effective and efficient with their planning. . the model outputs geotiffs (raster maps of forest attributes, such as tree density and tree species data)."
Esri ArcGIS Pro Deep Learning Modules,Department of Agriculture,USDA,FS: Forest Service,Mission-Enabling,None of the above.,"The purpose of this tool is to enhance scientific modeling and analysis, which will standardize Geographic Information System (GIS) workflows for modeling and analytics.",The tools will output image classifications.,Operation and Maintenance,Neither,4/1/2024,4/1/2024,4/1/2024,Developed in-house.,Unknown,No,No,No,No,Yes,The data is comprised of Natural Resource image data.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,Yes,FS ACE,More than 12 months,Yes,Yes,Unknown,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The purpose of this tool is to enhance scientific modeling and analysis, which will standardize Geographic Information System (GIS) workflows for modeling and analytics. . The tools will output image classifications.","the purpose of this tool is to enhance scientific modeling and analysis, which will standardize geographic information system (gis) workflows for modeling and analytics. . the tools will output image classifications."
Analysis of prescribed fire turbulence data,Department of Agriculture,USDA,FS: Forest Service,Science & Space,None of the above.,The purpose of this project is to find connections between the heat from a wildfire and the turbulence it creates in the air. Current tools are not very accurate and can make mistakes. This AI effort helps create better tools that can assist fire and smoke managers in making decisions about smoke management.,The model outputs correlation analysis of how temperature change is associated with air turbulence measurements above a prescribed fire.,Acquisition and/or Development,Neither,6/12/2023,7/10/2023,Unknown,Developed in-house.,Unknown,No,Unknown,Unknown,No,Yes,The data is comprised of temperature and turbulence kinetic energy readings collected over time from 16 sonic anemometers (measure wind speed and direction) and 112 thermocouples (measures temperature). The datasets are published in the US Forest Service data archive. ,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,The purpose of this project is to find connections between the heat from a wildfire and the turbulence it creates in the air. Current tools are not very accurate and can make mistakes. This AI effort helps create better tools that can assist fire and smoke managers in making decisions about smoke management. . The model outputs correlation analysis of how temperature change is associated with air turbulence measurements above a prescribed fire.,the purpose of this project is to find connections between the heat from a wildfire and the turbulence it creates in the air. current tools are not very accurate and can make mistakes. this ai effort helps create better tools that can assist fire and smoke managers in making decisions about smoke management. . the model outputs correlation analysis of how temperature change is associated with air turbulence measurements above a prescribed fire.
Anomaly Detection for Data Quality,Commodity Futures Trading Commission,CFTC,DOD,Mission-Enabling,None of the above.,Anomaly detection model designed to identify potentially erroneous data loads in TCR data. Uses an isolation forest model with and automatically runs daily. ,Model provides an anomaly score on the full day of data for a single exchange,Operation and Maintenance,Neither,2/1/2023,5/1/2023,7/1/2023,Developed with both contracting and in-house resources.,REDACTED,No,No,No,Yes,Other,Transactions by market and type,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,REDACTED,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,Anomaly detection model designed to identify potentially erroneous data loads in TCR data. Uses an isolation forest model with and automatically runs daily. . Model provides an anomaly score on the full day of data for a single exchange,anomaly detection model designed to identify potentially erroneous data loads in tcr data. uses an isolation forest model with and automatically runs daily. . model provides an anomaly score on the full day of data for a single exchange
Spoofing Detection AI/ML Project,Commodity Futures Trading Commission,CFTC,DOE,Law & Justice,None of the above.,Pilot project that applied AI/ML techniques to identify spoofing patterns in order message data. Project used a variety of supervised and unsupervised techniques trained with DOE's existing set of expert based spoofing detection algorithms.,Model provides probability of spoofing behavior for every individual trader in a specified market on a specified day,Retired,Both,8/1/2023,9/1/2023,12/1/2023,Developed with both contracting and in-house resources.,REDACTED,No,No,No,Yes,Other,Order message data,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,REDACTED,Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,No,Risks inherent to any analytical identification: risk of false positives; risks identified by discussion among developers and use case owners,Yes – by another appropriate agency office that was not directly involved in the system’s development,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,"Other – Immediate human intervention is not practicable; however, an appropriate fail-safe has been implemented. ",We follow the same procedures as we do for all enforcement matters.,None of the above,"No – Law, operational limitations, or governmentwide guidance precludes an opportunity for an individual to appeal.",Both,0.7936507936507936,Pilot project that applied AI/ML techniques to identify spoofing patterns in order message data. Project used a variety of supervised and unsupervised techniques trained with DOE's existing set of expert based spoofing detection algorithms. . Model provides probability of spoofing behavior for every individual trader in a specified market on a specified day . Risks inherent to any analytical identification: risk of false positives; risks identified by discussion among developers and use case owners,pilot project that applied ai/ml techniques to identify spoofing patterns in order message data. project used a variety of supervised and unsupervised techniques trained with doe's existing set of expert based spoofing detection algorithms. . model provides probability of spoofing behavior for every individual trader in a specified market on a specified day . risks inherent to any analytical identification: risk of false positives; risks identified by discussion among developers and use case owners
Stress Testing Scenarios with Deep Learning,Commodity Futures Trading Commission,CFTC,DCR,Mission-Enabling,None of the above.,Pilot project to explore neural-network based machine learning methods for creating stress testing scenarios and estimating PnL (profit and loss) on FO (Futures and Options) portfolios based on the current/recent market states/conditions. ,The AI generates predicted price movements based on provided input probabilities ,Acquisition and/or Development,Neither,1/6/2023,2/14/2023,Unknown,Developed with both contracting and in-house resources.,REDACTED,No,No,Yes,No,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.31746031746031744,Pilot project to explore neural-network based machine learning methods for creating stress testing scenarios and estimating PnL (profit and loss) on FO (Futures and Options) portfolios based on the current/recent market states/conditions. . The AI generates predicted price movements based on provided input probabilities,pilot project to explore neural-network based machine learning methods for creating stress testing scenarios and estimating pnl (profit and loss) on fo (futures and options) portfolios based on the current/recent market states/conditions. . the ai generates predicted price movements based on provided input probabilities
MPD Entity Risk Modeling,Commodity Futures Trading Commission,CFTC,MPD,Mission-Enabling,None of the above.,Entity-level risk modeling project. Use statistical and probabalistic models to predict firms experiencing changes in capital levels. Very early stage R&D effort. ,Now-cast score for each registrant and each day indicating probability or risk category for near-future capital adjustment indicator statisitc,Initiated,Neither,1/1/2025,Unknown,Unknown,Developed in-house.,Unknown,No,No,Yes,No,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.2698412698412698,Entity-level risk modeling project. Use statistical and probabalistic models to predict firms experiencing changes in capital levels. Very early stage R&D effort. . Now-cast score for each registrant and each day indicating probability or risk category for near-future capital adjustment indicator statisitc,entity-level risk modeling project. use statistical and probabalistic models to predict firms experiencing changes in capital levels. very early stage r&d effort. . now-cast score for each registrant and each day indicating probability or risk category for near-future capital adjustment indicator statisitc
Microsoft Copilot Integration,Election Assistance Commission,EAC,OCIO,Mission-Enabling,Summarizing the key points of a lengthy report using AI.,Efficiency and improved administrative tasks.,Text-based summaries and AI generated text. ,Operation and Maintenance,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,No – Agency did not request an extension for this use case.,No,AI is not safety or rights-impacting.,AI is not safety or rights-impacting.,AI is not safety or rights-impacting.,AI is not safety or rights-impacting.,AI is not safety or rights-impacting.,None of the above,No – it is not operationally practical to offer this.,Neither,0.4126984126984127,Efficiency and improved administrative tasks. . Text-based summaries and AI generated text. . AI is not safety or rights-impacting.,efficiency and improved administrative tasks. . text-based summaries and ai generated text. . ai is not safety or rights-impacting.
Invoice and Contract Data Extraction,Federal Deposit Insurance Corporation,FDIC,Chief Information Officer Organization,Mission-Enabling,None of the above.,The purpose is to extract data from invoices and contracts (PDFs) and use the data for reconciliation.  The benefit is to improve the accuracy of invoice and  contract reconciliation and to reduce the time for Oversight Managers (OMs) to perform this activity.,"The system generates an excel output sent via email to OMs that show the results of the reconciliation, including discrepancies and errors.",Acquisition and/or Development,Neither,3/15/2024,4/15/2024,Unknown,Developed with both contracting and in-house resources.,FDIC does not use PIIDs,No,Unknown,Unknown,Unknown,Other,Data from FDIC invoices and contracts,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,Yes,Commodities Invoice Reconciliation,Less than 6 months,Other,Other,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The purpose is to extract data from invoices and contracts (PDFs) and use the data for reconciliation.  The benefit is to improve the accuracy of invoice and  contract reconciliation and to reduce the time for Oversight Managers (OMs) to perform this activity. . The system generates an excel output sent via email to OMs that show the results of the reconciliation, including discrepancies and errors.","the purpose is to extract data from invoices and contracts (pdfs) and use the data for reconciliation. the benefit is to improve the accuracy of invoice and contract reconciliation and to reduce the time for oversight managers (oms) to perform this activity. . the system generates an excel output sent via email to oms that show the results of the reconciliation, including discrepancies and errors."
Immersive Learning,Federal Deposit Insurance Corporation,FDIC,Corporate University,Mission-Enabling,None of the above.,"Immersive learning offers an engaging approach to training programs by creating realistic environments using GenAI where learners can practice and refine their skills. A controlled environment wherein learners interact with immersive technology to solve pre-determined complex situations and problems. This approach amplifies employee engagement and retention, performance management, learning and development, administrative tasks, workforce planning, employee onboarding, recruiting, interviewing as well as others. ",Prediction based on sequencing provided by developer; completions reports with objectives met.,Acquisition and/or Development,Neither,2/6/2024,11/22/2024,Unknown,Developed with contracting resources.,FDIC does not use PIIDs,No,Unknown,Unknown,Unknown,Other,No specific FDIC datasets go into this model,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Immersive learning offers an engaging approach to training programs by creating realistic environments using GenAI where learners can practice and refine their skills. A controlled environment wherein learners interact with immersive technology to solve pre-determined complex situations and problems. This approach amplifies employee engagement and retention, performance management, learning and development, administrative tasks, workforce planning, employee onboarding, recruiting, interviewing as well as others. . Prediction based on sequencing provided by developer; completions reports with objectives met.","immersive learning offers an engaging approach to training programs by creating realistic environments using genai where learners can practice and refine their skills. a controlled environment wherein learners interact with immersive technology to solve pre-determined complex situations and problems. this approach amplifies employee engagement and retention, performance management, learning and development, administrative tasks, workforce planning, employee onboarding, recruiting, interviewing as well as others. . prediction based on sequencing provided by developer; completions reports with objectives met."
AI Assisted Data Collection,Federal Deposit Insurance Corporation,FDIC,Division of Depositor & Consumer Protection,Other,None of the above.,"The Assistant enhances the efficiency and effectiveness of our fair lending and compliance functions by automating data collection from PDF form documents to support internal supervisory consultations.  Through using the Assistant. data collection takes significantly less time, retains and improves accuracy, and allows the FDIC examiners to focus on examination activities",The Assistant provides extracted data from credit reports and similar type documents in excel readable formats,Acquisition and/or Development,Neither,4/27/2023,4/27/2023,Unknown,Developed in-house.,Unknown,No,Unknown,Unknown,Unknown,Other,This program uses bank-specific data from examinations containing bank-provided customer information from credit reports and loan applications often through FDICConnect,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,Yes,Microsoft Power Apps - AI Builder,Less than 6 months,Other,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"The Assistant enhances the efficiency and effectiveness of our fair lending and compliance functions by automating data collection from PDF form documents to support internal supervisory consultations.  Through using the Assistant. data collection takes significantly less time, retains and improves accuracy, and allows the FDIC examiners to focus on examination activities . The Assistant provides extracted data from credit reports and similar type documents in excel readable formats","the assistant enhances the efficiency and effectiveness of our fair lending and compliance functions by automating data collection from pdf form documents to support internal supervisory consultations. through using the assistant. data collection takes significantly less time, retains and improves accuracy, and allows the fdic examiners to focus on examination activities . the assistant provides extracted data from credit reports and similar type documents in excel readable formats"
Compliance Risk Monitoring,Federal Deposit Insurance Corporation,FDIC,Division of Depositor & Consumer Protection,Other,None of the above.,Enhance effectiveness of off-site risk monitoring on several regulation areas.,A report from Division of Consumer Protection (DCP) Analytics that identifies whether a financial institution has high / low relative risk of a compliance violation.  Used by examiners to scope exams.,Acquisition and/or Development,Neither,8/14/2023,8/14/2023,Unknown,Developed in-house.,Unknown,No,Unknown,Unknown,Unknown,Other,"Data comes from three major FDIC data sources, i.e. Research Information System  database (RIS), compliance exam data (Framework for the Oversight of Compliance and CRA Activities User Suite ((FOCUS)) and consumer complaints data ((Enterprise Public Inquiries and Complaints ((EPIC)).
","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Unknown,Unknown,Yes,R,Less than 6 months,Other,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,Enhance effectiveness of off-site risk monitoring on several regulation areas. . A report from Division of Consumer Protection (DCP) Analytics that identifies whether a financial institution has high / low relative risk of a compliance violation.  Used by examiners to scope exams.,enhance effectiveness of off-site risk monitoring on several regulation areas. . a report from division of consumer protection (dcp) analytics that identifies whether a financial institution has high / low relative risk of a compliance violation. used by examiners to scope exams.
Pre-Examination Planning Monitoring,Federal Deposit Insurance Corporation,FDIC,Division of Depositor & Consumer Protection,Other,None of the above.,Assist examiners in pre-exam planning and identify pre-exam questions that are misaligned or do not inform risk assessments,A report from  Division of Consumer Protection (DCP) Analytics to DCP Compliance and UDAP (Unfair Deceptive Acts and Practices) section related to pre-exam questions on their alignment with risk identification,Operation and Maintenance,Neither,2/8/2023,2/8/2023,12/14/2023,Developed in-house.,Unknown,No,No,No,Yes,Other,"PEP IR (Pre-Examination Planning Instrument Report) uses pre-exam planning documents such as ARCH (Assessment of Risk of Consumer Harm), Call Report data, to identify pre exam planning questions for further review.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,R,6-12 months,Other,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,Assist examiners in pre-exam planning and identify pre-exam questions that are misaligned or do not inform risk assessments . A report from  Division of Consumer Protection (DCP) Analytics to DCP Compliance and UDAP (Unfair Deceptive Acts and Practices) section related to pre-exam questions on their alignment with risk identification,assist examiners in pre-exam planning and identify pre-exam questions that are misaligned or do not inform risk assessments . a report from division of consumer protection (dcp) analytics to dcp compliance and udap (unfair deceptive acts and practices) section related to pre-exam questions on their alignment with risk identification
HMDA (Home Mortgage Disclosure Act) Outlier Screen,Federal Deposit Insurance Corporation,FDIC,Division of Depositor & Consumer Protection,Other,None of the above.,"The HMDA (Home Mortgage Disclosure Act) Outlier program pre-screens off-site mortgage data to identify heightened fair lending risks related to pricing, underwriting, and redlining",Reports results of statistical analysis to support examination supervisory actions,Operation and Maintenance,Neither,1/1/2018,1/1/2018,1/1/2019,Developed in-house.,Unknown,No,No,Yes,Yes,Other,The HMDA Outlier program uses regulator version HMDA data provided by the CFPB.  The public version of HMDA data strips out or obfuscates PII.,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,SAS,Less than 6 months,Other,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"The HMDA (Home Mortgage Disclosure Act) Outlier program pre-screens off-site mortgage data to identify heightened fair lending risks related to pricing, underwriting, and redlining . Reports results of statistical analysis to support examination supervisory actions","the hmda (home mortgage disclosure act) outlier program pre-screens off-site mortgage data to identify heightened fair lending risks related to pricing, underwriting, and redlining . reports results of statistical analysis to support examination supervisory actions"
Automatically-Scored Writing Assessment (AWA),Federal Deposit Insurance Corporation,FDIC,Division of Administration,Mission-Enabling,None of the above.,The Automatically-Scored Writing Assessment (AWA) is intended to significantly increase speed and reduce cost of essay scoring for the purpose of employment decisions.,"A single overall score for the essay on a scale from 1 to 5, as well as three subscale scores ranging from 1 to 5 across the following dimensions: grammar and mechanics, analysis and reasoning, as well as organization and structure.",Operation and Maintenance,"Rights-Impacting
",1/17/2018,1/17/2018,1/17/2018,Developed with contracting resources.,FDIC does not use PIIDs,No,No,Yes,No,Other,Unknown,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,No – agency does not have access to source code.,Yes,PDRI Performance-Fit,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Rights-Impacting,0.6190476190476191,"The Automatically-Scored Writing Assessment (AWA) is intended to significantly increase speed and reduce cost of essay scoring for the purpose of employment decisions. . A single overall score for the essay on a scale from 1 to 5, as well as three subscale scores ranging from 1 to 5 across the following dimensions: grammar and mechanics, analysis and reasoning, as well as organization and structure.","the automatically-scored writing assessment (awa) is intended to significantly increase speed and reduce cost of essay scoring for the purpose of employment decisions. . a single overall score for the essay on a scale from 1 to 5, as well as three subscale scores ranging from 1 to 5 across the following dimensions: grammar and mechanics, analysis and reasoning, as well as organization and structure."
CFOO Assessment Chatbot,Federal Deposit Insurance Corporation,FDIC,Division of Finance,Mission-Enabling,None of the above.,"Compiling data from external FDIC website and targeted documents to answer public questions about Bank Assessments that are currently available on FDIC gov. Makes information more readily available to the public, searchable and readable.",An text-based answer to a question the user asks the bot.,Acquisition and/or Development,Neither,2/15/2022,4/4/2022,Unknown,Developed in-house.,Unknown,No,Unknown,Unknown,Unknown,Other,"CFOO (Chief Financial Officer Organization) Assessment Chatbot use the FDIC Assessment data set for training. The data contains assessment related data elements. The assessment data set is internally/externally available for all the FDIC employees, contractors and the public through FDIC.gov external site.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Unknown,Unknown,Yes,External Chatbot Rasa (ECR) - Assessment Chatbot,Less than 6 months,Other,Other,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Compiling data from external FDIC website and targeted documents to answer public questions about Bank Assessments that are currently available on FDIC gov. Makes information more readily available to the public, searchable and readable. . An text-based answer to a question the user asks the bot.","compiling data from external fdic website and targeted documents to answer public questions about bank assessments that are currently available on fdic gov. makes information more readily available to the public, searchable and readable. . an text-based answer to a question the user asks the bot."
CFOO Admin Chatbot,Federal Deposit Insurance Corporation,FDIC,Division of Finance,Mission-Enabling,None of the above.,"Compiling data from internal FDIC website and targeted documents to answer questions about CFOO admin. Makes information more readily available to the public, searchable and readable.",An text-based answer to a question the user asks the bot.,Operation and Maintenance,Neither,9/1/2022,1/10/2023,6/13/2023,Developed in-house.,Unknown,No,No,No,Yes,Other,CFOO (Chief Financial Officer Organization) Admin Chatbot use the FDIC CFOO Administrative data set for training. The data contains administrative data elements. The administrative data set is internally available for all the FDIC employees and contractors.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Internal Chatbot Rasa (ICR) - Admin Chatbot,6-12 months,Other,Other,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"Compiling data from internal FDIC website and targeted documents to answer questions about CFOO admin. Makes information more readily available to the public, searchable and readable. . An text-based answer to a question the user asks the bot.","compiling data from internal fdic website and targeted documents to answer questions about cfoo admin. makes information more readily available to the public, searchable and readable. . an text-based answer to a question the user asks the bot."
CFOO Travel Chatbot,Federal Deposit Insurance Corporation,FDIC,Division of Finance,Mission-Enabling,None of the above.,"Compiling data from internal FDIC website and targeted documents to answer questions about FDIC travel. Makes information more readily available, searchable and readable.",An text-based answer to a question the user asks the bot.,Operation and Maintenance,Neither,8/10/2020,9/1/2020,12/21/2021,Developed in-house.,Unknown,No,No,No,Yes,Other,CFOO (Chief Financial Officer Organization) Travel Chatbot use the FDIC Travel Policy data set for training. The data contains travel policy data elements. The travel policy data set is internally available for all the FDIC employees and contractors.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Internal Chatbot Rasa (ICR) - Travel Chatbot,6-12 months,Other,Other,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"Compiling data from internal FDIC website and targeted documents to answer questions about FDIC travel. Makes information more readily available, searchable and readable. . An text-based answer to a question the user asks the bot.","compiling data from internal fdic website and targeted documents to answer questions about fdic travel. makes information more readily available, searchable and readable. . an text-based answer to a question the user asks the bot."
Trust Document Entity Extraction,Federal Deposit Insurance Corporation,FDIC,Division of Resolutions and Receiverships,Other,None of the above.,"Extract key data elements, such as grantor/settler and beneficiaries, from Trust Agreements. This will reduce the time and cost to transpose information from Trust Agreements.","The extracted data, along with the original Trust Agreement, will be presented to users through an application user interface.",Acquisition and/or Development,Neither,6/7/2024,6/10/2024,Unknown,Developed in-house.,Unknown,No,Unknown,Unknown,Unknown,Other,"We used open source, sample trust agreements available on the web, as well as an internal repository of Trust Agreements retained by the FDIC after they were used in earlier insurance determination activities.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,Yes,Resolution and Receivership Management Portal,Less than 6 months,Other,Other,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"Extract key data elements, such as grantor/settler and beneficiaries, from Trust Agreements. This will reduce the time and cost to transpose information from Trust Agreements. . The extracted data, along with the original Trust Agreement, will be presented to users through an application user interface.","extract key data elements, such as grantor/settler and beneficiaries, from trust agreements. this will reduce the time and cost to transpose information from trust agreements. . the extracted data, along with the original trust agreement, will be presented to users through an application user interface."
Leverage AI in the Rulemaking Process Use Case ,Federal Energy Regulatory Commission,FERC,Unknown,Mission-Enabling,None of the above.,"The AI solution will be designed to streamline the manual review of public feedback, providing insights on sentiment and key themes for FERC’s rulemaking process. Enabling FERC analysts to complete their work more efficiently and accurately saving costs, shorten the regulatory timelines, and ensure timely implementation of policies that benefit the energy industry and the public.",Initial Comment Sentiment Analysis,Unknown,Neither,1/16/2024,1/16/2024,Unknown,Developed with both contracting and in-house resources.,89603024F0006,No,No,No,Yes,Yes,Comments submitted into the agency via a public facing application will be utilized to train.,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Microsoft Azure Commerical Environment for prototype.,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The AI solution will be designed to streamline the manual review of public feedback, providing insights on sentiment and key themes for FERC’s rulemaking process. Enabling FERC analysts to complete their work more efficiently and accurately saving costs, shorten the regulatory timelines, and ensure timely implementation of policies that benefit the energy industry and the public. . Initial Comment Sentiment Analysis","the ai solution will be designed to streamline the manual review of public feedback, providing insights on sentiment and key themes for ferc’s rulemaking process. enabling ferc analysts to complete their work more efficiently and accurately saving costs, shorten the regulatory timelines, and ensure timely implementation of policies that benefit the energy industry and the public. . initial comment sentiment analysis"
Improve Safety Inspections Use Case ,Federal Energy Regulatory Commission,FERC,Unknown,Energy & the Environment,None of the above.,"The AI solution is intended to enhance the efficiency and accuracy of dam and LNG pipeline structure inspections by FERCs Dam inspectors. It will analyze historical and real-time images, videos, and notes to identify potential safety defects and trends. This approach reduces the manual workload of safety inspectors and improve inspection timelines. ",Digital Images for Review,Unknown,Neither,Unknown,Unknown,Unknown,Unknown,89603024F0006,No,No,No,Yes,Other,Images obtained by the AI will build a repository for training.,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,Unknown,No,Unknown,Unknown,Other,Yes,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"The AI solution is intended to enhance the efficiency and accuracy of dam and LNG pipeline structure inspections by FERCs Dam inspectors. It will analyze historical and real-time images, videos, and notes to identify potential safety defects and trends. This approach reduces the manual workload of safety inspectors and improve inspection timelines. . Digital Images for Review","the ai solution is intended to enhance the efficiency and accuracy of dam and lng pipeline structure inspections by fercs dam inspectors. it will analyze historical and real-time images, videos, and notes to identify potential safety defects and trends. this approach reduces the manual workload of safety inspectors and improve inspection timelines. . digital images for review"
Enhance Market Surveillance and Fraud Detection Use Case ,Federal Energy Regulatory Commission,FERC,Unknown,Mission-Enabling,None of the above.,"The AI solution aims to assist staff in analyzing large and complex market datasets for market integrity and compliance purposes. It will process and interpret vast amounts of trading data at high speed, identifying anomalies, trends, and potential regulatory violations. This capability will help maintain fair and transparent markets, enabling staff to focus on strategic decision-making. The AI’s efficiency and precision will save time, improve compliance, and uphold public trust in market operations.",Initial Analysis of market data,Unknown,Neither,Unknown,Unknown,Unknown,Unknown,89603024F0006,No,No,No,Yes,Yes,We have not started work on this use case and have not identified specific data sets needed to support all market surveillance activities. ,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,Unknown,No,Unknown,Unknown,Other,Yes,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"The AI solution aims to assist staff in analyzing large and complex market datasets for market integrity and compliance purposes. It will process and interpret vast amounts of trading data at high speed, identifying anomalies, trends, and potential regulatory violations. This capability will help maintain fair and transparent markets, enabling staff to focus on strategic decision-making. The AI’s efficiency and precision will save time, improve compliance, and uphold public trust in market operations. . Initial Analysis of market data","the ai solution aims to assist staff in analyzing large and complex market datasets for market integrity and compliance purposes. it will process and interpret vast amounts of trading data at high speed, identifying anomalies, trends, and potential regulatory violations. this capability will help maintain fair and transparent markets, enabling staff to focus on strategic decision-making. the ai’s efficiency and precision will save time, improve compliance, and uphold public trust in market operations. . initial analysis of market data"
Support Interconnection Request Responses Use Case ,Federal Energy Regulatory Commission,FERC,Unknown,Energy & the Environment,None of the above.,"The AI solution will be designed to address the growing backlog of project interconnection requests and reduce the average three-year wait time. By accelerating the processing of proposals, the AI will help FERC with cost savings and shorting the timelines of requests. ",Request analysis of interconnection requests,Unknown,Neither,Unknown,Unknown,Unknown,Unknown,89603024F0006,No,No,No,Yes,Yes,We have not started work on this use case and have not identified specific data sets needed to support all interconnection requests. ,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,Unknown,No,Unknown,Unknown,Other,Yes,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"The AI solution will be designed to address the growing backlog of project interconnection requests and reduce the average three-year wait time. By accelerating the processing of proposals, the AI will help FERC with cost savings and shorting the timelines of requests. . Request analysis of interconnection requests","the ai solution will be designed to address the growing backlog of project interconnection requests and reduce the average three-year wait time. by accelerating the processing of proposals, the ai will help ferc with cost savings and shorting the timelines of requests. . request analysis of interconnection requests"
Neural Networks for FHFA Modeling Analytics Platform (FMAP),Federal Housing Finance Agency,FHFA,DHMG,Mission-Enabling,None of the above.,Help researchers optimize the framework and specifications of the production of the Single-Family FHFA Modeling Analytics Platform (FMAP).,"This tool uses neural networks to identify nonlinearities, anomalies, and important variables in loan-level mortgage data, which helps FHFA staff improve forecasts from the Single-Family Modeling Analytics Platform.",Acquisition and/or Development,Neither,1/8/2023,1/8/2023,Unknown,Developed in-house.,Unknown,No,No,No,No,No,Unknown,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",No,Machine Learning for SF FMAP specification  ,Less than 6 months,Yes,No,No,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"Help researchers optimize the framework and specifications of the production of the Single-Family FHFA Modeling Analytics Platform (FMAP). . This tool uses neural networks to identify nonlinearities, anomalies, and important variables in loan-level mortgage data, which helps FHFA staff improve forecasts from the Single-Family Modeling Analytics Platform.","help researchers optimize the framework and specifications of the production of the single-family fhfa modeling analytics platform (fmap). . this tool uses neural networks to identify nonlinearities, anomalies, and important variables in loan-level mortgage data, which helps fhfa staff improve forecasts from the single-family modeling analytics platform."
Public Comments Summarization and Topic Classifications Pilot Project,Federal Housing Finance Agency,FHFA,DBR,Mission-Enabling,None of the above.,To summarize and classify comments received by FHFA during public comment processes.,"This tool uses natural language processing to rapidly summarize and classify certain comments received from the public, increasing FHFA staff efficiency.",Initiated,Neither,10/1/2023,Unknown,Unknown,Developed in-house.,Unknown,No,No,No,No,Other,Publiccomments,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",No,Public Comments AnlysisTool Pilot Project (PCAT,Less than 6 months,Yes,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"To summarize and classify comments received by FHFA during public comment processes. . This tool uses natural language processing to rapidly summarize and classify certain comments received from the public, increasing FHFA staff efficiency.","to summarize and classify comments received by fhfa during public comment processes. . this tool uses natural language processing to rapidly summarize and classify certain comments received from the public, increasing fhfa staff efficiency."
Economic Trend Modeling,Board of Governors of the Federal Reserve System,FRB,Division of Monetary Affairs,Other,None of the above.,Additional data beyond traditional statistical models,Multiple methods calculate the probability of a economic event in the next twelve months between 0 and 1 and are aggregated,Operation and Maintenance,Neither,3/1/2019,3/1/2019,9/23/2019,Developed in-house.,Unknown,No,No,No,No,Yes,Internal- Fixed Income,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,No,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,Additional data beyond traditional statistical models . Multiple methods calculate the probability of a economic event in the next twelve months between 0 and 1 and are aggregated,additional data beyond traditional statistical models . multiple methods calculate the probability of a economic event in the next twelve months between 0 and 1 and are aggregated
PDF Optical Character Recognition (Text),Board of Governors of the Federal Reserve System,FRB,Division of Information Technology,Other,None of the above.,The intended purpose is to extract and to decide the correct column name extracted from a report in PDF format,The list of the appropriate column names in text format,Operation and Maintenance,Neither,9/19/2021,9/22/2021,9/26/2024,Developed in-house.,Unknown,No,No,No,Unknown,No,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,No,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,The intended purpose is to extract and to decide the correct column name extracted from a report in PDF format . The list of the appropriate column names in text format,the intended purpose is to extract and to decide the correct column name extracted from a report in pdf format . the list of the appropriate column names in text format
PDF Optical Character Recognition (Images),Board of Governors of the Federal Reserve System,FRB,Division of Information Technology,Other,Inputting large amounts of data from paper forms into a digital system using AI.,The intended purpose is to extract text from the image embedded in PDF file,The AI system's output is the image details in text format for further analysis,Acquisition and/or Development,Neither,6/11/2024,7/17/2024,Unknown,Developed in-house.,Unknown,No,No,No,No,No,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,No,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,The intended purpose is to extract text from the image embedded in PDF file . The AI system's output is the image details in text format for further analysis,the intended purpose is to extract text from the image embedded in pdf file . the ai system's output is the image details in text format for further analysis
Stock Market Analysis,Board of Governors of the Federal Reserve System,FRB,Division of Research and Statistics,Other,None of the above.,To understand media narratives associated with stock market events,Time series measuring association of different news topics frequencies with market volatility,Acquisition and/or Development,Neither,9/1/2022,9/1/2022,Unknown,Developed in-house.,Unknown,No,No,No,No,Yes,External- News  Feed,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,To understand media narratives associated with stock market events . Time series measuring association of different news topics frequencies with market volatility,to understand media narratives associated with stock market events . time series measuring association of different news topics frequencies with market volatility
Consumer Related Information Classification,Board of Governors of the Federal Reserve System,FRB,Division of Supervision and Regulation,Mission-Enabling,None of the above.,Classification of consumer related information to relevant regulations,Assignment of one or more regulations to a consumer related information,Implementation and Assessment,Neither,3/16/2023,5/1/2023,10/19/2023,Developed in-house.,Unknown,No,No,No,No,Yes,Internal- Firm Data,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,Classification of consumer related information to relevant regulations . Assignment of one or more regulations to a consumer related information,classification of consumer related information to relevant regulations . assignment of one or more regulations to a consumer related information
Commercial Real Estate Index,Board of Governors of the Federal Reserve System,FRB,Division of Supervision and Regulation,Mission-Enabling,None of the above.,Identify principal component using various data points to create a market index for commercial real estate,"A principal component, which then gets used as the index",Operation and Maintenance,Neither,10/1/2023,10/1/2023,2/1/2024,Developed in-house.,Unknown,No,Yes,No,No,Yes,External - Real Estate,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Identify principal component using various data points to create a market index for commercial real estate . A principal component, which then gets used as the index","identify principal component using various data points to create a market index for commercial real estate . a principal component, which then gets used as the index"
Variable Optimization,Board of Governors of the Federal Reserve System,FRB,Division of Supervision and Regulation,Mission-Enabling,None of the above.,Iterate through alternative lag structures to identify the optimal one for forecasting call report metrics,A forecast for select call report metrics,Implementation and Assessment,Neither,1/1/2022,2/1/2022,9/28/2023,Developed in-house.,Unknown,No,No,No,No,Yes,Internal- FFIEC Call Report,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,Iterate through alternative lag structures to identify the optimal one for forecasting call report metrics . A forecast for select call report metrics,iterate through alternative lag structures to identify the optimal one for forecasting call report metrics . a forecast for select call report metrics
Proposals and Public Comments,Board of Governors of the Federal Reserve System,FRB,Office of the Secretary,Government Services (includes Benefits and Service Delivery),None of the above.,"The Board of Governors of the Federal Reserve System (“Board”) is developing the Proposals and Public Comments (PPC) system to electronically process and manage comments from the public on regulatory rulemakings, information collections, and other proposals (collectively, “proposals”) and to post those comments to the Board’s public website. The first release of PPC is expected in mid-November 2024. The Board’s processing of comments may use artificial intelligence (AI) to provide more efficient processing of public comments (e.g., PII redaction recommendations, spam detection).","The Board’s processing of comments may use artificial intelligence (AI) to provide more efficient processing of public comments (e.g., PII redaction recommendations, sentiment analysis, text matching, entity identification, and text similarity matching) . A human verifies the system recommendations.
",Operation and Maintenance,Neither,9/1/2023,9/1/2023,11/16/2024,Developed in-house.,Unknown,No,No,Yes,Yes,No,External- Public Comments,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Proposals and Public Comments,Less than 6 months,No,No,No,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The Board of Governors of the Federal Reserve System (“Board”) is developing the Proposals and Public Comments (PPC) system to electronically process and manage comments from the public on regulatory rulemakings, information collections, and other proposals (collectively, “proposals”) and to post those comments to the Board’s public website. The first release of PPC is expected in mid-November 2024. The Board’s processing of comments may use artificial intelligence (AI) to provide more efficient processing of public comments (e.g., PII redaction recommendations, spam detection). . The Board’s processing of comments may use artificial intelligence (AI) to provide more efficient processing of public comments (e.g., PII redaction recommendations, sentiment analysis, text matching, entity identification, and text similarity matching) . A human verifies the system recommendations.","the board of governors of the federal reserve system (“board”) is developing the proposals and public comments (ppc) system to electronically process and manage comments from the public on regulatory rulemakings, information collections, and other proposals (collectively, “proposals”) and to post those comments to the board’s public website. the first release of ppc is expected in mid-november 2024. the board’s processing of comments may use artificial intelligence (ai) to provide more efficient processing of public comments (e.g., pii redaction recommendations, spam detection). . the board’s processing of comments may use artificial intelligence (ai) to provide more efficient processing of public comments (e.g., pii redaction recommendations, sentiment analysis, text matching, entity identification, and text similarity matching) . a human verifies the system recommendations."
Bank Subsidiary Identification,Board of Governors of the Federal Reserve System,FRB,Division of Research and Statistics,Other,None of the above.,Identifying auto loans reported as subsidiaries of the banks,Identification of loans issued by finance company subsidiaries of banks,Initiated,Neither,10/1/2024,Unknown,Unknown,Developed in-house.,Unknown,No,No,No,No,No,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.30158730158730157,Identifying auto loans reported as subsidiaries of the banks . Identification of loans issued by finance company subsidiaries of banks,identifying auto loans reported as subsidiaries of the banks . identification of loans issued by finance company subsidiaries of banks
Manufacturer Sentiment Analysis,Board of Governors of the Federal Reserve System,FRB,Division of Research and Statistics,Other,None of the above.,To quantify survey responses related to forecasts of industrial production,Time series measuring the sentiment of manufacturing via survey respondents,Operation and Maintenance,Neither,7/1/2022,7/1/2022,7/1/2022,Developed in-house.,Unknown,No,No,Yes,Yes,Yes,External- Manufacturing,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,No ,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,To quantify survey responses related to forecasts of industrial production . Time series measuring the sentiment of manufacturing via survey respondents,to quantify survey responses related to forecasts of industrial production . time series measuring the sentiment of manufacturing via survey respondents
Supply Chain Estimations,Board of Governors of the Federal Reserve System,FRB,Division of Research and Statistics,Other,None of the above.,To estimate supply chain bottlenecks,Time series measuring supply chain bottleneck sentiment data,Operation and Maintenance,Neither,12/1/2022,12/1/2022,12/1/2022,Developed in-house.,Unknown,No,Yes,No,No,Yes,Internal- Beige Book,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,No ,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,To estimate supply chain bottlenecks . Time series measuring supply chain bottleneck sentiment data,to estimate supply chain bottlenecks . time series measuring supply chain bottleneck sentiment data
Research And Development Analysis,Board of Governors of the Federal Reserve System,FRB,Division of Research and Statistics,Other,None of the above.,To quantify AI research and development intensity in industry,Time series measuring R&D intensity within industry,Acquisition and/or Development,Neither,8/1/2024,8/1/2024,8/1/2024,Developed in-house.,Unknown,No,No,No,No,Yes,"Internal and External- Government and Economic Data- 10K filings, FRED Macroeconomic Data","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,No ,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,To quantify AI research and development intensity in industry . Time series measuring R&D intensity within industry,to quantify ai research and development intensity in industry . time series measuring r&d intensity within industry
Market Fund Portfolio,Board of Governors of the Federal Reserve System,FRB,Division of Research and Statistics,Other,None of the above.,To assist with issuer classification of money market funds,A list of likely issuers of a particular security,Operation and Maintenance,Neither,10/1/2020,10/1/2020,5/1/2021,Developed in-house.,Unknown,No,No,No,No,No,External- SEC Filings,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,No,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,To assist with issuer classification of money market funds . A list of likely issuers of a particular security,to assist with issuer classification of money market funds . a list of likely issuers of a particular security
Sentiment Analysis of Earnings Transcripts,Board of Governors of the Federal Reserve System,FRB,Division of Monetary Affairs,Other,None of the above.,Additional insight into sentiments related to topics in bank earnings calls,"The probability that text inputs are positive, negative, or neutral and classifies them as such",Operation and Maintenance,Neither,5/23/2023,6/1/2023,10/1/2024,Developed in-house.,Unknown,No,No,No,No,Yes,Unknown,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,No,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Additional insight into sentiments related to topics in bank earnings calls . The probability that text inputs are positive, negative, or neutral and classifies them as such","additional insight into sentiments related to topics in bank earnings calls . the probability that text inputs are positive, negative, or neutral and classifies them as such"
Anomaly Detection,Board of Governors of the Federal Reserve System,FRB,Division of Supervision and Regulation,Other,None of the above.,Improve the iterative data quality process,Suggested messages regarding data quality of subsets of firm submitted data based on similar historical messages,Operation and Maintenance,Neither,8/1/2022,8/1/2022,12/1/2022,Developed in-house.,Unknown,No,No,No,No,Yes,Internal- QIS,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,Improve the iterative data quality process . Suggested messages regarding data quality of subsets of firm submitted data based on similar historical messages,improve the iterative data quality process . suggested messages regarding data quality of subsets of firm submitted data based on similar historical messages
Consumer Complaints Explorer,Board of Governors of the Federal Reserve System,FRB,Division of Consumer and Community Affairs,Other,None of the above.,Improve the classification of consumer complaints into topics using topic modeling,"Gamma value, topic number, and top five terms for the topic number for each narrative",Operation and Maintenance,Neither,3/1/2018,6/1/2018,1/1/2019,Developed in-house.,Unknown,No,No,No,No,Other,External- CFPB Consumer Complaints,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), and took place within during use case’s development. This documentation has not been shared beyond the data science team building the tool.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"Improve the classification of consumer complaints into topics using topic modeling . Gamma value, topic number, and top five terms for the topic number for each narrative","improve the classification of consumer complaints into topics using topic modeling . gamma value, topic number, and top five terms for the topic number for each narrative"
Regulatory Data Analysis,Board of Governors of the Federal Reserve System,FRB,Division of Supervision and Regulation,Other,None of the above.,Improved data quality for stakeholders,Values for various predicted percentile levels for a given reporter are provided to an analyst to compare to current reported values,Operation and Maintenance,Neither,6/1/2023,6/1/2023,9/9/2024,Developed in-house.,Unknown,No,No,No,No,No,Internal- Firm Data,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,No,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,Improved data quality for stakeholders . Values for various predicted percentile levels for a given reporter are provided to an analyst to compare to current reported values,improved data quality for stakeholders . values for various predicted percentile levels for a given reporter are provided to an analyst to compare to current reported values
Decision Tree for Deposits Data,Board of Governors of the Federal Reserve System,FRB,Division of Supervision and Regulation,Other,None of the above.,Improved process efficiency and data quality. ,Predetermined variables are calculated and then filtered to identify potential outliers in the current reporting period data,Implementation and Assessment,Neither,3/1/2023,3/1/2023,8/29/2024,Developed in-house.,Unknown,No,No,No,No,No,Internal- H.6 Money Stock Measures,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,No,No,No,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,Improved process efficiency and data quality. . Predetermined variables are calculated and then filtered to identify potential outliers in the current reporting period data,improved process efficiency and data quality. . predetermined variables are calculated and then filtered to identify potential outliers in the current reporting period data
Novel Activities Call Report Classification,Board of Governors of the Federal Reserve System,FRB,Division of Supervision and Regulation,Other,None of the above.,The random forest model would parse out what reported categories most significantly predict inclusion on novel banking activity lists,The model identifies call report line items that are correlated with the banks on internal supervisory lists and classifies banks based on their statistical similarity to banks engaged in novel activities,Retired,Neither,7/1/2024,7/1/2024,Unknown,Developed in-house.,Unknown,No,No,No,No,Yes,Internal- FDIC Deposits,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,The random forest model would parse out what reported categories most significantly predict inclusion on novel banking activity lists . The model identifies call report line items that are correlated with the banks on internal supervisory lists and classifies banks based on their statistical similarity to banks engaged in novel activities,the random forest model would parse out what reported categories most significantly predict inclusion on novel banking activity lists . the model identifies call report line items that are correlated with the banks on internal supervisory lists and classifies banks based on their statistical similarity to banks engaged in novel activities
Financial News Processing,Board of Governors of the Federal Reserve System,FRB,Division of Supervision and Regulation,Mission-Enabling,None of the above.,Assist with financial news processing,Interactive dashboards,Implementation and Assessment,Neither,6/1/2023,8/1/2023,9/1/2024,Developed in-house.,Unknown,No,No,No,No,No,External- News  Feed,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,No – agency does not have access to source code.,Yes,General Support System,Less than 6 months,No,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,Assist with financial news processing . Interactive dashboards,assist with financial news processing . interactive dashboards
Bank Exam Quality Control,Board of Governors of the Federal Reserve System,FRB,Division of Supervision and Regulation,Mission-Enabling,None of the above.,A traditional machine learning NLP model of bank performance that utilizes information in news articles among various inputs,Model outcomes are used as a component of quality control,Acquisition and/or Development,Neither,1/1/2022,1/1/2022,Unknown,Developed in-house.,Unknown,No,No,No,No,No,External- News  Feed,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,No – agency does not have access to source code.,Yes,General Support System,Less than 6 months,No,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,A traditional machine learning NLP model of bank performance that utilizes information in news articles among various inputs . Model outcomes are used as a component of quality control,a traditional machine learning nlp model of bank performance that utilizes information in news articles among various inputs . model outcomes are used as a component of quality control
Document Summarization Statistics,Board of Governors of the Federal Reserve System,FRB,Division of Supervision and Regulation,Mission-Enabling,None of the above.,Provide statistics about bank review letters based on the length of the letters and the frequency of common financial terms,Statistics on letters contained in a PDF,Operation and Maintenance,Neither,1/1/2022,1/1/2022,12/22/2024,Developed in-house.,Unknown,No,No,No,No,No,Internal- Firm Data,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,No – agency does not have access to source code.,Yes,General Support System,Less than 6 months,No,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,Provide statistics about bank review letters based on the length of the letters and the frequency of common financial terms . Statistics on letters contained in a PDF,provide statistics about bank review letters based on the length of the letters and the frequency of common financial terms . statistics on letters contained in a pdf
Writing Quality Analysis Model,Board of Governors of the Federal Reserve System,FRB,Division of Supervision and Regulation,Mission-Enabling,None of the above.,A NLP model to help leaders better understand the writing quality and consistency of documents,Provides analysis score of consistency of writing style,Initiated,Neither,1/1/2022,1/1/2022,Unknown,Developed in-house.,Unknown,No,No,No,No,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.2857142857142857,A NLP model to help leaders better understand the writing quality and consistency of documents . Provides analysis score of consistency of writing style,a nlp model to help leaders better understand the writing quality and consistency of documents . provides analysis score of consistency of writing style
Comment Review System,Board of Governors of the Federal Reserve System,FRB,Division of Information Technology,Government Services (includes Benefits and Service Delivery),None of the above.,"The Comment Review System (CRS) is a system used by the Board of Governors of the Federal Reserve System (“Board”) to electronically process and manage comments from the public on regulatory?rulemakings, information collections, and other proposals (collectively, “proposals”). The Board’s processing of comments may use artificial?intelligence (AI) to provide more efficient processing of public comments (e.g., text matching, entity identification, and text similarity matching).? ","To assist the analyst in reviewing each comment, CRS uses traditional machine learning natural language processing (NLP) to provide text summarization, text matching with lists of topics, entity identification, and text similarity matching. In addition, CRS identifies duplicative or near-duplicative comment letters, provides full text search (including metadata properties), and provides the optionality of providing notes or labelling comments based on various metadata attributes. All public comments are reviewed in their entirety, and summaries are used to assist with these reviews.",Operation and Maintenance,Neither,2/1/2021,2/1/2021,7/1/2021,Developed in-house.,Unknown,No,No,Yes,Yes,No,External- Public Comments,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Comment Review System,Less than 6 months,No,No,No,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The Comment Review System (CRS) is a system used by the Board of Governors of the Federal Reserve System (“Board”) to electronically process and manage comments from the public on regulatory?rulemakings, information collections, and other proposals (collectively, “proposals”). The Board’s processing of comments may use artificial?intelligence (AI) to provide more efficient processing of public comments (e.g., text matching, entity identification, and text similarity matching).? . To assist the analyst in reviewing each comment, CRS uses traditional machine learning natural language processing (NLP) to provide text summarization, text matching with lists of topics, entity identification, and text similarity matching. In addition, CRS identifies duplicative or near-duplicative comment letters, provides full text search (including metadata properties), and provides the optionality of providing notes or labelling comments based on various metadata attributes. All public comments are reviewed in their entirety, and summaries are used to assist with these reviews.","the comment review system (crs) is a system used by the board of governors of the federal reserve system (“board”) to electronically process and manage comments from the public on regulatory?rulemakings, information collections, and other proposals (collectively, “proposals”). the board’s processing of comments may use artificial?intelligence (ai) to provide more efficient processing of public comments (e.g., text matching, entity identification, and text similarity matching).? . to assist the analyst in reviewing each comment, crs uses traditional machine learning natural language processing (nlp) to provide text summarization, text matching with lists of topics, entity identification, and text similarity matching. in addition, crs identifies duplicative or near-duplicative comment letters, provides full text search (including metadata properties), and provides the optionality of providing notes or labelling comments based on various metadata attributes. all public comments are reviewed in their entirety, and summaries are used to assist with these reviews."
Trading Desk Grouping,Board of Governors of the Federal Reserve System,FRB,Division of Supervision and Regulation,Other,None of the above.,Grouping Trading Desk Descriptions,An asset class type for each desk,Implementation and Assessment,Neither,1/1/2024,1/1/2024,4/1/2024,Developed in-house.,Unknown,No,No,No,No,Yes,External - Banking Data,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,Grouping Trading Desk Descriptions . An asset class type for each desk,grouping trading desk descriptions . an asset class type for each desk
Labor Market Analysis,Board of Governors of the Federal Reserve System,FRB,Division of Research and Statistics,Other,None of the above.,Estimates factors in the labor market,Time series measuring the quantity of layoffs,Operation and Maintenance,Neither,6/1/2022,6/1/2022,4/1/2024,Developed in-house.,Unknown,No,No,No,No,Yes,External- SEC Filings,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,Estimates factors in the labor market . Time series measuring the quantity of layoffs,estimates factors in the labor market . time series measuring the quantity of layoffs
Entity Name Matching Tool,Board of Governors of the Federal Reserve System,FRB,Division of Supervision and Regulation,Other,None of the above.,Improved process efficiency,Aids analysts in comparing a reported entity name to a database of legal entity names,Acquisition and/or Development,Neither,8/1/2022,8/1/2022,11/1/2023,Developed in-house.,Unknown,No,No,No,No,No,External- Corporate Structure,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,No,No,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,Improved process efficiency . Aids analysts in comparing a reported entity name to a database of legal entity names,improved process efficiency . aids analysts in comparing a reported entity name to a database of legal entity names
Bank Exam Quality Control,Board of Governors of the Federal Reserve System,FRB,Division of Supervision and Regulation,Mission-Enabling,None of the above.,A traditional machine learning NLP model of bank ratings that utilizes information in news articles among various inputs,Model outcomes are used as a component of quality control,Acquisition and/or Development,Neither,1/1/2022,1/1/2022,Unknown,Developed in-house.,Unknown,No,No,No,No,No,External- News  Feed,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,No – agency does not have access to source code.,Yes,General Support System,Less than 6 months,No,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,A traditional machine learning NLP model of bank ratings that utilizes information in news articles among various inputs . Model outcomes are used as a component of quality control,a traditional machine learning nlp model of bank ratings that utilizes information in news articles among various inputs . model outcomes are used as a component of quality control
Risk Rating Model- Community Banks,Board of Governors of the Federal Reserve System,FRB,Division of Supervision and Regulation,Government Services (includes Benefits and Service Delivery),None of the above.,"To improve upon the risk classification framework used to tier community banks according to risk, and to tailor examination intensity according to risk",Statistical methods are used in the variable selection process for a risk model,Operation and Maintenance,Neither,5/1/2020,5/2/2020,10/30/2023,Developed in-house.,Unknown,No,No,No,No,Yes,"Internal- FFIEC Call Report, Uniform Bank Performance Report","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,Yes,Other,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"To improve upon the risk classification framework used to tier community banks according to risk, and to tailor examination intensity according to risk . Statistical methods are used in the variable selection process for a risk model","to improve upon the risk classification framework used to tier community banks according to risk, and to tailor examination intensity according to risk . statistical methods are used in the variable selection process for a risk model"
Risk Rating Model,Board of Governors of the Federal Reserve System,FRB,Division of Supervision and Regulation,Government Services (includes Benefits and Service Delivery),None of the above.,"Identify a set of factors that may be helpful in predicting  the likelihood that a bank will experience an adverse outcome in the future, and to tier banks into High-, Moderate- and Low-risk tiers for review
",Statistical methods are used in the variable selection process for a risk model,Implementation and Assessment,Neither,10/1/2019,10/2/2019,Unknown,Developed in-house.,Unknown,No,No,No,No,Yes,"Internal- FFIEC Call Report, Uniform Bank Performance Report","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,General Support System,Less than 6 months,Yes,Other,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Identify a set of factors that may be helpful in predicting  the likelihood that a bank will experience an adverse outcome in the future, and to tier banks into High-, Moderate- and Low-risk tiers for review . Statistical methods are used in the variable selection process for a risk model","identify a set of factors that may be helpful in predicting the likelihood that a bank will experience an adverse outcome in the future, and to tier banks into high-, moderate- and low-risk tiers for review . statistical methods are used in the variable selection process for a risk model"
Automatic PSC Classification,Federal Trade Commission,FTC,BCP,Other,None of the above.,"FTC receives uncategorized complaints from multiple channels and contents of comments varies widely by source.

",Product and Service code,Operation and Maintenance,Neither,7/10/1905,7/10/1905,7/11/1905,Developed with contracting resources.,Unknown,No,No,No,Yes,Yes,Consumer Sentinal Network database,Unknown,Unknown,No – agency does not have access to source code.,Yes,Sentinal Network System,Unknown,Yes,Yes,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,FTC receives uncategorized complaints from multiple channels and contents of comments varies widely by source. . Product and Service code,ftc receives uncategorized complaints from multiple channels and contents of comments varies widely by source. . product and service code
Group Duplicate Complaints,Federal Trade Commission,FTC,BCP,Other,None of the above.,"Consumers submit multiple complaints for the same issue using various channels, which impacts the ability of Law Enforcement to quickly search and identify leads.

",Links to duplicate complaints,Operation and Maintenance,Neither,7/10/1905,7/10/1905,7/11/1905,Developed with contracting resources.,Unknown,No,No,No,Yes,Yes,Consumer Sentinal Network database,Unknown,Unknown,No – agency does not have access to source code.,Yes,Sentinal Network System,Unknown,Yes,Yes,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"Consumers submit multiple complaints for the same issue using various channels, which impacts the ability of Law Enforcement to quickly search and identify leads. . Links to duplicate complaints","consumers submit multiple complaints for the same issue using various channels, which impacts the ability of law enforcement to quickly search and identify leads. . links to duplicate complaints"
Chatbot,Federal Trade Commission,FTC,BCP,Government Services (includes Benefits and Service Delivery),None of the above.,Delivers unattended chat 24/7 for Identity Tehft.gov and ReportFraud.gov after business hours. Assisted chat is only available during business hours. ,Chats,Operation and Maintenance,Neither,7/10/1905,7/10/1905,7/11/1905,Developed with contracting resources.,Unknown,No,No,No,Yes,Yes,Consumer Sentinal Network database,Unknown,Unknown,No – agency does not have access to source code.,Yes,Sentinal Network System,Unknown,Yes,Yes,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,Delivers unattended chat 24/7 for Identity Tehft.gov and ReportFraud.gov after business hours. Assisted chat is only available during business hours. . Chats,delivers unattended chat 24/7 for identity tehft.gov and reportfraud.gov after business hours. assisted chat is only available during business hours. . chats
Word Cloud,Federal Trade Commission,FTC,BCP,Other,None of the above.,Uses natural language processing to visually explore complaints using word cloud and assist Law Enforcement to reduce the time required to analyze search results,word cloud,Operation and Maintenance,Neither,7/10/1905,7/10/1905,7/11/1905,Developed with contracting resources.,Unknown,No,No,No,Yes,Yes,Consumer Sentinal Network database,Unknown,Unknown,No – agency does not have access to source code.,Yes,Sentinal Network System,Unknown,Yes,Yes,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,Uses natural language processing to visually explore complaints using word cloud and assist Law Enforcement to reduce the time required to analyze search results . word cloud,uses natural language processing to visually explore complaints using word cloud and assist law enforcement to reduce the time required to analyze search results . word cloud
Graph Analytics,Federal Trade Commission,FTC,BCP,Other,None of the above.,Assist Law Enforcement in identifying connections between complaints,node diagram,Operation and Maintenance,Neither,7/10/1905,7/10/1905,7/11/1905,Developed with contracting resources.,Unknown,No,No,No,Yes,Yes,Consumer Sentinal Network database,Unknown,Unknown,No – agency does not have access to source code.,Yes,Sentinal Network System,Unknown,Yes,Yes,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,Assist Law Enforcement in identifying connections between complaints . node diagram,assist law enforcement in identifying connections between complaints . node diagram
Developer productivity,Federal Trade Commission,FTC,BCP,Other,None of the above.,"FTC is integrating Generative AI (Gen AI), specifically Azure OpenAI GPT-4 (AOI), into our workflow to boost development efficiency. ",Application code,Initiated,Neither,7/14/1905,7/15/1905,7/16/1905,Developed with contracting resources.,Unknown,No,No,No,Yes,Yes,Unknown,Unknown,Unknown,No – agency does not have access to source code.,Yes,Sentinal Network System,Unknown,Yes,Yes,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4126984126984127,"FTC is integrating Generative AI (Gen AI), specifically Azure OpenAI GPT-4 (AOI), into our workflow to boost development efficiency. . Application code","ftc is integrating generative ai (gen ai), specifically azure openai gpt-4 (aoi), into our workflow to boost development efficiency. . application code"
AI Pilot Project to Screen and Flag for Personally Identifiable Information (PII) in Digitized Archival Records,National Archives and Records Administration,NARA,Unknown,Mission-Enabling,Digitizing text from scanned documents or smart forms for archival purposes using AI.,"NARA is using AI to automatically find and remove sensitive personal information from its digitized historical records. This will make the process faster, more accurate, and better protect privacy while allowing for greater public access to these records. The AI will also help NARA manage its vast collection and adapt to future privacy needs.","This AI use case will produce redacted records, a prioritized list for redaction, and a tool for staff to redact unpublished records.",Acquisition and/or Development,Neither,7/15/1905,7/15/1905,TBD,Developed in-house.,Unknown,No,Yes,Yes,Yes,Yes,Agency is strictly using its data to test the pre-trained model for the purpose of prototype work. The data is publicly available at catalog.archives.gov.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,National Archive Catalog (NAC) ,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Planned or in-progress.,,Yes – by another appropriate agency office that was not directly involved in the system’s development,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Unknown,Direct user testing,Unknown,Neither,0.6507936507936508,"NARA is using AI to automatically find and remove sensitive personal information from its digitized historical records. This will make the process faster, more accurate, and better protect privacy while allowing for greater public access to these records. The AI will also help NARA manage its vast collection and adapt to future privacy needs. . This AI use case will produce redacted records, a prioritized list for redaction, and a tool for staff to redact unpublished records.","nara is using ai to automatically find and remove sensitive personal information from its digitized historical records. this will make the process faster, more accurate, and better protect privacy while allowing for greater public access to these records. the ai will also help nara manage its vast collection and adapt to future privacy needs. . this ai use case will produce redacted records, a prioritized list for redaction, and a tool for staff to redact unpublished records."
Freedom of Information Act (FOIA) Discovery AI Pilot,National Archives and Records Administration,NARA,Unknown,Mission-Enabling,Searching for information using AI.,"NARA is piloting the use of AI to improve its responses to Freedom of Information Act requests. This includes AI tools for finding relevant records and automatically redacting sensitive information, which will make the process faster, more accurate, and compliant with privacy laws.",This AI use case will output relevant records matching FOIA requests and redacted versions of those records with sensitive information removed.,Acquisition and/or Development,Neither,7/16/1905,7/16/1905,TBD,Developed in-house.,Unknown,No,Yes,Yes,Yes,Yes,NARA will train this AI use case based on previous FOIA requests that were released to public ,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,TBD,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Planned or in-progress.,,Yes – by another appropriate agency office that was not directly involved in the system’s development,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Unknown,Direct user testing,Unknown,Neither,0.6349206349206349,"NARA is piloting the use of AI to improve its responses to Freedom of Information Act requests. This includes AI tools for finding relevant records and automatically redacting sensitive information, which will make the process faster, more accurate, and compliant with privacy laws. . This AI use case will output relevant records matching FOIA requests and redacted versions of those records with sensitive information removed.","nara is piloting the use of ai to improve its responses to freedom of information act requests. this includes ai tools for finding relevant records and automatically redacting sensitive information, which will make the process faster, more accurate, and compliant with privacy laws. . this ai use case will output relevant records matching foia requests and redacted versions of those records with sensitive information removed."
Auto-fill of Descriptive Metadata for Archival Descriptions ,National Archives and Records Administration,NARA,Unknown,Mission-Enabling,Digitizing text from scanned documents or smart forms for archival purposes using AI.,"NARA is using AI to automatically create descriptions (metadata) for its digital archives. This will save archivists time, make records easier to find, and help the public better understand the information in the National Archives Catalog.","This AI use case will create more complete and informative descriptions for records, making them easier to find in the National Archives Catalog.",Acquisition and/or Development,Neither,7/16/1905,7/16/1905,TBD,Developed in-house.,Unknown,No,Yes,No,Yes,Yes,"The data is publicly available at 
catalog.archives.gov","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Description and Authority Service (DAS),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Planned or in-progress.,,Yes – by another appropriate agency office that was not directly involved in the system’s development,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Unknown,Direct user testing,Unknown,Neither,0.6349206349206349,"NARA is using AI to automatically create descriptions (metadata) for its digital archives. This will save archivists time, make records easier to find, and help the public better understand the information in the National Archives Catalog. . This AI use case will create more complete and informative descriptions for records, making them easier to find in the National Archives Catalog.","nara is using ai to automatically create descriptions (metadata) for its digital archives. this will save archivists time, make records easier to find, and help the public better understand the information in the national archives catalog. . this ai use case will create more complete and informative descriptions for records, making them easier to find in the national archives catalog."
AI based Semantic Search for National Archives Catalog,National Archives and Records Administration,NARA,Unknown,Mission-Enabling,Searching for information using AI.,"NARA is using AI to create a smarter search engine for its online catalog. This new search understands the meaning behind your search, not just the keywords, and can even find hidden connections between documents. This will make research faster, easier, and more insightful.","This AI search tool will provide better search results and show connections between records, making research easier and more insightful.",Acquisition and/or Development,Neither,7/16/1905,7/16/1905,TBD,Developed in-house.,Unknown,No,Yes,No,Yes,Yes,Agency is strictly using its data to test the pre-trained model for the purpose of prototype work. The data is publicly available at catalog.archives.gov.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,ArchisAI,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,,Yes – by another appropriate agency office that was not directly involved in the system’s development,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Unknown,Direct user testing,Unknown,Neither,0.6349206349206349,"NARA is using AI to create a smarter search engine for its online catalog. This new search understands the meaning behind your search, not just the keywords, and can even find hidden connections between documents. This will make research faster, easier, and more insightful. . This AI search tool will provide better search results and show connections between records, making research easier and more insightful.","nara is using ai to create a smarter search engine for its online catalog. this new search understands the meaning behind your search, not just the keywords, and can even find hidden connections between documents. this will make research faster, easier, and more insightful. . this ai search tool will provide better search results and show connections between records, making research easier and more insightful."
Synthetic Data Generation,Pension Benefit Guaranty Corporation,PBGC,Office of Information Technology (OIT) ,Other,None of the above.,"To create artificial data that mimics the features, structures, and statistical attributes of end user/pensioner data","Unlike real data, which may contain sensitive or Personally Identifiable Information (PII), the synthetic data ensures data privacy, while enabling data analysis, research, and software testing",Initiated,Neither,11/1/2022,Unknown,Unknown,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,No,N/A.  Currently conducting market research to identify requirements for acquisition/implementation. ,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,N/a.  Acquisition has not been completed as of yet. ,Unknown,No,No,Unknown,Unknown,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Planned or in-progress.,TBD.  Currently doing market research to assess potential risks involved with deployment. ,Yes – by the CAIO,Unknown,Unknown,Unknown,None of the above,Unknown,Neither,0.6190476190476191,"To create artificial data that mimics the features, structures, and statistical attributes of end user/pensioner data . Unlike real data, which may contain sensitive or Personally Identifiable Information (PII), the synthetic data ensures data privacy, while enabling data analysis, research, and software testing . TBD.  Currently doing market research to assess potential risks involved with deployment.","to create artificial data that mimics the features, structures, and statistical attributes of end user/pensioner data . unlike real data, which may contain sensitive or personally identifiable information (pii), the synthetic data ensures data privacy, while enabling data analysis, research, and software testing . tbd. currently doing market research to assess potential risks involved with deployment."
IT Security Monitoring,Pension Benefit Guaranty Corporation,PBGC,Office of Information Technology (OIT) ,Mission-Enabling,Identifying unusual patterns in system logs from a single incident report using AI.,Analyzing large data sets of end user activity to detect anamolies and potential security events  ,Enhanced detection and reporting of potential IT Security events.,Implementation and Assessment,Neither,9/1/2022,9/1/2022,9/1/2023,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,No,Raw data events to clear and concise anomalies and threats for review and resolution. ,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,Yes,Embedded within PBGC's IT Infrastructure General Support System,6-12 months,Yes,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,No,Risks are associated with noncompliance with NIST 800-53 Rev 5 requirements.  Security control assessments performed to ensure operational itent and desired outcomes.  ,Planned or in-progress,No monitoring protocols have been established: Necessary infrastructure needed to perform monitoring of AI systems in production is not available and/or a plan to monitor models has not yet been established.,No - Some individual decisions or actions require direct human oversight.,Unknown,None of the above,Yes,Neither,0.7619047619047619,Analyzing large data sets of end user activity to detect anamolies and potential security events . Enhanced detection and reporting of potential IT Security events. . Risks are associated with noncompliance with NIST 800-53 Rev 5 requirements.  Security control assessments performed to ensure operational itent and desired outcomes.,analyzing large data sets of end user activity to detect anamolies and potential security events . enhanced detection and reporting of potential it security events. . risks are associated with noncompliance with nist 800-53 rev 5 requirements. security control assessments performed to ensure operational itent and desired outcomes.
Lexis +AI,Postal Regulatory Commission,PRC,OGC,Law & Justice,Improving the quality of written communications using AI tools.,Legal Research,Research,Operation and Maintenance,Neither,7/1/2024,7/1/2024,7/1/2024,Developed with contracting resources.,Sole Source,Yes,No,No,No,Other,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Lexis +AI,Less than 6 months,No,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,No,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,Legal Research . Research,legal research . research
Comment Letter Review and Analysis ,Securities and Exchange Commission,SEC,Office of Information Technology (OIT),Other,Summarizing the key points of a lengthy report using AI.,Enhances the review of public comment letters.,Comment letter summaries.,Implementation and Assessment,Neither,22-Jul,22-Dec,24-Jun,Developed with both contracting and in-house resources.,50310220F0059,No,No,Yes,Yes,Yes,Comment letter data,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Comment Letter Review and Analysis System,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,Enhances the review of public comment letters. . Comment letter summaries.,enhances the review of public comment letters. . comment letter summaries.
Federal Acquisition Research Solution,Securities and Exchange Commission,SEC,Office of Acquisitions (OA),Mission-Enabling,Summarizing the key points of a lengthy report using AI.,"Aggregates and summarizes federal acquisition policy, best practices, and templates for common issues raised for the federal acquisition workforce and stakeholders. This tool will support OA operations, training, and policy formulation.","AI-generated text aggregating and summarizing federal acquisition policy, best practices, and templates for common acquisition issues.",Operation and Maintenance,Neither,24-Aug,24-Aug,24-Aug,Developed with contracting resources.,50310224F0144,No,No,No,Yes,Other,Not Applicable,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Yes,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"Aggregates and summarizes federal acquisition policy, best practices, and templates for common issues raised for the federal acquisition workforce and stakeholders. This tool will support OA operations, training, and policy formulation. . AI-generated text aggregating and summarizing federal acquisition policy, best practices, and templates for common acquisition issues.","aggregates and summarizes federal acquisition policy, best practices, and templates for common issues raised for the federal acquisition workforce and stakeholders. this tool will support oa operations, training, and policy formulation. . ai-generated text aggregating and summarizing federal acquisition policy, best practices, and templates for common acquisition issues."
Human Resources Search,Securities and Exchange Commission,SEC,Office of Human Resources (OHR),Mission-Enabling,Searching for information using AI.,Provides results to staff inquiries. ,Search results identifying knowledge articles and services available.,Operation and Maintenance,Neither,22-Nov,22-Apr,22-Nov,Developed with both contracting and in-house resources.,503102-22-F-0107,No,No,No,Yes,Yes,Knowledge articles related to human resource information,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",No,"Yes – agency has access to source code, but it is not public.",Yes,ServiceNow HR Service Delivery ,Less than 6 months,Yes,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,Provides results to staff inquiries. . Search results identifying knowledge articles and services available.,provides results to staff inquiries. . search results identifying knowledge articles and services available.
8-K Filing Solution,Securities and Exchange Commission,SEC,Division of Corporate Finance (CF),Other,Searching for information using AI.,Searches and extracts information from certain 8-K filings (HTML files and images).,"Extracts of filing data.
",Acquisition and/or Development,Neither,24-Aug,24-Sep,Unknown,Developed in-house.,Unknown,No,Unknown,Unknown,Unknown,Yes,Not Applicable,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,Searches and extracts information from certain 8-K filings (HTML files and images). . Extracts of filing data.,searches and extracts information from certain 8-k filings (html files and images). . extracts of filing data.
Training Conversation Tool ,Securities and Exchange Commission,SEC,Office of Human Resources (OHR),Education & Workforce,Transcribing and summarizing a recorded meeting or interview using AI.,Improves communication and collaboration skills of the SEC workforce,Feedback on collaboration and communication.,Implementation and Assessment,Neither,24-Nov,24-Nov,24-Nov,Developed with contracting resources.,NNG15SC71B-50310224F0140,No,No,No,Yes,Other,Not Applicable,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Yes,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,Improves communication and collaboration skills of the SEC workforce . Feedback on collaboration and communication.,improves communication and collaboration skills of the sec workforce . feedback on collaboration and communication.
Create Work-Related Videos,Securities and Exchange Commission,SEC,Office of Human Resources (OHR),Other,Creating visually appealing presentations using AI-driven design suggestions.,Allows staff to make and edit work-related videos. ,"A video is created, which may be edited.",Operation and Maintenance,Neither,24-Nov,23-Jun,24-May,Developed with contracting resources.,47QTCA21D007B/50310223F0119/P00001,No,No,No,Yes,Other,Not Applicable,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,Yes,Vyond,Less than 6 months,Yes,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6190476190476191,"Allows staff to make and edit work-related videos. . A video is created, which may be edited.","allows staff to make and edit work-related videos. . a video is created, which may be edited."
CF Staff Review Comments ,Securities and Exchange Commission,SEC,Division of Corporate Finance (CF),Other,Searching for information using AI.,Classifies staff comments and improve CF's review process.,SEC staff comment categories,Acquisition and/or Development,Neither,24-Sep,24-Sep,Unknown,Developed in-house.,Unknown,No,Unknown,Unknown,Unknown,Yes,Filings review data,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Unknown,Unknown,Yes,DREAM - Disclosure Review Automation,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,Classifies staff comments and improve CF's review process. . SEC staff comment categories,classifies staff comments and improve cf's review process. . sec staff comment categories
EDGAR Filing Certification Review Solution ,Securities and Exchange Commission,SEC,Office of Information Technology (OIT),Other,Inputting large amounts of data from paper forms into a digital system using AI.,Identifies potentially non-compliant 302 certifications of EDGAR filings.,Tables containing potentially non-compliant filing data.,Implementation and Assessment,Neither,23-Jan,23-Apr,23-Dec,Developed with contracting resources.,503102-20-F-0059,No,No,Yes,Yes,Yes,EDGAR filing data,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,AI Disclosure Analytics (AIDA),Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,Identifies potentially non-compliant 302 certifications of EDGAR filings. . Tables containing potentially non-compliant filing data.,identifies potentially non-compliant 302 certifications of edgar filings. . tables containing potentially non-compliant filing data.
EDGAR Filing Audit Report Solution ,Securities and Exchange Commission,SEC,Office of Information Technology (OIT),Other,Inputting large amounts of data from paper forms into a digital system using AI.,Utilizing a tool to identify audit reports included in EDGAR filings that are potentially non-compliant with SEC Rules and Regulations.,Tables containing potentially non-compliant filing data.,Implementation and Assessment,Neither,24-Feb,24-Apr,25-Feb,Developed with contracting resources.,503102-20-F-0059,No,No,Yes,Yes,Yes,EDGAR filing data,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,AIDA,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,Utilizing a tool to identify audit reports included in EDGAR filings that are potentially non-compliant with SEC Rules and Regulations. . Tables containing potentially non-compliant filing data.,utilizing a tool to identify audit reports included in edgar filings that are potentially non-compliant with sec rules and regulations. . tables containing potentially non-compliant filing data.
Public Filing Disclosure Review,Securities and Exchange Commission,SEC,Office of the Chief Data Officer (OCDO),Other,None of the above.,"Summarizes public filing disclosures and identifying differences and similarities between disclosures.
",Summaries of disclosures and answers to user-selected questions.,Implementation and Assessment,Neither,24-Aug,24-Aug,24-Oct,Developed with contracting resources.,50310224F0089,No,No,No,Yes,Other,Not Applicable,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Yes,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,Summarizes public filing disclosures and identifying differences and similarities between disclosures. . Summaries of disclosures and answers to user-selected questions.,summarizes public filing disclosures and identifying differences and similarities between disclosures. . summaries of disclosures and answers to user-selected questions.
EDGAR Filing Signature Review Solution ,Securities and Exchange Commission,SEC,Office of Information Technology (OIT),Other,Inputting large amounts of data from paper forms into a digital system using AI.,Identifies when required signatures are potentially missing in EDGAR filings.,Tables containing potentially non-compliant filing data.,Implementation and Assessment,Neither,23-Oct,23-Dec,25-Feb,Developed with contracting resources.,503102-20-F-0059,No,No,Yes,Yes,Yes,EDGAR filing data,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,AIDA,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,Identifies when required signatures are potentially missing in EDGAR filings. . Tables containing potentially non-compliant filing data.,identifies when required signatures are potentially missing in edgar filings. . tables containing potentially non-compliant filing data.
EDGAR Filing Risk Disclosure Review Solution ,Securities and Exchange Commission,SEC,Office of Information Technology (OIT),Other,Inputting large amounts of data from paper forms into a digital system using AI.,Analyzes earnings call transcripts and EDGAR filing disclosure of risk factors.,Table of summarized earnings call data and EDGAR filing comparison.,Implementation and Assessment,Neither,23-Dec,23-Dec,25-Feb,Developed with contracting resources.,503102-20-F-0059,No,No,Yes,Yes,Yes,EDGAR filing data,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,AIDA,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,Analyzes earnings call transcripts and EDGAR filing disclosure of risk factors. . Table of summarized earnings call data and EDGAR filing comparison.,analyzes earnings call transcripts and edgar filing disclosure of risk factors. . table of summarized earnings call data and edgar filing comparison.
Name Matching,Securities and Exchange Commission,SEC,Division of Examinations (EXAMS),Other,None of the above.,Identifies certain similarities in names across data sources. This allows staff to more easily compare information across data sources. ,The system outputs a number which measures how closely names match.,Operation and Maintenance,Neither,18-Feb,18-Jan,18-Feb,Developed with both contracting and in-house resources.,SECHQ16A0013,No,No,Yes,Yes,Other,Registrant data,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,NEAT,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,Identifies certain similarities in names across data sources. This allows staff to more easily compare information across data sources. . The system outputs a number which measures how closely names match.,identifies certain similarities in names across data sources. this allows staff to more easily compare information across data sources. . the system outputs a number which measures how closely names match.
Parsing of Plain Language Descriptions to Machine-Readable Notations,Securities and Exchange Commission,SEC,Division of Examinations (EXAMS),Other,None of the above.,Automates conversion of plain-language text into machine-readable standardized notations.  This permits ease of analysis.,The system outputs machine-readable notations.,Operation and Maintenance,Neither,22-Feb,21-Oct,22-Feb,Developed with both contracting and in-house resources.,50310220D0012,No,No,No,Yes,Other,Registrant data,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,NEAT,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,Automates conversion of plain-language text into machine-readable standardized notations.  This permits ease of analysis. . The system outputs machine-readable notations.,automates conversion of plain-language text into machine-readable standardized notations. this permits ease of analysis. . the system outputs machine-readable notations.
Identification of Potentially Manipulative Trading Activity in Registrant Data,Securities and Exchange Commission,SEC,Division of Examinations (EXAMS),Other,Searching for information using AI.,Identifies potentially manipulative trading activity for further review by an examination team.,The system identifies activity of interest.,Acquisition and/or Development,Neither,22-Mar,21-May,Unknown,Developed with both contracting and in-house resources.,50310220D0012,No,Unknown,Unknown,Unknown,Yes,Registrant data,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Unknown,Unknown,Yes,HALSI,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Identifies potentially manipulative trading activity for further review by an examination team. . The system identifies activity of interest.,identifies potentially manipulative trading activity for further review by an examination team. . the system identifies activity of interest.
Using Machine Learning/Artificial Intelligence Techniques to Predict Entities With Certain Risk Characteristics,Securities and Exchange Commission,SEC,Division of Examinations (EXAMS),Other,None of the above.,Analyze data to predict entities with certain risk characteristics. Staff may consider the results of these efforts in their examination efforts. ,This system provides predictive information.  ,Operation and Maintenance,Neither,14-Jan,14-Jan,15-Apr,Developed with both contracting and in-house resources.,SECHQ113A0004,No,No,No,Yes,Yes,Filings and examination data,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,EDW,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,Analyze data to predict entities with certain risk characteristics. Staff may consider the results of these efforts in their examination efforts. . This system provides predictive information.,analyze data to predict entities with certain risk characteristics. staff may consider the results of these efforts in their examination efforts. . this system provides predictive information.
Developed Parsing Logic for Filings ,Securities and Exchange Commission,SEC,Division of Examinations (EXAMS),Other,None of the above.,Help staff more efficiently search and analyze disclosure text contained in various filings.,These efforts will help to search and analyze certain filings.,Operation and Maintenance,Neither,20-Jan,23-May,24-Jul,Developed with both contracting and in-house resources.,50310219D0008,No,No,No,Yes,Yes,Filings,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,AI Filing Parser,More than 12 months,Yes,Yes,No,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,Help staff more efficiently search and analyze disclosure text contained in various filings. . These efforts will help to search and analyze certain filings.,help staff more efficiently search and analyze disclosure text contained in various filings. . these efforts will help to search and analyze certain filings.
Identification of Potentially Manipulative Activity in Certain Accounts,Securities and Exchange Commission,SEC,Division of Examinations (EXAMS),Other,Searching for information using AI.,Identifies accounts with certain activity of interest for further review by an examination team.,The system identifies activity of interest.,Implementation and Assessment,Neither,21-Oct,20-Oct,24-May,Developed with both contracting and in-house resources.,50310220D0012,No,No,No,Yes,Yes,Registrant and market data,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,NEAT,6-12 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,Identifies accounts with certain activity of interest for further review by an examination team. . The system identifies activity of interest.,identifies accounts with certain activity of interest for further review by an examination team. . the system identifies activity of interest.
Using AI to Help with Coding-Related Tasks/Questions,Securities and Exchange Commission,SEC,Division of Enforcement (ENF); Division of Economic and Risk Analysis (DERA),Education & Workforce,None of the above.,"Helps staff with coding-related tasks, for a subset of supported languages. Staff can also use this as an educational tool to learn how to code more efficiently.","It outputs code responsive to the user’s request. It is textual, non-self-executing, and typically has non-code text explaining the logic behind the code and how to use the code.",Operation and Maintenance,Neither,24-Jan,24-Jan,24-Jun,Developed in-house.,Unknown,No,No,No,Yes,Yes,Not Applicable,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,No – agency does not have access to source code.,Yes,SEARCH,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"Helps staff with coding-related tasks, for a subset of supported languages. Staff can also use this as an educational tool to learn how to code more efficiently. . It outputs code responsive to the user’s request. It is textual, non-self-executing, and typically has non-code text explaining the logic behind the code and how to use the code.","helps staff with coding-related tasks, for a subset of supported languages. staff can also use this as an educational tool to learn how to code more efficiently. . it outputs code responsive to the user’s request. it is textual, non-self-executing, and typically has non-code text explaining the logic behind the code and how to use the code."
Optical Character Recognition (OCR) for Records Analysis,Securities and Exchange Commission,SEC,Division of Enforcement (ENF),Other,Digitizing text from scanned documents or smart forms for archival purposes using AI.,Assists ENF staff in extracting relevant information from digital image files. This will accelerate associated ENF investigations.,A text string of all text recognized from the associated image. ,Operation and Maintenance,Neither,24-Mar,24-Mar,24-Oct,Developed with both contracting and in-house resources.,50310220D0012_50310221F0160 ,No,No,Yes,Yes,Yes,Image data,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,Yes,SEARCH,Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,Assists ENF staff in extracting relevant information from digital image files. This will accelerate associated ENF investigations. . A text string of all text recognized from the associated image.,assists enf staff in extracting relevant information from digital image files. this will accelerate associated enf investigations. . a text string of all text recognized from the associated image.
Microsoft Copilot Integration,United States Commission on Civil Rights,USCCR,ASCD/IT,Mission-Enabling,Summarizing the key points of a lengthy report using AI.,Efficiency and improved administrative tasks.,Text-based summaries and AI generated text. ,Operation and Maintenance,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,No – Agency did not request an extension for this use case.,No,AI is not safety or rights-impacting.,AI is not safety or rights-impacting.,AI is not safety or rights-impacting.,AI is not safety or rights-impacting.,AI is not safety or rights-impacting.,None of the above,No – it is not operationally practical to offer this.,Neither,0.4126984126984127,Efficiency and improved administrative tasks. . Text-based summaries and AI generated text. . AI is not safety or rights-impacting.,efficiency and improved administrative tasks. . text-based summaries and ai generated text. . ai is not safety or rights-impacting.
Lookout for mobile security,United States Trade and Development Agency,USTDA,Unknown,Mission-Enabling,None of the above.,"Lookout uses AI and machine learning to detect and respond to cyberattacks in real time. Lookout's threat intelligence services include data forensics, incident response, and research.",A Dashboard used for monitoring activity and reports,Operation and Maintenance,Neither,12/1/2022,5/1/2023,7/1/2023,Developed with contracting resources.,Shared Service through DHS CISA's CDM Program,No,No,No,No,No,"Not applicable. The Lookout software is a commercial-off-the-shelf platform, which is provided to USTDA by the Continuous Diagnostic and Mitigation Program under the Department of Homeland Security, Cybersecurity and Infrastructure Security Agency. The platform is one of several DHS/CISA services contracted by USTDA for security.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,Yes,Lookout (FEDRAMP) PK ID F1603297883,Less than 6 months,Other,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Lookout uses AI and machine learning to detect and respond to cyberattacks in real time. Lookout's threat intelligence services include data forensics, incident response, and research. . A Dashboard used for monitoring activity and reports","lookout uses ai and machine learning to detect and respond to cyberattacks in real time. lookout's threat intelligence services include data forensics, incident response, and research. . a dashboard used for monitoring activity and reports"
Qualys Cloud Detection and Response,United States Trade and Development Agency,USTDA,Unknown,Mission-Enabling,None of the above.,"Qualys uses AI to detect threats in real time, such as malware, ransomware, and unauthorized access.",A Dashboard used for monitoring activity and reports; real-time alerts when issues are discovered.,Operation and Maintenance,Neither,7/1/2021,11/1/2021,12/1/2021,Developed with contracting resources.,Shared Service through DHS CISA's CDM Program,No,No,No,No,No,"Not applicable. The Qualys tool is a commercial-off-the-shelf platform, which is provided to USTDA by the Continuous Diagnostic and Mitigation Program under the Department of Homeland Security, Cybersecurity and Infrastructure Security Agency. The platform is one of several DHS/CISA services contracted by USTDA for security.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,Yes,Qualys (FEDRAMP) PK ID F1508207205,Less than 6 months,Other,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Qualys uses AI to detect threats in real time, such as malware, ransomware, and unauthorized access. . A Dashboard used for monitoring activity and reports; real-time alerts when issues are discovered.","qualys uses ai to detect threats in real time, such as malware, ransomware, and unauthorized access. . a dashboard used for monitoring activity and reports; real-time alerts when issues are discovered."
Crowdstrike Falcon Threat Detection and Response,United States Trade and Development Agency,USTDA,Unknown,Mission-Enabling,None of the above.,"Crowdstrike uses AI to detect and respond to threats in real-time, such as malware, ransomware, and unauthorized access.",A Dashboard used for monitoring activity and reports; real-time alerts when issues are discovered.,Operation and Maintenance,Neither,7/1/2021,11/1/2021,12/1/2021,Developed with contracting resources.,Shared Service through DHS CISA's CDM Program,No,No,No,No,No,"Not applicable. The Crowdstrike tool is a commercial-off-the-shelf platform, which is provided to USTDA by the Continuous Diagnostic and Mitigation Program under the Department of Homeland Security, Cybersecurity and Infrastructure Security Agency. The platform is one of several DHS/CISA services contracted by USTDA for security.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,Yes,Crowdstrike (FEDRAMP) PK ID FR1807853629,Less than 6 months,Other,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Crowdstrike uses AI to detect and respond to threats in real-time, such as malware, ransomware, and unauthorized access. . A Dashboard used for monitoring activity and reports; real-time alerts when issues are discovered.","crowdstrike uses ai to detect and respond to threats in real-time, such as malware, ransomware, and unauthorized access. . a dashboard used for monitoring activity and reports; real-time alerts when issues are discovered."
Exiger Due Diligence IQ (“DDIQ”) Research Software,United States Trade and Development Agency,USTDA,Unknown,Mission-Enabling,Searching for information using AI.,"The AI utilized by Exiger’s team of analysts improves the efficiency of their research efforts into individuals and companies under consideration for partnership with USTDA by reviewing and consolidating relevant publicly available information, such as news reports.","Exiger’s DDIQ platform produces consolidated publicly available data on searched subjects for review, refinement, and analysis by Exiger’s due diligence and research professionals.",Operation and Maintenance,Neither,2/24/2024,3/1/2024,5/1/2024,Developed with contracting resources.,1131PL24CCP91094 ,No,No,No,No,No,"Not applicable. The DDIQ software is a proprietary product developed by Exiger as a commercial-off-the-shelf platform, which is available to purchase by anyone in the general public and has not been modified for government use. The DDIQ platform is one of several Exiger services contracted by USTDA for due diligence.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,Yes,Exiger Insight 3PM (FEDRAMP) PK ID FR2122140784,Less than 6 months,No,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The AI utilized by Exiger’s team of analysts improves the efficiency of their research efforts into individuals and companies under consideration for partnership with USTDA by reviewing and consolidating relevant publicly available information, such as news reports. . Exiger’s DDIQ platform produces consolidated publicly available data on searched subjects for review, refinement, and analysis by Exiger’s due diligence and research professionals.","the ai utilized by exiger’s team of analysts improves the efficiency of their research efforts into individuals and companies under consideration for partnership with ustda by reviewing and consolidating relevant publicly available information, such as news reports. . exiger’s ddiq platform produces consolidated publicly available data on searched subjects for review, refinement, and analysis by exiger’s due diligence and research professionals."
Evolv WDS - OSSO,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Law & Justice,Creating visual representations of data sets for reports and presentations using AI.,Using heat map technology to identify heat signature of known weapons.,Areas of interest for a heat map that include objects that would produce heat indicators aligned with heat signatures of known types of different weapons.,Operation and Maintenance,Both,10/1/2022,11/1/2022,2/1/2023,Developed with contracting resources.,36C25023P0332 ,No,No,No,No,No,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,Using heat map technology to identify heat signature of known weapons. . Areas of interest for a heat map that include objects that would produce heat indicators aligned with heat signatures of known types of different weapons.,using heat map technology to identify heat signature of known weapons. . areas of interest for a heat map that include objects that would produce heat indicators aligned with heat signatures of known types of different weapons.
Avigilon Camera/Search Function - OSSO,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Law & Justice,Searching for information using AI.,A search function in a security video surveillance system that search's for points of interest in the video.,Video or picture of requested searches from the video surveillance systems.,Operation and Maintenance,Both,3/1/2022,8/1/2022,10/1/2022,Developed with contracting resources.,Unknown,No,No,Yes,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.49206349206349204,A search function in a security video surveillance system that search's for points of interest in the video. . Video or picture of requested searches from the video surveillance systems.,a search function in a security video surveillance system that search's for points of interest in the video. . video or picture of requested searches from the video surveillance systems.
CareCentra Next Level Personalized AI Health Coach,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"This is an evidence based, randomized control trialed AI solution that focuses on precision nudging.  It is intended to improve quality of care and reduce view alerts.  Its focus is on identifying personalized behavioral change algorithms for each veteran to identify timing, channel, ",,Implementation and Assessment,Neither,1/1/2024,1/1/2024,5/1/2024,Developed with both contracting and in-house resources.,CRADA: Cooperative research and development agreement,Yes,No,Yes,Yes,Other,not applicable,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Arches,Less than 6 months,Yes,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5714285714285714,"This is an evidence based, randomized control trialed AI solution that focuses on precision nudging.  It is intended to improve quality of care and reduce view alerts.  Its focus is on identifying personalized behavioral change algorithms for each veteran to identify timing, channel,","this is an evidence based, randomized control trialed ai solution that focuses on precision nudging. it is intended to improve quality of care and reduce view alerts. its focus is on identifying personalized behavioral change algorithms for each veteran to identify timing, channel,"
SafePointe WDS - OSSO,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Law & Justice,Creating visual representations of data sets for reports and presentations using AI.,Heat map detection for weapon detection software.,Areas of interest for a heat map that include objects that would produce heat indicators aligned with heat signatures of known types of different weapons.,Retired,Both,6/23/2023,6/27/2022,2/1/2024,Developed with contracting resources.,Unknown,No,No,No,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.49206349206349204,Heat map detection for weapon detection software. . Areas of interest for a heat map that include objects that would produce heat indicators aligned with heat signatures of known types of different weapons.,heat map detection for weapon detection software. . areas of interest for a heat map that include objects that would produce heat indicators aligned with heat signatures of known types of different weapons.
Verkada Camera - OSSO,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Law & Justice,Searching for information using AI.,Video search function inside a physical security camera solution.,Video and images from the AI searches inside the physical security camera solutions.,Acquisition and/or Development,Both,5/1/2024,9/1/2024,Unknown,Developed with contracting resources.,Unknown,No,No,Yes,No,Yes,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.4603174603174603,Video search function inside a physical security camera solution. . Video and images from the AI searches inside the physical security camera solutions.,video search function inside a physical security camera solution. . video and images from the ai searches inside the physical security camera solutions.
Axon Body Camera and DMS - OSSO,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Law & Justice,Searching for information using AI.,"To be able to search the data repository with better results, better redaction abilities for certain capabilities and on the Fleet System only to have better license plate recognition. ","To have better outputs in searching the systems data repository for case files, evidence locker, and DMS; to have a better result when doing the redaction of evidence, files, videos and photos; and on the Fleet system only to have better results in identifying licenses plates in the licenses plate recognition part of the system.",Operation and Maintenance,Both,9/1/2020,9/26/2022,1/3/2023,Developed with contracting resources.,Unknown,No,No,Yes,No,Yes,Unknown,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",No,No – agency does not have access to source code.,Yes,Axon,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,"To be able to search the data repository with better results, better redaction abilities for certain capabilities and on the Fleet System only to have better license plate recognition. . To have better outputs in searching the systems data repository for case files, evidence locker, and DMS; to have a better result when doing the redaction of evidence, files, videos and photos; and on the Fleet system only to have better results in identifying licenses plates in the licenses plate recognition part of the system.","to be able to search the data repository with better results, better redaction abilities for certain capabilities and on the fleet system only to have better license plate recognition. . to have better outputs in searching the systems data repository for case files, evidence locker, and dms; to have a better result when doing the redaction of evidence, files, videos and photos; and on the fleet system only to have better results in identifying licenses plates in the licenses plate recognition part of the system."
Smart AI Bot Assistant,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"A machine learning and natural language processing algorithm will be trained to provide real-time prompts of resources to improve the quality of crisis calls for the Veterans Crisis Line (VCL). Curated, vetted resources will be prompted by keywords for responders to consider during a crisis call. Natural language processing will be used to listen and document during and after the call, organizing the documentation to include topic areas and key points. This will decrease the time spent on documentation, improve the responder workflow, and provide needed downtime for responders to debrief with colleagues and take breaks. Additionally, the chat bot could detect the need for supervisory support during a call, improving real-time communication between a VCL responder and their supervisor. The use cases for the AI will be reviewed by VA experts and subject matter experts, with the draft use cases requiring technical steps to build, pilot test, and refine.","Smart Responder Chat Bot Use Case #1 – Real time resources for VCL responders   Using machine learning and natural language processing an algorithm will be trained based on feedback from the Veterans Crisis Line staff and crisis line responders to provide real-time prompts of resources to improve the quality of the crisis call. Curated, vetted resources will be prompted by key words for the responder to consider using during a crisis call.   Smart Responder Chat Bot Use Case #2 – AI documentation for VCL responders   To improve the time each Veterans Crisis Line responder spends on documentation during and after a crisis call, natural language processing will be used to listen and document during and after the call. The documentation will be organized to include the topic areas and key points required in documentation and provide a final draft editable by the responder. This use case will; decrease the time spent by responders on documentation, improve the responder workflow for VCL operations, and provide needed downtime away from both answering calls and documentation for responder to debrief with colleagues, accomplish other tasks and take necessary breaks to improve overall responder wellbeing.   Smart Responder Chat Bot Use Case #3 – AI for supervisory support during a call   Using machine learning and natural language processing including sentiment analysis, a chat bot could detect the need for a responder to seek supervisory support. Improving the real-time communication between a VCL responder and their supervisor during a call that is challenging (eg. Inappropriate language, escalating rapidly, etc.) could be a useful use case.",Initiated,"Rights-Impacting
",9/19/2024,Unknown,Unknown,Developed with both contracting and in-house resources.,Unknown,Yes,No,No,No,Other,TBD,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,6-12 months,Other,Other,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Rights-Impacting,0.5396825396825397,"A machine learning and natural language processing algorithm will be trained to provide real-time prompts of resources to improve the quality of crisis calls for the Veterans Crisis Line (VCL). Curated, vetted resources will be prompted by keywords for responders to consider during a crisis call. Natural language processing will be used to listen and document during and after the call, organizing the documentation to include topic areas and key points. This will decrease the time spent on documentation, improve the responder workflow, and provide needed downtime for responders to debrief with colleagues and take breaks. Additionally, the chat bot could detect the need for supervisory support during a call, improving real-time communication between a VCL responder and their supervisor. The use cases for the AI will be reviewed by VA experts and subject matter experts, with the draft use cases requiring technical steps to build, pilot test, and refine. . Smart Responder Chat Bot Use Case #1 – Real time resources for VCL responders   Using machine learning and natural language processing an algorithm will be trained based on feedback from the Veterans Crisis Line staff and crisis line responders to provide real-time prompts of resources to improve the quality of the crisis call. Curated, vetted resources will be prompted by key words for the responder to consider using during a crisis call.   Smart Responder Chat Bot Use Case #2 – AI documentation for VCL responders   To improve the time each Veterans Crisis Line responder spends on documentation during and after a crisis call, natural language processing will be used to listen and document during and after the call. The documentation will be organized to include the topic areas and key points required in documentation and provide a final draft editable by the responder. This use case will; decrease the time spent by responders on documentation, improve the responder workflow for VCL operations, and provide needed downtime away from both answering calls and documentation for responder to debrief with colleagues, accomplish other tasks and take necessary breaks to improve overall responder wellbeing.   Smart Responder Chat Bot Use Case #3 – AI for supervisory support during a call   Using machine learning and natural language processing including sentiment analysis, a chat bot could detect the need for a responder to seek supervisory support. Improving the real-time communication between a VCL responder and their supervisor during a call that is challenging (eg. Inappropriate language, escalating rapidly, etc.) could be a useful use case.","a machine learning and natural language processing algorithm will be trained to provide real-time prompts of resources to improve the quality of crisis calls for the veterans crisis line (vcl). curated, vetted resources will be prompted by keywords for responders to consider during a crisis call. natural language processing will be used to listen and document during and after the call, organizing the documentation to include topic areas and key points. this will decrease the time spent on documentation, improve the responder workflow, and provide needed downtime for responders to debrief with colleagues and take breaks. additionally, the chat bot could detect the need for supervisory support during a call, improving real-time communication between a vcl responder and their supervisor. the use cases for the ai will be reviewed by va experts and subject matter experts, with the draft use cases requiring technical steps to build, pilot test, and refine. . smart responder chat bot use case #1 – real time resources for vcl responders using machine learning and natural language processing an algorithm will be trained based on feedback from the veterans crisis line staff and crisis line responders to provide real-time prompts of resources to improve the quality of the crisis call. curated, vetted resources will be prompted by key words for the responder to consider using during a crisis call. smart responder chat bot use case #2 – ai documentation for vcl responders to improve the time each veterans crisis line responder spends on documentation during and after a crisis call, natural language processing will be used to listen and document during and after the call. the documentation will be organized to include the topic areas and key points required in documentation and provide a final draft editable by the responder. this use case will; decrease the time spent by responders on documentation, improve the responder workflow for vcl operations, and provide needed downtime away from both answering calls and documentation for responder to debrief with colleagues, accomplish other tasks and take necessary breaks to improve overall responder wellbeing. smart responder chat bot use case #3 – ai for supervisory support during a call using machine learning and natural language processing including sentiment analysis, a chat bot could detect the need for a responder to seek supervisory support. improving the real-time communication between a vcl responder and their supervisor during a call that is challenging (eg. inappropriate language, escalating rapidly, etc.) could be a useful use case."
Nursing Proficiency Coach & Nurse Proficiency Evaluator...and beyond!,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Education & Workforce,None of the above.,"Two custom GPTs have been created: Nurse Proficiency Writer and Nursing Proficiency Coach.  A GS version is in development.  The Nurse GPTs are used to compare the nurse's self-input against 600+ functional statements and standards (externally accessible or approved by the Office of Nursing Service).  It evaluates the nurse's performance against the standards, reports on positive findings, identifies weak areas, provides an estimated rating, and grade assessment.  The interactive GPT allows nurses to have a dialog to better understand how their practice can be improved.   Many of our 90,000+ RNs are not proficient in understanding the standards nor adept in writing performance reports. This leads to end-user frustration, quality issues with content, submission gaps compared to the standard, interrater reliability variances, and a host of other challenges with the our current process. The tool aids the nurses in developing the content for their ePerformance submission.  No PHI is allowed in nursing proficiencies (by standard), and PHII is scrubbed prior to submission by the nurse. ","
The output takes the nurse's input and returns a summary of each practice dimension and positive accomplishments offering suggestions where the nurse can improve.  The tool can help the nurse in generating a written summary for ePerformance submission.  This is a tremendous time saver and improves the quality of their submission.  Most importantly, the Custom GPT does not impact the staff rating.  It is a tool is designed to help the nurse.  The nurse is ultimately responsible for the content and final submission (data accuracy and validity), and the proficiency evaluations are the responsibility of the Supervisor with knowledge regarding the staffs performance.  The improvment in content can greatly assist the Supervisor in their evaluation process (win-win).      
",Implementation and Assessment,Neither,8/20/2024,8/27/2024,10/5/2024,Developed in-house.,Unknown,No,No,No,No,No,"
Nursing Functional Statements
","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,"Custom ChatGPT paid for by owner, external to VA. ",Less than 6 months,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Two custom GPTs have been created: Nurse Proficiency Writer and Nursing Proficiency Coach.  A GS version is in development.  The Nurse GPTs are used to compare the nurse's self-input against 600+ functional statements and standards (externally accessible or approved by the Office of Nursing Service).  It evaluates the nurse's performance against the standards, reports on positive findings, identifies weak areas, provides an estimated rating, and grade assessment.  The interactive GPT allows nurses to have a dialog to better understand how their practice can be improved.   Many of our 90,000+ RNs are not proficient in understanding the standards nor adept in writing performance reports. This leads to end-user frustration, quality issues with content, submission gaps compared to the standard, interrater reliability variances, and a host of other challenges with the our current process. The tool aids the nurses in developing the content for their ePerformance submission.  No PHI is allowed in nursing proficiencies (by standard), and PHII is scrubbed prior to submission by the nurse. . The output takes the nurse's input and returns a summary of each practice dimension and positive accomplishments offering suggestions where the nurse can improve.  The tool can help the nurse in generating a written summary for ePerformance submission.  This is a tremendous time saver and improves the quality of their submission.  Most importantly, the Custom GPT does not impact the staff rating.  It is a tool is designed to help the nurse.  The nurse is ultimately responsible for the content and final submission (data accuracy and validity), and the proficiency evaluations are the responsibility of the Supervisor with knowledge regarding the staffs performance.  The improvment in content can greatly assist the Supervisor in their evaluation process (win-win).","two custom gpts have been created: nurse proficiency writer and nursing proficiency coach. a gs version is in development. the nurse gpts are used to compare the nurse's self-input against 600+ functional statements and standards (externally accessible or approved by the office of nursing service). it evaluates the nurse's performance against the standards, reports on positive findings, identifies weak areas, provides an estimated rating, and grade assessment. the interactive gpt allows nurses to have a dialog to better understand how their practice can be improved. many of our 90,000+ rns are not proficient in understanding the standards nor adept in writing performance reports. this leads to end-user frustration, quality issues with content, submission gaps compared to the standard, interrater reliability variances, and a host of other challenges with the our current process. the tool aids the nurses in developing the content for their eperformance submission. no phi is allowed in nursing proficiencies (by standard), and phii is scrubbed prior to submission by the nurse. . the output takes the nurse's input and returns a summary of each practice dimension and positive accomplishments offering suggestions where the nurse can improve. the tool can help the nurse in generating a written summary for eperformance submission. this is a tremendous time saver and improves the quality of their submission. most importantly, the custom gpt does not impact the staff rating. it is a tool is designed to help the nurse. the nurse is ultimately responsible for the content and final submission (data accuracy and validity), and the proficiency evaluations are the responsibility of the supervisor with knowledge regarding the staffs performance. the improvment in content can greatly assist the supervisor in their evaluation process (win-win)."
GE Portable Critical Care Suite 2.x,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf18/K183182.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf18/K183182.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf18/k183182.pdf).
Billing Claims Prediction,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,The intended purpose of the AI is to support Revenue Operations Billing activities by providing a propensity to bill flag based on previous user activity in the Revenue Operations Workflow Tool. The output of the AI is strictly intended to be used for decision support for users who are determining whether an encounter is billable or not. ,"The model was trained on Third Party Community Care Billing and Facility Revenue Activity Codes 10, 30, 31, 32, 33, 34, 35 applied from 2021-2023. Initial run at Florida Caribbean Consolidated Patient Account Center (FCCPAC) was trained on 1.2 Million accounts. By training the model on previous activity data, an account can be classified as likely to have a billable or non-billable outcome.",Implementation and Assessment,Neither,2/1/2024,2/1/2024,6/14/2024,Developed with contracting resources.,VA118-16-D-1006 36C10B22N10060040,No,No,Yes,Yes,Yes,"1.2 million claims trained from Community Care Claims Data from  Community Care Reimbursement System (CCRS), Electronic Claims Adjudication Management System (eCAMS) and VACS sourced via Central Data Warehouse (CDW).","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Revenue Operations Workflow Tool - RPA,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"The intended purpose of the AI is to support Revenue Operations Billing activities by providing a propensity to bill flag based on previous user activity in the Revenue Operations Workflow Tool. The output of the AI is strictly intended to be used for decision support for users who are determining whether an encounter is billable or not. . The model was trained on Third Party Community Care Billing and Facility Revenue Activity Codes 10, 30, 31, 32, 33, 34, 35 applied from 2021-2023. Initial run at Florida Caribbean Consolidated Patient Account Center (FCCPAC) was trained on 1.2 Million accounts. By training the model on previous activity data, an account can be classified as likely to have a billable or non-billable outcome.","the intended purpose of the ai is to support revenue operations billing activities by providing a propensity to bill flag based on previous user activity in the revenue operations workflow tool. the output of the ai is strictly intended to be used for decision support for users who are determining whether an encounter is billable or not. . the model was trained on third party community care billing and facility revenue activity codes 10, 30, 31, 32, 33, 34, 35 applied from 2021-2023. initial run at florida caribbean consolidated patient account center (fccpac) was trained on 1.2 million accounts. by training the model on previous activity data, an account can be classified as likely to have a billable or non-billable outcome."
ScienceLogic Artificial Intelligence Operations (AIOPS) Software Subscriptions with Maintenance and Professional Support Services.,Department of Veterans Affairs,VA,OIT: Office of Information & Technology,Other,None of the above.,Performance monitor of cloud applications ,"The SL1 AIOps solution enables VA to  diagnose reasons for outages and rapid capability to respond, resolve and prevent such  outages across the full domain.",Operation and Maintenance,Neither,9/24/2020,9/24/2020,9/24/2020,Developed with both contracting and in-house resources.,NNG15SC91B/36C10B20F0417,No,No,No,Yes,No,N/a,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,VA Enterprise Cloud ,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6349206349206349,"Performance monitor of cloud applications . The SL1 AIOps solution enables VA to  diagnose reasons for outages and rapid capability to respond, resolve and prevent such  outages across the full domain.","performance monitor of cloud applications . the sl1 aiops solution enables va to diagnose reasons for outages and rapid capability to respond, resolve and prevent such outages across the full domain."
ECG/EKG Machines- Interpretation of Results,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,This software aids with interpretation of  clinical Electrocardiogram tracings for VA clinicians. ,"The system integrates w/ CPRS and outputs patient demographics on the ECG print-out.  It then prints a list of notable findings from the ECG on the form that is later reviewed and confirmed or overturned by a licensed provider.   In many cases, the machine interpretation is correct and is confirmed by the provider.",Operation and Maintenance,Both,2/1/2022,4/1/2022,10/1/2022,Developed in-house.,Unknown,Yes,No,Yes,Yes,Other,none,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,Yes,Macview 360,Less than 6 months,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.6031746031746031,"This software aids with interpretation of  clinical Electrocardiogram tracings for VA clinicians. . The system integrates w/ CPRS and outputs patient demographics on the ECG print-out.  It then prints a list of notable findings from the ECG on the form that is later reviewed and confirmed or overturned by a licensed provider.   In many cases, the machine interpretation is correct and is confirmed by the provider.","this software aids with interpretation of clinical electrocardiogram tracings for va clinicians. . the system integrates w/ cprs and outputs patient demographics on the ecg print-out. it then prints a list of notable findings from the ecg on the form that is later reviewed and confirmed or overturned by a licensed provider. in many cases, the machine interpretation is correct and is confirmed by the provider."
Appointment Comments Categorization,Department of Veterans Affairs,VA,OIT: Office of Information & Technology,Health & Medical,None of the above.,To deliver additional timely options for patients that are having mental or physical issues during appointment signup.,"Right now, I'm focusing on alerting around mental health, so the model will return, ""Timely_Alert_Needed, 0.xxxxx""",Acquisition and/or Development,Safety-impacting,8/1/2024,8/1/2024,Unknown,Developed in-house.,Unknown,Yes,No,Yes,No,Yes,"

Comment data in cdw prod","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Summit,Less than 6 months,Yes,Other,No,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Safety-Impacting,0.5396825396825397,"To deliver additional timely options for patients that are having mental or physical issues during appointment signup. . Right now, I'm focusing on alerting around mental health, so the model will return, ""Timely_Alert_Needed, 0.xxxxx""","to deliver additional timely options for patients that are having mental or physical issues during appointment signup. . right now, i'm focusing on alerting around mental health, so the model will return, ""timely_alert_needed, 0.xxxxx"""
TeraRecon,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K220349.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K220349.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/k220349.pdf).
Pension Optimization Initiative (POI),Department of Veterans Affairs,VA,VBA: Veterans Benefits Administration,Government Services (includes Benefits and Service Delivery),None of the above.,"VBA’s POI is a transformative effort that seeks a Managed Services Provider (MSP) to convert an existing manual business process into automated processing of Pension and Survivor claims. The goal is to have 75% of claims completed by automation with claims processing quality that is equivalent to, or greater than, existing manual processes. ","The following is high-level scenarios describe the inputs --->automated processes--->outputs:  Scenario A:  Claimant submits application for benefits via handwritten, typed, or VA.gov submission  --->  POI automation extracts data from forms and other documents submitted with application  --->  extracted data and existing Veteran/Beneficiary file data is processed in accordance with business rules to determine outcome of claim  --->  outputs are:  -updated Veteran/Beneficiary files, -completed Pension-related claim actions to include stop and start of awards (delivery of benefits), and -Notification Letters to Claimants.  Scenario B:  same as above, except, claim is established via computer matching agreement with other Government agencies (e.g. Social Security Administration) or other VBA-internal process rather than actually receiving an application from a claimant.",Operation and Maintenance,"Rights-Impacting
",4/1/2021,9/12/2022,1/27/2023,Unknown,"36C10E19D0018, 36C10D22N0008",Yes,No,Yes,Yes,Other,Not sure.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,"VBA Automation Platform, VASI ID #2522",Less than 6 months,Other,Other,Other,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Rights-Impacting,0.6507936507936508,"VBA’s POI is a transformative effort that seeks a Managed Services Provider (MSP) to convert an existing manual business process into automated processing of Pension and Survivor claims. The goal is to have 75% of claims completed by automation with claims processing quality that is equivalent to, or greater than, existing manual processes. . The following is high-level scenarios describe the inputs --->automated processes--->outputs:  Scenario A:  Claimant submits application for benefits via handwritten, typed, or VA.gov submission  --->  POI automation extracts data from forms and other documents submitted with application  --->  extracted data and existing Veteran/Beneficiary file data is processed in accordance with business rules to determine outcome of claim  --->  outputs are:  -updated Veteran/Beneficiary files, -completed Pension-related claim actions to include stop and start of awards (delivery of benefits), and -Notification Letters to Claimants.  Scenario B:  same as above, except, claim is established via computer matching agreement with other Government agencies (e.g. Social Security Administration) or other VBA-internal process rather than actually receiving an application from a claimant.","vba’s poi is a transformative effort that seeks a managed services provider (msp) to convert an existing manual business process into automated processing of pension and survivor claims. the goal is to have 75% of claims completed by automation with claims processing quality that is equivalent to, or greater than, existing manual processes. . the following is high-level scenarios describe the inputs --->automated processes--->outputs: scenario a: claimant submits application for benefits via handwritten, typed, or va.gov submission ---> poi automation extracts data from forms and other documents submitted with application ---> extracted data and existing veteran/beneficiary file data is processed in accordance with business rules to determine outcome of claim ---> outputs are: -updated veteran/beneficiary files, -completed pension-related claim actions to include stop and start of awards (delivery of benefits), and -notification letters to claimants. scenario b: same as above, except, claim is established via computer matching agreement with other government agencies (e.g. social security administration) or other vba-internal process rather than actually receiving an application from a claimant."
Limited Use of Azure Speech Services in PETALS Platform,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"We have specific and limited use of Azure Speech Services in PETALS Platform, where Azure Speech Services will read either (a) a participant first name or (b) a numerical step count, and then read-aloud that content to validate the user and information input provided by user during program engagement. ",Audio verbalization (speak-aloud during telephone engagement) of a first name and of a numerical step count.,Acquisition and/or Development,Neither,6/14/2021,11/17/2023,Unknown,Developed with both contracting and in-house resources.,36C250P2111,Yes,No,Yes,Yes,No,Training is not applicable in this case given that the 'data' used is provided to the system. Extensive testing has been done to validate/evaluate accuracy of speech services during extensive testing using test data.,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,Patient Engagement Tracking and Longterm Support and PETALS CS (Communication Services) - (eMASS 1341 and 2589),6-12 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"We have specific and limited use of Azure Speech Services in PETALS Platform, where Azure Speech Services will read either (a) a participant first name or (b) a numerical step count, and then read-aloud that content to validate the user and information input provided by user during program engagement. . Audio verbalization (speak-aloud during telephone engagement) of a first name and of a numerical step count.","we have specific and limited use of azure speech services in petals platform, where azure speech services will read either (a) a participant first name or (b) a numerical step count, and then read-aloud that content to validate the user and information input provided by user during program engagement. . audio verbalization (speak-aloud during telephone engagement) of a first name and of a numerical step count."
Medallia SaaS - VSignals and ESignals,Department of Veterans Affairs,VA,VEO: Veterans Experience Office,Other,None of the above.,"Medallia is a customer experience management platform which can leverage AI (NLP) to review survey comments for actioning.   VSignal survey comments (from Veterans) are processed by NLP to HELP determine if a Veteran is potentially in risk of crisis (related to homelessness and mental health).   If NLP flags a comment as a potential risk, a case is created and sent to the Crisis Line and Homeless Program for review and actioning.",See above,Operation and Maintenance,Safety-impacting,1/1/2017,6/1/2017,1/1/2018,Developed with both contracting and in-house resources.,VA-22-00077985  this is the PIID for Call Order 25 ,No,No,Yes,Yes,Yes,Data from VHA's corporate data warehouse,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Medallia eCloud-e,Unknown,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Safety-Impacting,0.6190476190476191,"Medallia is a customer experience management platform which can leverage AI (NLP) to review survey comments for actioning.   VSignal survey comments (from Veterans) are processed by NLP to HELP determine if a Veteran is potentially in risk of crisis (related to homelessness and mental health).   If NLP flags a comment as a potential risk, a case is created and sent to the Crisis Line and Homeless Program for review and actioning. . See above","medallia is a customer experience management platform which can leverage ai (nlp) to review survey comments for actioning. vsignal survey comments (from veterans) are processed by nlp to help determine if a veteran is potentially in risk of crisis (related to homelessness and mental health). if nlp flags a comment as a potential risk, a case is created and sent to the crisis line and homeless program for review and actioning. . see above"
AGFA Dose Monitor system,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf18/K180589.pdf).,Operation and Maintenance,Both,12/28/2023,12/28/2023,12/28/2023,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5555555555555556,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf18/K180589.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf18/k180589.pdf).
Volpara Imaging Patient Hub,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf21/K211279.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf21/K211279.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf21/k211279.pdf).
Beckam Coulter DxH 800,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf12/K120771.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf12/K120771.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf12/k120771.pdf).
UIPath Document Understanding,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Mission-Enabling (internal agency support),None of the above.,The tool enables Optical Character Recognition and data extraction from submitted documents through AI document processing.  This tool can extract data accurately and efficiently to allow automations to input or use the data for other applications.  ,"UiPath's Document Understanding will provide computerized information from forms and documents (computer generated or handwritten) that can be used by the automation to add information to other applications, or present to assist in decisions by the Agents for Veterans.",Operation and Maintenance,Neither,7/23/2023,7/23/2023,11/15/2023,Developed with contracting resources.,"36C10B23F0307, NNG15SD66B",No,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Yes,VA Robotic Process Automation Platform 2755,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"The tool enables Optical Character Recognition and data extraction from submitted documents through AI document processing.  This tool can extract data accurately and efficiently to allow automations to input or use the data for other applications. . UiPath's Document Understanding will provide computerized information from forms and documents (computer generated or handwritten) that can be used by the automation to add information to other applications, or present to assist in decisions by the Agents for Veterans.","the tool enables optical character recognition and data extraction from submitted documents through ai document processing. this tool can extract data accurately and efficiently to allow automations to input or use the data for other applications. . uipath's document understanding will provide computerized information from forms and documents (computer generated or handwritten) that can be used by the automation to add information to other applications, or present to assist in decisions by the agents for veterans."
Potential Fraud or Waste,Department of Veterans Affairs,VA,OM: Office of Management,Mission-Enabling (internal agency support),None of the above.,"The Purchase Card dashboards are a suite of dashboards that allow various purchase card managers to monitor purchase card spending of employees. The dashboards help monitor for fraud, waste, and abuse, and assist in providing oversight for compliance with purchase card laws and policies. We have now incorporated advanced data analytics within the dashboard. The Advanced data Analytics model generates an expected received date for items. It then flags items that have gone beyond the expected generated date by flagging items that were received late (possible waste) and/or items that have not been received yet (possible fraud). This has the potential to save the analyst time by investigating cases that have been flagged as opposed to investigating potentially an entire dataset, which is what they have been doing up until now. The model analyzes all purchase card transactions including ordered, delivery, and recieved dates as its inputs. It outputs expected recieved dates. The intended users of the product are purhase card managers. ",The Advanced data Analytics model generates an expected received date for items. It then flags items that have gone beyond the expected generated date by flagging items that were received late (possible waste) and/or items that have not been received yet (possible fraud).,Initiated,Neither,4/1/2024,Unknown,Unknown,Developed in-house.,Unknown,No,No,No,No,Yes,Purchase card transaction details,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,6-12 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The Purchase Card dashboards are a suite of dashboards that allow various purchase card managers to monitor purchase card spending of employees. The dashboards help monitor for fraud, waste, and abuse, and assist in providing oversight for compliance with purchase card laws and policies. We have now incorporated advanced data analytics within the dashboard. The Advanced data Analytics model generates an expected received date for items. It then flags items that have gone beyond the expected generated date by flagging items that were received late (possible waste) and/or items that have not been received yet (possible fraud). This has the potential to save the analyst time by investigating cases that have been flagged as opposed to investigating potentially an entire dataset, which is what they have been doing up until now. The model analyzes all purchase card transactions including ordered, delivery, and recieved dates as its inputs. It outputs expected recieved dates. The intended users of the product are purhase card managers. . The Advanced data Analytics model generates an expected received date for items. It then flags items that have gone beyond the expected generated date by flagging items that were received late (possible waste) and/or items that have not been received yet (possible fraud).","the purchase card dashboards are a suite of dashboards that allow various purchase card managers to monitor purchase card spending of employees. the dashboards help monitor for fraud, waste, and abuse, and assist in providing oversight for compliance with purchase card laws and policies. we have now incorporated advanced data analytics within the dashboard. the advanced data analytics model generates an expected received date for items. it then flags items that have gone beyond the expected generated date by flagging items that were received late (possible waste) and/or items that have not been received yet (possible fraud). this has the potential to save the analyst time by investigating cases that have been flagged as opposed to investigating potentially an entire dataset, which is what they have been doing up until now. the model analyzes all purchase card transactions including ordered, delivery, and recieved dates as its inputs. it outputs expected recieved dates. the intended users of the product are purhase card managers. . the advanced data analytics model generates an expected received date for items. it then flags items that have gone beyond the expected generated date by flagging items that were received late (possible waste) and/or items that have not been received yet (possible fraud)."
Roche Digital Pathology ,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,To aid pathologists in diagnosing pathology cases.,It analyzes slide images to aid in diagnosing,Acquisition and/or Development,Both,4/23/2024,12/15/2022,Unknown,Developed with contracting resources.,GS-07F-0446V,Yes,No,Yes,Yes,No,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,Unknown,No,Navify,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5396825396825397,To aid pathologists in diagnosing pathology cases. . It analyzes slide images to aid in diagnosing,to aid pathologists in diagnosing pathology cases. . it analyzes slide images to aid in diagnosing
Activity recognition using wearable sensors for use in closed loop deep brain stimulation systems,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"To accurately recognize/classify a number of different activities of daily living (walking, standing, sitting, lying down, etc.) using accelerometry/gyroscopic data. This could be used in a number of different applications. ",Described above. A classifier that outputs a recognized behavior in real-time based on changes in accelerometry/gyroscopic data.,Acquisition and/or Development,Safety-impacting,8/1/2019,12/16/2019,Unknown,Developed with both contracting and in-house resources.,Unknown,Yes,Yes,Yes,No,No,researcher-acquired accelerometer/gyroscope data,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,Yes – source code is publicly available.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Safety-Impacting,0.5238095238095238,"To accurately recognize/classify a number of different activities of daily living (walking, standing, sitting, lying down, etc.) using accelerometry/gyroscopic data. This could be used in a number of different applications. . Described above. A classifier that outputs a recognized behavior in real-time based on changes in accelerometry/gyroscopic data.","to accurately recognize/classify a number of different activities of daily living (walking, standing, sitting, lying down, etc.) using accelerometry/gyroscopic data. this could be used in a number of different applications. . described above. a classifier that outputs a recognized behavior in real-time based on changes in accelerometry/gyroscopic data."
AgileMD eCART Clinical Deterioration Model,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K233253.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K233253.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/k233253.pdf).
PINGOO.AI,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,Edcuational infomrational app that uses a RAG model to provide up to date and trustrowthy infomration to Veterans who are diabetic. ,Output is infomation that is related to the RAG model communicated to the Veteran through a language model interface.,Acquisition and/or Development,Both,9/26/2024,9/26/2024,Unknown,Developed with contracting resources.,Unknown,Yes,Yes,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,6-12 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,Edcuational infomrational app that uses a RAG model to provide up to date and trustrowthy infomration to Veterans who are diabetic. . Output is infomation that is related to the RAG model communicated to the Veteran through a language model interface.,edcuational infomrational app that uses a rag model to provide up to date and trustrowthy infomration to veterans who are diabetic. . output is infomation that is related to the rag model communicated to the veteran through a language model interface.
Persyst 14 EEG Review And Analysis Software,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"Persyst EEG Review and Analysis Software provides the complete set of tools needed for C.A.R.E (Computer Assisted Review of EEG), resulting in accurate, efficient and rapid review of EEG data.Link to the vendors website -
https://www.persyst.com/","It uses prespecified but customizable parameters for seizure detection, spike detection, trending of EEG signals, artifact reduction and monitoring. Its a commercial off the shelf product used widely in the field as well as public and private hospitals.
",Operation and Maintenance,Both,2/1/2024,Unknown,Unknown,Developed with contracting resources.,"506-A40115, 36F79718D0330",Yes,No,Yes,Yes,Other,"
N/A - COTS product with pre-specified parameters used for EEG analysis.
","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.6190476190476191,"Persyst EEG Review and Analysis Software provides the complete set of tools needed for C.A.R.E (Computer Assisted Review of EEG), resulting in accurate, efficient and rapid review of EEG data.Link to the vendors website -
https://www.persyst.com/ . It uses prespecified but customizable parameters for seizure detection, spike detection, trending of EEG signals, artifact reduction and monitoring. Its a commercial off the shelf product used widely in the field as well as public and private hospitals.","persyst eeg review and analysis software provides the complete set of tools needed for c.a.r.e (computer assisted review of eeg), resulting in accurate, efficient and rapid review of eeg data.link to the vendors website - https://www.persyst.com/ . it uses prespecified but customizable parameters for seizure detection, spike detection, trending of eeg signals, artifact reduction and monitoring. its a commercial off the shelf product used widely in the field as well as public and private hospitals."
Computer Aided Detection (CADe) of Neoplasia during Colonoscopy - “GI Genius”,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"GI Genius™, a commercial product distributed by Medtronic, Inc., is a computer-aided detection module, powered by artificial intelligence, designed to aid endoscopists in detecting colonic mucosal lesions (such as polyps and adenomas) during colonoscopy. The FDA cleared this commercial product in April 2021.
The device physically connects to existing endoscopes, video processors, and display monitors, but does not connect to the VA network.
During endoscopy, the CADe device automatically detects, and highlights suspected neoplastic lesions / polyps in real time. The module was trained and validated with white-light endoscopy videos and continues to be refined with growing datasets. Information about the indications, safety and warnings can be found here: GI Genius™ Intelligent Endoscopy Module - Indications, Safety, and Warnings | Medtronic (https://www.medtronic.com/covidien/en-us/products/gastrointestinal-artificial-intelligence/gi-genius-intelligent-endoscopy/indications-safety-warnings.html)
Multiple studies and meta-analyses have found these devices increase adenoma detection rates, a well-established metric for colonoscopy quality that is related to colon cancer incidence and death. Initial randomized implementation of the devices across VA facilities allowed for a pragmatic evaluation of the impact of these devices on adenoma detection. The evaluation demonstrated that the provision of colonoscopy CADe devices resulted in a statistically significant 21% increase in the odds of adenoma detection and an absolute increase in ADR of approximately 4% compared to colonoscopy without CADe. These devices will improve ADR, which should decrease colorectal cancer incidence and mortality amongst Veterans over time.
","The device physically connects to existing endoscopes, video processors, and display monitors, but does not connect to the VA network.  During endoscopy, the CADe device automatically detects, and highlights suspected neoplastic lesions / polyps in real time on the display monitor.",Operation and Maintenance,Both,6/1/2022,6/30/2022,10/12/2023,Developed with contracting resources.,36C10X22X0045,Yes,No,No,No,Other,"No agency-owned data have been, nor are being, used to train the device. However, initial output/outcome evaluations of the device were conducted. The impact of CADe on adenoma detection rate (ADR) was not moderated by patient race, ethnicity, gender, or rurality (p>.05 for each); however, it was moderated by patient age (OR 1.02, 95% CI 1.00-1.03, p=.02), indicating that improvement in ADR after CADe availability was greater for older patients. The impact of CADe on ADR was moderated by the number of years since an endoscopist’s medical degree (OR 1.02; 95% CI 1.00-1.04, p =.04), indicating that ADR improvement with CADe availability was greater for endoscopists with more years since their medical degree. The impact of CADe on ADR was not moderated by endoscopist specialty or sex (p >.05 for both).","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",No,No – agency does not have access to source code.,No,Unknown,Unknown,No,Other,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,"Possible risk of increased false positive detection of colorectal lesions leading to unnecessary biopsies or polypectomy. This could result in increased costs and potential complications. Risk of over-reliance on detection by the physician (i.e., automation bias) Risk to education / training of fellows who may become reliant on the technology.",TRUE,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"This FDA cleared AI device was formally evaluated by VA specifically looking for evidence of disparities in performance. The AI device was equally beneficial for all demographic groups evaluated, including by sex, race, and ethnicity. There was a modest increase in benefit with increasing patient age, which may be related to the increased prevalence of precancerous polyps with age.","[""Direct user testing"",""Post-transaction customer feedback collections""]",No – it is not operationally practical to offer this.,Both,0.8095238095238095,"GI Genius™, a commercial product distributed by Medtronic, Inc., is a computer-aided detection module, powered by artificial intelligence, designed to aid endoscopists in detecting colonic mucosal lesions (such as polyps and adenomas) during colonoscopy. The FDA cleared this commercial product in April 2021.
The device physically connects to existing endoscopes, video processors, and display monitors, but does not connect to the VA network.
During endoscopy, the CADe device automatically detects, and highlights suspected neoplastic lesions / polyps in real time. The module was trained and validated with white-light endoscopy videos and continues to be refined with growing datasets. Information about the indications, safety and warnings can be found here: GI Genius™ Intelligent Endoscopy Module - Indications, Safety, and Warnings | Medtronic (https://www.medtronic.com/covidien/en-us/products/gastrointestinal-artificial-intelligence/gi-genius-intelligent-endoscopy/indications-safety-warnings.html)
Multiple studies and meta-analyses have found these devices increase adenoma detection rates, a well-established metric for colonoscopy quality that is related to colon cancer incidence and death. Initial randomized implementation of the devices across VA facilities allowed for a pragmatic evaluation of the impact of these devices on adenoma detection. The evaluation demonstrated that the provision of colonoscopy CADe devices resulted in a statistically significant 21% increase in the odds of adenoma detection and an absolute increase in ADR of approximately 4% compared to colonoscopy without CADe. These devices will improve ADR, which should decrease colorectal cancer incidence and mortality amongst Veterans over time. . The device physically connects to existing endoscopes, video processors, and display monitors, but does not connect to the VA network.  During endoscopy, the CADe device automatically detects, and highlights suspected neoplastic lesions / polyps in real time on the display monitor. . Possible risk of increased false positive detection of colorectal lesions leading to unnecessary biopsies or polypectomy. This could result in increased costs and potential complications. Risk of over-reliance on detection by the physician (i.e., automation bias) Risk to education / training of fellows who may become reliant on the technology.","gi genius™, a commercial product distributed by medtronic, inc., is a computer-aided detection module, powered by artificial intelligence, designed to aid endoscopists in detecting colonic mucosal lesions (such as polyps and adenomas) during colonoscopy. the fda cleared this commercial product in april 2021. the device physically connects to existing endoscopes, video processors, and display monitors, but does not connect to the va network. during endoscopy, the cade device automatically detects, and highlights suspected neoplastic lesions / polyps in real time. the module was trained and validated with white-light endoscopy videos and continues to be refined with growing datasets. information about the indications, safety and warnings can be found here: gi genius™ intelligent endoscopy module - indications, safety, and warnings | medtronic (https://www.medtronic.com/covidien/en-us/products/gastrointestinal-artificial-intelligence/gi-genius-intelligent-endoscopy/indications-safety-warnings.html) multiple studies and meta-analyses have found these devices increase adenoma detection rates, a well-established metric for colonoscopy quality that is related to colon cancer incidence and death. initial randomized implementation of the devices across va facilities allowed for a pragmatic evaluation of the impact of these devices on adenoma detection. the evaluation demonstrated that the provision of colonoscopy cade devices resulted in a statistically significant 21% increase in the odds of adenoma detection and an absolute increase in adr of approximately 4% compared to colonoscopy without cade. these devices will improve adr, which should decrease colorectal cancer incidence and mortality amongst veterans over time. . the device physically connects to existing endoscopes, video processors, and display monitors, but does not connect to the va network. during endoscopy, the cade device automatically detects, and highlights suspected neoplastic lesions / polyps in real time on the display monitor. . possible risk of increased false positive detection of colorectal lesions leading to unnecessary biopsies or polypectomy. this could result in increased costs and potential complications. risk of over-reliance on detection by the physician (i.e., automation bias) risk to education / training of fellows who may become reliant on the technology."
Digizens,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,Behavioral prediction model used to create clinical trials without the need for actual patients or patient information. ,Output is behavioral analysis and predictions based on messaging and circumstances.,Acquisition and/or Development,Neither,9/26/2024,9/26/2024,Unknown,Developed with contracting resources.,Unknown,Yes,No,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,6-12 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Behavioral prediction model used to create clinical trials without the need for actual patients or patient information. . Output is behavioral analysis and predictions based on messaging and circumstances.,behavioral prediction model used to create clinical trials without the need for actual patients or patient information. . output is behavioral analysis and predictions based on messaging and circumstances.
TrueFidelity CT Deep Learning Image Reconstruction,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,Decreases noise in CT images ,,Operation and Maintenance,Both,4/11/2019,4/11/2019,4/11/2019,Unknown,Unknown,Unknown,Unknown,Unknown,Yes,Unknown,Unknown,Unknown,Unknown,Unknown,Yes,MD-LITE ID: 2278,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.2698412698412698,Decreases noise in CT images,decreases noise in ct images
Pangaea,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,Devellop predictive model for chronic disease with treatment recommendations based on current guidelines. ,Output will be identification of chronic disease in Veterans and treatment modalities required prior to consulting specialty services.,Acquisition and/or Development,Both,9/26/2024,9/26/2024,Unknown,Developed with contracting resources.,Unknown,Yes,No,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.49206349206349204,Devellop predictive model for chronic disease with treatment recommendations based on current guidelines. . Output will be identification of chronic disease in Veterans and treatment modalities required prior to consulting specialty services.,devellop predictive model for chronic disease with treatment recommendations based on current guidelines. . output will be identification of chronic disease in veterans and treatment modalities required prior to consulting specialty services.
AIDOC,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K230020.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K230020.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/k230020.pdf).
Cogitativo,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,Predict chronic disease,Output is based on Veteran profiles utilizing data model built from previously obtained Veteran profiles by the company Cogitativo. It is unclear how the company obtained the data.,Acquisition and/or Development,Both,9/26/2024,9/26/2024,Unknown,Developed with contracting resources.,none,Yes,No,Yes,No,No,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,6-12 months,No,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5238095238095238,Predict chronic disease . Output is based on Veteran profiles utilizing data model built from previously obtained Veteran profiles by the company Cogitativo. It is unclear how the company obtained the data.,predict chronic disease . output is based on veteran profiles utilizing data model built from previously obtained veteran profiles by the company cogitativo. it is unclear how the company obtained the data.
"Analytics, Data, and Decision Support Unified Platform (ADDSUP)",Department of Veterans Affairs,VA,"OALC: Office of Acquisitions Logistics, and Construction",Other,None of the above.,"Using AI enhancements, this tool can quickly filter contracts and line items to facilitate easy searching, as well as provide quick answers to enhance our ability to obtain cost avoidance.  ",The system will output a list of contracts or line items based on search criteria entered by the user.,Operation and Maintenance,Neither,10/1/2023,1/1/2022,10/1/2023,Developed with contracting resources.,36C10B21F0353,No,No,No,No,Yes,"VA owned data comes from eCMS, FPDS and D2D.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,6-12 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"Using AI enhancements, this tool can quickly filter contracts and line items to facilitate easy searching, as well as provide quick answers to enhance our ability to obtain cost avoidance. . The system will output a list of contracts or line items based on search criteria entered by the user.","using ai enhancements, this tool can quickly filter contracts and line items to facilitate easy searching, as well as provide quick answers to enhance our ability to obtain cost avoidance. . the system will output a list of contracts or line items based on search criteria entered by the user."
iCAD ProFound AI,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf20/K203822.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf20/K203822.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf20/k203822.pdf).
VA Section 508 Office URL Ownership Prediction Model,Department of Veterans Affairs,VA,OIT: Office of Information & Technology,Mission-Enabling (internal agency support),None of the above.,"Purpose of the AI system is to predict the administrative agency that owns URLs.  The AI system reduces the level of effort and turnaround time for the OIT 508 Office to curate URLs and coordinate with VA Administrations to review and, as necessary, remediate URLs for accessibility compliance.  In the absence of the AI system both the 508 Office and VA Administrations would be required to undertake a lengthy and iterative process to identify the VA Administration that is the formal owner of a URL.","The AI Model has two outputs.  1) Agency Owner with responses as: VHA, VBA, NCA, VACO, OIT, OCTO, and Unknown.  2) Prediction Score ranging from 0 to 1.  The system is comprised of manually executed scripts using the AI model on local GFE.  Besides the AI model, the scripts additionally apply manually constructed business rules and create prepopulated excel files for each VA Administration to use for quarterly reviews and data entry.",Operation and Maintenance,Neither,11/1/2023,11/1/2023,11/13/2023,Developed in-house.,Unknown,No,No,No,No,Other,The data of available URLs is obtained from Google Analytics as part of the GSA Digital Analytics Program.  Data to train the model is based on SME input from the VA administrations as part of their review process.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,None.  The system is GFE operated by OIT staff member providing consultative support to 508 Office.,Less than 6 months,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"Purpose of the AI system is to predict the administrative agency that owns URLs.  The AI system reduces the level of effort and turnaround time for the OIT 508 Office to curate URLs and coordinate with VA Administrations to review and, as necessary, remediate URLs for accessibility compliance.  In the absence of the AI system both the 508 Office and VA Administrations would be required to undertake a lengthy and iterative process to identify the VA Administration that is the formal owner of a URL. . The AI Model has two outputs.  1) Agency Owner with responses as: VHA, VBA, NCA, VACO, OIT, OCTO, and Unknown.  2) Prediction Score ranging from 0 to 1.  The system is comprised of manually executed scripts using the AI model on local GFE.  Besides the AI model, the scripts additionally apply manually constructed business rules and create prepopulated excel files for each VA Administration to use for quarterly reviews and data entry.","purpose of the ai system is to predict the administrative agency that owns urls. the ai system reduces the level of effort and turnaround time for the oit 508 office to curate urls and coordinate with va administrations to review and, as necessary, remediate urls for accessibility compliance. in the absence of the ai system both the 508 office and va administrations would be required to undertake a lengthy and iterative process to identify the va administration that is the formal owner of a url. . the ai model has two outputs. 1) agency owner with responses as: vha, vba, nca, vaco, oit, octo, and unknown. 2) prediction score ranging from 0 to 1. the system is comprised of manually executed scripts using the ai model on local gfe. besides the ai model, the scripts additionally apply manually constructed business rules and create prepopulated excel files for each va administration to use for quarterly reviews and data entry."
VA CART Adenoma Detection NLP,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above., Extracts adenoma status for CART Veterans using Natural Language Processing (NLP) of VistA colonoscopy surgical pathology reports,Results contain 0 for no evidence of adenoma or 1 for evidence of adenoma in the pathology report,Operation and Maintenance,Neither,1/8/2024,5/31/2024,7/31/2024,Developed in-house.,Unknown,Yes,No,Yes,Yes,Yes,VistA colonoscopy surgical pathology reports,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,"Clinical Assessment, Reporting and Tracking Application (VASI ID 1079, EMAS ID 1808) ",6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,Extracts adenoma status for CART Veterans using Natural Language Processing (NLP) of VistA colonoscopy surgical pathology reports . Results contain 0 for no evidence of adenoma or 1 for evidence of adenoma in the pathology report,extracts adenoma status for cart veterans using natural language processing (nlp) of vista colonoscopy surgical pathology reports . results contain 0 for no evidence of adenoma or 1 for evidence of adenoma in the pathology report
Call Center Knowledge Navigator,Department of Veterans Affairs,VA,VBA: Veterans Benefits Administration,Mission-Enabling (internal agency support),None of the above.,"Using VA-provided procedural reference material, the tool will respond to call center agent questions with an answer and a cited reference to verify the answer. The tool is intended to speed responses by quickly identifying relevant references and summarizing procedures in the form of an answer, while also providing the agent with the means to look up the referenced material.",The system will output a generated response to a user's question.,Implementation and Assessment,Neither,4/29/2024,5/9/2024,11/11/2024,Developed with contracting resources.,47QTCK18D0036,Yes,No,No,Yes,Yes,"The model uses data obtained from Education Service procedural resources (e.g., the School Certifying Official Handbook) to provide its responses.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,No – agency does not have access to source code.,No,Unknown,6-12 months,Yes,Other,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"Using VA-provided procedural reference material, the tool will respond to call center agent questions with an answer and a cited reference to verify the answer. The tool is intended to speed responses by quickly identifying relevant references and summarizing procedures in the form of an answer, while also providing the agent with the means to look up the referenced material. . The system will output a generated response to a user's question.","using va-provided procedural reference material, the tool will respond to call center agent questions with an answer and a cited reference to verify the answer. the tool is intended to speed responses by quickly identifying relevant references and summarizing procedures in the form of an answer, while also providing the agent with the means to look up the referenced material. . the system will output a generated response to a user's question."
National Training Team | Schools — FAQ Dashboard,Department of Veterans Affairs,VA,VBA: Veterans Benefits Administration,Mission-Enabling (internal agency support),None of the above.,"The 1.0 dashboard clusters thousands of submitted questions during a monthly Office Hours session with School Certifying Officials and presents a ""top ten"" list of associated categories that have the most questions.",Classification and sentiment score,Implementation and Assessment,Neither,4/12/2023,4/28/2023,10/24/2023,Developed with contracting resources.,47QTCK18D0036,No,No,No,No,Yes,Question-sets are obtained by exporting them from the monthly office hours sessions in Adobe Connect. These files are used to train and as the model's inputs.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The 1.0 dashboard clusters thousands of submitted questions during a monthly Office Hours session with School Certifying Officials and presents a ""top ten"" list of associated categories that have the most questions. . Classification and sentiment score","the 1.0 dashboard clusters thousands of submitted questions during a monthly office hours session with school certifying officials and presents a ""top ten"" list of associated categories that have the most questions. . classification and sentiment score"
National Training Team | Schools — FAQ Dashboard,Department of Veterans Affairs,VA,VBA: Veterans Benefits Administration,Mission-Enabling (internal agency support),None of the above.,"The model should classify thousands of questions into categories, generate a sample question based on the category, and provide a suggested answer based on previous responses to similar questions.","Classification, list of questions, answers to questions",Acquisition and/or Development,Neither,10/24/2023,10/24/2023,Unknown,Developed with contracting resources.,47QTCK18D0036,No,No,No,Yes,Other,Unknown,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"The model should classify thousands of questions into categories, generate a sample question based on the category, and provide a suggested answer based on previous responses to similar questions. . Classification, list of questions, answers to questions","the model should classify thousands of questions into categories, generate a sample question based on the category, and provide a suggested answer based on previous responses to similar questions. . classification, list of questions, answers to questions"
Audit of Service Connection Designations Associated with Prescriptions,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Government Services (includes Benefits and Service Delivery),None of the above.,"Reduction in waste, fraud and abuse. ",Binary,Acquisition and/or Development,Neither,7/11/2011,8/1/2011,Unknown,Developed in-house.,Unknown,Yes,No,Yes,Yes,Yes,The information to train\validate the model is obtainable from the Corporate Data Warehouse environment.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,Yes – source code is publicly available.,Yes,Data and Analytical Support for Healthcare (DASH) - #2515,Less than 6 months,Other,Yes,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"Reduction in waste, fraud and abuse. . Binary","reduction in waste, fraud and abuse. . binary"
ReflexAI,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Education & Workforce,None of the above.,"AI-powered simulations to provide nuanced, high-quality practice and real-time feedback to every individual undergoing training to serve as a responder on Veterans Crisis Line.","All information is accessible for training staff within the secure platform hosted on VA Enterprise Cloud. This data include simulation content and the instantaneous feedback across the scoring dimensions. This output includes insights on training effectiveness overall, as well as progress of individual trainees who are going through the training process.",Operation and Maintenance,Neither,9/9/2022,6/6/2023,5/10/2024,Developed with both contracting and in-house resources.,36C10B23F0271,Yes,No,No,Yes,Yes,Unknown,Unknown,Yes,"Yes – agency has access to source code, but it is not public.",Yes,"Hosted in ETIL, moving to Arches",6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"AI-powered simulations to provide nuanced, high-quality practice and real-time feedback to every individual undergoing training to serve as a responder on Veterans Crisis Line. . All information is accessible for training staff within the secure platform hosted on VA Enterprise Cloud. This data include simulation content and the instantaneous feedback across the scoring dimensions. This output includes insights on training effectiveness overall, as well as progress of individual trainees who are going through the training process.","ai-powered simulations to provide nuanced, high-quality practice and real-time feedback to every individual undergoing training to serve as a responder on veterans crisis line. . all information is accessible for training staff within the secure platform hosted on va enterprise cloud. this data include simulation content and the instantaneous feedback across the scoring dimensions. this output includes insights on training effectiveness overall, as well as progress of individual trainees who are going through the training process."
"Prostate Cancer, Genetic Risk, and Equitable Screening Study (ProGRESS) - Prostate Cancer Risk Prediction Model",Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Other,None of the above.,"This risk model is being used within the framework of a nationwide, VA funded randomized controlled trial to deliver a precision prostate cancer screening intervention to research participants. The model uses machine learning components (e.g., training and validation sets from MVP and other external sources, LASSO-regularized Cox model) and genetic, ancestry, and self-report family history of prostate cancer data to develop and validate a risk prediction model to help stratify new participants into three distinct prostate cancer risk categories (low risk, average risk, high risk). The risk classification is then included as part of a comprehensive report, including a full risk model report from a VA-approved genetic testing laboratory, a monogenic genetic variant report from a VA-approved genetic testing laboratory, and a summary sheet which provide general information and risk-specific recommendations surrounding prostate cancer screening. The report is shared with the participant and their VA healthcare providers as additional information to be used within the  framework of shared decision-making regarding future prostate cancer screening. ","The risk model uses new participant data to output a risk classification for prostate cancer based on genetic, ancestry, and self-report family history of prostate cancer data (low risk, average risk, high risk). The classification is is then included as part of a comprehensive report that is delivered to the participant and their VA healthcare providers to be used within the framework of shared decision-making regarding future prostate cancer screening.",Acquisition and/or Development,Both,8/1/2023,3/1/2024,Unknown,Developed with both contracting and in-house resources.,"36C24124D0061, 36C24124C0018",No,No,Yes,Yes,Other,Million Veteran Program (MVP) data,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5714285714285714,"This risk model is being used within the framework of a nationwide, VA funded randomized controlled trial to deliver a precision prostate cancer screening intervention to research participants. The model uses machine learning components (e.g., training and validation sets from MVP and other external sources, LASSO-regularized Cox model) and genetic, ancestry, and self-report family history of prostate cancer data to develop and validate a risk prediction model to help stratify new participants into three distinct prostate cancer risk categories (low risk, average risk, high risk). The risk classification is then included as part of a comprehensive report, including a full risk model report from a VA-approved genetic testing laboratory, a monogenic genetic variant report from a VA-approved genetic testing laboratory, and a summary sheet which provide general information and risk-specific recommendations surrounding prostate cancer screening. The report is shared with the participant and their VA healthcare providers as additional information to be used within the  framework of shared decision-making regarding future prostate cancer screening. . The risk model uses new participant data to output a risk classification for prostate cancer based on genetic, ancestry, and self-report family history of prostate cancer data (low risk, average risk, high risk). The classification is is then included as part of a comprehensive report that is delivered to the participant and their VA healthcare providers to be used within the framework of shared decision-making regarding future prostate cancer screening.","this risk model is being used within the framework of a nationwide, va funded randomized controlled trial to deliver a precision prostate cancer screening intervention to research participants. the model uses machine learning components (e.g., training and validation sets from mvp and other external sources, lasso-regularized cox model) and genetic, ancestry, and self-report family history of prostate cancer data to develop and validate a risk prediction model to help stratify new participants into three distinct prostate cancer risk categories (low risk, average risk, high risk). the risk classification is then included as part of a comprehensive report, including a full risk model report from a va-approved genetic testing laboratory, a monogenic genetic variant report from a va-approved genetic testing laboratory, and a summary sheet which provide general information and risk-specific recommendations surrounding prostate cancer screening. the report is shared with the participant and their va healthcare providers as additional information to be used within the framework of shared decision-making regarding future prostate cancer screening. . the risk model uses new participant data to output a risk classification for prostate cancer based on genetic, ancestry, and self-report family history of prostate cancer data (low risk, average risk, high risk). the classification is is then included as part of a comprehensive report that is delivered to the participant and their va healthcare providers to be used within the framework of shared decision-making regarding future prostate cancer screening."
"Using AI to enhance/augment classification of reported patient safety events, and to proactively identify patient safety vulnerabilities and potential areas of improvement",Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Mission-Enabling (internal agency support),None of the above.,This system allows statisticians to identify trends and anomalies to proactively determine patient safety areas of improvement by identifying any new type of safety event and any abnormal increase in frequency of events and stop them before they become widespread.,Augmented event type classifications,Initiated,Neither,8/5/2024,Unknown,Unknown,Developed in-house.,Unknown,Yes,No,No,No,Yes,Unknown,Unknown,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,This system allows statisticians to identify trends and anomalies to proactively determine patient safety areas of improvement by identifying any new type of safety event and any abnormal increase in frequency of events and stop them before they become widespread. . Augmented event type classifications,this system allows statisticians to identify trends and anomalies to proactively determine patient safety areas of improvement by identifying any new type of safety event and any abnormal increase in frequency of events and stop them before they become widespread. . augmented event type classifications
App feedback model for NLP tasks,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"Functional Overview: The objective is to utilize Natural Language Processing (NLP) with comment reviews for App feedback, specifically to identify named entities (NER), profanity, and stop words, and provide an automated approach to pre-processing and cleaning text for downstream analytics tools.
 
System Overview: The system uses spaCy's en_core_web_sm model, an open-source software and model, to analyze text reviews from various sources, including external sources (Google and Apple stores) and internal sources (FeedbackUI and VA Mobile). The model provides an out-of-the-box approach for NLP tasks, including NER, profanity detection, and stop word removal.
 
Data: The data sources include:
 
Text reviews from the VA's mobile applications on the Google and Apple stores (external sources)

Reviews from FeedbackUI (internal source, available in the OIA_MobileHealth database)

Reviews from VA Mobile (internal source, available via CSV files on the Mobile VA's internal website)

Users: The users of this system are likely the OCC Data Science Team, who are responsible for developing and maintaining the pipeline.
 
Target Audience: The target audience mobile application developers and other internal stakeholders who may be interested in analyzing and understanding the sentiment and feedback from these reviews.
 ","The objective is to utilize Natural Language Processing (NLP) with comment reviews for App feedback, specifically to identify named entities (NER), profanity, and stop words, and provide an automated approach to pre-processing and cleaning text for downstream analytics tools.",Operation and Maintenance,Neither,12/1/2022,2/1/2024,4/26/2024,Developed with contracting resources.,"Unable to identify in time, working with Contracting team to identify. Iron Bow (Prime) - InnoVet (Sub)",No,No,Yes,No,Yes,"CDW -VA application feedback data (Ex. Share My Health Data - application download count, user review)","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,SDP / HDAP - working with contract team to better identify system nomenclature this falls under,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"Functional Overview: The objective is to utilize Natural Language Processing (NLP) with comment reviews for App feedback, specifically to identify named entities (NER), profanity, and stop words, and provide an automated approach to pre-processing and cleaning text for downstream analytics tools.
 
System Overview: The system uses spaCy's en_core_web_sm model, an open-source software and model, to analyze text reviews from various sources, including external sources (Google and Apple stores) and internal sources (FeedbackUI and VA Mobile). The model provides an out-of-the-box approach for NLP tasks, including NER, profanity detection, and stop word removal.
 
Data: The data sources include:
 
Text reviews from the VA's mobile applications on the Google and Apple stores (external sources)

Reviews from FeedbackUI (internal source, available in the OIA_MobileHealth database)

Reviews from VA Mobile (internal source, available via CSV files on the Mobile VA's internal website)

Users: The users of this system are likely the OCC Data Science Team, who are responsible for developing and maintaining the pipeline.
 
Target Audience: The target audience mobile application developers and other internal stakeholders who may be interested in analyzing and understanding the sentiment and feedback from these reviews. . The objective is to utilize Natural Language Processing (NLP) with comment reviews for App feedback, specifically to identify named entities (NER), profanity, and stop words, and provide an automated approach to pre-processing and cleaning text for downstream analytics tools.","functional overview: the objective is to utilize natural language processing (nlp) with comment reviews for app feedback, specifically to identify named entities (ner), profanity, and stop words, and provide an automated approach to pre-processing and cleaning text for downstream analytics tools. system overview: the system uses spacy's en_core_web_sm model, an open-source software and model, to analyze text reviews from various sources, including external sources (google and apple stores) and internal sources (feedbackui and va mobile). the model provides an out-of-the-box approach for nlp tasks, including ner, profanity detection, and stop word removal. data: the data sources include: text reviews from the va's mobile applications on the google and apple stores (external sources) reviews from feedbackui (internal source, available in the oia_mobilehealth database) reviews from va mobile (internal source, available via csv files on the mobile va's internal website) users: the users of this system are likely the occ data science team, who are responsible for developing and maintaining the pipeline. target audience: the target audience mobile application developers and other internal stakeholders who may be interested in analyzing and understanding the sentiment and feedback from these reviews. . the objective is to utilize natural language processing (nlp) with comment reviews for app feedback, specifically to identify named entities (ner), profanity, and stop words, and provide an automated approach to pre-processing and cleaning text for downstream analytics tools."
XtractOne,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Law & Justice,None of the above.,"The new WDS will allow the VA Police to centralize all the facilities WDS reporting to one place allowing the VA to use manpower properly, allow them to also have better eyes on the WDS, and also allow them to have Enterprise solutions to allow upper management to see what is going on through the SaaS solution. This solution will also allow the VA Police to send out reports or alerts to Upper management and VA Police so they can mobilize and report to where the issues are quicker, efficiently, and effectively. This will also allow the VA Police to lock down facilities easier than before where that was not possible without going to said doors and locking them physically. The WDS can do that for the VA Police now.

The key benefits are having a fully modernized weapons detection system to help the VA Police better detect unauthorized weapons entering and exiting the VA Facilities allowing the VA Police to do their job more efficiently and effectively. This will also all the VA Police to use their manpower in proper means instead of wasting man hours and money.","Xtract One View is equipped with advanced analytical tools that process vast amounts of data to detect unusual patterns and potential threats before they escalate. Using machine learning algorithms, it continuously improves its concealed weapons detection capabilities, adapting to new types of risks and optimizing security protocols accordingly.",Acquisition and/or Development,Both,9/23/2024,Unknown,Unknown,Unknown,Contract has not been awarded yet.,No,No,No,Unknown,No,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.47619047619047616,"The new WDS will allow the VA Police to centralize all the facilities WDS reporting to one place allowing the VA to use manpower properly, allow them to also have better eyes on the WDS, and also allow them to have Enterprise solutions to allow upper management to see what is going on through the SaaS solution. This solution will also allow the VA Police to send out reports or alerts to Upper management and VA Police so they can mobilize and report to where the issues are quicker, efficiently, and effectively. This will also allow the VA Police to lock down facilities easier than before where that was not possible without going to said doors and locking them physically. The WDS can do that for the VA Police now.

The key benefits are having a fully modernized weapons detection system to help the VA Police better detect unauthorized weapons entering and exiting the VA Facilities allowing the VA Police to do their job more efficiently and effectively. This will also all the VA Police to use their manpower in proper means instead of wasting man hours and money. . Xtract One View is equipped with advanced analytical tools that process vast amounts of data to detect unusual patterns and potential threats before they escalate. Using machine learning algorithms, it continuously improves its concealed weapons detection capabilities, adapting to new types of risks and optimizing security protocols accordingly.","the new wds will allow the va police to centralize all the facilities wds reporting to one place allowing the va to use manpower properly, allow them to also have better eyes on the wds, and also allow them to have enterprise solutions to allow upper management to see what is going on through the saas solution. this solution will also allow the va police to send out reports or alerts to upper management and va police so they can mobilize and report to where the issues are quicker, efficiently, and effectively. this will also allow the va police to lock down facilities easier than before where that was not possible without going to said doors and locking them physically. the wds can do that for the va police now. the key benefits are having a fully modernized weapons detection system to help the va police better detect unauthorized weapons entering and exiting the va facilities allowing the va police to do their job more efficiently and effectively. this will also all the va police to use their manpower in proper means instead of wasting man hours and money. . xtract one view is equipped with advanced analytical tools that process vast amounts of data to detect unusual patterns and potential threats before they escalate. using machine learning algorithms, it continuously improves its concealed weapons detection capabilities, adapting to new types of risks and optimizing security protocols accordingly."
 Using AI to weekly monitoring of any harms resulting from the use of newly implemented electronic health record system – FEHR across VA,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Mission-Enabling (internal agency support),None of the above.,This AI based monitoring system will be used to inform and improve the EHR system and it's functionalities to prevent and mitigate any patient safety events.,Classification of harm categories from EHR,Initiated,Neither,10/3/2022,Unknown,Unknown,Developed in-house.,Unknown,Yes,No,No,No,Yes,Unknown,Unknown,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,This AI based monitoring system will be used to inform and improve the EHR system and it's functionalities to prevent and mitigate any patient safety events. . Classification of harm categories from EHR,this ai based monitoring system will be used to inform and improve the ehr system and it's functionalities to prevent and mitigate any patient safety events. . classification of harm categories from ehr
Podimetrics,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"Our involvement with Podimetrics is limited to receiving and processing escalation reports generated by their SmartMat system. Podimetrics’ platform uses its proprietary technology, to generate these reports. However, our role does not involve developing, deploying, or controlling  AI systems. Instead, we serve as a conduit, ensuring that these escalation reports are shared with the appropriate clinicians to facilitate timely and informed care for Veterans.","Our interaction with the Podimetrics platform is strictly limited to data transfer and integration of the generated reports. We do not engage with their proprietary technology, or decision-making processes inherent in the Podimetrics platform. Our role is supportive and integration-focused, solely aimed at ensuring seamless communication and clinical follow-up based on the provided reports.",Operation and Maintenance,Both,12/1/2019,12/1/2019,12/1/2019,Developed with contracting resources.,Unknown,No,No,No,Yes,No,Unknown,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",No,No – agency does not have access to source code.,Unknown,Unknown,Unknown,Other,Other,Unknown,Unknown,Unknown,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.4444444444444444,"Our involvement with Podimetrics is limited to receiving and processing escalation reports generated by their SmartMat system. Podimetrics’ platform uses its proprietary technology, to generate these reports. However, our role does not involve developing, deploying, or controlling  AI systems. Instead, we serve as a conduit, ensuring that these escalation reports are shared with the appropriate clinicians to facilitate timely and informed care for Veterans. . Our interaction with the Podimetrics platform is strictly limited to data transfer and integration of the generated reports. We do not engage with their proprietary technology, or decision-making processes inherent in the Podimetrics platform. Our role is supportive and integration-focused, solely aimed at ensuring seamless communication and clinical follow-up based on the provided reports.","our involvement with podimetrics is limited to receiving and processing escalation reports generated by their smartmat system. podimetrics’ platform uses its proprietary technology, to generate these reports. however, our role does not involve developing, deploying, or controlling ai systems. instead, we serve as a conduit, ensuring that these escalation reports are shared with the appropriate clinicians to facilitate timely and informed care for veterans. . our interaction with the podimetrics platform is strictly limited to data transfer and integration of the generated reports. we do not engage with their proprietary technology, or decision-making processes inherent in the podimetrics platform. our role is supportive and integration-focused, solely aimed at ensuring seamless communication and clinical follow-up based on the provided reports."
Thematic Analysis of Chief Resident in Quality and Patient Safety (CRQS) Capstone Projects Using BERT Modeling,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Mission-Enabling (internal agency support),None of the above.,"The intended purpose of the AI is to perform a detailed thematic analysis of CRQS capstone projects to identify key themes and trends, with the expected benefit of enhancing the national curriculum for quality and patient safety in healthcare.","The AI system's outputs will be categorized themes, patterns, and trends derived from the analysis of CRQS capstone projects, which will inform curriculum development and improvement strategies.",Initiated,Neither,7/1/2024,Unknown,Unknown,Developed in-house.,Unknown,No,No,No,No,No,"The model used in this use case will be trained, fine-tuned, and evaluated using agency-owned data comprising Chief Resident in Quality and Patient Safety (CRQS) capstone project documents, which detail various quality improvement initiatives and patient safety measures undertaken at VA Medical Centers.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,No,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"The intended purpose of the AI is to perform a detailed thematic analysis of CRQS capstone projects to identify key themes and trends, with the expected benefit of enhancing the national curriculum for quality and patient safety in healthcare. . The AI system's outputs will be categorized themes, patterns, and trends derived from the analysis of CRQS capstone projects, which will inform curriculum development and improvement strategies.","the intended purpose of the ai is to perform a detailed thematic analysis of crqs capstone projects to identify key themes and trends, with the expected benefit of enhancing the national curriculum for quality and patient safety in healthcare. . the ai system's outputs will be categorized themes, patterns, and trends derived from the analysis of crqs capstone projects, which will inform curriculum development and improvement strategies."
My HealtheVet VSignals Main Improvement Summary,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"My HealtheVet is the VHA patient portal to access to refill medications, secure message with care teams, access medical records and view appointment information.  To ensure Veterans are being provided the best patient portal experiences, My HealtheVet collects many surveys (over 26k a month).  AI provides initial summarizing and grouping functions for open-text questions to identify trending issues and new features requested.  Rather than having humans read each comment and manually sorting, AI enhances trend identification and resource prioritization.","Veteran feedback is stripped of PII and then sorts comments based on product area, summary is primarily based on how often key words are mentioned.",Operation and Maintenance,Neither,7/19/2022,9/6/2022,9/9/2022,Developed with both contracting and in-house resources.,36C10X20D0007,Yes,No,Yes,No,No,"This is using open-sourced code requiring no training, essentially performance was evaluated if correct keys words were logged to provide useable outputs.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5555555555555556,"My HealtheVet is the VHA patient portal to access to refill medications, secure message with care teams, access medical records and view appointment information.  To ensure Veterans are being provided the best patient portal experiences, My HealtheVet collects many surveys (over 26k a month).  AI provides initial summarizing and grouping functions for open-text questions to identify trending issues and new features requested.  Rather than having humans read each comment and manually sorting, AI enhances trend identification and resource prioritization. . Veteran feedback is stripped of PII and then sorts comments based on product area, summary is primarily based on how often key words are mentioned.","my healthevet is the vha patient portal to access to refill medications, secure message with care teams, access medical records and view appointment information. to ensure veterans are being provided the best patient portal experiences, my healthevet collects many surveys (over 26k a month). ai provides initial summarizing and grouping functions for open-text questions to identify trending issues and new features requested. rather than having humans read each comment and manually sorting, ai enhances trend identification and resource prioritization. . veteran feedback is stripped of pii and then sorts comments based on product area, summary is primarily based on how often key words are mentioned."
Privacy Act Automation Services - Disclosure.AI,Department of Veterans Affairs,VA,VBA: Veterans Benefits Administration,Government Services (includes Benefits and Service Delivery),None of the above.,"The PAA System will automate PA workflows that are currently highly manual. Automating PA processing will benefit Veterans by decreasing the time they have to wait to receive requested records, and it will benefit the VA by reducing the time that staff need to support PA request processing. Tools/techniques that may be categorized as AI improve the automated processing that provide these benefits, but AI technology is ancillary to other technology utilized in the PAA system. Machine learning is used to identify specific documents (DD-214, Decision Letters, Service Records, etc.) and AI is used to improve and expedite data extraction as well as identify and apply automated redactions. The process is designed with Human in the Loop (HITL) and Government in the Loop (GITL) roles.","The PAA System automates the process of ingesting PA requests, retrieving requested records, and redacting requested records when necessary. The outputs of the PAA System are reviewed by Deloitte’s managed service team and by the VA for accuracy and completeness before the requested records are sent to the requester. The outputs consist of records held by VBA within Claim Evidence (VBMS) that have appropriate redactions applied for 3rd party information before release to the 1st party requestor or their designee.",Implementation and Assessment,Neither,1/10/2023,9/27/2023,11/18/2024,Developed in-house.,Unknown,Yes,No,Yes,Yes,Yes,"Agency Enterprise Data Warehouse (EDW) data is used to track pending and completed Privacy Act requests for information and their respective performance metrics, to include average days pending and average days to complete to ensure the D.AI solution exceeds the metrics for the existing manual process and meets the contractually obligated service levels for accuracy and turn around (completion) times.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,No – agency does not have access to source code.,Yes,Privacy Act Automation/Disclosure.AI,More than 12 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"The PAA System will automate PA workflows that are currently highly manual. Automating PA processing will benefit Veterans by decreasing the time they have to wait to receive requested records, and it will benefit the VA by reducing the time that staff need to support PA request processing. Tools/techniques that may be categorized as AI improve the automated processing that provide these benefits, but AI technology is ancillary to other technology utilized in the PAA system. Machine learning is used to identify specific documents (DD-214, Decision Letters, Service Records, etc.) and AI is used to improve and expedite data extraction as well as identify and apply automated redactions. The process is designed with Human in the Loop (HITL) and Government in the Loop (GITL) roles. . The PAA System automates the process of ingesting PA requests, retrieving requested records, and redacting requested records when necessary. The outputs of the PAA System are reviewed by Deloitte’s managed service team and by the VA for accuracy and completeness before the requested records are sent to the requester. The outputs consist of records held by VBA within Claim Evidence (VBMS) that have appropriate redactions applied for 3rd party information before release to the 1st party requestor or their designee.","the paa system will automate pa workflows that are currently highly manual. automating pa processing will benefit veterans by decreasing the time they have to wait to receive requested records, and it will benefit the va by reducing the time that staff need to support pa request processing. tools/techniques that may be categorized as ai improve the automated processing that provide these benefits, but ai technology is ancillary to other technology utilized in the paa system. machine learning is used to identify specific documents (dd-214, decision letters, service records, etc.) and ai is used to improve and expedite data extraction as well as identify and apply automated redactions. the process is designed with human in the loop (hitl) and government in the loop (gitl) roles. . the paa system automates the process of ingesting pa requests, retrieving requested records, and redacting requested records when necessary. the outputs of the paa system are reviewed by deloitte’s managed service team and by the va for accuracy and completeness before the requested records are sent to the requester. the outputs consist of records held by vba within claim evidence (vbms) that have appropriate redactions applied for 3rd party information before release to the 1st party requestor or their designee."
Sentiment analysis for app feedback,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"Functional Overview: The objective is to perform sentiment analysis on text reviews to classify their content as positive, negative, or neutral, and to provide a processed dataset for further downstream analytics.
 
System Overview: The system uses a trained model, derived from an open-source model and fine-tuned for the specific dataset, to analyze text reviews from various sources, including external sources (Google and Apple stores) and internal sources (FeedbackUI and VA Mobile). The model classifies reviews as positive, negative, or neutral.
 
Data: The data sources include:
 
Text reviews from the VA's mobile applications on the Google and Apple stores (external sources)

Reviews from FeedbackUI (internal source, available in the OIA_MobileHealth database)

Reviews from VA Mobile (internal source, available via CSV files on the Mobile VA's internal website)

Users: The users of this system are likely the OCC Data Science Team, who are responsible for developing and maintaining the pipeline.
 
Target Audience: The target audience OCC mobile application developers and internal stakeholders who may be interested in analyzing and understanding the sentiment and feedback from these reviews.
 ","perform sentiment analysis on text reviews to classify their content as positive, negative, or neutral, and to provide a processed dataset for further downstream analytics.",Operation and Maintenance,Neither,12/21/2022,3/8/2024,4/24/2024,Developed with contracting resources.,"Unable to identify in time, working with contracting team to close",No,No,Yes,No,Yes,CDW - VA application text review commentary for sentiment analysis,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,SDP / HDAP - working with contract team to identify most applicable ATO system name,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"Functional Overview: The objective is to perform sentiment analysis on text reviews to classify their content as positive, negative, or neutral, and to provide a processed dataset for further downstream analytics.
 
System Overview: The system uses a trained model, derived from an open-source model and fine-tuned for the specific dataset, to analyze text reviews from various sources, including external sources (Google and Apple stores) and internal sources (FeedbackUI and VA Mobile). The model classifies reviews as positive, negative, or neutral.
 
Data: The data sources include:
 
Text reviews from the VA's mobile applications on the Google and Apple stores (external sources)

Reviews from FeedbackUI (internal source, available in the OIA_MobileHealth database)

Reviews from VA Mobile (internal source, available via CSV files on the Mobile VA's internal website)

Users: The users of this system are likely the OCC Data Science Team, who are responsible for developing and maintaining the pipeline.
 
Target Audience: The target audience OCC mobile application developers and internal stakeholders who may be interested in analyzing and understanding the sentiment and feedback from these reviews. . perform sentiment analysis on text reviews to classify their content as positive, negative, or neutral, and to provide a processed dataset for further downstream analytics.","functional overview: the objective is to perform sentiment analysis on text reviews to classify their content as positive, negative, or neutral, and to provide a processed dataset for further downstream analytics. system overview: the system uses a trained model, derived from an open-source model and fine-tuned for the specific dataset, to analyze text reviews from various sources, including external sources (google and apple stores) and internal sources (feedbackui and va mobile). the model classifies reviews as positive, negative, or neutral. data: the data sources include: text reviews from the va's mobile applications on the google and apple stores (external sources) reviews from feedbackui (internal source, available in the oia_mobilehealth database) reviews from va mobile (internal source, available via csv files on the mobile va's internal website) users: the users of this system are likely the occ data science team, who are responsible for developing and maintaining the pipeline. target audience: the target audience occ mobile application developers and internal stakeholders who may be interested in analyzing and understanding the sentiment and feedback from these reviews. . perform sentiment analysis on text reviews to classify their content as positive, negative, or neutral, and to provide a processed dataset for further downstream analytics."
ESD-Predictive Intelligence,Department of Veterans Affairs,VA,OIT: Office of Information & Technology,Mission-Enabling (internal agency support),None of the above.,The intended purpose is to leverage Machine Learning on existing ticketing data supporting internal customers (non-veterans) to reduce mean time to resolution for internal customers while also increasing ticket routing accuracy and identifying major incidents more quickly as they occur.,Outputs involve pre-populated assignment groups in tickets within the ServiceNow IT Service Management platform to provide more accurate ticket routing and dashboard information for Incident Management staff to review highlighting potential major incidents beginning to occur in the environment.,Initiated,Neither,7/1/2024,Unknown,Unknown,Developed in-house.,Unknown,No,No,No,Yes,Yes,Ticketing data in the ServiceNow IT Service Management platform.,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,"Yes – agency has access to source code, but it is not public.",Yes,ServiceNow ,More than 12 months,Other,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,The intended purpose is to leverage Machine Learning on existing ticketing data supporting internal customers (non-veterans) to reduce mean time to resolution for internal customers while also increasing ticket routing accuracy and identifying major incidents more quickly as they occur. . Outputs involve pre-populated assignment groups in tickets within the ServiceNow IT Service Management platform to provide more accurate ticket routing and dashboard information for Incident Management staff to review highlighting potential major incidents beginning to occur in the environment.,the intended purpose is to leverage machine learning on existing ticketing data supporting internal customers (non-veterans) to reduce mean time to resolution for internal customers while also increasing ticket routing accuracy and identifying major incidents more quickly as they occur. . outputs involve pre-populated assignment groups in tickets within the servicenow it service management platform to provide more accurate ticket routing and dashboard information for incident management staff to review highlighting potential major incidents beginning to occur in the environment.
AI Health Coach,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,The Health Coach and AI components are different. The Health Coach helps improve adherence by sending evidence based nudges (this doesn't involve AI). The AI makes the health coach more effective by adjusting the number of nudges sent to the engagement of the Veteran.,"The Health Coach has a pre-selected decision tree (ex: did you miss your medication -> if yes, how many medications did you miss -> did you miss them because you ran out of refills). The AI simply helps determine how often to send those nudges based on response rates.",Operation and Maintenance,Neither,5/1/2024,9/22/2022,5/1/2024,Developed with both contracting and in-house resources.,NG15SD26B,No,No,Yes,Yes,Yes,"Master Patient Index (IEN, DOB, Address, Phone, PCMM)","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",No,"Yes – agency has access to source code, but it is not public.",Yes,1366,More than 12 months,Yes,Other,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5873015873015873,"The Health Coach and AI components are different. The Health Coach helps improve adherence by sending evidence based nudges (this doesn't involve AI). The AI makes the health coach more effective by adjusting the number of nudges sent to the engagement of the Veteran. . The Health Coach has a pre-selected decision tree (ex: did you miss your medication -> if yes, how many medications did you miss -> did you miss them because you ran out of refills). The AI simply helps determine how often to send those nudges based on response rates.","the health coach and ai components are different. the health coach helps improve adherence by sending evidence based nudges (this doesn't involve ai). the ai makes the health coach more effective by adjusting the number of nudges sent to the engagement of the veteran. . the health coach has a pre-selected decision tree (ex: did you miss your medication -> if yes, how many medications did you miss -> did you miss them because you ran out of refills). the ai simply helps determine how often to send those nudges based on response rates."
Verathon Prime Plus,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,It's a bladder scanner.  The AI component is used in relation with detection of bladder.  It's aim is to increase accuracy.,It outputs assisted measurements related to bladder size and shape.,Operation and Maintenance,Both,1/1/2024,1/1/2018,1/1/2018,Developed with both contracting and in-house resources.,Unknown,Yes,No,No,No,No,Unknown,Unknown,No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,No,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,It's a bladder scanner.  The AI component is used in relation with detection of bladder.  It's aim is to increase accuracy. . It outputs assisted measurements related to bladder size and shape.,it's a bladder scanner. the ai component is used in relation with detection of bladder. it's aim is to increase accuracy. . it outputs assisted measurements related to bladder size and shape.
App Feedback categorization model,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"Functional Overview: The objective is to categorize review texts into specific categories to provide more granularity and enable users to identify trends and perform analytics on the data.
 
System Overview: The system uses a trained open-source model that has been fine-tuned on the app feedback data to categorize review texts into specific categories, including Tech, Usability, Content, Idea, and Other.
 
Data: The data sources include:
 
Text reviews from the VA's mobile applications on the Google and Apple stores (external sources)

Reviews from FeedbackUI (internal source, available in the OIA_MobileHealth database)

Reviews from VA Mobile (internal source, available via CSV files on the Mobile VA's internal website)

Users: The users of this system are likely the OCC Data Science Team, who are responsible for developing and maintaining the pipeline.
 ",The objective is to categorize review texts into specific categories to provide more granularity and enable users to identify trends and perform analytics on the data.,Operation and Maintenance,Neither,12/2/2023,3/1/2024,4/16/2024,Developed with contracting resources.,"Unable to identify, working with contracting team to find most applicable PIID",No,No,Yes,No,Yes,CDW - VA application feedback,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,SDP / HDAP - working with contract team to better identify applicable system name,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Functional Overview: The objective is to categorize review texts into specific categories to provide more granularity and enable users to identify trends and perform analytics on the data.
 
System Overview: The system uses a trained open-source model that has been fine-tuned on the app feedback data to categorize review texts into specific categories, including Tech, Usability, Content, Idea, and Other.
 
Data: The data sources include:
 
Text reviews from the VA's mobile applications on the Google and Apple stores (external sources)

Reviews from FeedbackUI (internal source, available in the OIA_MobileHealth database)

Reviews from VA Mobile (internal source, available via CSV files on the Mobile VA's internal website)

Users: The users of this system are likely the OCC Data Science Team, who are responsible for developing and maintaining the pipeline. . The objective is to categorize review texts into specific categories to provide more granularity and enable users to identify trends and perform analytics on the data.","functional overview: the objective is to categorize review texts into specific categories to provide more granularity and enable users to identify trends and perform analytics on the data. system overview: the system uses a trained open-source model that has been fine-tuned on the app feedback data to categorize review texts into specific categories, including tech, usability, content, idea, and other. data: the data sources include: text reviews from the va's mobile applications on the google and apple stores (external sources) reviews from feedbackui (internal source, available in the oia_mobilehealth database) reviews from va mobile (internal source, available via csv files on the mobile va's internal website) users: the users of this system are likely the occ data science team, who are responsible for developing and maintaining the pipeline. . the objective is to categorize review texts into specific categories to provide more granularity and enable users to identify trends and perform analytics on the data."
LINQ and  LINQ II Loop Recorder ,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,Implantable heart monitoring device. This continuously checks the heart rhythm and rate.  ,The information (recordings) are remotely transmitted and recorded for interpretation by a Medical provider.,Operation and Maintenance,Both,12/21/2021,12/7/2021,12/27/2021,Developed with contracting resources.,Sara Lind Boston Scientific Peripheral Interventions Field Inventory Management sara.lind@bsci.com P: 763.255.0037,Yes,No,Yes,Yes,No,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5396825396825397,Implantable heart monitoring device. This continuously checks the heart rhythm and rate. . The information (recordings) are remotely transmitted and recorded for interpretation by a Medical provider.,implantable heart monitoring device. this continuously checks the heart rhythm and rate. . the information (recordings) are remotely transmitted and recorded for interpretation by a medical provider.
VA.gov Chatbot: Use of AI for summative and/or intent classification of Va.gov content,Department of Veterans Affairs,VA,OIT: Office of Information & Technology,Government Services (includes Benefits and Service Delivery),None of the above.,"The VA.gov Chatbot currently uses Natural Language Understanding (primarily driven by Microsoft Power Virtual Agents (PVA) and Custom Question Answer (CQA)) to interpret Veterans' intent and match it to responses within the knowledge base.

The future state vision for the VA’s chatbot solution involves leveraging advanced AI services and capabilities to increase the breadth of questions that the Chatbot will be able answer, to improve the accuracy of each answer, and in cases of necessary channel escalation, to accurately match user utterances to contact center routing queues.",Input: Understanding user intent: Copilot can determine what the user is trying to accomplish based on their query. Output: Generating relevant responses: The AI can then provide a suitable response or action based on the user's intent.,Operation and Maintenance,Neither,9/1/2021,9/1/2021,3/2/2022,Developed with both contracting and in-house resources.,"VA118-16-D-1015,36C10B21N10150056",No,Yes,No,Yes,Yes,Customer Experience Insights (CXI) data lake - all chatbot transcripts are written here and leveraged in analysis,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,VA.gov Chatbot,Unknown,No,No,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"The VA.gov Chatbot currently uses Natural Language Understanding (primarily driven by Microsoft Power Virtual Agents (PVA) and Custom Question Answer (CQA)) to interpret Veterans' intent and match it to responses within the knowledge base.

The future state vision for the VA’s chatbot solution involves leveraging advanced AI services and capabilities to increase the breadth of questions that the Chatbot will be able answer, to improve the accuracy of each answer, and in cases of necessary channel escalation, to accurately match user utterances to contact center routing queues. . Input: Understanding user intent: Copilot can determine what the user is trying to accomplish based on their query. Output: Generating relevant responses: The AI can then provide a suitable response or action based on the user's intent.","the va.gov chatbot currently uses natural language understanding (primarily driven by microsoft power virtual agents (pva) and custom question answer (cqa)) to interpret veterans' intent and match it to responses within the knowledge base. the future state vision for the va’s chatbot solution involves leveraging advanced ai services and capabilities to increase the breadth of questions that the chatbot will be able answer, to improve the accuracy of each answer, and in cases of necessary channel escalation, to accurately match user utterances to contact center routing queues. . input: understanding user intent: copilot can determine what the user is trying to accomplish based on their query. output: generating relevant responses: the ai can then provide a suitable response or action based on the user's intent."
Pharmacy AI Managed Inventory System (PHAIMIS),Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"PHAIMIS will be an AI-powered inventory management system to centralize and optimize inventory processes within the VA Pharmacy Service. This system will integrate AI and data analytics to streamline inventory management, provide real-time notifications, and enhance user experience through seamless integration into existing workflows.","System outputs will include demand forecasting, procurement strategy optimization, and dynamic inventory thresholds.  It may also include anomaly detection alerts and other features.",Initiated,Safety-impacting,5/21/2024,Unknown,Unknown,Developed with both contracting and in-house resources.,Unknown,No,No,No,No,Other,Prescription fill time-series data (note: PHI/PII not needed) needed for demand forecasting.  Pharmacy inventory data needed for ordering recommendations.,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,Unknown,Yes,Summit Data Platform,Less than 6 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Safety-Impacting,0.47619047619047616,"PHAIMIS will be an AI-powered inventory management system to centralize and optimize inventory processes within the VA Pharmacy Service. This system will integrate AI and data analytics to streamline inventory management, provide real-time notifications, and enhance user experience through seamless integration into existing workflows. . System outputs will include demand forecasting, procurement strategy optimization, and dynamic inventory thresholds.  It may also include anomaly detection alerts and other features.","phaimis will be an ai-powered inventory management system to centralize and optimize inventory processes within the va pharmacy service. this system will integrate ai and data analytics to streamline inventory management, provide real-time notifications, and enhance user experience through seamless integration into existing workflows. . system outputs will include demand forecasting, procurement strategy optimization, and dynamic inventory thresholds. it may also include anomaly detection alerts and other features."
Silverberry Surgery Planning AI,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"Assist in planning and operations in the perioperative process including sps, supplies, personell requirements and patinet planning. ",Output will be multiple AI agents responsible for various tasks related to the peri operative process.,Acquisition and/or Development,Both,9/26/2024,9/26/2024,Unknown,Developed with contracting resources.,Unknown,Yes,No,No,No,Yes,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.49206349206349204,"Assist in planning and operations in the perioperative process including sps, supplies, personell requirements and patinet planning. . Output will be multiple AI agents responsible for various tasks related to the peri operative process.","assist in planning and operations in the perioperative process including sps, supplies, personell requirements and patinet planning. . output will be multiple ai agents responsible for various tasks related to the peri operative process."
ICU CIS/ARK PDF Medical Entity and Clinical Scoring Extraction (PHI 3.5 LLM),Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"Extract clinical data elements from unstructured clinical note text into structured and semi-structured formats, mapping to standardized terminologies, thus making unstructured note text available to downstream analytics processes and clinical information systems.","Extracted data elements in structured and semi-structured formats, and mappings to standardized terminologies such as SNOMED, LOINC, ICD-10 and RxNorm.",Acquisition and/or Development,Safety-impacting,6/1/2023,6/3/2024,Unknown,Developed in-house.,Unknown,No,No,Yes,Yes,Yes,"TIU notes, CISARK PDF documents and other unstructured and semi-structured text-based patient care records","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Arches,Less than 6 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Safety-Impacting,0.49206349206349204,"Extract clinical data elements from unstructured clinical note text into structured and semi-structured formats, mapping to standardized terminologies, thus making unstructured note text available to downstream analytics processes and clinical information systems. . Extracted data elements in structured and semi-structured formats, and mappings to standardized terminologies such as SNOMED, LOINC, ICD-10 and RxNorm.","extract clinical data elements from unstructured clinical note text into structured and semi-structured formats, mapping to standardized terminologies, thus making unstructured note text available to downstream analytics processes and clinical information systems. . extracted data elements in structured and semi-structured formats, and mappings to standardized terminologies such as snomed, loinc, icd-10 and rxnorm."
Multi-Modal Digital Image Exchange - AI (MDIE-AI),Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,It creates an AI driven orchestration layer and generative AI summary for  automating computer vision models with saved dicom diagnostic images.,The system will output the results of each model into a generated summary PDF that is available for review directly from the dicom file.,Acquisition and/or Development,Safety-impacting,1/22/2024,2/19/2024,Unknown,Developed in-house.,Unknown,No,No,Yes,Yes,Yes,"radiology images, reports, problem list, demographics","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,ARCHES,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Safety-Impacting,0.5238095238095238,It creates an AI driven orchestration layer and generative AI summary for  automating computer vision models with saved dicom diagnostic images. . The system will output the results of each model into a generated summary PDF that is available for review directly from the dicom file.,it creates an ai driven orchestration layer and generative ai summary for automating computer vision models with saved dicom diagnostic images. . the system will output the results of each model into a generated summary pdf that is available for review directly from the dicom file.
Clinical outcomes for asynchronous teledermatology,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,Determine clinical outcomes of patients with skin disease for research and QI purposes.,"Classify dermatology progress notes into 6 classes:  Resolved, improved, unchanged (clinically significant), unchanged (not clinically significant), worse, and No outcome stated.",Acquisition and/or Development,Neither,6/9/2020,6/1/2022,Unknown,Developed in-house.,Unknown,No,No,No,No,Yes,Clinical notes and encounter data from CDW,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,Yes – source code is publicly available.,No,VINCI,Less than 6 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Determine clinical outcomes of patients with skin disease for research and QI purposes. . Classify dermatology progress notes into 6 classes:  Resolved, improved, unchanged (clinically significant), unchanged (not clinically significant), worse, and No outcome stated.","determine clinical outcomes of patients with skin disease for research and qi purposes. . classify dermatology progress notes into 6 classes: resolved, improved, unchanged (clinically significant), unchanged (not clinically significant), worse, and no outcome stated."
VA TryOpenAI,Department of Veterans Affairs,VA,OIT: Office of Information & Technology,Mission-Enabling (internal agency support),None of the above.,"TryOpenAI aims to improve VA employee efficiency and satisfaction. TryOpenAI can be used complete basic administrative tasks like drafting emails and communications, summarizing long text, and draft template project plans (among other similar uses). ","Tasks pilot users use TryOpenAI for    Writing Enhancement and Communication Support:  - Drafting and editing professional emails, letters of recommendation, and narrative write-ups.  - Generating and refining performance appraisals, award write-ups, and administrative correspondence.  - Summarizing large documents including meeting transcripts and project reports for email announcements    Administrative Task Automation:  - Streamlining the creation of meeting minutes to improve efficiency in documentation workflows.  - Automating responses for customer service inquiries or generating template-based outputs for routine tasks.  - Organizing thoughts and prioritizing information to aid in decision-making    Creative and Technical Problem Solving:  - Brainstorming ideas for project development and strategy formulation.  - Assisting with technical problems such as understanding PowerShell scripting or SQL configuration settings.  - Designing website layouts, drafting emails with complex content, and conceptualizing Power apps    Research Aid and Learning Tool:  - Gathering information on specific subjects like VA public information or medical guidelines.  - Finding synonyms or generating ideas for naming conventions in research contexts.  - Seeking deep insights into subject matter for personal knowledge enhancement or educational purposes    Amongst other similar tasks",Implementation and Assessment,Neither,9/1/2023,10/1/2023,11/1/2023,Developed with contracting resources.,"47QTCA22D003G, 36C10B22F0089",No,No,Yes,Yes,No,Unknown,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,TryOpenAI,Less than 6 months,Yes,Other,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"TryOpenAI aims to improve VA employee efficiency and satisfaction. TryOpenAI can be used complete basic administrative tasks like drafting emails and communications, summarizing long text, and draft template project plans (among other similar uses). . Tasks pilot users use TryOpenAI for    Writing Enhancement and Communication Support:  - Drafting and editing professional emails, letters of recommendation, and narrative write-ups.  - Generating and refining performance appraisals, award write-ups, and administrative correspondence.  - Summarizing large documents including meeting transcripts and project reports for email announcements    Administrative Task Automation:  - Streamlining the creation of meeting minutes to improve efficiency in documentation workflows.  - Automating responses for customer service inquiries or generating template-based outputs for routine tasks.  - Organizing thoughts and prioritizing information to aid in decision-making    Creative and Technical Problem Solving:  - Brainstorming ideas for project development and strategy formulation.  - Assisting with technical problems such as understanding PowerShell scripting or SQL configuration settings.  - Designing website layouts, drafting emails with complex content, and conceptualizing Power apps    Research Aid and Learning Tool:  - Gathering information on specific subjects like VA public information or medical guidelines.  - Finding synonyms or generating ideas for naming conventions in research contexts.  - Seeking deep insights into subject matter for personal knowledge enhancement or educational purposes    Amongst other similar tasks","tryopenai aims to improve va employee efficiency and satisfaction. tryopenai can be used complete basic administrative tasks like drafting emails and communications, summarizing long text, and draft template project plans (among other similar uses). . tasks pilot users use tryopenai for writing enhancement and communication support: - drafting and editing professional emails, letters of recommendation, and narrative write-ups. - generating and refining performance appraisals, award write-ups, and administrative correspondence. - summarizing large documents including meeting transcripts and project reports for email announcements administrative task automation: - streamlining the creation of meeting minutes to improve efficiency in documentation workflows. - automating responses for customer service inquiries or generating template-based outputs for routine tasks. - organizing thoughts and prioritizing information to aid in decision-making creative and technical problem solving: - brainstorming ideas for project development and strategy formulation. - assisting with technical problems such as understanding powershell scripting or sql configuration settings. - designing website layouts, drafting emails with complex content, and conceptualizing power apps research aid and learning tool: - gathering information on specific subjects like va public information or medical guidelines. - finding synonyms or generating ideas for naming conventions in research contexts. - seeking deep insights into subject matter for personal knowledge enhancement or educational purposes amongst other similar tasks"
Lyssn for Mental Health,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"To improve fidelity to evidence based practices in Behavioral Health, such as Motivational Interviewing. ","Lyssn uses AI algorithms to automatically generate clinical quality metrics from a recording (or  transcript) of a provider-patient session or appointment. Metrics range from general counseling  and patient-centered communication to fidelity to evidence-based counseling, including  Motivational Interviewing and Cognitive Behavioral Therapy.",Operation and Maintenance,Neither,10/3/2022,6/15/2023,9/5/2023,Unknown,Unknown,Yes,No,No,No,No,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"To improve fidelity to evidence based practices in Behavioral Health, such as Motivational Interviewing. . Lyssn uses AI algorithms to automatically generate clinical quality metrics from a recording (or  transcript) of a provider-patient session or appointment. Metrics range from general counseling  and patient-centered communication to fidelity to evidence-based counseling, including  Motivational Interviewing and Cognitive Behavioral Therapy.","to improve fidelity to evidence based practices in behavioral health, such as motivational interviewing. . lyssn uses ai algorithms to automatically generate clinical quality metrics from a recording (or transcript) of a provider-patient session or appointment. metrics range from general counseling and patient-centered communication to fidelity to evidence-based counseling, including motivational interviewing and cognitive behavioral therapy."
SOMATOM go.Up; SOMATOM go.Now; SOMATOM go.All; SOMATOM go.Top; SOMATOM go.Sim; SOMATOM go.Open Pro; SOMATOM X.cite; SOMATOM X.ceed,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K233650.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K233650.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/k233650.pdf).
Vivid iq,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K221148.pdf),Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K221148.pdf),fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/k221148.pdf)
Vivid E80/ Vivid E90/ Vivid E95,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K220882.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K220882.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/k220882.pdf).
Payment Redirect Fraud (PRF) Model,Department of Veterans Affairs,VA,VBA: Veterans Benefits Administration,Government Services (includes Benefits and Service Delivery),None of the above.,"Criminals make direct deposit (DD) changes to steal Veterans’ benefit payments. Most direct deposit changes are safe, but 1-2 out of 1,000 (.1% - .2%) are fraudulent. The goal of the Payment Redirect Fraud (PRF) model is to identify which DD changes are likely to be fraudulent and refer them to team investigators for review and remediation.",The model identifies referrals (Veterans) with the highest scores indicating likely fraudulent activity. The outputs include Veteran PII and associated direct deposit bank account information.  These outputs are in the form of an R data file and an Excel file.,Operation and Maintenance,"Rights-Impacting
",10/1/2019,12/1/2021,10/21/2022,Developed with both contracting and in-house resources.,GS-00F-008DA,No,No,No,No,Yes,We use the population data from all historical fraud cases as well as model learning as a result of daily outputs.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,"The PRF model does not run in a systems or a cloud, it runs by manual intervention on employee/contractor GFE machines",More than 12 months,No,Yes,No,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,"False positives could result from the model output; however, this is mitigated by the fact that all referrals from the model output undergo a manual investigation by a fraud investigator.",TRUE,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,The model does not incorporate race or gender into the model features.,"[""None of the above""]",No – it is not operationally practical to offer this.,Rights-Impacting,0.7777777777777778,"Criminals make direct deposit (DD) changes to steal Veterans’ benefit payments. Most direct deposit changes are safe, but 1-2 out of 1,000 (.1% - .2%) are fraudulent. The goal of the Payment Redirect Fraud (PRF) model is to identify which DD changes are likely to be fraudulent and refer them to team investigators for review and remediation. . The model identifies referrals (Veterans) with the highest scores indicating likely fraudulent activity. The outputs include Veteran PII and associated direct deposit bank account information.  These outputs are in the form of an R data file and an Excel file. . False positives could result from the model output; however, this is mitigated by the fact that all referrals from the model output undergo a manual investigation by a fraud investigator.","criminals make direct deposit (dd) changes to steal veterans’ benefit payments. most direct deposit changes are safe, but 1-2 out of 1,000 (.1% - .2%) are fraudulent. the goal of the payment redirect fraud (prf) model is to identify which dd changes are likely to be fraudulent and refer them to team investigators for review and remediation. . the model identifies referrals (veterans) with the highest scores indicating likely fraudulent activity. the outputs include veteran pii and associated direct deposit bank account information. these outputs are in the form of an r data file and an excel file. . false positives could result from the model output; however, this is mitigated by the fact that all referrals from the model output undergo a manual investigation by a fraud investigator."
"EchoPAC Software Only, EchoPAC Plug-In",Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K220940.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K220940.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/k220940.pdf).
Using Partially Observed Markov Decision Process to Implement a Response to Intervention Framework for Veterans with Post-Traumatic Stress Disorder ,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,The Partially Observed Markov Decision Process (POMDP) model will be eligible to help clinicians to provide more personalized treatment to minimize or eliminate the PTSD symptoms in a short amount of time (thus costing less) and enable veterans to return to civilian life more quickly than the current practice,,Initiated,Both,12/4/2023,Unknown,Unknown,Developed in-house.,research IRB approved,No,No,Yes,Yes,Other,"labs, demographics, visits, problems, medications, social history","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,VINCI,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5238095238095238,The Partially Observed Markov Decision Process (POMDP) model will be eligible to help clinicians to provide more personalized treatment to minimize or eliminate the PTSD symptoms in a short amount of time (thus costing less) and enable veterans to return to civilian life more quickly than the current practice,the partially observed markov decision process (pomdp) model will be eligible to help clinicians to provide more personalized treatment to minimize or eliminate the ptsd symptoms in a short amount of time (thus costing less) and enable veterans to return to civilian life more quickly than the current practice
Microsoft Power Automate and AI  ,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Transportation,None of the above.,"To retrieve information from external facilities (SNF, CNH) in order to allow our facility (Veterans Transport Program) to facilitate scheduling of transport of eligible veterans to and from our facility or to community care appointments.","The AI program extracts information from a request form (pdf) attached to an email. The information is then populated in an Excel spreadsheet.  The data points include the date of request, date transport needed, name of requesting facility, person completing the form, Veteran's last name, first name and last 4, attendance needed, trip type, pick up time, appointment time, facility pick up address, appt location, destination address, oxygen needed, weight of patient.",Operation and Maintenance,Neither,7/5/2023,5/1/2023,6/1/2023,Unknown,Unknown,Yes,No,Yes,Yes,No,Unknown,Unknown,No,Unknown,No,MS Office 365 - Power Automate ,Less than 6 months,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,"To retrieve information from external facilities (SNF, CNH) in order to allow our facility (Veterans Transport Program) to facilitate scheduling of transport of eligible veterans to and from our facility or to community care appointments. . The AI program extracts information from a request form (pdf) attached to an email. The information is then populated in an Excel spreadsheet.  The data points include the date of request, date transport needed, name of requesting facility, person completing the form, Veteran's last name, first name and last 4, attendance needed, trip type, pick up time, appointment time, facility pick up address, appt location, destination address, oxygen needed, weight of patient.","to retrieve information from external facilities (snf, cnh) in order to allow our facility (veterans transport program) to facilitate scheduling of transport of eligible veterans to and from our facility or to community care appointments. . the ai program extracts information from a request form (pdf) attached to an email. the information is then populated in an excel spreadsheet. the data points include the date of request, date transport needed, name of requesting facility, person completing the form, veteran's last name, first name and last 4, attendance needed, trip type, pick up time, appointment time, facility pick up address, appt location, destination address, oxygen needed, weight of patient."
IVC Access Transformation Transformative Care Modalities VA Health Connect Using Machine Learning to Evaluate First Contact Resolution (FCR),Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,For analytics purposes to generate performance metrics on First Contact Resolution for the Clinical Triage core service. Does not influence medical or critical decisions and does not use any sensitive Veteran data.,"The output of the model allows us to classify each clinical triage as resolved or not resolved, so that we can generate performance metrics on First Contact Resolution for the Clinical Triage core service.",Acquisition and/or Development,Neither,10/23/2023,10/24/2023,Unknown,Developed with contracting resources.,"VA118-16-D-1015, 36C10B21N10150056",No,No,No,No,Yes,The language model is being used to compare free text fields within the Customer Relationship Management (CRM) application that capture details about the purpose of each case.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Summit Data Platform,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"For analytics purposes to generate performance metrics on First Contact Resolution for the Clinical Triage core service. Does not influence medical or critical decisions and does not use any sensitive Veteran data. . The output of the model allows us to classify each clinical triage as resolved or not resolved, so that we can generate performance metrics on First Contact Resolution for the Clinical Triage core service.","for analytics purposes to generate performance metrics on first contact resolution for the clinical triage core service. does not influence medical or critical decisions and does not use any sensitive veteran data. . the output of the model allows us to classify each clinical triage as resolved or not resolved, so that we can generate performance metrics on first contact resolution for the clinical triage core service."
Avicenna ICH ,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,The function of the AI is to detect possible intracranial hemorrhage (ICH) on CT head exams to aid and expedite diagnosis.,The input is DICOM format CT head image data from VHA facilities that submit the exam to VHA National Teleradiology Program (NTP) for interpretation.  The output is DICOM object written to the NTP Picture Archive Communication System (PACS) for review by an interpreting VHA NTP Diagnostic Radiologist.  Future output will also include a DICOM structured report to the VHA NTP PACS Powerscribe dictation software.,Operation and Maintenance,Both,4/1/2020,6/1/2020,10/1/2021,Developed with contracting resources.,36C10B19C0016,Yes,No,Yes,Yes,Yes,"N/A - FDA 510K cleared COTS product, trained using OEM data","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,National Teleradiology Program Next Generation Picture Archive and Communication System #2418,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5714285714285714,The function of the AI is to detect possible intracranial hemorrhage (ICH) on CT head exams to aid and expedite diagnosis. . The input is DICOM format CT head image data from VHA facilities that submit the exam to VHA National Teleradiology Program (NTP) for interpretation.  The output is DICOM object written to the NTP Picture Archive Communication System (PACS) for review by an interpreting VHA NTP Diagnostic Radiologist.  Future output will also include a DICOM structured report to the VHA NTP PACS Powerscribe dictation software.,the function of the ai is to detect possible intracranial hemorrhage (ich) on ct head exams to aid and expedite diagnosis. . the input is dicom format ct head image data from vha facilities that submit the exam to vha national teleradiology program (ntp) for interpretation. the output is dicom object written to the ntp picture archive communication system (pacs) for review by an interpreting vha ntp diagnostic radiologist. future output will also include a dicom structured report to the vha ntp pacs powerscribe dictation software.
Densitas Density AI,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"The function of the AI is to incorporate quantitative mammographic density with the qualitative 4-category (A, B, C, D) breast density scale that aligns with the ACR BI-RADS Atlas 5th edition breast density scale.  The AI accounts for both the degree of mammographic density and the presence of localized areas of dense breast tissue to aid and expedite mammographic diagnosis.",The input is DICOM mammography image data from VHA facilities that submit digital mammogram exams to VHA National Teleradiology Program (NTP) for interpretation.  The output is DICOM object written to the NTP Picture Archive Communication System (PACS) for review by an interpreting VHA NTP Diagnostic Radiologist.  Future output will also include a DICOM structured report to the VHA NTP PACS Powerscribe dictation software.,Operation and Maintenance,Both,6/1/2022,9/1/2022,11/1/2022,Developed with contracting resources.,36C10B22C0042,Yes,No,Yes,Yes,Yes,"N/A - FDA 510K cleared COTS product, trained using OEM data","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,National Teleradiology Program Next Generation Picture Archive and Communication System #2418,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5714285714285714,"The function of the AI is to incorporate quantitative mammographic density with the qualitative 4-category (A, B, C, D) breast density scale that aligns with the ACR BI-RADS Atlas 5th edition breast density scale.  The AI accounts for both the degree of mammographic density and the presence of localized areas of dense breast tissue to aid and expedite mammographic diagnosis. . The input is DICOM mammography image data from VHA facilities that submit digital mammogram exams to VHA National Teleradiology Program (NTP) for interpretation.  The output is DICOM object written to the NTP Picture Archive Communication System (PACS) for review by an interpreting VHA NTP Diagnostic Radiologist.  Future output will also include a DICOM structured report to the VHA NTP PACS Powerscribe dictation software.","the function of the ai is to incorporate quantitative mammographic density with the qualitative 4-category (a, b, c, d) breast density scale that aligns with the acr bi-rads atlas 5th edition breast density scale. the ai accounts for both the degree of mammographic density and the presence of localized areas of dense breast tissue to aid and expedite mammographic diagnosis. . the input is dicom mammography image data from vha facilities that submit digital mammogram exams to vha national teleradiology program (ntp) for interpretation. the output is dicom object written to the ntp picture archive communication system (pacs) for review by an interpreting vha ntp diagnostic radiologist. future output will also include a dicom structured report to the vha ntp pacs powerscribe dictation software."
CLARUS,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf19/K191194.pdf).,Operation and Maintenance,Both,2/4/2022,2/4/2022,2/4/2022,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5555555555555556,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf19/K191194.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf19/k191194.pdf).
Riverain ClearRead CT,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"The function of the AI is to analyze CT chest exams and support identification and characterization of pulmonary nodules of all types (sold, part-solid, ground glass), especially for lung cancer screening exams.  The CT Vessel Suppress produces a secondary series of CT images, suppressing vessels and other normal structures within the lungs to improve conspicuity of pulmonary nodules.  ClearRead CT Compare extends detection by automatically matching nodules found in the current exam to the same nodule on a prior exam.

The AI is managed by VHA National Teleradiology Program (NTP) and is hosted in VA Enterprise Cloud (VAEC) or on-premise servers depending on the individual VISN/facility. ","The input is DICOM CT chest image data.   The output is DICOM object written to the PACS for review by an interpreting Diagnostic Radiologist.   Future output will also include a DICOM structured report to the voice recognition dictation software.  The AI is deployed by NTP for the use of the individual VISN and facility Radiologists interpreting the exam, or for NTP Radiologist use.",Operation and Maintenance,Both,6/1/2021,9/1/2022,2/1/2023,Developed with contracting resources.,36H79719D0016,Yes,No,Yes,Yes,Yes,"N/A - FDA 510K cleared COTS product, trained using OEM data","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,VHA Centralized Lung CT CAD System #2219,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5714285714285714,"The function of the AI is to analyze CT chest exams and support identification and characterization of pulmonary nodules of all types (sold, part-solid, ground glass), especially for lung cancer screening exams.  The CT Vessel Suppress produces a secondary series of CT images, suppressing vessels and other normal structures within the lungs to improve conspicuity of pulmonary nodules.  ClearRead CT Compare extends detection by automatically matching nodules found in the current exam to the same nodule on a prior exam.

The AI is managed by VHA National Teleradiology Program (NTP) and is hosted in VA Enterprise Cloud (VAEC) or on-premise servers depending on the individual VISN/facility. . The input is DICOM CT chest image data.   The output is DICOM object written to the PACS for review by an interpreting Diagnostic Radiologist.   Future output will also include a DICOM structured report to the voice recognition dictation software.  The AI is deployed by NTP for the use of the individual VISN and facility Radiologists interpreting the exam, or for NTP Radiologist use.","the function of the ai is to analyze ct chest exams and support identification and characterization of pulmonary nodules of all types (sold, part-solid, ground glass), especially for lung cancer screening exams. the ct vessel suppress produces a secondary series of ct images, suppressing vessels and other normal structures within the lungs to improve conspicuity of pulmonary nodules. clearread ct compare extends detection by automatically matching nodules found in the current exam to the same nodule on a prior exam. the ai is managed by vha national teleradiology program (ntp) and is hosted in va enterprise cloud (vaec) or on-premise servers depending on the individual visn/facility. . the input is dicom ct chest image data. the output is dicom object written to the pacs for review by an interpreting diagnostic radiologist. future output will also include a dicom structured report to the voice recognition dictation software. the ai is deployed by ntp for the use of the individual visn and facility radiologists interpreting the exam, or for ntp radiologist use."
ACS NSQIP Risk Score,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,Clinical decision support by providing surgical risk assessment,Provides a surgical risk assessment for a patient based on a set of values input by a provider.,Operation and Maintenance,Both,12/16/2013,11/1/2014,11/1/2014,Unknown,"none, free product",Yes,Yes,No,No,No,"The history of the ACS/NSQIP initially started in response to issues around surgical quality and the VA in the VASQIP national database.  There are around 600 or so publications regarding the tool including a number with use case assessment using VA data.  There may be agency owned data used previously in the ACS/NSQIP development process, but I cannot confirm that based on review work completed so far, but given the history of association with VASQIP and the VA, the model was in part influenced originally from VA findings per the history on the tool.    The history of the tool's development is noted here:  https://www.facs.org/quality-programs/data-and-registries/acs-nsqip/history/","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,No,Unknown,Unknown,No,No,Other,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5714285714285714,Clinical decision support by providing surgical risk assessment . Provides a surgical risk assessment for a patient based on a set of values input by a provider.,clinical decision support by providing surgical risk assessment . provides a surgical risk assessment for a patient based on a set of values input by a provider.
Avicenna LVO,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"The function of the AI is to detect large vessel occlusion in the anterior intracranial circulation (distal Internal Carotid Artery, M1 and proximal M2 Middle Cerebral Artery) on Head CT Angiography exams to aid and expedite diagnosis.",The input is DICOM Head CT image data from VHA facilities that submit the exam to VHA NTP for interpretation.   The output is DICOM object written to the NTP Picture Archive Communication System (PACS) for review by the interpreting VHA NTP Diagnostic Radiologist.  Future output will also include a DICOM structured report to the VHA NTP PACS Powerscribe dictation software.,Implementation and Assessment,Both,3/1/2021,6/1/2022,12/1/2024,Developed with contracting resources.,36C10B23C0025,Yes,No,Yes,Yes,Yes,"N/A - FDA 510K cleared COTS product, trained using OEM data","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,National Teleradiology Program Next Generation Picture Archive and Communication System #2418,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5714285714285714,"The function of the AI is to detect large vessel occlusion in the anterior intracranial circulation (distal Internal Carotid Artery, M1 and proximal M2 Middle Cerebral Artery) on Head CT Angiography exams to aid and expedite diagnosis. . The input is DICOM Head CT image data from VHA facilities that submit the exam to VHA NTP for interpretation.   The output is DICOM object written to the NTP Picture Archive Communication System (PACS) for review by the interpreting VHA NTP Diagnostic Radiologist.  Future output will also include a DICOM structured report to the VHA NTP PACS Powerscribe dictation software.","the function of the ai is to detect large vessel occlusion in the anterior intracranial circulation (distal internal carotid artery, m1 and proximal m2 middle cerebral artery) on head ct angiography exams to aid and expedite diagnosis. . the input is dicom head ct image data from vha facilities that submit the exam to vha ntp for interpretation. the output is dicom object written to the ntp picture archive communication system (pacs) for review by the interpreting vha ntp diagnostic radiologist. future output will also include a dicom structured report to the vha ntp pacs powerscribe dictation software."
Ysio Max,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf13/K133259.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf13/K133259.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf13/k133259.pdf).
Transpara Breast Care,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"The function of the AI is to analyze digital mammograms to identify and characterize patterns of breast tissue suspicious for breast cancer, potentially identifying cancers more quickly and at an earlier stage.",The input is DICOM mammography image data from VHA facilities that submit mammography exams to VHA National Teleradiology Program (NTP) for interpretation.  The output is DICOM object written to the NTP PACS for review by an interpreting VHA NTP Diagnostic Radiologist.  Future output will also include a DICOM structured report to the VHA NTP PACS Powerscribe dictation software.,Implementation and Assessment,Both,6/1/2022,6/1/2022,12/1/2024,Developed with contracting resources.,36C10B23C0025,Yes,No,Yes,Yes,Yes,"N/A - FDA 510K cleared COTS product, trained using OEM data","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,National Teleradiology Program Next Generation Picture Archive and Communication System #2418,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5714285714285714,"The function of the AI is to analyze digital mammograms to identify and characterize patterns of breast tissue suspicious for breast cancer, potentially identifying cancers more quickly and at an earlier stage. . The input is DICOM mammography image data from VHA facilities that submit mammography exams to VHA National Teleradiology Program (NTP) for interpretation.  The output is DICOM object written to the NTP PACS for review by an interpreting VHA NTP Diagnostic Radiologist.  Future output will also include a DICOM structured report to the VHA NTP PACS Powerscribe dictation software.","the function of the ai is to analyze digital mammograms to identify and characterize patterns of breast tissue suspicious for breast cancer, potentially identifying cancers more quickly and at an earlier stage. . the input is dicom mammography image data from vha facilities that submit mammography exams to vha national teleradiology program (ntp) for interpretation. the output is dicom object written to the ntp pacs for review by an interpreting vha ntp diagnostic radiologist. future output will also include a dicom structured report to the vha ntp pacs powerscribe dictation software."
"XIDF-AWS801, Angio Workstation (Alphenix Workstation), V9.5",Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K232526.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K232526.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/k232526.pdf).
Synthetic Data Creation,Department of Veterans Affairs,VA,VBA: Veterans Benefits Administration,Education & Workforce,None of the above.,"Synthetic (non-PII) test data creation, using Generative AI, at the scale and representation of the diverse characteristics of the beneficiary population necessary to support EDU UAT.  ",Synthetic beneficiary population data,Initiated,Neither,7/18/2024,Unknown,Unknown,Developed with contracting resources.,VA118-16-D-1013 36C10D21N0007,No,No,No,Yes,No,GI Bill beneficiary demographic data.  This model's purpose is to replicate real data but with synthetic identifiers to avoid using PII in User Acceptance Testing.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",No,No – agency does not have access to source code.,Yes,Digital GI Bill,Less than 6 months,No,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Synthetic (non-PII) test data creation, using Generative AI, at the scale and representation of the diverse characteristics of the beneficiary population necessary to support EDU UAT. . Synthetic beneficiary population data","synthetic (non-pii) test data creation, using generative ai, at the scale and representation of the diverse characteristics of the beneficiary population necessary to support edu uat. . synthetic beneficiary population data"
Billie GPT,Department of Veterans Affairs,VA,VBA: Veterans Benefits Administration,Education & Workforce,None of the above.,Generative AI knowledge portal to support School Certifying Officials in addressing questions regarding VA education benefits and use of the Enrollment Manager System.,Generative AI response to the user query,Acquisition and/or Development,Neither,3/5/2024,4/29/2024,Unknown,Developed with both contracting and in-house resources.,VA118-16-D-1013 36C10D21N0007,No,Yes,No,Yes,No,"Training: VA-created user guides, GI Bill-related documents, and training resources.  Evaluating performance: Call Center volume, User feedback scores, Increased use over current rule-based NLP bot.","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,Digital GI Bill,Less than 6 months,No,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,Generative AI knowledge portal to support School Certifying Officials in addressing questions regarding VA education benefits and use of the Enrollment Manager System. . Generative AI response to the user query,generative ai knowledge portal to support school certifying officials in addressing questions regarding va education benefits and use of the enrollment manager system. . generative ai response to the user query
X100HT with Slide Loader with Full Field Peripheral Blood Smear (PBS) Application,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K220013.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K220013.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/k220013.pdf).
WRDensity by Whiterabbit.ai,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf20/K202013.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf20/K202013.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf20/k202013.pdf).
Workflow Box,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf18/K181572.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf18/K181572.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf18/k181572.pdf).
Withings Scan Monitor 2.0,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K230812.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K230812.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/k230812.pdf).
HTM-LLM,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Mission-Enabling (internal agency support),None of the above.,RAG chatbot using technical service manuals to aid Healthcare Technology Management staff in medical device maintenance and troubleshooting,Chatbot responses,Initiated,Neither,5/1/2024,Unknown,Unknown,Developed in-house.,Unknown,No,No,No,No,No,"none, uses manufacturer service manuals","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,Yes – source code is publicly available.,No,Unknown,Unknown,No,No,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,RAG chatbot using technical service manuals to aid Healthcare Technology Management staff in medical device maintenance and troubleshooting . Chatbot responses,rag chatbot using technical service manuals to aid healthcare technology management staff in medical device maintenance and troubleshooting . chatbot responses
Clinician-Administered PTSD Scale for DSM-5 (CAPS-5) Clinician Training Simulator,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Education & Workforce,None of the above.,"The CAPS-5 Clinician Training Simulator comprises three online courses, available in TMS and TRAIN, that provide instruction in the administration and scoring of the CAPS-5. The courses use voice recognition, 3-D technology, and artificial intelligence to create an immersive learning experience. The courses are intended for clinicians, trainees, researchers, and examiners in VA and in the community. They use AI in the following ways: 1. In the three current CAPS-5 virtual patient courses, learners have the option to speak the prompts, with their spoken prompts transformed to text using speech recognition. 2. The learners’ prompts, whether spoken or typed, are programmatically compared to the required prompts in the CAPS-5. The system analyzes whether the prompts were delivered exactly (“verbatim”), with slight variations (“paraphrased”), or with unacceptable variations (“off-script”). 3. One of the courses analyzes learner prompts in order to trigger comments –corrections or praise -- from a virtual coach. 4. We’re currently revising one of the courses to use generative AI. Here’s how the vendor described it in their response to our solicitation: “This new design will also leverage a separate backend to handle AI multi-agency and retrieval augmented generation (RAG) to ensure large language models (LLMs) used will always operate as designed.” This will be a closed system that does not pull information from or input information to publicly-available platforms (e.g., GPT-4, Bard). More information about the three courses and access to them is available at https://www.ptsd.va.gov/professional/continuing_ed/caps5_clinician_training.asp.

","The AI system outputs, narrowly defined, are the virtual patients' responses to the learners' prompts. More broadly, the system is designed to help learners learn to more accurately administer and score the CAPS-5. Learners receive a summary document describing their performance at the conclusion of each course.",Operation and Maintenance,Neither,10/15/2018,11/29/2018,3/13/2020,Developed with contracting resources.,3610G8Q9155,No,No,No,No,No,Unknown,Unknown,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Unknown,No,No,No,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"The CAPS-5 Clinician Training Simulator comprises three online courses, available in TMS and TRAIN, that provide instruction in the administration and scoring of the CAPS-5. The courses use voice recognition, 3-D technology, and artificial intelligence to create an immersive learning experience. The courses are intended for clinicians, trainees, researchers, and examiners in VA and in the community. They use AI in the following ways: 1. In the three current CAPS-5 virtual patient courses, learners have the option to speak the prompts, with their spoken prompts transformed to text using speech recognition. 2. The learners’ prompts, whether spoken or typed, are programmatically compared to the required prompts in the CAPS-5. The system analyzes whether the prompts were delivered exactly (“verbatim”), with slight variations (“paraphrased”), or with unacceptable variations (“off-script”). 3. One of the courses analyzes learner prompts in order to trigger comments –corrections or praise -- from a virtual coach. 4. We’re currently revising one of the courses to use generative AI. Here’s how the vendor described it in their response to our solicitation: “This new design will also leverage a separate backend to handle AI multi-agency and retrieval augmented generation (RAG) to ensure large language models (LLMs) used will always operate as designed.” This will be a closed system that does not pull information from or input information to publicly-available platforms (e.g., GPT-4, Bard). More information about the three courses and access to them is available at https://www.ptsd.va.gov/professional/continuing_ed/caps5_clinician_training.asp. . The AI system outputs, narrowly defined, are the virtual patients' responses to the learners' prompts. More broadly, the system is designed to help learners learn to more accurately administer and score the CAPS-5. Learners receive a summary document describing their performance at the conclusion of each course.","the caps-5 clinician training simulator comprises three online courses, available in tms and train, that provide instruction in the administration and scoring of the caps-5. the courses use voice recognition, 3-d technology, and artificial intelligence to create an immersive learning experience. the courses are intended for clinicians, trainees, researchers, and examiners in va and in the community. they use ai in the following ways: 1. in the three current caps-5 virtual patient courses, learners have the option to speak the prompts, with their spoken prompts transformed to text using speech recognition. 2. the learners’ prompts, whether spoken or typed, are programmatically compared to the required prompts in the caps-5. the system analyzes whether the prompts were delivered exactly (“verbatim”), with slight variations (“paraphrased”), or with unacceptable variations (“off-script”). 3. one of the courses analyzes learner prompts in order to trigger comments –corrections or praise -- from a virtual coach. 4. we’re currently revising one of the courses to use generative ai. here’s how the vendor described it in their response to our solicitation: “this new design will also leverage a separate backend to handle ai multi-agency and retrieval augmented generation (rag) to ensure large language models (llms) used will always operate as designed.” this will be a closed system that does not pull information from or input information to publicly-available platforms (e.g., gpt-4, bard). more information about the three courses and access to them is available at https://www.ptsd.va.gov/professional/continuing_ed/caps5_clinician_training.asp. . the ai system outputs, narrowly defined, are the virtual patients' responses to the learners' prompts. more broadly, the system is designed to help learners learn to more accurately administer and score the caps-5. learners receive a summary document describing their performance at the conclusion of each course."
WellDoc BlueStar,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf19/K190013.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf19/K190013.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf19/k190013.pdf).
Electronic Virtual Assistant (e-VA),Department of Veterans Affairs,VA,VBA: Veterans Benefits Administration,Government Services (includes Benefits and Service Delivery),None of the above.,"The electronic Virtual Assistant (e-VA) information system is a product acquired from SaraWorks (formerly The Career Index (TCI)) by the Veterans Benefits Administration (VBA) Veteran Readiness & Employment (VR&E) Business Line. e-VA is a breakthrough digital assistant that does most of the work a human assistant can do in terms of client follow-up, data entry, and documentation. The primary objective for implementing e-VA is to provide virtual support services that will alleviate administrative tasks from Vocational Rehabilitation Counselors (VRCs), which enable them to focus their time on the Veterans’ needs, fulfilling VR&E’s mission of guiding Veterans to successful outcomes. e-VA is the only digital assistant focused on applying these autonomous capabilities to the complex field of human services. The e-VA system will create reminders for appointments, grades, receipts, etc. and accept documents and images, in addition to connecting program participants directly to their Counselor. The application also enables automated routine electronic communication with program participants through either text message and/or e-mail. Participants can interact with the e-VA application via text or email to schedule and reschedule appointments, respond to reminders, and submit supporting documents and receipts related to their VR&E case. The e-VA information system is single-tenant, which resides in SaraWorks private AWS GovCloud. It is not a public-facing system. The SARA Sync windows executable will receive information from the e-VA system, such as supporting documents, receipts, etc., case notes, or updating of status on their case. This information are transmitted to the VA database of record for storage.","Allow VRCs to focus on providing world-class service to our Veterans. The e-VA system will create reminders for appointments, grades, receipts, etc. and accept documents and images, in addition to connecting program participants directly to their Counselor. The Case Manager ID will be used to identify case managers and assigned caseload which will determine the location of the requested appointment. The application will also enable automated routine electronic communication with program participant through either text message and/or e-mail. Participants can interact with the e-VA application via text or email to schedule and reschedule appointments, respond to reminders, and submit supporting documents and receipts related to their VR&E case.  The SARA Sync windows will receive information from the e-VA system, such as supporting documents, receipts, etc., case notes, or updating of status on their case. This information are transmitted to the VA database of record for storage.",Operation and Maintenance,Both,6/1/2019,6/1/2019,6/20/2020,Developed with both contracting and in-house resources.,36C10E19P0165,No,Yes,Yes,Yes,No,We do not train e-VA,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,Yes,electronic Virtual Assistant (e-VA) VASI: 2419,Unknown,No,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5714285714285714,"The electronic Virtual Assistant (e-VA) information system is a product acquired from SaraWorks (formerly The Career Index (TCI)) by the Veterans Benefits Administration (VBA) Veteran Readiness & Employment (VR&E) Business Line. e-VA is a breakthrough digital assistant that does most of the work a human assistant can do in terms of client follow-up, data entry, and documentation. The primary objective for implementing e-VA is to provide virtual support services that will alleviate administrative tasks from Vocational Rehabilitation Counselors (VRCs), which enable them to focus their time on the Veterans’ needs, fulfilling VR&E’s mission of guiding Veterans to successful outcomes. e-VA is the only digital assistant focused on applying these autonomous capabilities to the complex field of human services. The e-VA system will create reminders for appointments, grades, receipts, etc. and accept documents and images, in addition to connecting program participants directly to their Counselor. The application also enables automated routine electronic communication with program participants through either text message and/or e-mail. Participants can interact with the e-VA application via text or email to schedule and reschedule appointments, respond to reminders, and submit supporting documents and receipts related to their VR&E case. The e-VA information system is single-tenant, which resides in SaraWorks private AWS GovCloud. It is not a public-facing system. The SARA Sync windows executable will receive information from the e-VA system, such as supporting documents, receipts, etc., case notes, or updating of status on their case. This information are transmitted to the VA database of record for storage. . Allow VRCs to focus on providing world-class service to our Veterans. The e-VA system will create reminders for appointments, grades, receipts, etc. and accept documents and images, in addition to connecting program participants directly to their Counselor. The Case Manager ID will be used to identify case managers and assigned caseload which will determine the location of the requested appointment. The application will also enable automated routine electronic communication with program participant through either text message and/or e-mail. Participants can interact with the e-VA application via text or email to schedule and reschedule appointments, respond to reminders, and submit supporting documents and receipts related to their VR&E case.  The SARA Sync windows will receive information from the e-VA system, such as supporting documents, receipts, etc., case notes, or updating of status on their case. This information are transmitted to the VA database of record for storage.","the electronic virtual assistant (e-va) information system is a product acquired from saraworks (formerly the career index (tci)) by the veterans benefits administration (vba) veteran readiness & employment (vr&e) business line. e-va is a breakthrough digital assistant that does most of the work a human assistant can do in terms of client follow-up, data entry, and documentation. the primary objective for implementing e-va is to provide virtual support services that will alleviate administrative tasks from vocational rehabilitation counselors (vrcs), which enable them to focus their time on the veterans’ needs, fulfilling vr&e’s mission of guiding veterans to successful outcomes. e-va is the only digital assistant focused on applying these autonomous capabilities to the complex field of human services. the e-va system will create reminders for appointments, grades, receipts, etc. and accept documents and images, in addition to connecting program participants directly to their counselor. the application also enables automated routine electronic communication with program participants through either text message and/or e-mail. participants can interact with the e-va application via text or email to schedule and reschedule appointments, respond to reminders, and submit supporting documents and receipts related to their vr&e case. the e-va information system is single-tenant, which resides in saraworks private aws govcloud. it is not a public-facing system. the sara sync windows executable will receive information from the e-va system, such as supporting documents, receipts, etc., case notes, or updating of status on their case. this information are transmitted to the va database of record for storage. . allow vrcs to focus on providing world-class service to our veterans. the e-va system will create reminders for appointments, grades, receipts, etc. and accept documents and images, in addition to connecting program participants directly to their counselor. the case manager id will be used to identify case managers and assigned caseload which will determine the location of the requested appointment. the application will also enable automated routine electronic communication with program participant through either text message and/or e-mail. participants can interact with the e-va application via text or email to schedule and reschedule appointments, respond to reminders, and submit supporting documents and receipts related to their vr&e case. the sara sync windows will receive information from the e-va system, such as supporting documents, receipts, etc., case notes, or updating of status on their case. this information are transmitted to the va database of record for storage."
WAVE Clinical Platform,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf17/K171056.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf17/K171056.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf17/k171056.pdf).
"VX1, VX1+",Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K223516.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K223516.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/k223516.pdf).
Sybil - Lung Cancer Prediction Model by MIT,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"To be used to personalize screening regime, calling high risk patients earlier to catch cancers in the early stages and potentially reducing screening burden for low risk patients.","It takes as input a lung low-dose CT scan, and produces a JSON return object with predictions of the odds of developing cancer within the next 6 years (one prediction for each year).",Acquisition and/or Development,Both,4/10/2024,10/16/2023,Unknown,Developed in-house.,Unknown,Yes,No,Yes,No,No,We (Dayton HTM) have developed a web based front end to point DICOM images to the API. The resulting JSON cancer prediction results are transcribed into a excel table for validation. Our aim is to have the model on server behind the VA's isolation architecture (MDIA or SDAI) and have results stored in SQL database. With access secured via LDAP.,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,Yes – source code is publicly available.,No,Sybil,6-12 months,Other,Yes,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5873015873015873,"To be used to personalize screening regime, calling high risk patients earlier to catch cancers in the early stages and potentially reducing screening burden for low risk patients. . It takes as input a lung low-dose CT scan, and produces a JSON return object with predictions of the odds of developing cancer within the next 6 years (one prediction for each year).","to be used to personalize screening regime, calling high risk patients earlier to catch cancers in the early stages and potentially reducing screening burden for low risk patients. . it takes as input a lung low-dose ct scan, and produces a json return object with predictions of the odds of developing cancer within the next 6 years (one prediction for each year)."
eCaremanager (Philips),Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,Creating visual representations of data sets for reports and presentations using AI.,"eCaremanager integrates clinical, physiological, and demographic data from all patients entered into the National TeleCC Program.  The data are presented graphically to users.  Additionally, the data are used to generate acuity scores to assess each patients acuity in comparison to all other patients.  Trend and threshold alerts are also generated to notify providers to take a closer look at those patients and initiate treatments to remediate derangements and facilitate return to homeostasis and health.",The outputs of eCaremanager are acuity scores and trend and threshold alerts.  The software also integrates and presents patient care information to enable providers to care for larger number of patients and provide critical care at a population level.,Operation and Maintenance,Both,1/1/2010,1/1/2010,1/1/2010,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Yes,TCC ID: 2209,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.2698412698412698,"eCaremanager integrates clinical, physiological, and demographic data from all patients entered into the National TeleCC Program.  The data are presented graphically to users.  Additionally, the data are used to generate acuity scores to assess each patients acuity in comparison to all other patients.  Trend and threshold alerts are also generated to notify providers to take a closer look at those patients and initiate treatments to remediate derangements and facilitate return to homeostasis and health. . The outputs of eCaremanager are acuity scores and trend and threshold alerts.  The software also integrates and presents patient care information to enable providers to care for larger number of patients and provide critical care at a population level.","ecaremanager integrates clinical, physiological, and demographic data from all patients entered into the national telecc program. the data are presented graphically to users. additionally, the data are used to generate acuity scores to assess each patients acuity in comparison to all other patients. trend and threshold alerts are also generated to notify providers to take a closer look at those patients and initiate treatments to remediate derangements and facilitate return to homeostasis and health. . the outputs of ecaremanager are acuity scores and trend and threshold alerts. the software also integrates and presents patient care information to enable providers to care for larger number of patients and provide critical care at a population level."
VUNO Med-DeepBrain,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K231398.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K231398.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/k231398.pdf).
"Voluson Expert 22, Voluson Expert 20, Voluson Expert 18",Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K231965.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K231965.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/k231965.pdf).
Volta AF-Xplorer,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K232616.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K232616.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/k232616.pdf).
Biotronik Home Monitoring System,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"The Biotronik home monitoring system collects data transmitted by patients with implanted cardiac monitors from their homes for review by clinicians on a password protected Web page. Transmissions are made by the monitors when certain criteria are met suggesting an important heart rhythm abnormality is present. A proprietary industry-created AI system evaluates the transmitted data and classifies whether the findings that triggered the transmissions are true arrhythmias or false positives. The home monitoring system for BioMonitor IV implanted cardiac monitor uses AI to detect false positives for atrial fibrillation, bradycardia, tachycardia and pauses. These technologies leverage machine learning algorithms and vast datasets to provide more accurate diagnoses and patient care. The staff of the VA National Cardiac Device Surveillance program and the patients’ providers at local VA clinics use this AI when evaluating patient implanted cardiac monitor transmissions.",It filters possible arrhythmia events in to those that are false positive and those that are true positive.,Initiated,Both,4/10/2024,Unknown,Unknown,Unknown,Unknown,Yes,No,No,No,Other,This is a commercial product. We are not involved in development.,Unknown,No,No – agency does not have access to source code.,No,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.4126984126984127,"The Biotronik home monitoring system collects data transmitted by patients with implanted cardiac monitors from their homes for review by clinicians on a password protected Web page. Transmissions are made by the monitors when certain criteria are met suggesting an important heart rhythm abnormality is present. A proprietary industry-created AI system evaluates the transmitted data and classifies whether the findings that triggered the transmissions are true arrhythmias or false positives. The home monitoring system for BioMonitor IV implanted cardiac monitor uses AI to detect false positives for atrial fibrillation, bradycardia, tachycardia and pauses. These technologies leverage machine learning algorithms and vast datasets to provide more accurate diagnoses and patient care. The staff of the VA National Cardiac Device Surveillance program and the patients’ providers at local VA clinics use this AI when evaluating patient implanted cardiac monitor transmissions. . It filters possible arrhythmia events in to those that are false positive and those that are true positive.","the biotronik home monitoring system collects data transmitted by patients with implanted cardiac monitors from their homes for review by clinicians on a password protected web page. transmissions are made by the monitors when certain criteria are met suggesting an important heart rhythm abnormality is present. a proprietary industry-created ai system evaluates the transmitted data and classifies whether the findings that triggered the transmissions are true arrhythmias or false positives. the home monitoring system for biomonitor iv implanted cardiac monitor uses ai to detect false positives for atrial fibrillation, bradycardia, tachycardia and pauses. these technologies leverage machine learning algorithms and vast datasets to provide more accurate diagnoses and patient care. the staff of the va national cardiac device surveillance program and the patients’ providers at local va clinics use this ai when evaluating patient implanted cardiac monitor transmissions. . it filters possible arrhythmia events in to those that are false positive and those that are true positive."
"Synthetic Data Generation:  Experimenting with OpenSource, Third Party and GenAI ",Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Other,None of the above.,Improve agility of research and operational dev,Synthetic patient clinical and demographic data,Initiated,Neither,3/1/2024,Unknown,Unknown,Developed in-house.,Unknown,No,No,No,No,Yes,CDW-ng is going to be used to train,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,2266,More than 12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,Improve agility of research and operational dev . Synthetic patient clinical and demographic data,improve agility of research and operational dev . synthetic patient clinical and demographic data
CareLink Home Monitoring,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,The CareLink home monitoring system collects data transmitted by patients with implanted cardiac monitors from their homes for review by clinicians on a password protected Web page. Transmissions are made by the monitors when certain criteria are met suggesting an important heart rhythm abnormality is present. A proprietary industry-created AI system evaluates the transmitted data and classifies whether the findings that triggered the transmissions are true arrhythmias or false positives. The home monitoring system for Carelink Reveal Linq and Linq II uses AI deep learning algorithms flowing into the CareLink network to remove false AFib and false pause episodes. The staff of the VA National Cardiac Device Surveillance program and the patients’ providers at local VA clinics use this AI when evaluating patient implanted cardiac monitor transmissions.,It eliminates false positive atrial fibrillation and pause events recorded by Medtronic implanted loop recorders when you are reviewing data from these devices on the Medtronic Carelink web page.,Initiated,Both,11/13/2023,Unknown,Unknown,Unknown,Unknown,Yes,No,No,No,Other,This is an commercially developed and deployed FDA approved AI use. The VA was not part of its development.,Unknown,No,No – agency does not have access to source code.,No,Unknown,Unknown,Unknown,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.4603174603174603,The CareLink home monitoring system collects data transmitted by patients with implanted cardiac monitors from their homes for review by clinicians on a password protected Web page. Transmissions are made by the monitors when certain criteria are met suggesting an important heart rhythm abnormality is present. A proprietary industry-created AI system evaluates the transmitted data and classifies whether the findings that triggered the transmissions are true arrhythmias or false positives. The home monitoring system for Carelink Reveal Linq and Linq II uses AI deep learning algorithms flowing into the CareLink network to remove false AFib and false pause episodes. The staff of the VA National Cardiac Device Surveillance program and the patients’ providers at local VA clinics use this AI when evaluating patient implanted cardiac monitor transmissions. . It eliminates false positive atrial fibrillation and pause events recorded by Medtronic implanted loop recorders when you are reviewing data from these devices on the Medtronic Carelink web page.,the carelink home monitoring system collects data transmitted by patients with implanted cardiac monitors from their homes for review by clinicians on a password protected web page. transmissions are made by the monitors when certain criteria are met suggesting an important heart rhythm abnormality is present. a proprietary industry-created ai system evaluates the transmitted data and classifies whether the findings that triggered the transmissions are true arrhythmias or false positives. the home monitoring system for carelink reveal linq and linq ii uses ai deep learning algorithms flowing into the carelink network to remove false afib and false pause episodes. the staff of the va national cardiac device surveillance program and the patients’ providers at local va clinics use this ai when evaluating patient implanted cardiac monitor transmissions. . it eliminates false positive atrial fibrillation and pause events recorded by medtronic implanted loop recorders when you are reviewing data from these devices on the medtronic carelink web page.
Stratification Tool for Opioid Risk Mitigation (STORM) predictive model ,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"The STORM model is a predictive model to estimate risk of a suicide or overdose-related health care event or death in the next year.  The model uses patient level predictors extracted from the patient’s medical records (into the VA’s Corporate Data Warehouse), such as diagnoses, prescriptions, and health care utilization, etc.  
This predictive model is used for three purposes. (1) Primary use case is to identify patients at elevated risk of suicide or overdose-related events for referral to an interdisciplinary team for clinical case review.  This team makes treatment recommendations to share with treating providers and documents them in the medical record. The STORM algorithm is used to identify patients to refer to this program at each facility.  Assignment to this case management program has been shown to reduce all-cause mortality in patients estimated at “very high” risk on the STORM predictive model by 22% in the next 4 months. (2) Secondary use case is to help risk stratify patients based on prior health care interactions to support augmentation of risk mitigation interventions and treatments for patients with patterns of care associated with risk of suicide or overdose events. Risk estimates are provided as a sorting/filtering and informational feature on the STORM clinical decision support system.  This decision support system enables population management of patients prescribed opioids or with opioid use disorders, as well as providing curated information to support treatment planning for individual patients. (3) The last use case is to meet the requirements of the Comprehensive Addiction Recovery Act (CARA) mandating that clinicians use VA decision support tools to review patient risks prior to initiating opioid therapy for pain.  Direct synchronized links from the VISTA medical record to the STORM report integrate this decision support summary information into clinician workflows as a support for risk assessment before writing and initial opioid prescription.  Each of these use cases is built into existing clinical workflows completed by overlapping, but different health care providers. For use case 1, designated interdisciplinary risk review teams at each health care system use the risk estimates to determine which patient require risk review.  For use case 2, VA health care providers (e.g., opioid prescribers) and teams (e.g., primary care and general mental health teams) use the risk estimates in combination with decision support recommendations of interventions to consider facilitate and augment risk assessment and treatment planning for their patients on opioid analgesics or being treated for opioid use disorder.  For use case 3, VA medication prescribers or their delegated team members use the categorized risk scores in the context of curated descriptive data from the patient’s medical record to review risk factors and consider risk and benefits of treatment options for pain management before prescribing opioids.  Risk considerations are documented in a clinical note for tracking.   Use of STORM is expected to improve decision-making about opioid prescribing and treatment of patients with opioid use disorders, as well as consistency of use of clinical practice guideline recommended treatment practices for safe management of patients exposed to opioids.  STORM also focuses on ensuring recognition of patient risks and facilitating care coordination across treatment providers for complex patients with multiple interacting disorders. By improving use of risk mitigation practices and treatment interventions, we expect STORM will reduce adverse outcomes for patients exposed to opioids.  Program evaluations support this expectation, finding that implementation of the targeted interdisciplinary review requirements associated with use case 1 significantly reduced all cause mortality in the targeted patient population. 

Links: https://psycnet.apa.org/buy/2017-03732-004
https://www.tandfonline.com/doi/10.1080/08897077.2018.1540376
https://link.springer.com/article/10.1007/s11606-022-07622-1
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10698768/","The STORM predictive model estimates the risk of having an overdose or suicide-related event in next year for all patients currently receiving a VA opioid analgesic prescription with nightly updates.  For all other active VHA patients, the STORM predictive model estimates the hypothetical risk of having an overdose or suicide-related event in next year, if the patient was prescribed or otherwise had access to low, medium, or high dose opioid analgesics.  These risk estimates are provided to clinicians on a family of dashboards, both as percentage likelihood of an event in the next year, and as a categorical grouping of estimated low, medium, high or very high risk.   Paired with this risk estimate is a summary of all risk factors identified and considered in generating the risk estimate for that patient, and recommendations for risk mitigation and treatment options to consider.  These recommendations are tailored based on patient characteristics identified in the medical record (e.g. Medications for opioid use disorder are recommended for patients seen for an opioid use disorder diagnosis) and patient receipt of these interventions is tracked and displayed on the dashboard.  The STORM dashboards are used by clinicians as an informational tool to summarize and organize medical record information to support risk reviews during treatment planning (e.g. when considering an opioid prescription), for management of patients prescribed opioids or treated for opioid use disorder, and for interdisciplinary case reviews of very high risk patients to ensure safe and effective care coordination and treatment planning across health care providers.",Operation and Maintenance,Both,6/1/2012,6/1/2012,6/1/2015,Developed in-house.,Unknown,No,No,Yes,Yes,Yes,"Clinical data came from the VHA National Corporate Data Warehouse, an archival data set that contains comprehensive patient-by-patient, encounter-by-encounter clinical and administrative data derived from the Department of Veterans Affairs’ electronic medical records. Data regarding vital status and cause of death were from the National Death Index, a centralized database maintained by the National Center for Health Statistics of death record information on file in state vital statistics offices.","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,VA Corporate Data Warehouse - 1152,Less than 6 months,Yes,Yes,Yes,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,"(1) Providers might over-rely on statistical risk assessment. This risk was a concern based on known tendencies of some providers to over-interpret recommendations as requirements.  This tendency was observed in a minority of providers and addressed explicitly in training and training materials moving forward.  (2) The program could be clinically inefficient and use provider time that might be more effectively focused elsewhere.  This is a concern for all VHA clinical programs, and VHA management consistently seeks to optimize clinical time and attention towards achieving the greatest patient benefit.  (3) The program could lead to unfair allocation of clinical resources if the model is biased and other clinical programming does not compensate for areas of model under-performance. Here, a targeted case review is required for patients identified as ""very high"" risk per the STORM predictive model, though all clinical services are available and recommended for all patients based on clinical indication in the STORM decision support system.   After review of the common risks template, the following risks were identified: Fair & Equitable (FE) - Algorithm target not reflective of real-world outcome of interest Response: There is a risk that changes in population exposures and behaviors related to opioids may alter patterns of risk away from those the model was trained on.    Transparent & Explainable (TE) - Degradation of End-User Trust and Ineffective Challenge or Remedy Processes Response: This is a risk, there has been public misunderstanding of how VA predictive models are developed and used. VA has responded and clarified misconceptions where concerns have been raised.  Accountable & Monitored (AM) - Performance Efficacy and Fairness (e.g. Model/Data Drift, Model Degradation, or inappropriate application)  Response: This is a risk, model performance is degrading over time and an update is in process. However all identified patients are clinically complex and appropriate for clinical attention.  Accountable & Monitored (AM) - User-Introduced Errors Response: There is a low risk, there is a community of practice and regular monthly education to support proper implementation of the VA opioid safety program, including the STORM decision support system and predictive model.  Accountable & Monitored (AM) - System Performance Not as Intended Response: There is a low risk, team has validation processes to review all steps.",TRUE,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"VA has examined performance of the STORM model for estimating overdose and suicide -related risk among Veterans in VHA care, stratified by patient sex, race and ethnicity.  The first iteration of model performance evaluation is described in List et al., 2023, available at: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10698768/.   The main finding from this evaluation was that the model tended to underestimate risk in non-white populations in years subsequent to the year the model was trained on.  This underestimation tracked population level changes in opioid overdose rates across this period, as there was a significant increase in observed overdose rates in non-white populations between the model training year and later years.  This finding emphasized the importance of regular updating of the predictive model to address changes in risk contributions in the U.S. population over time.  Additional efforts to optimize bias evaluation methods were undertaken utilizing the STORM model as an example (Meerwijk et al., 2024).  While this evaluation emphasized the complexity of bias evaluation, it also suggested opportunities for improvement of model performance, particularly for older adults, who tend to utilize VHA mental health services less than younger VHA patients. Guided by these findings, VHA is actively working on an update to the STORM predictive model, incorporating a significantly expanded set of candidate predictor variables in hopes of better capturing indications of risk across subpopulations and optimizing the model to reflect evolving trends in opioid exposure and outcomes. As part of this effort, VHA has built a decision support infrastructure to enable remodeling, model evaluation, and model deployment into VHA clinical decision support systems on an annual or as-needed cycle.    References:  Meerwijk EL, McElfresh DC, Martins S, Tamang SR. Evaluating accuracy and fairness of clinical decision support algorithms when health care resources are limited. J Biomed Inform. 2024 Aug;156:104664. doi: 10.1016/j.jbi.2024.104664. Epub 2024 Jun 6. PMID: 38851413.   List JM, Palevsky P, Tamang S, Crowley S, Au D, Yarbrough WC, Navathe AS, Kreisler C, Parikh RB, Wang-Rodriguez J, Klutts JS, Conlin P, Pogach L, Meerwijk E, Moy E. Eliminating Algorithmic Racial Bias in Clinical Decision Support Algorithms: Use Cases from the Veterans Health Administration. Health Equity. 2023 Nov 30;7(1):809-816. doi: 10.1089/heq.2023.0037. PMID: 38076213; PMCID: PMC10698768.","[""Direct user testing""]",No – it is not operationally practical to offer this.,Both,0.746031746031746,"The STORM model is a predictive model to estimate risk of a suicide or overdose-related health care event or death in the next year.  The model uses patient level predictors extracted from the patient’s medical records (into the VA’s Corporate Data Warehouse), such as diagnoses, prescriptions, and health care utilization, etc.  
This predictive model is used for three purposes. (1) Primary use case is to identify patients at elevated risk of suicide or overdose-related events for referral to an interdisciplinary team for clinical case review.  This team makes treatment recommendations to share with treating providers and documents them in the medical record. The STORM algorithm is used to identify patients to refer to this program at each facility.  Assignment to this case management program has been shown to reduce all-cause mortality in patients estimated at “very high” risk on the STORM predictive model by 22% in the next 4 months. (2) Secondary use case is to help risk stratify patients based on prior health care interactions to support augmentation of risk mitigation interventions and treatments for patients with patterns of care associated with risk of suicide or overdose events. Risk estimates are provided as a sorting/filtering and informational feature on the STORM clinical decision support system.  This decision support system enables population management of patients prescribed opioids or with opioid use disorders, as well as providing curated information to support treatment planning for individual patients. (3) The last use case is to meet the requirements of the Comprehensive Addiction Recovery Act (CARA) mandating that clinicians use VA decision support tools to review patient risks prior to initiating opioid therapy for pain.  Direct synchronized links from the VISTA medical record to the STORM report integrate this decision support summary information into clinician workflows as a support for risk assessment before writing and initial opioid prescription.  Each of these use cases is built into existing clinical workflows completed by overlapping, but different health care providers. For use case 1, designated interdisciplinary risk review teams at each health care system use the risk estimates to determine which patient require risk review.  For use case 2, VA health care providers (e.g., opioid prescribers) and teams (e.g., primary care and general mental health teams) use the risk estimates in combination with decision support recommendations of interventions to consider facilitate and augment risk assessment and treatment planning for their patients on opioid analgesics or being treated for opioid use disorder.  For use case 3, VA medication prescribers or their delegated team members use the categorized risk scores in the context of curated descriptive data from the patient’s medical record to review risk factors and consider risk and benefits of treatment options for pain management before prescribing opioids.  Risk considerations are documented in a clinical note for tracking.   Use of STORM is expected to improve decision-making about opioid prescribing and treatment of patients with opioid use disorders, as well as consistency of use of clinical practice guideline recommended treatment practices for safe management of patients exposed to opioids.  STORM also focuses on ensuring recognition of patient risks and facilitating care coordination across treatment providers for complex patients with multiple interacting disorders. By improving use of risk mitigation practices and treatment interventions, we expect STORM will reduce adverse outcomes for patients exposed to opioids.  Program evaluations support this expectation, finding that implementation of the targeted interdisciplinary review requirements associated with use case 1 significantly reduced all cause mortality in the targeted patient population. 

Links: https://psycnet.apa.org/buy/2017-03732-004
https://www.tandfonline.com/doi/10.1080/08897077.2018.1540376
https://link.springer.com/article/10.1007/s11606-022-07622-1
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10698768/ . The STORM predictive model estimates the risk of having an overdose or suicide-related event in next year for all patients currently receiving a VA opioid analgesic prescription with nightly updates.  For all other active VHA patients, the STORM predictive model estimates the hypothetical risk of having an overdose or suicide-related event in next year, if the patient was prescribed or otherwise had access to low, medium, or high dose opioid analgesics.  These risk estimates are provided to clinicians on a family of dashboards, both as percentage likelihood of an event in the next year, and as a categorical grouping of estimated low, medium, high or very high risk.   Paired with this risk estimate is a summary of all risk factors identified and considered in generating the risk estimate for that patient, and recommendations for risk mitigation and treatment options to consider.  These recommendations are tailored based on patient characteristics identified in the medical record (e.g. Medications for opioid use disorder are recommended for patients seen for an opioid use disorder diagnosis) and patient receipt of these interventions is tracked and displayed on the dashboard.  The STORM dashboards are used by clinicians as an informational tool to summarize and organize medical record information to support risk reviews during treatment planning (e.g. when considering an opioid prescription), for management of patients prescribed opioids or treated for opioid use disorder, and for interdisciplinary case reviews of very high risk patients to ensure safe and effective care coordination and treatment planning across health care providers. . (1) Providers might over-rely on statistical risk assessment. This risk was a concern based on known tendencies of some providers to over-interpret recommendations as requirements.  This tendency was observed in a minority of providers and addressed explicitly in training and training materials moving forward.  (2) The program could be clinically inefficient and use provider time that might be more effectively focused elsewhere.  This is a concern for all VHA clinical programs, and VHA management consistently seeks to optimize clinical time and attention towards achieving the greatest patient benefit.  (3) The program could lead to unfair allocation of clinical resources if the model is biased and other clinical programming does not compensate for areas of model under-performance. Here, a targeted case review is required for patients identified as ""very high"" risk per the STORM predictive model, though all clinical services are available and recommended for all patients based on clinical indication in the STORM decision support system.   After review of the common risks template, the following risks were identified: Fair & Equitable (FE) - Algorithm target not reflective of real-world outcome of interest Response: There is a risk that changes in population exposures and behaviors related to opioids may alter patterns of risk away from those the model was trained on.    Transparent & Explainable (TE) - Degradation of End-User Trust and Ineffective Challenge or Remedy Processes Response: This is a risk, there has been public misunderstanding of how VA predictive models are developed and used. VA has responded and clarified misconceptions where concerns have been raised.  Accountable & Monitored (AM) - Performance Efficacy and Fairness (e.g. Model/Data Drift, Model Degradation, or inappropriate application)  Response: This is a risk, model performance is degrading over time and an update is in process. However all identified patients are clinically complex and appropriate for clinical attention.  Accountable & Monitored (AM) - User-Introduced Errors Response: There is a low risk, there is a community of practice and regular monthly education to support proper implementation of the VA opioid safety program, including the STORM decision support system and predictive model.  Accountable & Monitored (AM) - System Performance Not as Intended Response: There is a low risk, team has validation processes to review all steps.","the storm model is a predictive model to estimate risk of a suicide or overdose-related health care event or death in the next year. the model uses patient level predictors extracted from the patient’s medical records (into the va’s corporate data warehouse), such as diagnoses, prescriptions, and health care utilization, etc. this predictive model is used for three purposes. (1) primary use case is to identify patients at elevated risk of suicide or overdose-related events for referral to an interdisciplinary team for clinical case review. this team makes treatment recommendations to share with treating providers and documents them in the medical record. the storm algorithm is used to identify patients to refer to this program at each facility. assignment to this case management program has been shown to reduce all-cause mortality in patients estimated at “very high” risk on the storm predictive model by 22% in the next 4 months. (2) secondary use case is to help risk stratify patients based on prior health care interactions to support augmentation of risk mitigation interventions and treatments for patients with patterns of care associated with risk of suicide or overdose events. risk estimates are provided as a sorting/filtering and informational feature on the storm clinical decision support system. this decision support system enables population management of patients prescribed opioids or with opioid use disorders, as well as providing curated information to support treatment planning for individual patients. (3) the last use case is to meet the requirements of the comprehensive addiction recovery act (cara) mandating that clinicians use va decision support tools to review patient risks prior to initiating opioid therapy for pain. direct synchronized links from the vista medical record to the storm report integrate this decision support summary information into clinician workflows as a support for risk assessment before writing and initial opioid prescription. each of these use cases is built into existing clinical workflows completed by overlapping, but different health care providers. for use case 1, designated interdisciplinary risk review teams at each health care system use the risk estimates to determine which patient require risk review. for use case 2, va health care providers (e.g., opioid prescribers) and teams (e.g., primary care and general mental health teams) use the risk estimates in combination with decision support recommendations of interventions to consider facilitate and augment risk assessment and treatment planning for their patients on opioid analgesics or being treated for opioid use disorder. for use case 3, va medication prescribers or their delegated team members use the categorized risk scores in the context of curated descriptive data from the patient’s medical record to review risk factors and consider risk and benefits of treatment options for pain management before prescribing opioids. risk considerations are documented in a clinical note for tracking. use of storm is expected to improve decision-making about opioid prescribing and treatment of patients with opioid use disorders, as well as consistency of use of clinical practice guideline recommended treatment practices for safe management of patients exposed to opioids. storm also focuses on ensuring recognition of patient risks and facilitating care coordination across treatment providers for complex patients with multiple interacting disorders. by improving use of risk mitigation practices and treatment interventions, we expect storm will reduce adverse outcomes for patients exposed to opioids. program evaluations support this expectation, finding that implementation of the targeted interdisciplinary review requirements associated with use case 1 significantly reduced all cause mortality in the targeted patient population. links: https://psycnet.apa.org/buy/2017-03732-004 https://www.tandfonline.com/doi/10.1080/08897077.2018.1540376 https://link.springer.com/article/10.1007/s11606-022-07622-1 https://www.ncbi.nlm.nih.gov/pmc/articles/pmc10698768/ . the storm predictive model estimates the risk of having an overdose or suicide-related event in next year for all patients currently receiving a va opioid analgesic prescription with nightly updates. for all other active vha patients, the storm predictive model estimates the hypothetical risk of having an overdose or suicide-related event in next year, if the patient was prescribed or otherwise had access to low, medium, or high dose opioid analgesics. these risk estimates are provided to clinicians on a family of dashboards, both as percentage likelihood of an event in the next year, and as a categorical grouping of estimated low, medium, high or very high risk. paired with this risk estimate is a summary of all risk factors identified and considered in generating the risk estimate for that patient, and recommendations for risk mitigation and treatment options to consider. these recommendations are tailored based on patient characteristics identified in the medical record (e.g. medications for opioid use disorder are recommended for patients seen for an opioid use disorder diagnosis) and patient receipt of these interventions is tracked and displayed on the dashboard. the storm dashboards are used by clinicians as an informational tool to summarize and organize medical record information to support risk reviews during treatment planning (e.g. when considering an opioid prescription), for management of patients prescribed opioids or treated for opioid use disorder, and for interdisciplinary case reviews of very high risk patients to ensure safe and effective care coordination and treatment planning across health care providers. . (1) providers might over-rely on statistical risk assessment. this risk was a concern based on known tendencies of some providers to over-interpret recommendations as requirements. this tendency was observed in a minority of providers and addressed explicitly in training and training materials moving forward. (2) the program could be clinically inefficient and use provider time that might be more effectively focused elsewhere. this is a concern for all vha clinical programs, and vha management consistently seeks to optimize clinical time and attention towards achieving the greatest patient benefit. (3) the program could lead to unfair allocation of clinical resources if the model is biased and other clinical programming does not compensate for areas of model under-performance. here, a targeted case review is required for patients identified as ""very high"" risk per the storm predictive model, though all clinical services are available and recommended for all patients based on clinical indication in the storm decision support system. after review of the common risks template, the following risks were identified: fair & equitable (fe) - algorithm target not reflective of real-world outcome of interest response: there is a risk that changes in population exposures and behaviors related to opioids may alter patterns of risk away from those the model was trained on. transparent & explainable (te) - degradation of end-user trust and ineffective challenge or remedy processes response: this is a risk, there has been public misunderstanding of how va predictive models are developed and used. va has responded and clarified misconceptions where concerns have been raised. accountable & monitored (am) - performance efficacy and fairness (e.g. model/data drift, model degradation, or inappropriate application) response: this is a risk, model performance is degrading over time and an update is in process. however all identified patients are clinically complex and appropriate for clinical attention. accountable & monitored (am) - user-introduced errors response: there is a low risk, there is a community of practice and regular monthly education to support proper implementation of the va opioid safety program, including the storm decision support system and predictive model. accountable & monitored (am) - system performance not as intended response: there is a low risk, team has validation processes to review all steps."
Vitrea CT Brain Perfusion,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf18/K181247.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf18/K181247.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf18/k181247.pdf).
Deep-Learning approaches to develop candidate lists of terms for use in text searches ,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"The intended use is to increase the comprehensiveness of term lists to be used in text-matching approaches to identifying concepts of interest in free-text medical record notes. Text-matching approaches are an efficient method for identifying text of interest in medical notes.  The success of text-matching approaches depends on the quality of term lists used to search for a concept of interest.  While humans can generate word lists for text-matching approaches, the generated word lists are likely to be limited and biased by the vocabulary, language habits, population exposure, and local dialects of the human generating the list.  Deep learning methods can help to augment candidate term lists to help overcome some of these challenges.  Once generated these augmented candidate terms are reviewed by subject matter experts for appropriateness and accuracy of the suggested terms for the concept and use case of interest.  Here we (1) query pre-trained large language models (e.g., ChatGPT or Llama) through a user interface that allows prompting (such as the OpenAI API or CodeLlama) and/or (2) query word and phrase “embedding” models (e.g., Word2Vec and Phrase2Vec) to expand candidate term lists for review by subject matter experts.  Our team has found both approaches effective for expediting the generation of more comprehensive term to concept mappings for use in text-matching algorithms that increase the accuracy of clinical concept extraction.   We expect this use of AI to improve initial brainstorming of terms related to a given concept, resulting in a more sensitive approach to finding information of interest in free-text clinical notes. ","The outputs of this use of AI is a candidate list of terms related to a concept of interest.  This candidate list is combined with suggestions generated by human experts and then these are reviewed by a subject matter expert to generate a final list of terms for use in text-matching algorithms to extract medical record free text mentions of interest. For the pre-trained Large Language model approach, the input is a query prompting for synonyms of terms related to the concept of interest.  The output is the answer provided by the LLM, including the suggested candidate terms and the original terms that were stated in the query.  For the word and phrase embedding models, the input is an initial list of terms that are relevant to the concept of interest and the output are the suggested candidate terms, ranked by their statistical similarity to the original terms, based on the transformation of a clinical corpus into vector space or an “embedding model”.  Regardless of the type of model used, all suggested candidate terms are reviewed by subject matter experts to ensure quality term lists are used in all text searches.",Operation and Maintenance,Neither,10/1/2022,10/1/2022,10/1/2022,Developed in-house.,Unknown,No,No,No,No,Other,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Yes,Yes,Yes,Re-use production level code and/or data products: This use case re-uses production level code as well as data products that have been published or made widely available for data science teams for within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.49206349206349204,"The intended use is to increase the comprehensiveness of term lists to be used in text-matching approaches to identifying concepts of interest in free-text medical record notes. Text-matching approaches are an efficient method for identifying text of interest in medical notes.  The success of text-matching approaches depends on the quality of term lists used to search for a concept of interest.  While humans can generate word lists for text-matching approaches, the generated word lists are likely to be limited and biased by the vocabulary, language habits, population exposure, and local dialects of the human generating the list.  Deep learning methods can help to augment candidate term lists to help overcome some of these challenges.  Once generated these augmented candidate terms are reviewed by subject matter experts for appropriateness and accuracy of the suggested terms for the concept and use case of interest.  Here we (1) query pre-trained large language models (e.g., ChatGPT or Llama) through a user interface that allows prompting (such as the OpenAI API or CodeLlama) and/or (2) query word and phrase “embedding” models (e.g., Word2Vec and Phrase2Vec) to expand candidate term lists for review by subject matter experts.  Our team has found both approaches effective for expediting the generation of more comprehensive term to concept mappings for use in text-matching algorithms that increase the accuracy of clinical concept extraction.   We expect this use of AI to improve initial brainstorming of terms related to a given concept, resulting in a more sensitive approach to finding information of interest in free-text clinical notes. . The outputs of this use of AI is a candidate list of terms related to a concept of interest.  This candidate list is combined with suggestions generated by human experts and then these are reviewed by a subject matter expert to generate a final list of terms for use in text-matching algorithms to extract medical record free text mentions of interest. For the pre-trained Large Language model approach, the input is a query prompting for synonyms of terms related to the concept of interest.  The output is the answer provided by the LLM, including the suggested candidate terms and the original terms that were stated in the query.  For the word and phrase embedding models, the input is an initial list of terms that are relevant to the concept of interest and the output are the suggested candidate terms, ranked by their statistical similarity to the original terms, based on the transformation of a clinical corpus into vector space or an “embedding model”.  Regardless of the type of model used, all suggested candidate terms are reviewed by subject matter experts to ensure quality term lists are used in all text searches.","the intended use is to increase the comprehensiveness of term lists to be used in text-matching approaches to identifying concepts of interest in free-text medical record notes. text-matching approaches are an efficient method for identifying text of interest in medical notes. the success of text-matching approaches depends on the quality of term lists used to search for a concept of interest. while humans can generate word lists for text-matching approaches, the generated word lists are likely to be limited and biased by the vocabulary, language habits, population exposure, and local dialects of the human generating the list. deep learning methods can help to augment candidate term lists to help overcome some of these challenges. once generated these augmented candidate terms are reviewed by subject matter experts for appropriateness and accuracy of the suggested terms for the concept and use case of interest. here we (1) query pre-trained large language models (e.g., chatgpt or llama) through a user interface that allows prompting (such as the openai api or codellama) and/or (2) query word and phrase “embedding” models (e.g., word2vec and phrase2vec) to expand candidate term lists for review by subject matter experts. our team has found both approaches effective for expediting the generation of more comprehensive term to concept mappings for use in text-matching algorithms that increase the accuracy of clinical concept extraction. we expect this use of ai to improve initial brainstorming of terms related to a given concept, resulting in a more sensitive approach to finding information of interest in free-text clinical notes. . the outputs of this use of ai is a candidate list of terms related to a concept of interest. this candidate list is combined with suggestions generated by human experts and then these are reviewed by a subject matter expert to generate a final list of terms for use in text-matching algorithms to extract medical record free text mentions of interest. for the pre-trained large language model approach, the input is a query prompting for synonyms of terms related to the concept of interest. the output is the answer provided by the llm, including the suggested candidate terms and the original terms that were stated in the query. for the word and phrase embedding models, the input is an initial list of terms that are relevant to the concept of interest and the output are the suggested candidate terms, ranked by their statistical similarity to the original terms, based on the transformation of a clinical corpus into vector space or an “embedding model”. regardless of the type of model used, all suggested candidate terms are reviewed by subject matter experts to ensure quality term lists are used in all text searches."
Vereos PET/CT,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf21/K211764.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf21/K211764.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf21/k211764.pdf).
Recovery Engagement and Coordination for Health – Veterans Enhanced Treatment (REACH VET) 1.0 predictive model,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"The REACH VET 1.0 model is a predictive model using predictors extracted from the patient’s medical records (into the VA’s Corporate Data Warehouse), such as diagnoses, prescriptions, and health care utilization, to estimate statistical risk of a suicide death in the next month.  
This predictive model is used for two purposes. (1) Primary use case is to support identification of patients at elevated statistical risk of suicide in the next month for referral to a clinical case review and outreach program. Staffing guidance to ensure each health care system is staffed to conduct the case review and outreach protocol is established per clinical policy. To date, the population recommended to the clinical case review program has been limited to a number equivalent to 0.1% of the total treated patient population at each facility each month; the number of patients in the program may be modified based on guidance from the Office of Suicide Prevention. The REACH VET 1.0 statistical risk estimates contribute to an algorithm used to identify patients to refer to this program at each facility. (2) Secondary use case is to help risk stratify patients based on prior health care interactions to support augmentation of clinical attention and interventions for patients with patterns of care associated with high suicide risk. This is used, for example, to support clinical review of relevant health care history during triage of Veteran Crisis Line calls and during risk assessment of patients in mental health care.
For use case 1, REACH VET coordinators and REACH VET providers use the list and supporting information to guide execution of the targeted prevention program. For use case 2, Veterans Crisis Line responders and VHA health care providers use the categorized risk scores in the context of curated descriptive data from the patient’s medical record to review risk factors towards optimizing assessment and clinical follow-up.   
The REACH VET 1.0 model is expected to identify a population of patients at elevated risk of suicide and other negative outcomes for review by clinicians; clinicians conduct telephone outreach to identified patients when they deem such outreach appropriate.  This model and targeted prevention in which it is used (i.e. the REACH VET program) is used to augment VHA's extensive clinical suicide prevention program. While VHA conducts universal screening for suicide risk by asking patients structured questions regarding suicidality, this screening process does not identify all patients who go on to die of  suicide. The REACH VET model is intended to identify patients who might be missed in clinical assessment or whose risk may have changed since their last assessment. Here, the REACH VET model identifies patients at statistical risk of suicide, that is they share similarities in their health care data to patients who did die of suicide in the past.  The patients at highest statistical risk are highlighted to mental health providers for additional review. This targeted prevention program has been nationally implemented in VHA health care systems since 2017 and an evaluation of the effects of the REACHVET program was conducted. This evaluation found that REACH VET implementation was associated with greater treatment engagement and new safety plan documentation and fewer mental health admissions, emergency department visits, and suicide attempts. Full findings of this program evaluation are available at:  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8524305/","The REACH VET predictive model estimates the likelihood of dying of suicide in the next month for each active patient in VHA. This estimate is used as part of an algorithm that considers factors such as the size of the health care system, current patient treatment engagement, and whether the patient has had previous engagement with the REACH VET program to generate a dashboard with patients at high statistical risk of suicide for review by a REACH VET coordinator and identified clinicians at each health care system. The REACH VET dashboard provides information about identified risk factors for the included patients and supports a coordinated case review and, where applicable, clinical outreach process to support identification of care gaps and offer additional services as appropriate.",Operation and Maintenance,Both,1/1/2014,1/1/2014,11/1/2016,Developed in-house.,Unknown,No,No,Yes,Yes,Yes,"As quoted in publication 'Clinical data came from the VHA National Patient Care Database, an archival data set that contains comprehensive patient-by-patient, encounter-by-encounter clinical and administrative data derived from the Department of Veterans Affairs’ electronic medical records. Data regarding vital status and cause of death were from the National Death Index, a centralized database maintained by the National Center for Health Statistics of death record information on file in state vital statistics offices. We conducted searches with established procedures.24 We constructed 3 samples: (1) model development, (2) model validation, and (3) prediction. The development and validation samples contained data for patient-months from all patients who died from suicide from October 1, 2008, to September 30, 2011, and had received VHA services in the previous 2 years (case patients) and a random 1% of patients who survived the month and received VHA services in the previous 2 years (control patients). We randomly assigned half of the case patients and half of the control patients to the development sample and half to the validation sample. Each contained patient-months from 3180 case patients and 1 056 004 control patients.' (McCarthy et al, 2015).  Data from the VA Corporate Data Warehouse was used to evaluate performance of the models.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Corporate Data Warehouse - 1152,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Yes,"Our main concerns regarding risks are as follows:  (1) Providers might over-rely on statistical risk estimate. To reduce this risk, we provide information regarding which risk factors were identified by the predictive model to encourage transparent understanding of the basis for the model estimate.  Moreover, the clinical prevention program provides only general guidelines to clinicians indicating that identified patients should be reviewed.  This forces clinicians to rely on their clinical skills to plan intervention responses. (2) The program could be clinically inefficient and use provider time that might be more effectively focused elsewhere.  This is a real risk which may evolve over time as other components of the overall VA suicide prevention program improve, and potentially reduce the need for this targeted prevention program. VHA conducts ongoing evaluations to assess for potential inefficiencies in the model and test methods to improve clinical efficiency.  For example, VA is currently piloting a process to expand focus of the REACH VET program to patients not currently engaged in mental health care.  (3) The program could lead to unfair allocation of clinical resources if the model is biased and other clinical programming does not compensate for areas of model underperformance. This is also a risk and thus VHA evaluates model bias across key demographic populations and actively develops methods to reduce model bias or address bias by adjusting other components of the treatment system to compensate.   After review of the common risks template, the following additional risks were identified: Fair & Equitable (FE) - Algorithm target not reflective of real-world outcome of interest Response: There is a minor risk that the county coroners who assign cause of death might have miscategorized some Veteran deaths, reducing the validity of the outcome variable used.   Transparent & Explainable (TE) - Degradation of End-User Trust and Ineffective Challenge or Remedy Processes Response: This is a risk. There has been public misunderstanding of how the model performs and is used. VA has responded and clarified misconceptions in the public forums in which misunderstanding has arisen.   Accountable & Monitored (AM) - Performance Efficacy and Fairness (e.g. Model/Data Drift, Model Degradation, or inappropriate application)  Response: This is a risk, model performance is degrading over time and an update is in process. However all identified patients are clinically complex and appropriate for clinical attention.  Accountable & Monitored (AM) - User-Introduced Errors Response: There is a low risk, there is a community of practice to support proper implementation of the REACH VET Program, which is supervised and trained by national program leads.   Accountable & Monitored (AM) - System Performance Not as Intended Response: There is a low risk, the internal VA team that manages the model and risk estimates based on it has validation processes in place to review all steps of the process.",TRUE,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,"VA has examined suicide risk concentration among Veterans in VHA care, stratified by patient sex, race and ethnicity.  Model performance was reasonably similar across demographic subgroups. Because the REACH VET clinical program is just one augmentative component of the overall suicide prevention program, weaknesses in the ability of the REACH VET algorithm to identify high risk patients for intervention have been considered in design of other components of the overall suicide prevention program and mitigated where possible.  For example, the REACH VET model lacks information about patients who have had minimal engagement with the health care system for suicide risk factors they may have experienced.  As such, the model will inherently underestimate risk for new patients and patients experiencing new episodes of symptoms or stressors that could elevate risk.  To combat this weakness, VA implemented a universal suicide risk screening process, including at least annual screening of all patients in primary care and mandatory suicide screening in VA emergency departments which are likely to be the entry point for patients with new challenges that are not yet documented in medical records. This screening triggers mandatory follow-up for positive cases by a clinician with mental health care training, who can offer services (as would be done in REACH VET triggered outreach).  As another example, some known risk factors for suicide were not well captured in medical records at the time of model training.  VHA has implemented clinical screening programs for some of these risk factors (e.g. military sexual trauma, food insecurity) and trained clinicians regarding their contribution to risk and the importance of offering services to address challenges.  Likewise, VA decision support systems that provide drill-in summaries of suicide related risks for REACH VET and other VHA patients (i.e. CRISTAL) summarize and display information from these clinical screeners with other information about suicide risk factors to facilitate clinical evaluation and response to suicide risks.  While it is impossible to build a REACH VET model that is not affected by biases in detection of risk factors due to variation in access to/utilization of VA health care, VHA has taken steps to address expected/known gaps in model performance through implementation of other suicide prevention programming.","[""Post-transaction customer feedback collections"",""Other"",""Public hearings or meetings""]",Yes,Both,0.746031746031746,"The REACH VET 1.0 model is a predictive model using predictors extracted from the patient’s medical records (into the VA’s Corporate Data Warehouse), such as diagnoses, prescriptions, and health care utilization, to estimate statistical risk of a suicide death in the next month.  
This predictive model is used for two purposes. (1) Primary use case is to support identification of patients at elevated statistical risk of suicide in the next month for referral to a clinical case review and outreach program. Staffing guidance to ensure each health care system is staffed to conduct the case review and outreach protocol is established per clinical policy. To date, the population recommended to the clinical case review program has been limited to a number equivalent to 0.1% of the total treated patient population at each facility each month; the number of patients in the program may be modified based on guidance from the Office of Suicide Prevention. The REACH VET 1.0 statistical risk estimates contribute to an algorithm used to identify patients to refer to this program at each facility. (2) Secondary use case is to help risk stratify patients based on prior health care interactions to support augmentation of clinical attention and interventions for patients with patterns of care associated with high suicide risk. This is used, for example, to support clinical review of relevant health care history during triage of Veteran Crisis Line calls and during risk assessment of patients in mental health care.
For use case 1, REACH VET coordinators and REACH VET providers use the list and supporting information to guide execution of the targeted prevention program. For use case 2, Veterans Crisis Line responders and VHA health care providers use the categorized risk scores in the context of curated descriptive data from the patient’s medical record to review risk factors towards optimizing assessment and clinical follow-up.   
The REACH VET 1.0 model is expected to identify a population of patients at elevated risk of suicide and other negative outcomes for review by clinicians; clinicians conduct telephone outreach to identified patients when they deem such outreach appropriate.  This model and targeted prevention in which it is used (i.e. the REACH VET program) is used to augment VHA's extensive clinical suicide prevention program. While VHA conducts universal screening for suicide risk by asking patients structured questions regarding suicidality, this screening process does not identify all patients who go on to die of  suicide. The REACH VET model is intended to identify patients who might be missed in clinical assessment or whose risk may have changed since their last assessment. Here, the REACH VET model identifies patients at statistical risk of suicide, that is they share similarities in their health care data to patients who did die of suicide in the past.  The patients at highest statistical risk are highlighted to mental health providers for additional review. This targeted prevention program has been nationally implemented in VHA health care systems since 2017 and an evaluation of the effects of the REACHVET program was conducted. This evaluation found that REACH VET implementation was associated with greater treatment engagement and new safety plan documentation and fewer mental health admissions, emergency department visits, and suicide attempts. Full findings of this program evaluation are available at:  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8524305/ . The REACH VET predictive model estimates the likelihood of dying of suicide in the next month for each active patient in VHA. This estimate is used as part of an algorithm that considers factors such as the size of the health care system, current patient treatment engagement, and whether the patient has had previous engagement with the REACH VET program to generate a dashboard with patients at high statistical risk of suicide for review by a REACH VET coordinator and identified clinicians at each health care system. The REACH VET dashboard provides information about identified risk factors for the included patients and supports a coordinated case review and, where applicable, clinical outreach process to support identification of care gaps and offer additional services as appropriate. . Our main concerns regarding risks are as follows:  (1) Providers might over-rely on statistical risk estimate. To reduce this risk, we provide information regarding which risk factors were identified by the predictive model to encourage transparent understanding of the basis for the model estimate.  Moreover, the clinical prevention program provides only general guidelines to clinicians indicating that identified patients should be reviewed.  This forces clinicians to rely on their clinical skills to plan intervention responses. (2) The program could be clinically inefficient and use provider time that might be more effectively focused elsewhere.  This is a real risk which may evolve over time as other components of the overall VA suicide prevention program improve, and potentially reduce the need for this targeted prevention program. VHA conducts ongoing evaluations to assess for potential inefficiencies in the model and test methods to improve clinical efficiency.  For example, VA is currently piloting a process to expand focus of the REACH VET program to patients not currently engaged in mental health care.  (3) The program could lead to unfair allocation of clinical resources if the model is biased and other clinical programming does not compensate for areas of model underperformance. This is also a risk and thus VHA evaluates model bias across key demographic populations and actively develops methods to reduce model bias or address bias by adjusting other components of the treatment system to compensate.   After review of the common risks template, the following additional risks were identified: Fair & Equitable (FE) - Algorithm target not reflective of real-world outcome of interest Response: There is a minor risk that the county coroners who assign cause of death might have miscategorized some Veteran deaths, reducing the validity of the outcome variable used.   Transparent & Explainable (TE) - Degradation of End-User Trust and Ineffective Challenge or Remedy Processes Response: This is a risk. There has been public misunderstanding of how the model performs and is used. VA has responded and clarified misconceptions in the public forums in which misunderstanding has arisen.   Accountable & Monitored (AM) - Performance Efficacy and Fairness (e.g. Model/Data Drift, Model Degradation, or inappropriate application)  Response: This is a risk, model performance is degrading over time and an update is in process. However all identified patients are clinically complex and appropriate for clinical attention.  Accountable & Monitored (AM) - User-Introduced Errors Response: There is a low risk, there is a community of practice to support proper implementation of the REACH VET Program, which is supervised and trained by national program leads.   Accountable & Monitored (AM) - System Performance Not as Intended Response: There is a low risk, the internal VA team that manages the model and risk estimates based on it has validation processes in place to review all steps of the process.","the reach vet 1.0 model is a predictive model using predictors extracted from the patient’s medical records (into the va’s corporate data warehouse), such as diagnoses, prescriptions, and health care utilization, to estimate statistical risk of a suicide death in the next month. this predictive model is used for two purposes. (1) primary use case is to support identification of patients at elevated statistical risk of suicide in the next month for referral to a clinical case review and outreach program. staffing guidance to ensure each health care system is staffed to conduct the case review and outreach protocol is established per clinical policy. to date, the population recommended to the clinical case review program has been limited to a number equivalent to 0.1% of the total treated patient population at each facility each month; the number of patients in the program may be modified based on guidance from the office of suicide prevention. the reach vet 1.0 statistical risk estimates contribute to an algorithm used to identify patients to refer to this program at each facility. (2) secondary use case is to help risk stratify patients based on prior health care interactions to support augmentation of clinical attention and interventions for patients with patterns of care associated with high suicide risk. this is used, for example, to support clinical review of relevant health care history during triage of veteran crisis line calls and during risk assessment of patients in mental health care. for use case 1, reach vet coordinators and reach vet providers use the list and supporting information to guide execution of the targeted prevention program. for use case 2, veterans crisis line responders and vha health care providers use the categorized risk scores in the context of curated descriptive data from the patient’s medical record to review risk factors towards optimizing assessment and clinical follow-up. the reach vet 1.0 model is expected to identify a population of patients at elevated risk of suicide and other negative outcomes for review by clinicians; clinicians conduct telephone outreach to identified patients when they deem such outreach appropriate. this model and targeted prevention in which it is used (i.e. the reach vet program) is used to augment vha's extensive clinical suicide prevention program. while vha conducts universal screening for suicide risk by asking patients structured questions regarding suicidality, this screening process does not identify all patients who go on to die of suicide. the reach vet model is intended to identify patients who might be missed in clinical assessment or whose risk may have changed since their last assessment. here, the reach vet model identifies patients at statistical risk of suicide, that is they share similarities in their health care data to patients who did die of suicide in the past. the patients at highest statistical risk are highlighted to mental health providers for additional review. this targeted prevention program has been nationally implemented in vha health care systems since 2017 and an evaluation of the effects of the reachvet program was conducted. this evaluation found that reach vet implementation was associated with greater treatment engagement and new safety plan documentation and fewer mental health admissions, emergency department visits, and suicide attempts. full findings of this program evaluation are available at: https://www.ncbi.nlm.nih.gov/pmc/articles/pmc8524305/ . the reach vet predictive model estimates the likelihood of dying of suicide in the next month for each active patient in vha. this estimate is used as part of an algorithm that considers factors such as the size of the health care system, current patient treatment engagement, and whether the patient has had previous engagement with the reach vet program to generate a dashboard with patients at high statistical risk of suicide for review by a reach vet coordinator and identified clinicians at each health care system. the reach vet dashboard provides information about identified risk factors for the included patients and supports a coordinated case review and, where applicable, clinical outreach process to support identification of care gaps and offer additional services as appropriate. . our main concerns regarding risks are as follows: (1) providers might over-rely on statistical risk estimate. to reduce this risk, we provide information regarding which risk factors were identified by the predictive model to encourage transparent understanding of the basis for the model estimate. moreover, the clinical prevention program provides only general guidelines to clinicians indicating that identified patients should be reviewed. this forces clinicians to rely on their clinical skills to plan intervention responses. (2) the program could be clinically inefficient and use provider time that might be more effectively focused elsewhere. this is a real risk which may evolve over time as other components of the overall va suicide prevention program improve, and potentially reduce the need for this targeted prevention program. vha conducts ongoing evaluations to assess for potential inefficiencies in the model and test methods to improve clinical efficiency. for example, va is currently piloting a process to expand focus of the reach vet program to patients not currently engaged in mental health care. (3) the program could lead to unfair allocation of clinical resources if the model is biased and other clinical programming does not compensate for areas of model underperformance. this is also a risk and thus vha evaluates model bias across key demographic populations and actively develops methods to reduce model bias or address bias by adjusting other components of the treatment system to compensate. after review of the common risks template, the following additional risks were identified: fair & equitable (fe) - algorithm target not reflective of real-world outcome of interest response: there is a minor risk that the county coroners who assign cause of death might have miscategorized some veteran deaths, reducing the validity of the outcome variable used. transparent & explainable (te) - degradation of end-user trust and ineffective challenge or remedy processes response: this is a risk. there has been public misunderstanding of how the model performs and is used. va has responded and clarified misconceptions in the public forums in which misunderstanding has arisen. accountable & monitored (am) - performance efficacy and fairness (e.g. model/data drift, model degradation, or inappropriate application) response: this is a risk, model performance is degrading over time and an update is in process. however all identified patients are clinically complex and appropriate for clinical attention. accountable & monitored (am) - user-introduced errors response: there is a low risk, there is a community of practice to support proper implementation of the reach vet program, which is supervised and trained by national program leads. accountable & monitored (am) - system performance not as intended response: there is a low risk, the internal va team that manages the model and risk estimates based on it has validation processes in place to review all steps of the process."
Venue; Venue Fit; Venue Go,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfPMN/pmn.cfm?ID=K180599).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfPMN/pmn.cfm?ID=K180599).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm?id=k180599).
Velacur,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K223287.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K223287.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/k223287.pdf).
Syngo Application Software,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf17/K170747.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf17/K170747.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf17/k170747.pdf).
Operationalizing Coronary Artery Calcium Computer Vision Model,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,This AI model's purpose is to screen Chest CT scans for coronary artery calcification and notify physicians of patients with increased cardiovascular risk. This will help physicians detect cardiovascular disease earlier and better treat their patients.,The model will output a coronary artery calcium score and also output image masks showing physicians where the model detected coronary artery calcium on the patients chest images.,Acquisition and/or Development,Both,1/1/2023,5/5/2023,Unknown,Developed in-house.,Unknown,Yes,No,Yes,Yes,Yes,"VA Imaging data, VINCI-CDW based data","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,"ARCHES, VA Long Beach AIOC",Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,This AI model's purpose is to screen Chest CT scans for coronary artery calcification and notify physicians of patients with increased cardiovascular risk. This will help physicians detect cardiovascular disease earlier and better treat their patients. . The model will output a coronary artery calcium score and also output image masks showing physicians where the model detected coronary artery calcium on the patients chest images.,this ai model's purpose is to screen chest ct scans for coronary artery calcification and notify physicians of patients with increased cardiovascular risk. this will help physicians detect cardiovascular disease earlier and better treat their patients. . the model will output a coronary artery calcium score and also output image masks showing physicians where the model detected coronary artery calcium on the patients chest images.
Rapid AI,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K233582.pdf).,Operation and Maintenance,Both,5/13/2023,5/13/2023,5/13/2023,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5555555555555556,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K233582.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/k233582.pdf).
"Spine Planning (2.0), Elements Spine Planning, Elements Planning Spine",Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K223553.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K223553.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/k223553.pdf).
Post discharge 30-day readmission or death prediction model to select patients for expedited postdischarge clinic follow-up.,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"The purpose of this AI is to act as an aid not a replacement for clinical decision-making about whether a patient should have clinical follow-up in the week following discharge (an accelerated timeframe) or follow-up in 2-4 weeks (the regular timeframe). The AI will consider dozens of factors to which a clinician does not have ready access or time to review during a typical discharge process. Patients that are predicted to have an adverse outcome will be recommended a quick follow-up but this will not be mandated, similarly patients not predicted to have an adverse outcome can still be scheduled for quick post discharge follow-up. Predicting which patients will die or be readmitted after discharge is a famously difficult unsolved clinical problem, and this tool will be one more data point for clinicians to aid but not replace clinical judgment during the discharge process.

The AI’s intended benefit is to make our healthcare system more systematic and accurate in identifying high-risk patients at discharge as a primary goal. An aspirational benefit is that this process of more systematic more accurate follow-up for high risk patients will lead to decreased re-admissions and mortality after discharge.",The XG boost system outputs a 0 and 100% probability of readmission or death within 30 days.,Acquisition and/or Development,Both,6/1/2023,6/1/2023,Unknown,Developed in-house.,Unknown,Yes,No,No,No,Yes,The AI training data set is our healthcare systems admissions and subsequent readmissions or death at 30 days between the fiscal years 2018 and 2023. This was derived from CDW data.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,More than 12 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5238095238095238,"The purpose of this AI is to act as an aid not a replacement for clinical decision-making about whether a patient should have clinical follow-up in the week following discharge (an accelerated timeframe) or follow-up in 2-4 weeks (the regular timeframe). The AI will consider dozens of factors to which a clinician does not have ready access or time to review during a typical discharge process. Patients that are predicted to have an adverse outcome will be recommended a quick follow-up but this will not be mandated, similarly patients not predicted to have an adverse outcome can still be scheduled for quick post discharge follow-up. Predicting which patients will die or be readmitted after discharge is a famously difficult unsolved clinical problem, and this tool will be one more data point for clinicians to aid but not replace clinical judgment during the discharge process.

The AI’s intended benefit is to make our healthcare system more systematic and accurate in identifying high-risk patients at discharge as a primary goal. An aspirational benefit is that this process of more systematic more accurate follow-up for high risk patients will lead to decreased re-admissions and mortality after discharge. . The XG boost system outputs a 0 and 100% probability of readmission or death within 30 days.","the purpose of this ai is to act as an aid not a replacement for clinical decision-making about whether a patient should have clinical follow-up in the week following discharge (an accelerated timeframe) or follow-up in 2-4 weeks (the regular timeframe). the ai will consider dozens of factors to which a clinician does not have ready access or time to review during a typical discharge process. patients that are predicted to have an adverse outcome will be recommended a quick follow-up but this will not be mandated, similarly patients not predicted to have an adverse outcome can still be scheduled for quick post discharge follow-up. predicting which patients will die or be readmitted after discharge is a famously difficult unsolved clinical problem, and this tool will be one more data point for clinicians to aid but not replace clinical judgment during the discharge process. the ai’s intended benefit is to make our healthcare system more systematic and accurate in identifying high-risk patients at discharge as a primary goal. an aspirational benefit is that this process of more systematic more accurate follow-up for high risk patients will lead to decreased re-admissions and mortality after discharge. . the xg boost system outputs a 0 and 100% probability of readmission or death within 30 days."
RayStation 11B,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K220141.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K220141.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/k220141.pdf).
Enterprise Command Center - Generative AI ,Department of Veterans Affairs,VA,OIT: Office of Information & Technology,Mission-Enabling (internal agency support),None of the above.,The benefit of this use case is to provide plain English alert messages to service owners. The current alert language has proven suboptimal and results in a inordinate volume of alerts not being actioned. By feeding alerts into an LLM the message can be reconfigured to be understood by a human who in turn can take appropriate steps to resolve the alert.,Human readable alert messages,Initiated,Neither,3/22/2024,Unknown,Unknown,Developed in-house.,Unknown,No,No,No,Yes,Yes,"ServiceNow, SolarWinds, Splunk, Dynatrace, AppDynamics, and ScienceLogic all host alert data that would be used to train the model.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,"2879, 2462, 2464, 2818, 2463",6-12 months,Yes,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,The benefit of this use case is to provide plain English alert messages to service owners. The current alert language has proven suboptimal and results in a inordinate volume of alerts not being actioned. By feeding alerts into an LLM the message can be reconfigured to be understood by a human who in turn can take appropriate steps to resolve the alert. . Human readable alert messages,the benefit of this use case is to provide plain english alert messages to service owners. the current alert language has proven suboptimal and results in a inordinate volume of alerts not being actioned. by feeding alerts into an llm the message can be reconfigured to be understood by a human who in turn can take appropriate steps to resolve the alert. . human readable alert messages
QLAB Advanced Quantification Software,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf20/K200974.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf20/K200974.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf20/k200974.pdf).
"Philips EPIQ Diagnostic Ultrasound System, Philips Affiniti Diagnostic Ultrasound System",Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf21/K212704.pdf).,Operation and Maintenance,Both,11/28/2022,11/28/2022,11/28/2022,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5555555555555556,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf21/K212704.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf21/k212704.pdf).
"OPTIS Mobile Next Imaging System, OPTIS Integrated Next Imaging System",Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf21/K210458.pdf).,Operation and Maintenance,Both,7/17/2023,7/17/2023,7/17/2023,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5555555555555556,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf21/K210458.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf21/k210458.pdf).
Automated Decision Support,Department of Veterans Affairs,VA,VBA: Veterans Benefits Administration,Government Services (includes Benefits and Service Delivery),None of the above.,"Intake of VA Claims materials as well as other Veteran, VSO, and stakeholder submissions results in contentions which are prescribed as eligible.  The platform ingests and takes initial actions on claims and contentions (largely those impacted by the PACT Act) which include medical record retrieval, review of the VBMS eFolder, generates a summary review document, drafting or submitting contract examination requests via robotics process automation (RPA), and/or changing the claim status to Ready For Decision (RFD).  
","Utilizing prescribed machine learning and Natural Language Processing (NLP), ADS utilizes Intelligent Form Recognition (IFR), Optical Character Recognition (OCR), Intelligent Character Recognition (ICR), and NLP to extract an average of 95 fields on over 1500 form layouts within VA’s purview.  The system generates a summary review document, drafting or submitting contract examination requests via robotics process automation (RPA), and/or changing the claim status to Ready For Decision (RFD).",Operation and Maintenance,Both,7/1/2021,8/1/2021,1/27/2022,Developed with both contracting and in-house resources.,IDIQ 36C10E19D0018   APCAS 36C10D21N0013   MAS 36C10E20N0028   BTP 36C10D23N0010  ,Yes,No,Yes,Yes,Yes,NLP/ML has primarily focused around contents within the eFolder.  Please see the MOU/iSA within eMass and Vasi as the 46 page document delineates data optimization and boundaries.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,VBAAP - Vasi 2522 eMASS 1143,Less than 6 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5714285714285714,"Intake of VA Claims materials as well as other Veteran, VSO, and stakeholder submissions results in contentions which are prescribed as eligible.  The platform ingests and takes initial actions on claims and contentions (largely those impacted by the PACT Act) which include medical record retrieval, review of the VBMS eFolder, generates a summary review document, drafting or submitting contract examination requests via robotics process automation (RPA), and/or changing the claim status to Ready For Decision (RFD). . Utilizing prescribed machine learning and Natural Language Processing (NLP), ADS utilizes Intelligent Form Recognition (IFR), Optical Character Recognition (OCR), Intelligent Character Recognition (ICR), and NLP to extract an average of 95 fields on over 1500 form layouts within VA’s purview.  The system generates a summary review document, drafting or submitting contract examination requests via robotics process automation (RPA), and/or changing the claim status to Ready For Decision (RFD).","intake of va claims materials as well as other veteran, vso, and stakeholder submissions results in contentions which are prescribed as eligible. the platform ingests and takes initial actions on claims and contentions (largely those impacted by the pact act) which include medical record retrieval, review of the vbms efolder, generates a summary review document, drafting or submitting contract examination requests via robotics process automation (rpa), and/or changing the claim status to ready for decision (rfd). . utilizing prescribed machine learning and natural language processing (nlp), ads utilizes intelligent form recognition (ifr), optical character recognition (ocr), intelligent character recognition (icr), and nlp to extract an average of 95 fields on over 1500 form layouts within va’s purview. the system generates a summary review document, drafting or submitting contract examination requests via robotics process automation (rpa), and/or changing the claim status to ready for decision (rfd)."
O-arm O2 Imaging System,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf24/K240465.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf24/K240465.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf24/k240465.pdf).
Mail Automation Services,Department of Veterans Affairs,VA,VBA: Veterans Benefits Administration,Government Services (includes Benefits and Service Delivery),None of the above.,"Mail Automation Services (MAS) is a ITTT service for intake of VA Claims materials as well as other Veteran, VSO, and stakeholder submissions.  The platform ingests and reviews 25k to 40k packets of information per day, primarily derived from the Centralized Mail Portal.  Utilizing prescribed machine learning and Natural Language Processing (NLP), MAS utilizes Intelligent Form Recognition (IFR), Optical Character Recognition (OCR), Intelligent Character Recognition (ICR), and NLP to extract an average of 95 fields on over 1500 form layouts within VA’s purview.",VBA End-Product establishment and correct business line orientation for ingested submissions are the most common outcomes and outputs for the MAS platform which runs under Veterans Benefits Administration Automation Platform (VBAAP).,Operation and Maintenance,"Rights-Impacting
",7/1/2019,10/1/2019,1/6/2020,Developed with both contracting and in-house resources.,IDIQ 36C10E19D0018   APCAS 36C10D21N0013   MAS 36C10E20N0028   BTP 36C10D23N0010  ,Yes,No,Yes,Yes,Yes,NLP/ML has primarily focused around contents submitted via the centralized mail portal.  Please see the MOU/iSA within eMass and Vasi as the 46 page document delineates data optimization and boundaries.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,VBAAP - Vasi 2522 eMASS 1143,Less than 6 months,No,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Rights-Impacting,0.5714285714285714,"Mail Automation Services (MAS) is a ITTT service for intake of VA Claims materials as well as other Veteran, VSO, and stakeholder submissions.  The platform ingests and reviews 25k to 40k packets of information per day, primarily derived from the Centralized Mail Portal.  Utilizing prescribed machine learning and Natural Language Processing (NLP), MAS utilizes Intelligent Form Recognition (IFR), Optical Character Recognition (OCR), Intelligent Character Recognition (ICR), and NLP to extract an average of 95 fields on over 1500 form layouts within VA’s purview. . VBA End-Product establishment and correct business line orientation for ingested submissions are the most common outcomes and outputs for the MAS platform which runs under Veterans Benefits Administration Automation Platform (VBAAP).","mail automation services (mas) is a ittt service for intake of va claims materials as well as other veteran, vso, and stakeholder submissions. the platform ingests and reviews 25k to 40k packets of information per day, primarily derived from the centralized mail portal. utilizing prescribed machine learning and natural language processing (nlp), mas utilizes intelligent form recognition (ifr), optical character recognition (ocr), intelligent character recognition (icr), and nlp to extract an average of 95 fields on over 1500 form layouts within va’s purview. . vba end-product establishment and correct business line orientation for ingested submissions are the most common outcomes and outputs for the mas platform which runs under veterans benefits administration automation platform (vbaap)."
NeuroQuant,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf17/K170981.pdf).,Operation and Maintenance,Both,10/31/2017,10/31/2017,10/31/2017,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5555555555555556,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf17/K170981.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf17/k170981.pdf).
MAGNETOM Vida; MAGNETOM Lumina; MAGNETOM Aera; MAGNETOM Skyra; MAGNETOM Prisma; MAGNETOM Prisma fit,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfPMN/pmn.cfm?ID=K231560).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfPMN/pmn.cfm?ID=K231560).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm?id=k231560).
3D Quorum ,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,Mammography. AI-powered analytics - Generates 6mm 'SmartSlices' from the original high-resolution 3D data. Identifies clinically relevant regions of interest through overlapping slices (3mm) to ensure no loss of 3D data. Reduces the number of 3D images to be reviewed by the Radiologist by 2/3. ,Produces 6mm 'SmartSlices' from native 3D data to highlight clinically relevant or potentially clinically relevant regions of interest. Augments the decision making process of Board Certified or Board Eligible Radiologists. Interpreting Radiologists retain the ability to 'accept' or 'reject' the suggested outputs.,Operation and Maintenance,Both,1/1/2019,1/1/2020,1/1/2021,Unknown,Unknown,Yes,No,Yes,No,No,This use case will access PHI/PII from VA 'Home of Record' systems (Vista/CPRS) and peripheral Radiology PACS & Dictation Systems.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,No – agency does not have access to source code.,No,Unknown,More than 12 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5238095238095238,Mammography. AI-powered analytics - Generates 6mm 'SmartSlices' from the original high-resolution 3D data. Identifies clinically relevant regions of interest through overlapping slices (3mm) to ensure no loss of 3D data. Reduces the number of 3D images to be reviewed by the Radiologist by 2/3. . Produces 6mm 'SmartSlices' from native 3D data to highlight clinically relevant or potentially clinically relevant regions of interest. Augments the decision making process of Board Certified or Board Eligible Radiologists. Interpreting Radiologists retain the ability to 'accept' or 'reject' the suggested outputs.,mammography. ai-powered analytics - generates 6mm 'smartslices' from the original high-resolution 3d data. identifies clinically relevant regions of interest through overlapping slices (3mm) to ensure no loss of 3d data. reduces the number of 3d images to be reviewed by the radiologist by 2/3. . produces 6mm 'smartslices' from native 3d data to highlight clinically relevant or potentially clinically relevant regions of interest. augments the decision making process of board certified or board eligible radiologists. interpreting radiologists retain the ability to 'accept' or 'reject' the suggested outputs.
INtuition-Structural Heart Module,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfPMN/pmn.cfm?ID=K191585).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfPMN/pmn.cfm?ID=K191585).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm?id=k191585).
Intelligent 2D ,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"Mammography - Produces synthetic 2D images from 3D mammography data to improve the detection of breast lesions while reducing radiation dose. Reduces the 'noise' if the images, helping to eliminate random elements that may distort the image. Provides Interpreting Radiologists improved detection ability through reduction of artifacts that may reduce visibility of lesions - ultimately providing a clearer picture of breast tissue. FDA Approved. ",Provides synthetic 2D image to augment the Interpreting Radiologist's ability to detect breast lesions.,Operation and Maintenance,Both,1/1/2019,1/1/2020,1/1/2021,Unknown,Unknown,Yes,No,Yes,No,No,This use case will access PHI/PII from VA 'Home of Record' systems (Vista/CPRS) and peripheral Radiology PACS & Dictation Systems.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,No – agency does not have access to source code.,No,Unknown,More than 12 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5238095238095238,"Mammography - Produces synthetic 2D images from 3D mammography data to improve the detection of breast lesions while reducing radiation dose. Reduces the 'noise' if the images, helping to eliminate random elements that may distort the image. Provides Interpreting Radiologists improved detection ability through reduction of artifacts that may reduce visibility of lesions - ultimately providing a clearer picture of breast tissue. FDA Approved. . Provides synthetic 2D image to augment the Interpreting Radiologist's ability to detect breast lesions.","mammography - produces synthetic 2d images from 3d mammography data to improve the detection of breast lesions while reducing radiation dose. reduces the 'noise' if the images, helping to eliminate random elements that may distort the image. provides interpreting radiologists improved detection ability through reduction of artifacts that may reduce visibility of lesions - ultimately providing a clearer picture of breast tissue. fda approved. . provides synthetic 2d image to augment the interpreting radiologist's ability to detect breast lesions."
QV CAD,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,Automated Breast Ultrasound (ABUS) solution from QView Medical. Aids Radiologists with interpreting screening procedures for women who have negative mammograms but have had a 3D breast ultrasound due to dense breast tissue. Receives input images via standard DICOM format. Native imaging from the ABUS together with the output of the QVCAD images are displayed on the viewer. QVCAD engine uses several image patter recognition processes and artificial neural networks to detect suspicious areas in the breast with the objective to distinguish potential breast lesions from normal breast tissue. ,Detects suspicious or potentially suspicious areas in the breast.  Augments the decision making process of Board Certified or Board Eligible Radiologists. Interpreting Radiologists retain the ability to 'accept' or 'reject' the suggested outputs.,Acquisition and/or Development,Both,10/1/2023,9/6/2024,Unknown,Unknown,Unknown,Yes,No,Yes,No,No,This use case will access PHI/PII from VA 'Home of Record' systems (Vista/CPRS) and peripheral Radiology PACS & Dictation Systems.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,No – agency does not have access to source code.,No,Unknown,More than 12 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,Automated Breast Ultrasound (ABUS) solution from QView Medical. Aids Radiologists with interpreting screening procedures for women who have negative mammograms but have had a 3D breast ultrasound due to dense breast tissue. Receives input images via standard DICOM format. Native imaging from the ABUS together with the output of the QVCAD images are displayed on the viewer. QVCAD engine uses several image patter recognition processes and artificial neural networks to detect suspicious areas in the breast with the objective to distinguish potential breast lesions from normal breast tissue. . Detects suspicious or potentially suspicious areas in the breast.  Augments the decision making process of Board Certified or Board Eligible Radiologists. Interpreting Radiologists retain the ability to 'accept' or 'reject' the suggested outputs.,automated breast ultrasound (abus) solution from qview medical. aids radiologists with interpreting screening procedures for women who have negative mammograms but have had a 3d breast ultrasound due to dense breast tissue. receives input images via standard dicom format. native imaging from the abus together with the output of the qvcad images are displayed on the viewer. qvcad engine uses several image patter recognition processes and artificial neural networks to detect suspicious areas in the breast with the objective to distinguish potential breast lesions from normal breast tissue. . detects suspicious or potentially suspicious areas in the breast. augments the decision making process of board certified or board eligible radiologists. interpreting radiologists retain the ability to 'accept' or 'reject' the suggested outputs.
FlexLine: Detect patterns and make corrective action recommendations in application and dependency time to mitigation data. ,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Mission-Enabling (internal agency support),None of the above.,"Within the FlexLine materials management application, this AI capability would look at patterns across the time-to-mitigation data and potentially other scans for application vulnerabilities. If teams are not resolving vulnerabilities within the 30, 45, 60 day periods, then it could expose our products and degrade Veteran care. The goal would be to connect the duration to mitigate to other available data trends and recommend actions to the application teams that could help them resolve the issues faster.",Will provide detailed text data hat combines/correlates scan results for the app team and what might be causing the inability to resolve. This additional intelligence and recommended resolution(text) could be provided to the app team through automated Jira tickets to help them understand a way to resolve the issue.,Initiated,Neither,9/19/2024,Unknown,Unknown,Developed with both contracting and in-house resources.,"36C10G24D0048, 36C10G24N0112",No,No,No,Yes,Yes,OCC FlexLine vulnerability and scan data.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,2313,Unknown,Yes,Yes,Other,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"Within the FlexLine materials management application, this AI capability would look at patterns across the time-to-mitigation data and potentially other scans for application vulnerabilities. If teams are not resolving vulnerabilities within the 30, 45, 60 day periods, then it could expose our products and degrade Veteran care. The goal would be to connect the duration to mitigate to other available data trends and recommend actions to the application teams that could help them resolve the issues faster. . Will provide detailed text data hat combines/correlates scan results for the app team and what might be causing the inability to resolve. This additional intelligence and recommended resolution(text) could be provided to the app team through automated Jira tickets to help them understand a way to resolve the issue.","within the flexline materials management application, this ai capability would look at patterns across the time-to-mitigation data and potentially other scans for application vulnerabilities. if teams are not resolving vulnerabilities within the 30, 45, 60 day periods, then it could expose our products and degrade veteran care. the goal would be to connect the duration to mitigate to other available data trends and recommend actions to the application teams that could help them resolve the issues faster. . will provide detailed text data hat combines/correlates scan results for the app team and what might be causing the inability to resolve. this additional intelligence and recommended resolution(text) could be provided to the app team through automated jira tickets to help them understand a way to resolve the issue."
FFRangio,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf19/K192442.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf19/K192442.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf19/k192442.pdf).
Quantra ,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,Mammography. Uses multi-class support vector machine (SVM) based classification technique to segregate breast types into four categories based on breast parenchymal tissue pattern and texture representation to assist Radiologists in interpretations and reduce intra- and inter-reader variability. ,Classifies breast type. Augments the decision making process of Board Certified or Board Eligible Radiologists. Interpreting Radiologists retain the ability to 'accept' or 'reject' the suggested outputs.,Operation and Maintenance,Both,1/1/2019,1/1/2020,1/1/2022,Unknown,Unknown,Yes,No,Yes,No,No,This use case will access PHI/PII from VA 'Home of Record' systems (Vista/CPRS) and peripheral Radiology PACS & Dictation Systems.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,No – agency does not have access to source code.,No,Unknown,More than 12 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5238095238095238,Mammography. Uses multi-class support vector machine (SVM) based classification technique to segregate breast types into four categories based on breast parenchymal tissue pattern and texture representation to assist Radiologists in interpretations and reduce intra- and inter-reader variability. . Classifies breast type. Augments the decision making process of Board Certified or Board Eligible Radiologists. Interpreting Radiologists retain the ability to 'accept' or 'reject' the suggested outputs.,mammography. uses multi-class support vector machine (svm) based classification technique to segregate breast types into four categories based on breast parenchymal tissue pattern and texture representation to assist radiologists in interpretations and reduce intra- and inter-reader variability. . classifies breast type. augments the decision making process of board certified or board eligible radiologists. interpreting radiologists retain the ability to 'accept' or 'reject' the suggested outputs.
 Surveillance and Reporting of Suicidal Ideation Assessment in PTSD Specialty Care Clinical Notes using Natural Language Processing (NLP),Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,We endeavor to surveil and monitor nationally both the conduct and quality of suicide ideation (SI) assessment during treatment in PTSD specialty care. We hope eventually to increase the consistency and quality of SI assessment in PTSD care to enhance clinical management of suicide risk. ,"Our methods entail the use of natural language processing (NLP) of progress note documentation of treatment within the PTSD Clinical Team (PCT) specialty care clinics, to identify SI mentions, to quantify SI assessment documentation, and to classify SI mention clinical quality. We will provide reports of consistency and quality and the national, VISN, and medical center levels.",Acquisition and/or Development,Neither,8/1/2023,10/1/2023,Unknown,Developed in-house.,Unknown,No,No,Yes,Yes,Yes,VA Corporate Data Warehouse (CDW) provides clinical progress notes in text form through its TIU for our use in this project.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,CDW,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"We endeavor to surveil and monitor nationally both the conduct and quality of suicide ideation (SI) assessment during treatment in PTSD specialty care. We hope eventually to increase the consistency and quality of SI assessment in PTSD care to enhance clinical management of suicide risk. . Our methods entail the use of natural language processing (NLP) of progress note documentation of treatment within the PTSD Clinical Team (PCT) specialty care clinics, to identify SI mentions, to quantify SI assessment documentation, and to classify SI mention clinical quality. We will provide reports of consistency and quality and the national, VISN, and medical center levels.","we endeavor to surveil and monitor nationally both the conduct and quality of suicide ideation (si) assessment during treatment in ptsd specialty care. we hope eventually to increase the consistency and quality of si assessment in ptsd care to enhance clinical management of suicide risk. . our methods entail the use of natural language processing (nlp) of progress note documentation of treatment within the ptsd clinical team (pct) specialty care clinics, to identify si mentions, to quantify si assessment documentation, and to classify si mention clinical quality. we will provide reports of consistency and quality and the national, visn, and medical center levels."
"FastStroke, CT Perfusion 4D",Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf19/K193289.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf19/K193289.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf19/k193289.pdf).
Koios,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,Used with Automated Breast Ultrasound (ABUS). Classifies breast ultrasound lesions and automates reporting for improved efficiency and streamlined workflows. Increased sensitivity and specificity with reduced variability. Reduces unnecessary treatments (existing BI-RADS 3's can be accurately and safely downgraded) and provides probability of malignancy aligned to BI-RADS categorization. ,Classifies breast ultrasound lesions. Augments the decision making process of Board Certified or Board Eligible Radiologists. Interpreting Radiologists retain the ability to 'accept' or 'reject' the suggested outputs.,Acquisition and/or Development,Both,10/1/2023,9/6/2024,Unknown,Unknown,Unknown,Yes,No,Yes,No,No,This use case will access PHI/PII from VA 'Home of Record' systems (Vista/CPRS).,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,No – agency does not have access to source code.,No,Unknown,More than 12 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,Used with Automated Breast Ultrasound (ABUS). Classifies breast ultrasound lesions and automates reporting for improved efficiency and streamlined workflows. Increased sensitivity and specificity with reduced variability. Reduces unnecessary treatments (existing BI-RADS 3's can be accurately and safely downgraded) and provides probability of malignancy aligned to BI-RADS categorization. . Classifies breast ultrasound lesions. Augments the decision making process of Board Certified or Board Eligible Radiologists. Interpreting Radiologists retain the ability to 'accept' or 'reject' the suggested outputs.,used with automated breast ultrasound (abus). classifies breast ultrasound lesions and automates reporting for improved efficiency and streamlined workflows. increased sensitivity and specificity with reduced variability. reduces unnecessary treatments (existing bi-rads 3's can be accurately and safely downgraded) and provides probability of malignancy aligned to bi-rads categorization. . classifies breast ultrasound lesions. augments the decision making process of board certified or board eligible radiologists. interpreting radiologists retain the ability to 'accept' or 'reject' the suggested outputs.
Evaluation of Autonomous Assistive Feeding System in Spinal Cord Injury Patients,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"The AI use is to improve autonomous bite acquisition by robotic arm as well as to improve transfer to the participant's mouth.  Inputs include video analysis of caretakers administering/transferring bite to participants, as well as a web interface where feedback from stakeholders will be collected to improve the robotic arm's actions and to provide additional contextual information for bite acquisition and transfer.  This information will be used by the AI to improve the robotic arms autonomous functions of bite acquisition and transfer.

The ultimate goal of the robotic arm function is to provide autonomous or semi-autonomous support to improve functional independence for patients with upper extremity weakness from spinal cord injury or other causes."," Inputs and Outputs: Our experimental setup will consist of bite acquisition trials under two settings. In the autonomous setting, the robot will acquire the food autonomously. In the shared autonomy setting, the robot may acquire the food autonomously, but during certain times, may query the end-user or an expert research assistant based on its confidence about its manipulation actions and use their feedback to complete the bite acquisition process. For both the settings, the robot will use our online learning framework to design its behaviors. In the shared autonomy setting, we will develop an accessible web interface that the stakeholders can use to provide feedback on what actions to use or what additional contexts may be useful for bite-acquisition",Acquisition and/or Development,Safety-impacting,5/13/2024,5/13/2024,Unknown,Developed in-house.,Unknown,Yes,No,Yes,No,No,Will obtain video and input data from trials of robotic feeding arm once research begins proper,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,No – agency does not have access to source code.,No,Unknown,6-12 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Safety-Impacting,0.5238095238095238,"The AI use is to improve autonomous bite acquisition by robotic arm as well as to improve transfer to the participant's mouth.  Inputs include video analysis of caretakers administering/transferring bite to participants, as well as a web interface where feedback from stakeholders will be collected to improve the robotic arm's actions and to provide additional contextual information for bite acquisition and transfer.  This information will be used by the AI to improve the robotic arms autonomous functions of bite acquisition and transfer.

The ultimate goal of the robotic arm function is to provide autonomous or semi-autonomous support to improve functional independence for patients with upper extremity weakness from spinal cord injury or other causes. . Inputs and Outputs: Our experimental setup will consist of bite acquisition trials under two settings. In the autonomous setting, the robot will acquire the food autonomously. In the shared autonomy setting, the robot may acquire the food autonomously, but during certain times, may query the end-user or an expert research assistant based on its confidence about its manipulation actions and use their feedback to complete the bite acquisition process. For both the settings, the robot will use our online learning framework to design its behaviors. In the shared autonomy setting, we will develop an accessible web interface that the stakeholders can use to provide feedback on what actions to use or what additional contexts may be useful for bite-acquisition","the ai use is to improve autonomous bite acquisition by robotic arm as well as to improve transfer to the participant's mouth. inputs include video analysis of caretakers administering/transferring bite to participants, as well as a web interface where feedback from stakeholders will be collected to improve the robotic arm's actions and to provide additional contextual information for bite acquisition and transfer. this information will be used by the ai to improve the robotic arms autonomous functions of bite acquisition and transfer. the ultimate goal of the robotic arm function is to provide autonomous or semi-autonomous support to improve functional independence for patients with upper extremity weakness from spinal cord injury or other causes. . inputs and outputs: our experimental setup will consist of bite acquisition trials under two settings. in the autonomous setting, the robot will acquire the food autonomously. in the shared autonomy setting, the robot may acquire the food autonomously, but during certain times, may query the end-user or an expert research assistant based on its confidence about its manipulation actions and use their feedback to complete the bite acquisition process. for both the settings, the robot will use our online learning framework to design its behaviors. in the shared autonomy setting, we will develop an accessible web interface that the stakeholders can use to provide feedback on what actions to use or what additional contexts may be useful for bite-acquisition"
Discovery MI Gen2,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf21/K211846.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf21/K211846.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf21/k211846.pdf).
Genius AI,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,Mammography. Assists the Radiologist in locating lesions likely representing breast cancer by searching each slice of the tomosynthesis image set. Lesions are marked on the appropriate slices that can be overlaid on a synthesized 2D image and on 3DQuorum SmartSlices. These 2D overlays augment Radiologists by providing an overview image with suspicious areas indicated with quick navigation to the tomosynthesis slice that the mark was originally identified on. ,Lesions are marked on identified imaging slices with reference back to corresponding tomosynthesis slice.,Operation and Maintenance,Both,1/1/2019,1/1/2021,1/1/2022,Unknown,Unknown,Yes,No,Yes,No,No,This use case will access PHI/PII from VA 'Home of Record' systems (Vista/CPRS) and peripheral Radiology PACS & Dictation Systems.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,No – agency does not have access to source code.,No,Unknown,More than 12 months,Yes,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5238095238095238,Mammography. Assists the Radiologist in locating lesions likely representing breast cancer by searching each slice of the tomosynthesis image set. Lesions are marked on the appropriate slices that can be overlaid on a synthesized 2D image and on 3DQuorum SmartSlices. These 2D overlays augment Radiologists by providing an overview image with suspicious areas indicated with quick navigation to the tomosynthesis slice that the mark was originally identified on. . Lesions are marked on identified imaging slices with reference back to corresponding tomosynthesis slice.,mammography. assists the radiologist in locating lesions likely representing breast cancer by searching each slice of the tomosynthesis image set. lesions are marked on the appropriate slices that can be overlaid on a synthesized 2d image and on 3dquorum smartslices. these 2d overlays augment radiologists by providing an overview image with suspicious areas indicated with quick navigation to the tomosynthesis slice that the mark was originally identified on. . lesions are marked on identified imaging slices with reference back to corresponding tomosynthesis slice.
Implementation of a Severe COVID-19 Risk Prediction Tool,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"The project seeks to implement a Severe COVID-19 Risk Prediction Tool to effectively identify Veterans at higher risk of developing severe COVID-19 symptoms through a machine-learning model to address one of the key barriers to increased vaccination offerings identified, i.e., the lack of tools for assessing risk of severe disease progression and potential vaccination benefit. It is expected by facilitating the estimated risk for severe COVID-19 progression, as well as a patient's current vaccine eligibility, that these patients can be outreached to and offered the vaccine in a more timely manner to improve vaccination rates and ultimately health outcomes.","The model will output a relative risk score for currently hospitalized patients based on their COVID-19 progression severity risk, and will generate a task notification to clinical teams to notify them of patients within that cohort output who also currently meet eligibility for receiving the COVID vaccine based on CDC guidelines (i.e. no recent history of already receiving vaccine, or recent COVID-19 infection).",Acquisition and/or Development,Both,3/1/2024,3/1/2024,Unknown,Developed with both contracting and in-house resources.,Contract Number: GS-35F-0086U Contract Order Number: 36C10B21F0353,Yes,No,Yes,Yes,Yes,"VA data input variables for this model were adapted from prior work done at VA Boston through Office of Research / IRB work, and included: Time since infection prior to vaccination Use of glucocorticoids Lymphocyte levels Presence of various comorbidities including:  -Alzheimer's Disease -Anemia -Atrial fibrillation -Chronic Kidney Disease -Chronic obstructive pulmonary disease -Epilepsy -Heart failure -Liver disease -Mobility impairments -Pressure and chronic ulcers -Acute myocardial infarction -Hypertension -Lung Cancer -Schizophrenia Tobacco use Body Mass Index categorization  -Extreme Obesity -Obesity I / Obesity II -Overweight -Unknown -Underweight Vaccine type (manufacturer) Age group","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Corporate Data Warehouse (VASI ID #1152),6-12 months,No,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5555555555555556,"The project seeks to implement a Severe COVID-19 Risk Prediction Tool to effectively identify Veterans at higher risk of developing severe COVID-19 symptoms through a machine-learning model to address one of the key barriers to increased vaccination offerings identified, i.e., the lack of tools for assessing risk of severe disease progression and potential vaccination benefit. It is expected by facilitating the estimated risk for severe COVID-19 progression, as well as a patient's current vaccine eligibility, that these patients can be outreached to and offered the vaccine in a more timely manner to improve vaccination rates and ultimately health outcomes. . The model will output a relative risk score for currently hospitalized patients based on their COVID-19 progression severity risk, and will generate a task notification to clinical teams to notify them of patients within that cohort output who also currently meet eligibility for receiving the COVID vaccine based on CDC guidelines (i.e. no recent history of already receiving vaccine, or recent COVID-19 infection).","the project seeks to implement a severe covid-19 risk prediction tool to effectively identify veterans at higher risk of developing severe covid-19 symptoms through a machine-learning model to address one of the key barriers to increased vaccination offerings identified, i.e., the lack of tools for assessing risk of severe disease progression and potential vaccination benefit. it is expected by facilitating the estimated risk for severe covid-19 progression, as well as a patient's current vaccine eligibility, that these patients can be outreached to and offered the vaccine in a more timely manner to improve vaccination rates and ultimately health outcomes. . the model will output a relative risk score for currently hospitalized patients based on their covid-19 progression severity risk, and will generate a task notification to clinical teams to notify them of patients within that cohort output who also currently meet eligibility for receiving the covid vaccine based on cdc guidelines (i.e. no recent history of already receiving vaccine, or recent covid-19 infection)."
CVI42,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf14/K141480.pdf).,Operation and Maintenance,Both,8/2/2024,8/2/2024,8/2/2024,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5555555555555556,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf14/K141480.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf14/k141480.pdf).
Discharge Predictive Model,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"Implementation of a predictive model that can identify acute inpatients likely to discharge within 24 hours can facilitate communication across clinical and ancillary staff teams who are otherwise currently reliant on a manual process that requires clinical teams to enter a Pre-Discharge order, a process which can be inconsistent due to trainee turnover. The expectation of this project is increased awareness and communication of the hospital's anticipated aggregate discharge volume resulting in potentially improved visibility on hospital throughput for bed coordination, including improved capability and flexibility to accommodate additional hospital admissions and transfers.",The model estimates the overall clinical readiness of inpatient patients using fundamental parameters including vital signs and lab result trends and then produces an output that aggregates the total anticipated number of patients across the facility that would be expected to be ready for discharge within 24 hours based on basic clinical criteria.,Acquisition and/or Development,Neither,6/21/2023,6/21/2023,Unknown,Developed with both contracting and in-house resources.,"GS-35F-0086U, 36C10B21F0353",Yes,No,Yes,Yes,Yes,"VHA Corporate Data Warehouse for Facility / Station 600 (VA Long Beach Healthcare System). Data types included in Training and Development: • Admission Date • Discharge Date • Laboratory Results (Basic and comprehensive metabolic panel, Complete Blood Count) • Vital Signs (Blood Pressure, Heart Rate, Pulse Oximetry, Temperature, Height, Weight, Pain rating) • Inpatient Specialty Service • Age • Race • Ethnicity • Gender","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,Corporate Data Warehouse (VASI ID #1152),6-12 months,No,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5396825396825397,"Implementation of a predictive model that can identify acute inpatients likely to discharge within 24 hours can facilitate communication across clinical and ancillary staff teams who are otherwise currently reliant on a manual process that requires clinical teams to enter a Pre-Discharge order, a process which can be inconsistent due to trainee turnover. The expectation of this project is increased awareness and communication of the hospital's anticipated aggregate discharge volume resulting in potentially improved visibility on hospital throughput for bed coordination, including improved capability and flexibility to accommodate additional hospital admissions and transfers. . The model estimates the overall clinical readiness of inpatient patients using fundamental parameters including vital signs and lab result trends and then produces an output that aggregates the total anticipated number of patients across the facility that would be expected to be ready for discharge within 24 hours based on basic clinical criteria.","implementation of a predictive model that can identify acute inpatients likely to discharge within 24 hours can facilitate communication across clinical and ancillary staff teams who are otherwise currently reliant on a manual process that requires clinical teams to enter a pre-discharge order, a process which can be inconsistent due to trainee turnover. the expectation of this project is increased awareness and communication of the hospital's anticipated aggregate discharge volume resulting in potentially improved visibility on hospital throughput for bed coordination, including improved capability and flexibility to accommodate additional hospital admissions and transfers. . the model estimates the overall clinical readiness of inpatient patients using fundamental parameters including vital signs and lab result trends and then produces an output that aggregates the total anticipated number of patients across the facility that would be expected to be ready for discharge within 24 hours based on basic clinical criteria."
HSRD IIR 19-069 Optimizing Renin Angiotensin System Blocker Use among Veterans with Kidney Disease,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"Intended purpose: clinical summarization of narrative clinical notes within the electronic health records among patients with CKD to give providers a summary of relevant terms (side effects, adverse events) noted in those notes (exact phrase matches, pointer to document) along with medication and chronic condition information (not using AI) delivered to primary care providers before a clinic encounter to assist in clinical decision making.","Narrative note that goes into the patient's record with the PCP as an additional signer with a summary of why they qualify for the clinical summarization (CKD), when the ACE/ARB was stopped, documentation of notes and phrases/sentences that are relevant to decision making (exact text from note).",Implementation and Assessment,Neither,6/1/2017,6/1/2018,10/1/2024,Developed in-house.,Unknown,Yes,No,Yes,Yes,No,VHA clinical notes during routine conduct of care.,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,"The primary application that supports this is CirrODS, maintained in VA-TRM, was originally developed for different patient population but has been adapted to this use case, and is a minor application under the VA Innovations Office ATO.",More than 12 months,No,Other,Other,Re-use production level code from a different use-case: This use case re-uses production level code and/or internally developed code libraries for model development from existing AI systems or repositories inside the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.6031746031746031,"Intended purpose: clinical summarization of narrative clinical notes within the electronic health records among patients with CKD to give providers a summary of relevant terms (side effects, adverse events) noted in those notes (exact phrase matches, pointer to document) along with medication and chronic condition information (not using AI) delivered to primary care providers before a clinic encounter to assist in clinical decision making. . Narrative note that goes into the patient's record with the PCP as an additional signer with a summary of why they qualify for the clinical summarization (CKD), when the ACE/ARB was stopped, documentation of notes and phrases/sentences that are relevant to decision making (exact text from note).","intended purpose: clinical summarization of narrative clinical notes within the electronic health records among patients with ckd to give providers a summary of relevant terms (side effects, adverse events) noted in those notes (exact phrase matches, pointer to document) along with medication and chronic condition information (not using ai) delivered to primary care providers before a clinic encounter to assist in clinical decision making. . narrative note that goes into the patient's record with the pcp as an additional signer with a summary of why they qualify for the clinical summarization (ckd), when the ace/arb was stopped, documentation of notes and phrases/sentences that are relevant to decision making (exact text from note)."
CT CoPilot,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf16/K161322.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf16/K161322.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf16/k161322.pdf).
"Cranial Navigation, Navigation Software Cranial, Navigation Software Craniofacial, Cranial EM System, Automatic Registration iMRI",Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K223288.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K223288.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/k223288.pdf).
Aquilion ONE (TSX-305A/6) V8.9 With AiCE,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,AI Deep Learning CT Reconstruction,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf18/K183046.pdf).,Implementation and Assessment,Both,5/12/2023,5/12/2023,5/12/2023,Unknown,644-B08013,Yes,No,Yes,No,Yes,"
CT scans
","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,No – agency does not have access to source code.,Yes,MD-LITE ID: 2278,More than 12 months,Other,Other,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.6031746031746031,AI Deep Learning CT Reconstruction . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf18/K183046.pdf).,ai deep learning ct reconstruction . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf18/k183046.pdf).
CellaVision DM1200 with the body fluid application,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf10/K102778.pdf).,Operation and Maintenance,Both,8/22/2023,8/22/2023,8/22/2023,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5555555555555556,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf10/K102778.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf10/k102778.pdf).
Nediser reports QA,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"Nediser is a continuously trained artificial intelligence “radiology resident” that assists radiologists in drafting their reports. Using computer vision/ML, Nediser selects normal templates (for example, the knee, shoulder joint), detects absence or presence of hardware in the image, evaluate patella alignment and leg length and angle discrepancy, measure Cobb angles, and evaluate hip exams for osteoarthritis. The output is directed to a QC field in the report that the Radiologist may use. The output helps standardize reports and improve accuracy of reporting.","The system classifies arthritis grade and performs measurements which are output to a QC field in dictation software as a draft that the Radiologist may or may not incorporate into their report templates. This is similar to a Radiology resident who provides draft reports that the Attending Radiologist reviews. The system can also detect objects and landmarks to assist in making measurements and identify abnormality. The output are landmark annotations and measurements. However, these outputs are not sent to the production PACS. They are stored in a secure directory which the Radiologists can refer to and do not become part of the medical record. All original production DICOM images are deleted after processing. Nediser saves de-identified JPG images and reports (no PHI) for audit and review of system performance, and to further train the model is necessary.",Operation and Maintenance,Both,12/1/2020,12/1/2020,1/1/2021,Developed in-house.,Unknown,No,No,No,No,Yes,"
Radiology images within VISN 21 are used for training and fine-tuning. The original data set for training the system comes from Stanford University through IRB-approved research.
","Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",Yes,"Yes – agency has access to source code, but it is not public.",No,Nediser,Less than 6 months,No,No,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Yes,"Nediser is a system that receives Radiology images from the PACS. The system uses AI/computer vision/CNNs to perform several tasks including: identify landmarks on the images to perform measurements of lengths and angles of the lower legs and spine, identifies the absence or presence of hardware, and classifies the grade of hip arthritis. Before the models were deployed in operation, their performance were evaluated with testing and validation, and performance have been published in peer reviewed scientific conferences and/or publications:   Ha, AY., Do, BH, Bartret, AL, Fang, CX, Hsiao, A., Lutz, AM, Banerjee, I., Riley, GM., Rubin, DL., Stevens, KJ, Wang, E., Wang, W., Beaulieu, CF., Hurt, B. Automating scoliosis measurements in radiographic studies with machine learning: Comparing artificial intelligence and clinical reports. Journal of Digital Imaging, Feb 2022. https://link.springer.com/article/10.1007/s10278-022-00595-x   Larson, N., Nguyen, C., Do, B., Kaul, A., Larson, A., Wang, S., Wang, E., Bultman, E., Stevens, K., Pai, J., Ha, A., Boutin, R., Fredericson, M., Do, L., Fang, C. Artificial Intelligence System for Automatic Quantitative Analysis and Radiology Reporting of Leg Length Radiographs. J Digit Imaging July 2022. https://doi.org/10.1007/s10278-022-00671-2.   Ha, A., Fang, C., Baig, S., Do, BH. Automatic Detection of Aortic Aneurysm on CT Exams Using Deep Convolutional Neural Networks. Scientific presentation. AMIA 2020 Annual Symposium, Virtual, November, 2020.   Aryan Kaul1, Jason Pai2, Charles Fang2, Ed Boas3, Kathryn Stevens2, Jason Saleh2, Vananh Nguyen2, Constance Chu, Jamie Schroder2, Michelle Nguyen2, Joshua Reicher2, Woon Teck Yap2, Amelie Lutz2, Bao H. Do. Automatic Diagnosis of Knee Patella Malalignment on X-ray Using Artificial Intelligence: Performance Comparison with Physicians. Scientific presentation. AMIA 2020 Annual Symposium, Virtual, November, 2020.  Audrey Ha, John Vorhies, Andrew Campion, Charles Fang, Michael Fadell II, Steve Dou, Safwan Halabi, David Larson, Emily Wang, YongJin Lee, Joanna Langner, Japsimran Kaur, Bao Do. Automatic Extraction of Skeletal Maturity from Whole Body Pediatric Scoliosis X-rays Using Regional Proposal and Compound Scaling Convolutional Neural Networks. Scientific presentation, Virtual. IEEE International Conference on Bioinformatics and Biomedicine (BIBM), December 2020.  The goal is to deploy this system VISN wide in VISN21. However, the past 3 years have been spent testing in a limited, ""shadow"" deployment. Limited = Palo Alto VA only. ""Shadow"" deployment = the system is never deployed in an autonomous workflow, requires human in the loop, and outputs of the system (annotations of measurements) are never sent back to the PACS or EHRM. Instead, the model's output is sent as text to a marge field/QC field in the Radiologist dictation software called Powerscribe. This does not go to VistA or CPRS or the patient medical record. The merge field is then used by Radiologists if they desire, imported into their draft report. They can review this output, validate visually, and agree or disagree. The program deletes original DICOM data. All annotations of the program, predictions are saved with final radiologist report for audit.  Because the AI system requires human in the loop, never sends output to the ERHM/VistA, and is an optional QC/merge field in the report draft, the technical risks are minimized. Clinically, the model performs orthopedic measurements (patella, leg lengths) and arthritis. The models are not involved in real-time decision support of clinical diseases such as stroke, lung cancer.   However, all systems have risk. We identify the key risk of the Nediser system is that there may be discrepancy between the system draft output and the final report if the Radiologist does not read the AI draft report and inadvertently saves the report.",TRUE,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,This AI performs computer vision to detect and classify hip arthritis based on the images.,"[""None of the above""]",No – it is not operationally practical to offer this.,Both,0.746031746031746,"Nediser is a continuously trained artificial intelligence “radiology resident” that assists radiologists in drafting their reports. Using computer vision/ML, Nediser selects normal templates (for example, the knee, shoulder joint), detects absence or presence of hardware in the image, evaluate patella alignment and leg length and angle discrepancy, measure Cobb angles, and evaluate hip exams for osteoarthritis. The output is directed to a QC field in the report that the Radiologist may use. The output helps standardize reports and improve accuracy of reporting. . The system classifies arthritis grade and performs measurements which are output to a QC field in dictation software as a draft that the Radiologist may or may not incorporate into their report templates. This is similar to a Radiology resident who provides draft reports that the Attending Radiologist reviews. The system can also detect objects and landmarks to assist in making measurements and identify abnormality. The output are landmark annotations and measurements. However, these outputs are not sent to the production PACS. They are stored in a secure directory which the Radiologists can refer to and do not become part of the medical record. All original production DICOM images are deleted after processing. Nediser saves de-identified JPG images and reports (no PHI) for audit and review of system performance, and to further train the model is necessary. . Nediser is a system that receives Radiology images from the PACS. The system uses AI/computer vision/CNNs to perform several tasks including: identify landmarks on the images to perform measurements of lengths and angles of the lower legs and spine, identifies the absence or presence of hardware, and classifies the grade of hip arthritis. Before the models were deployed in operation, their performance were evaluated with testing and validation, and performance have been published in peer reviewed scientific conferences and/or publications:   Ha, AY., Do, BH, Bartret, AL, Fang, CX, Hsiao, A., Lutz, AM, Banerjee, I., Riley, GM., Rubin, DL., Stevens, KJ, Wang, E., Wang, W., Beaulieu, CF., Hurt, B. Automating scoliosis measurements in radiographic studies with machine learning: Comparing artificial intelligence and clinical reports. Journal of Digital Imaging, Feb 2022. https://link.springer.com/article/10.1007/s10278-022-00595-x   Larson, N., Nguyen, C., Do, B., Kaul, A., Larson, A., Wang, S., Wang, E., Bultman, E., Stevens, K., Pai, J., Ha, A., Boutin, R., Fredericson, M., Do, L., Fang, C. Artificial Intelligence System for Automatic Quantitative Analysis and Radiology Reporting of Leg Length Radiographs. J Digit Imaging July 2022. https://doi.org/10.1007/s10278-022-00671-2.   Ha, A., Fang, C., Baig, S., Do, BH. Automatic Detection of Aortic Aneurysm on CT Exams Using Deep Convolutional Neural Networks. Scientific presentation. AMIA 2020 Annual Symposium, Virtual, November, 2020.   Aryan Kaul1, Jason Pai2, Charles Fang2, Ed Boas3, Kathryn Stevens2, Jason Saleh2, Vananh Nguyen2, Constance Chu, Jamie Schroder2, Michelle Nguyen2, Joshua Reicher2, Woon Teck Yap2, Amelie Lutz2, Bao H. Do. Automatic Diagnosis of Knee Patella Malalignment on X-ray Using Artificial Intelligence: Performance Comparison with Physicians. Scientific presentation. AMIA 2020 Annual Symposium, Virtual, November, 2020.  Audrey Ha, John Vorhies, Andrew Campion, Charles Fang, Michael Fadell II, Steve Dou, Safwan Halabi, David Larson, Emily Wang, YongJin Lee, Joanna Langner, Japsimran Kaur, Bao Do. Automatic Extraction of Skeletal Maturity from Whole Body Pediatric Scoliosis X-rays Using Regional Proposal and Compound Scaling Convolutional Neural Networks. Scientific presentation, Virtual. IEEE International Conference on Bioinformatics and Biomedicine (BIBM), December 2020.  The goal is to deploy this system VISN wide in VISN21. However, the past 3 years have been spent testing in a limited, ""shadow"" deployment. Limited = Palo Alto VA only. ""Shadow"" deployment = the system is never deployed in an autonomous workflow, requires human in the loop, and outputs of the system (annotations of measurements) are never sent back to the PACS or EHRM. Instead, the model's output is sent as text to a marge field/QC field in the Radiologist dictation software called Powerscribe. This does not go to VistA or CPRS or the patient medical record. The merge field is then used by Radiologists if they desire, imported into their draft report. They can review this output, validate visually, and agree or disagree. The program deletes original DICOM data. All annotations of the program, predictions are saved with final radiologist report for audit.  Because the AI system requires human in the loop, never sends output to the ERHM/VistA, and is an optional QC/merge field in the report draft, the technical risks are minimized. Clinically, the model performs orthopedic measurements (patella, leg lengths) and arthritis. The models are not involved in real-time decision support of clinical diseases such as stroke, lung cancer.   However, all systems have risk. We identify the key risk of the Nediser system is that there may be discrepancy between the system draft output and the final report if the Radiologist does not read the AI draft report and inadvertently saves the report.","nediser is a continuously trained artificial intelligence “radiology resident” that assists radiologists in drafting their reports. using computer vision/ml, nediser selects normal templates (for example, the knee, shoulder joint), detects absence or presence of hardware in the image, evaluate patella alignment and leg length and angle discrepancy, measure cobb angles, and evaluate hip exams for osteoarthritis. the output is directed to a qc field in the report that the radiologist may use. the output helps standardize reports and improve accuracy of reporting. . the system classifies arthritis grade and performs measurements which are output to a qc field in dictation software as a draft that the radiologist may or may not incorporate into their report templates. this is similar to a radiology resident who provides draft reports that the attending radiologist reviews. the system can also detect objects and landmarks to assist in making measurements and identify abnormality. the output are landmark annotations and measurements. however, these outputs are not sent to the production pacs. they are stored in a secure directory which the radiologists can refer to and do not become part of the medical record. all original production dicom images are deleted after processing. nediser saves de-identified jpg images and reports (no phi) for audit and review of system performance, and to further train the model is necessary. . nediser is a system that receives radiology images from the pacs. the system uses ai/computer vision/cnns to perform several tasks including: identify landmarks on the images to perform measurements of lengths and angles of the lower legs and spine, identifies the absence or presence of hardware, and classifies the grade of hip arthritis. before the models were deployed in operation, their performance were evaluated with testing and validation, and performance have been published in peer reviewed scientific conferences and/or publications: ha, ay., do, bh, bartret, al, fang, cx, hsiao, a., lutz, am, banerjee, i., riley, gm., rubin, dl., stevens, kj, wang, e., wang, w., beaulieu, cf., hurt, b. automating scoliosis measurements in radiographic studies with machine learning: comparing artificial intelligence and clinical reports. journal of digital imaging, feb 2022. https://link.springer.com/article/10.1007/s10278-022-00595-x larson, n., nguyen, c., do, b., kaul, a., larson, a., wang, s., wang, e., bultman, e., stevens, k., pai, j., ha, a., boutin, r., fredericson, m., do, l., fang, c. artificial intelligence system for automatic quantitative analysis and radiology reporting of leg length radiographs. j digit imaging july 2022. https://doi.org/10.1007/s10278-022-00671-2. ha, a., fang, c., baig, s., do, bh. automatic detection of aortic aneurysm on ct exams using deep convolutional neural networks. scientific presentation. amia 2020 annual symposium, virtual, november, 2020. aryan kaul1, jason pai2, charles fang2, ed boas3, kathryn stevens2, jason saleh2, vananh nguyen2, constance chu, jamie schroder2, michelle nguyen2, joshua reicher2, woon teck yap2, amelie lutz2, bao h. do. automatic diagnosis of knee patella malalignment on x-ray using artificial intelligence: performance comparison with physicians. scientific presentation. amia 2020 annual symposium, virtual, november, 2020. audrey ha, john vorhies, andrew campion, charles fang, michael fadell ii, steve dou, safwan halabi, david larson, emily wang, yongjin lee, joanna langner, japsimran kaur, bao do. automatic extraction of skeletal maturity from whole body pediatric scoliosis x-rays using regional proposal and compound scaling convolutional neural networks. scientific presentation, virtual. ieee international conference on bioinformatics and biomedicine (bibm), december 2020. the goal is to deploy this system visn wide in visn21. however, the past 3 years have been spent testing in a limited, ""shadow"" deployment. limited = palo alto va only. ""shadow"" deployment = the system is never deployed in an autonomous workflow, requires human in the loop, and outputs of the system (annotations of measurements) are never sent back to the pacs or ehrm. instead, the model's output is sent as text to a marge field/qc field in the radiologist dictation software called powerscribe. this does not go to vista or cprs or the patient medical record. the merge field is then used by radiologists if they desire, imported into their draft report. they can review this output, validate visually, and agree or disagree. the program deletes original dicom data. all annotations of the program, predictions are saved with final radiologist report for audit. because the ai system requires human in the loop, never sends output to the erhm/vista, and is an optional qc/merge field in the report draft, the technical risks are minimized. clinically, the model performs orthopedic measurements (patella, leg lengths) and arthritis. the models are not involved in real-time decision support of clinical diseases such as stroke, lung cancer. however, all systems have risk. we identify the key risk of the nediser system is that there may be discrepancy between the system draft output and the final report if the radiologist does not read the ai draft report and inadvertently saves the report."
"CARTO 3 EP Navigation System Software V8.0 (FG-5400-00, FG-5400-00U)",Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K231207.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K231207.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/k231207.pdf).
Care Assessment Needs (CAN) Score,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,CAN is a set of risk-stratifying logistic regression models run on Veterans receiving health care through VHA.,90-day hospitalization or 90-day death,Operation and Maintenance,Both,1/1/2011,6/1/2011,1/1/2012,Developed in-house.,Unknown,Yes,No,No,No,Yes,"Patient demographics, medical conditions, laboratory results, medications, health care utilization, social determinants of health, social history, vital signs.","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Yes,Drift. Monitoring routinely,TRUE,"Automated and Regularly Scheduled Updates: Some aspects of the plan to monitor the AI system post-deployment are automated including re-training of models after detecting drift; however, data science teams are still significantly involved in the monitoring and re-deployment process.",No - Some individual decisions or actions require direct human oversight.,Included in pool of candidate variables and model tested for bias,"[""Direct user testing""]",No – it is not operationally practical to offer this.,Both,0.7619047619047619,CAN is a set of risk-stratifying logistic regression models run on Veterans receiving health care through VHA. . 90-day hospitalization or 90-day death . Drift. Monitoring routinely,can is a set of risk-stratifying logistic regression models run on veterans receiving health care through vha. . 90-day hospitalization or 90-day death . drift. monitoring routinely
Cartesion Prime (PCD-1000A/3) V10.15,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K231748.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K231748.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/k231748.pdf).
VA CART Percutaneous Coronary Intervention SYNTAX Score ,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,The VA CART SYNTAX risk model predicts 30-day post-PCI mortality using a combination of clinical and anatomic variables. The model is available to facilitate personalized informed consent discussions and appropriate preparation for high-risk PCI cases.,"The calculated score indicates a risk of major adverse cardiovascular events (death, MI, stroke, and repeat revascularization) over time.",Operation and Maintenance,Both,1/1/2018,3/1/2018,7/2/2019,Developed in-house.,Unknown,Yes,No,No,Yes,Yes,"coronary anatomic data including, coronary dominance, lesion stenosis, lesion characteristics (ostial, occluded, thrombus, bifurcation, calcification).","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,"Clinical Assessment, Reporting and Tracking Application (VASI ID 1079, EMAS ID 1808) ",6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Yes,Risk: Drift.  The model is reassessed periodically.,TRUE,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Included in pool of candidate variables and model tested for bias,"[""Direct user testing""]",No – it is not operationally practical to offer this.,Both,0.7777777777777778,"The VA CART SYNTAX risk model predicts 30-day post-PCI mortality using a combination of clinical and anatomic variables. The model is available to facilitate personalized informed consent discussions and appropriate preparation for high-risk PCI cases. . The calculated score indicates a risk of major adverse cardiovascular events (death, MI, stroke, and repeat revascularization) over time. . Risk: Drift.  The model is reassessed periodically.","the va cart syntax risk model predicts 30-day post-pci mortality using a combination of clinical and anatomic variables. the model is available to facilitate personalized informed consent discussions and appropriate preparation for high-risk pci cases. . the calculated score indicates a risk of major adverse cardiovascular events (death, mi, stroke, and repeat revascularization) over time. . risk: drift. the model is reassessed periodically."
Cardiac CT Function Software Application,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf24/K241038.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf24/K241038.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf24/k241038.pdf).
Falls Prediction Tool,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Government Services (includes Benefits and Service Delivery),None of the above.,"Background: Pact Act Co-Pay exemption determination is currently a manual process, and some veterans may not know if they had toxic exposure and may be eligible for co-pay exemption.
Objective: Develop an algorithm for the PACT Act to predict if patients are eligible for co-pay exemption.
",Automatically generate a binary output that indicates whether a veteran is eligible for Pact Act Co-Pay exemption.,Acquisition and/or Development,Safety-impacting,5/15/2024,5/20/2024,Unknown,Developed in-house.,Unknown,Yes,No,Yes,No,Yes,Corporate Data Warehouse (CDW); HDAP Self Service Client Portal;,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Safety-Impacting,0.5238095238095238,"Background: Pact Act Co-Pay exemption determination is currently a manual process, and some veterans may not know if they had toxic exposure and may be eligible for co-pay exemption.
Objective: Develop an algorithm for the PACT Act to predict if patients are eligible for co-pay exemption. . Automatically generate a binary output that indicates whether a veteran is eligible for Pact Act Co-Pay exemption.","background: pact act co-pay exemption determination is currently a manual process, and some veterans may not know if they had toxic exposure and may be eligible for co-pay exemption. objective: develop an algorithm for the pact act to predict if patients are eligible for co-pay exemption. . automatically generate a binary output that indicates whether a veteran is eligible for pact act co-pay exemption."
VA CART Percutaneous Coronary Intervention Mortality Model ,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,The VA CART mortality risk model is based on logistic regression using baseline clinical and procedural variables to predict post-PCI 30-day mortality. The model is available to facilitate risk stratification at the point of care., Post-PCI 30-day mortality,Operation and Maintenance,Both,1/1/2020,3/1/2021,12/14/2021,Developed in-house.,Unknown,Yes,No,No,Yes,Yes,"Patient characteristics (demographics, comorbidities and prior procedures, procedural characteristics, labs, and medication history.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,"Clinical Assessment, Reporting and Tracking Application (VASI ID 1079, EMAS ID 1808) ",6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Yes,Risk: Drift.  The model is reassessed periodically.,TRUE,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Included in pool of candidate variables,"[""Direct user testing""]",No – it is not operationally practical to offer this.,Both,0.7777777777777778,The VA CART mortality risk model is based on logistic regression using baseline clinical and procedural variables to predict post-PCI 30-day mortality. The model is available to facilitate risk stratification at the point of care. . Post-PCI 30-day mortality . Risk: Drift.  The model is reassessed periodically.,the va cart mortality risk model is based on logistic regression using baseline clinical and procedural variables to predict post-pci 30-day mortality. the model is available to facilitate risk stratification at the point of care. . post-pci 30-day mortality . risk: drift. the model is reassessed periodically.
Pact Act Co-Pay Exemption Prediction Tool,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Government Services (includes Benefits and Service Delivery),None of the above.,"Background: Pact Act Co-Pay exemption determination is currently a manual process, and some veterans may not know if they had toxic exposure and may be eligible for co-pay exemption.
Objective: Develop an algorithm for the PACT Act to predict if patients are eligible for co-pay exemption.",Automatically generate a binary output that indicates whether a veteran is eligible for Pact Act Co-Pay exemption.,Acquisition and/or Development,"Rights-Impacting
",5/15/2024,5/20/2024,Unknown,Developed in-house.,Unknown,Yes,No,Yes,No,Yes,Corporate Data Warehouse (CDW); HDAP Self Service Client Portal;,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Rights-Impacting,0.5238095238095238,"Background: Pact Act Co-Pay exemption determination is currently a manual process, and some veterans may not know if they had toxic exposure and may be eligible for co-pay exemption.
Objective: Develop an algorithm for the PACT Act to predict if patients are eligible for co-pay exemption. . Automatically generate a binary output that indicates whether a veteran is eligible for Pact Act Co-Pay exemption.","background: pact act co-pay exemption determination is currently a manual process, and some veterans may not know if they had toxic exposure and may be eligible for co-pay exemption. objective: develop an algorithm for the pact act to predict if patients are eligible for co-pay exemption. . automatically generate a binary output that indicates whether a veteran is eligible for pact act co-pay exemption."
VA CART Percutaneous Coronary Intervention Nephropathy Model ,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,The VA CART nephropathy risk model is based on logistic regression using baseline clinical and procedural variables to predict. The model is available to facilitate risk stratification at the point of care.,Post-PCI acute kidney injury,Operation and Maintenance,Both,1/1/2020,6/1/2020,10/1/2022,Developed in-house.,Unknown,Yes,No,No,Yes,Yes,"Patient characteristics (demographics, comorbidities and prior procedures, procedural characteristics, labs, and medication history.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,"Clinical Assessment, Reporting and Tracking Application (VASI ID 1079, EMAS ID 1808) ",6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,,TRUE,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Included in pool of candidate variables,"[""Direct user testing""]",No – it is not operationally practical to offer this.,Both,0.7619047619047619,The VA CART nephropathy risk model is based on logistic regression using baseline clinical and procedural variables to predict. The model is available to facilitate risk stratification at the point of care. . Post-PCI acute kidney injury,the va cart nephropathy risk model is based on logistic regression using baseline clinical and procedural variables to predict. the model is available to facilitate risk stratification at the point of care. . post-pci acute kidney injury
"Brainlab Elements Image Fusion, Contouring (4.5);Image Fusion (4.5);Fibertracking (2.0);BOLD MRI Mapping (1.0);Image Fusion Angio (1.0)",Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K223106.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K223106.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/k223106.pdf).
VA CART Myocardial Ischemia NLP,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,Extracts myocardial ischemia information in VistA using Natural Language Processing (NLP) of Radiology report. ,"Results of the NLP contain 0 indicating negative stress test result, 1 indicating positive stress test result, and -1 indicating inconclusive stress test result.",Operation and Maintenance,Neither,12/1/2021,5/1/2022,10/30/2023,Developed in-house.,Unknown,Yes,No,No,Yes,Yes,VistA Radiology report impression text,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,"Clinical Assessment, Reporting and Tracking Application (VASI ID 1079, EMAS ID 1808) ",6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Extracts myocardial ischemia information in VistA using Natural Language Processing (NLP) of Radiology report. . Results of the NLP contain 0 indicating negative stress test result, 1 indicating positive stress test result, and -1 indicating inconclusive stress test result.","extracts myocardial ischemia information in vista using natural language processing (nlp) of radiology report. . results of the nlp contain 0 indicating negative stress test result, 1 indicating positive stress test result, and -1 indicating inconclusive stress test result."
VA CART Ejection Fraction NLP,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,Extracts ejection fraction (EF) information in VistA using Natural Language Processing (NLP) of Echocardiogram TIU report text,"Results of the NLP are separated by EF determination method including Human Observation, Biplane, 4-Chamber, and 2-Chamber, with the Best EF Value following that hierarchy in order.  A NULL Best EF Value indicates that the LLM did not find an ejection fraction in the text.",Acquisition and/or Development,Neither,12/1/2021,5/1/2022,Unknown,Developed in-house.,Unknown,Yes,No,No,Yes,Yes,VistA Echocardiogram TIU report text,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,"Clinical Assessment, Reporting and Tracking Application (VASI ID 1079, EMAS ID 1808) ",6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5079365079365079,"Extracts ejection fraction (EF) information in VistA using Natural Language Processing (NLP) of Echocardiogram TIU report text . Results of the NLP are separated by EF determination method including Human Observation, Biplane, 4-Chamber, and 2-Chamber, with the Best EF Value following that hierarchy in order.  A NULL Best EF Value indicates that the LLM did not find an ejection fraction in the text.","extracts ejection fraction (ef) information in vista using natural language processing (nlp) of echocardiogram tiu report text . results of the nlp are separated by ef determination method including human observation, biplane, 4-chamber, and 2-chamber, with the best ef value following that hierarchy in order. a null best ef value indicates that the llm did not find an ejection fraction in the text."
BioPlex 2200 ANA Screen with Medical Decision Support Software for Use with BioPlex 2200 Multi-Analyte Detection System,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf4/K043341.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf4/K043341.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf4/k043341.pdf).
VA CART Percutaneous Coronary Intervention Bleeding Risk Model,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,The VA CART bleeding risk model is based on logistic regression using baseline clinical and procedural variables to predict post-PCI in-hospital bleeding events. The model is available to facilitate risk stratification at the point of care.,Post Percutaneous Coronary Intervention bleed prior to hospital discharge,Operation and Maintenance,Both,1/1/2020,6/5/2020,7/1/2021,Developed in-house.,Unknown,Yes,No,No,Yes,Yes,"Patient characteristics (demographics, comorbidities and prior procedures, procedural characteristics, labs, medication history, and  Post-PCI bleed prior to hospital discharge.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",Yes,"Clinical Assessment, Reporting and Tracking Application (VASI ID 1079, EMAS ID 1808) ",6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.,No – Agency did not request an extension for this use case.,Yes,Risk: Drift.  The model is reassessed periodically.,TRUE,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,Included in pool of candidate variables,"[""Direct user testing""]",No – it is not operationally practical to offer this.,Both,0.7936507936507936,The VA CART bleeding risk model is based on logistic regression using baseline clinical and procedural variables to predict post-PCI in-hospital bleeding events. The model is available to facilitate risk stratification at the point of care. . Post Percutaneous Coronary Intervention bleed prior to hospital discharge . Risk: Drift.  The model is reassessed periodically.,the va cart bleeding risk model is based on logistic regression using baseline clinical and procedural variables to predict post-pci in-hospital bleeding events. the model is available to facilitate risk stratification at the point of care. . post percutaneous coronary intervention bleed prior to hospital discharge . risk: drift. the model is reassessed periodically.
"Biograph Vision, Biograph MCT Family Of PET/CTs",Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf19/K193248.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf19/K193248.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf19/k193248.pdf).
Rythm Express,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,Predict potassium levels from ECG.,Potassium levels from ECG evaluation.,Acquisition and/or Development,Neither,9/26/2024,9/26/2024,Unknown,Developed with contracting resources.,Unknown,Yes,No,Yes,No,No,Unknown,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,No,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.47619047619047616,Predict potassium levels from ECG. . Potassium levels from ECG evaluation.,predict potassium levels from ecg. . potassium levels from ecg evaluation.
AutoContour Model RADAC V3,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K230685.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K230685.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/k230685.pdf).
A computer vision framework (CVF) for the image classification in the Veteran Health Administration (VHA),Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,"Intended purpose: To develop a computer vision framework (CVF) to classify medical test images in the VHA.
Benefits: Disease related decision generation from the medical test image.","The CVF can provide a decision for the given image (e.g., the patient has a disease or not).",Acquisition and/or Development,Safety-impacting,7/15/2024,8/1/2024,Unknown,Developed in-house.,Unknown,Yes,No,No,No,No,Here is the link: https://data.mendeley.com/datasets/rscbjbr9sj/2,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,No,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Safety-Impacting,0.49206349206349204,"Intended purpose: To develop a computer vision framework (CVF) to classify medical test images in the VHA.
Benefits: Disease related decision generation from the medical test image. . The CVF can provide a decision for the given image (e.g., the patient has a disease or not).","intended purpose: to develop a computer vision framework (cvf) to classify medical test images in the vha. benefits: disease related decision generation from the medical test image. . the cvf can provide a decision for the given image (e.g., the patient has a disease or not)."
aPROMISE X,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K220590.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K220590.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/k220590.pdf).
"Aplio i900, Aplio i800 and Aplio i700 Software V8.1 Diagnostic Ultrasound System",Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K233195.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/K233195.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf23/k233195.pdf).
"Alignment System Cranial, Alignment Software Cranial, Cirq Alignment Software Cranial Biopsy, Cirq Alignment Software Cranial sEEG, Varioguide Alignment Software Cranial",Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K223864.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/K223864.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf22/k223864.pdf).
"Acumen Hypotension Prediction Index - EV1000 Clinical Platform, Acumen Hypotension Prediction Index - Hemosphere Advanced Monitoring Platform, Acumen Hypotension Prediction Index, Hemosphere Advanced Monitoring Platform - Pressure",Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Health & Medical,None of the above.,FDA-cleared medical device to assist clinicians and improve healthcare outcomes.,Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf18/K183646.pdf).,Operation and Maintenance,Both,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,Other,Unknown,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Yes – Agency requested an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.5079365079365079,FDA-cleared medical device to assist clinicians and improve healthcare outcomes. . Described in FDA documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf18/K183646.pdf).,fda-cleared medical device to assist clinicians and improve healthcare outcomes. . described in fda documentation for this device (https://www.accessdata.fda.gov/cdrh_docs/pdf18/k183646.pdf).
Xtract WDS - OSSO,Department of Veterans Affairs,VA,VHA: Veterans Health Administration,Law & Justice,Creating visual representations of data sets for reports and presentations using AI.,Heat map detection for weapon detection software.,Areas of interest for a heat map that include objects that would produce heat indicators aligned with heat signatures of known types of different weapons.,Acquisition and/or Development,Both,1/26/2023,1/26/2023,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,No,Unknown,"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",No,No – agency does not have access to source code.,No,Unknown,Less than 6 months,Other,Other,Other,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,Unknown,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Both,0.4603174603174603,Heat map detection for weapon detection software. . Areas of interest for a heat map that include objects that would produce heat indicators aligned with heat signatures of known types of different weapons.,heat map detection for weapon detection software. . areas of interest for a heat map that include objects that would produce heat indicators aligned with heat signatures of known types of different weapons.
Transcription Services,Department of Veterans Affairs ,VA,OIG,Mission-Enabling,Transcribing and summarizing a recorded meeting or interview using AI.,Increased efficiency,transcribed meetings and interviews,Operation and Maintenance,Neither,7/1/2024,7/1/2024,7/1/2024,Developed in-house.,Unknown,No,No,Yes,No,Unknown,Unknown,Unknown,Yes,Yes – source code is publicly available.,No,Unknown,Less than 6 months,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Planned or in-progress.,,Unknown,Unknown,No - Some individual decisions or actions require direct human oversight.,Unknown,Unknown,Unknown,Neither,0.5714285714285714,Increased efficiency . transcribed meetings and interviews,increased efficiency . transcribed meetings and interviews
Chatbot within ESMS,Department of Veterans Affairs ,VA,OIG,Mission-Enabling,Searching for information using AI.,Increased efficiency,"internal facing, chatbot to assist with policy questions",Acquisition and/or Development,Neither,11/25/2024,12/2/2024,Unknown,Developed with contracting resources.,Unknown,No,No,No,No,No,Current policies and SOPs,Unknown,No,No – agency does not have access to source code.,No,Ivanti,Unknown,Yes,No,Unknown,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Unknown,No – Agency did not request an extension for this use case.,Planned or in-progress.,,Unknown,Unknown,No - Some individual decisions or actions require direct human oversight.,Unknown,Unknown,Unknown,Neither,0.5238095238095238,"Increased efficiency . internal facing, chatbot to assist with policy questions","increased efficiency . internal facing, chatbot to assist with policy questions"
Extreme Load Analysis,Tennessee Valley Authority,TVA,Enteprise Planning,Energy & the Environment,Creating visual representations of data sets for reports and presentations using AI.,Extreme load analysis involves estimating the likelihood of peaks in energy demand for different months of the year and hours of the day. This will aid in energy forecasting,"An RShiny Web App displays EVD for different months of the year, hours of the day, and plants. The Web App allows users to filter for different time periods and different plants. Users can also adjust
percentiles to see different quantile values.",Operation and Maintenance,Neither,1/30/2024,1/30/2024,5/1/2024,Developed in-house.,Unknown,No,No,No,Unknown,Yes,"Weather Data: Going back to 1960.
ELRF Data: Going to back to 1980.","Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,Yes,Yes,R Shiny Web application,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"No documentation is available: No documentation detailing model performance metrics, model architecture, features and intended use of the models have been created or are currently accessible to other data science teams within the agency.",No – Agency did not request an extension for this use case.,Yes,"Model accuracy, drift",Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6984126984126984,"Extreme load analysis involves estimating the likelihood of peaks in energy demand for different months of the year and hours of the day. This will aid in energy forecasting . An RShiny Web App displays EVD for different months of the year, hours of the day, and plants. The Web App allows users to filter for different time periods and different plants. Users can also adjust
percentiles to see different quantile values. . Model accuracy, drift","extreme load analysis involves estimating the likelihood of peaks in energy demand for different months of the year and hours of the day. this will aid in energy forecasting . an rshiny web app displays evd for different months of the year, hours of the day, and plants. the web app allows users to filter for different time periods and different plants. users can also adjust percentiles to see different quantile values. . model accuracy, drift"
Materials Intelligent Catalog Assistant,Tennessee Valley Authority,TVA,Supply Chain   ,Energy & the Environment,Searching for information using AI.,Transform how inventory is viewed in catalog. Provide intelligent search capability to quickly find what user needs. Reduce duplication of inventory ordering due to inventory and components,"Intelligent Search engine with filters to search through inventory. Also provides information on item, location, cost, and identifies like or similar items",Operation and Maintenance,Neither,12/1/2019,12/1/2019,9/30/2020,Developed in-house.,Unknown,No,No,No,Unknown,Yes,Maximo data,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,Yes,Yes,MS Azure ,More than 12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,"Model accuracy, data quality",Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6984126984126984,"Transform how inventory is viewed in catalog. Provide intelligent search capability to quickly find what user needs. Reduce duplication of inventory ordering due to inventory and components . Intelligent Search engine with filters to search through inventory. Also provides information on item, location, cost, and identifies like or similar items . Model accuracy, data quality","transform how inventory is viewed in catalog. provide intelligent search capability to quickly find what user needs. reduce duplication of inventory ordering due to inventory and components . intelligent search engine with filters to search through inventory. also provides information on item, location, cost, and identifies like or similar items . model accuracy, data quality"
Vegetation Management,Tennessee Valley Authority,TVA,Right of Way,Energy & the Environment,Creating visual representations of data sets for reports and presentations using AI.,Monitoring of vegetation around  transmission line and tower and predictive analytics on when/where to trim. Using NV5 to generate the reports,Report on areas identified where vegetation needs to be trimmed,Operation and Maintenance,Neither,Unknown,Unknown,Unknown,Developed with contracting resources.,Unknown,No,No,No,Unknown,Yes,image data,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",No,No – agency does not have access to source code.,No,Unknown,Unknown,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,,Unknown,"Established Process of Machine Learning Operations: Alongside automated testing and drift detection, model re-training and re-deployments are supported by continuous integration pipelines that are managed by machine learning and data engineers on the platform, adapting work done by data science team into repeatable scripts for re-training and re-testing a model once deployed.",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6031746031746031,Monitoring of vegetation around  transmission line and tower and predictive analytics on when/where to trim. Using NV5 to generate the reports . Report on areas identified where vegetation needs to be trimmed,monitoring of vegetation around transmission line and tower and predictive analytics on when/where to trim. using nv5 to generate the reports . report on areas identified where vegetation needs to be trimmed
CAP Automation,Tennessee Valley Authority,TVA,Enterprise Analytics & Innovation,Energy & the Environment,None of the above.,"Use NLP to classify condition reports as C, E, or WO level","A classification label - C, E, or WO",Operation and Maintenance,Neither,5/19/2023,5/19/2023,4/24/2024,Developed in-house.,Unknown,No,No,No,Unknown,Yes,Condition reports,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,Yes,Yes,Python application,More than 12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,Model accuracy. ,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6984126984126984,"Use NLP to classify condition reports as C, E, or WO level . A classification label - C, E, or WO . Model accuracy.","use nlp to classify condition reports as c, e, or wo level . a classification label - c, e, or wo . model accuracy."
CR intelligent search,Tennessee Valley Authority,TVA,Enterprise Analytics & Innovation,Energy & the Environment,Searching for information using AI.,Use NLP to search through CRs. Also tool to include pattern recognition and adding additional safety data to search for and do topic modeling on,Search engine for looking through condition reports,Operation and Maintenance,Neither,6/15/2021,6/15/2021,3/31/2022,Developed in-house.,Unknown,No,No,No,Unknown,Yes,Condition reports,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,Yes,Yes,Python Dash,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6825396825396826,Use NLP to search through CRs. Also tool to include pattern recognition and adding additional safety data to search for and do topic modeling on . Search engine for looking through condition reports,use nlp to search through crs. also tool to include pattern recognition and adding additional safety data to search for and do topic modeling on . search engine for looking through condition reports
Materials tracking,Tennessee Valley Authority,TVA,Supply Chain   ,Energy & the Environment,Creating visual representations of data sets for reports and presentations using AI.,Use predictive analytics and confidence levels to track materials as they go through the supply chain process and risk on being delivered on time,"Dashboard for monitoring where in the supply chain process, item is and if it is at risk for being late",Operation and Maintenance,Neither,1/5/2020,1/5/2020,5/1/2021,Developed in-house.,Unknown,No,No,No,Unknown,Yes,Maximo data,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,Yes,Yes,"MS Azure, PowerBI",More than 12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6825396825396826,"Use predictive analytics and confidence levels to track materials as they go through the supply chain process and risk on being delivered on time . Dashboard for monitoring where in the supply chain process, item is and if it is at risk for being late","use predictive analytics and confidence levels to track materials as they go through the supply chain process and risk on being delivered on time . dashboard for monitoring where in the supply chain process, item is and if it is at risk for being late"
Heliviewer,Tennessee Valley Authority,TVA,Enterprise Analytics & Innovation,Energy & the Environment,Searching for information using AI.,Use of helicopter image data for survey and project insights,"Search engine of helicopter data to search through images, projects, flight patterns, etc.",Operation and Maintenance,Neither,1/22/2022,1/22/2022,7/20/2022,Developed in-house.,Unknown,No,No,No,Unknown,Yes,Helicopter images,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,Yes,Yes,MS Azure,More than 12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6825396825396826,"Use of helicopter image data for survey and project insights . Search engine of helicopter data to search through images, projects, flight patterns, etc.","use of helicopter image data for survey and project insights . search engine of helicopter data to search through images, projects, flight patterns, etc."
ICI camera ,Tennessee Valley Authority,TVA,Enterprise Analytics & Innovation,Energy & the Environment,None of the above.,Use computer vision to detect rate of change of water temperature with thermal image data which is indicative of dam leakage,Predictions on what temperature of water should be and risk flag if over threshold,Operation and Maintenance,Neither,11/5/2020,11/5/2020,8/15/2021,Developed in-house.,Unknown,No,No,No,Unknown,Yes,Thermal camera imagery,"Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.",Yes,Yes,Yes,R Shiny Web application,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6984126984126984,Use computer vision to detect rate of change of water temperature with thermal image data which is indicative of dam leakage . Predictions on what temperature of water should be and risk flag if over threshold,use computer vision to detect rate of change of water temperature with thermal image data which is indicative of dam leakage . predictions on what temperature of water should be and risk flag if over threshold
GET,Tennessee Valley Authority,TVA,Generation Projects & Fleet Strategy,Energy & the Environment,Creating visual representations of data sets for reports and presentations using AI.,"Groundwater evaluation Tool - The GET Tool is designed to provide statistical analysis of groundwater monitoring data required under State regulatory programs and the federal Coal Combustion Residuals (CCR) rule. It uses state-of-the-art statistical and predictive methods to evaluate geochemical and geotechnical data under three phases of groundwater monitoring: Detection, Assessment, and Corrective Action. In “Detection”, background data is used to construct prediction limits for use in identifying statistically significant changes from background levels. In “Assessment” and “Corrective Action”, compliance data is used to compute confidence intervals and confidence bands to assess violations of groundwater protection standards (GWPS) to inform of corrective action needs.",Dashboard and reports of statistical analysis of groundwater data,Operation and Maintenance,Neither,12/1/2021,12/1/2021,7/21/2022,Developed in-house.,Unknown,No,No,No,Unknown,Yes,Groundwater data from Equis,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,Yes,Yes,R Shiny Web application,More than 12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6825396825396826,"Groundwater evaluation Tool - The GET Tool is designed to provide statistical analysis of groundwater monitoring data required under State regulatory programs and the federal Coal Combustion Residuals (CCR) rule. It uses state-of-the-art statistical and predictive methods to evaluate geochemical and geotechnical data under three phases of groundwater monitoring: Detection, Assessment, and Corrective Action. In “Detection”, background data is used to construct prediction limits for use in identifying statistically significant changes from background levels. In “Assessment” and “Corrective Action”, compliance data is used to compute confidence intervals and confidence bands to assess violations of groundwater protection standards (GWPS) to inform of corrective action needs. . Dashboard and reports of statistical analysis of groundwater data","groundwater evaluation tool - the get tool is designed to provide statistical analysis of groundwater monitoring data required under state regulatory programs and the federal coal combustion residuals (ccr) rule. it uses state-of-the-art statistical and predictive methods to evaluate geochemical and geotechnical data under three phases of groundwater monitoring: detection, assessment, and corrective action. in “detection”, background data is used to construct prediction limits for use in identifying statistically significant changes from background levels. in “assessment” and “corrective action”, compliance data is used to compute confidence intervals and confidence bands to assess violations of groundwater protection standards (gwps) to inform of corrective action needs. . dashboard and reports of statistical analysis of groundwater data"
GMET,Tennessee Valley Authority,TVA,Generation Projects & Fleet Strategy,Energy & the Environment,Creating visual representations of data sets for reports and presentations using AI.,The Groundwater Modeling Evaluation Tool (GMET) is an R Shiny application that uses Phreeqc geochemical models and analysis to evaluate groundwater chemistry data at each of TVA’s CCR sites ,Geochemical reports on chemical makeup of groundwater data,Operation and Maintenance,Neither,7/23/2023,7/23/2023,2/6/2024,Developed in-house.,Unknown,No,No,No,Unknown,Yes,Groundwater data from Equis,"Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",Yes,Yes,Yes,R Shiny Web application,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6825396825396826,The Groundwater Modeling Evaluation Tool (GMET) is an R Shiny application that uses Phreeqc geochemical models and analysis to evaluate groundwater chemistry data at each of TVA’s CCR sites . Geochemical reports on chemical makeup of groundwater data,the groundwater modeling evaluation tool (gmet) is an r shiny application that uses phreeqc geochemical models and analysis to evaluate groundwater chemistry data at each of tva’s ccr sites . geochemical reports on chemical makeup of groundwater data
Optiwatt EV Load Model,Tennessee Valley Authority,TVA,Commercial Energy Solutions,Energy & the Environment,None of the above.,Linear model to predict hourly unitized charging load from non-fleet EVs in a typical meteorological year (TMY).,Hourly load shape profiles for Evs,Operation and Maintenance,Neither,1/22/2024,1/22/2024,2/23/2024,Developed in-house.,Unknown,No,No,No,Unknown,Yes,"Optiwatt charging data, ELRF weather, TMY Weather","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,Yes,Yes,R Shiny Web application,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6825396825396826,Linear model to predict hourly unitized charging load from non-fleet EVs in a typical meteorological year (TMY). . Hourly load shape profiles for Evs,linear model to predict hourly unitized charging load from non-fleet evs in a typical meteorological year (tmy). . hourly load shape profiles for evs
Cold-Weather Heat Pump Model,Tennessee Valley Authority,TVA,Commercial Energy Solutions,Energy & the Environment,None of the above.,KNN regression to predict hourly unitized impact of an upgraded CWHP in a residential home under historical and forecasted weather conditions.,Hourly load shape profile for heat pump,Operation and Maintenance,Neither,7/19/2023,7/26/2023,9/7/2023,Developed in-house.,Unknown,No,No,No,Unknown,Yes,"TVA TRM, ELRF, TMY Weather","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,Yes,Yes,R Shiny Web application,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6825396825396826,KNN regression to predict hourly unitized impact of an upgraded CWHP in a residential home under historical and forecasted weather conditions. . Hourly load shape profile for heat pump,knn regression to predict hourly unitized impact of an upgraded cwhp in a residential home under historical and forecasted weather conditions. . hourly load shape profile for heat pump
EE/DR Smart Thermostat Impact Model,Tennessee Valley Authority,TVA,Commercial Energy Solutions,Energy & the Environment,None of the above.,DR dispatch simulation to demonstrate hourly unitized impact of an EE/DR smart thermostat in a residential home under historical and forecasted weather & load conditions.,Hourly load shape profile for EEDR,Operation and Maintenance,Neither,6/7/2023,6/7/2023,9/29/2023,Developed in-house.,Unknown,No,No,No,Unknown,Yes,"TVA TRM, TVA Smart Thermostat Pilot data, ELRF, TMY Weather","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,Yes,Yes,R Shiny Web application,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6825396825396826,DR dispatch simulation to demonstrate hourly unitized impact of an EE/DR smart thermostat in a residential home under historical and forecasted weather & load conditions. . Hourly load shape profile for EEDR,dr dispatch simulation to demonstrate hourly unitized impact of an ee/dr smart thermostat in a residential home under historical and forecasted weather & load conditions. . hourly load shape profile for eedr
Standard EE/BE Measure Impact Models,Tennessee Valley Authority,TVA,Commercial Energy Solutions,Energy & the Environment,None of the above.,Set of ~200 linear models for other EE & BE measures  to predict hourly unitized impactunder historical and forecasted weather conditions. Based on a standardized set of calendar and weather features.,Hourly load shape profiles for EE/BE,Operation and Maintenance,Neither,8/1/2019,8/1/2019,10/1/2019,Developed in-house.,Unknown,No,No,No,Unknown,Yes,"TVA TRM, ELRF","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,Yes,Yes,R Shiny Web application,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6825396825396826,Set of ~200 linear models for other EE & BE measures  to predict hourly unitized impactunder historical and forecasted weather conditions. Based on a standardized set of calendar and weather features. . Hourly load shape profiles for EE/BE,set of ~200 linear models for other ee & be measures to predict hourly unitized impactunder historical and forecasted weather conditions. based on a standardized set of calendar and weather features. . hourly load shape profiles for ee/be
EnergyRightPSPImpactsForecasting,Tennessee Valley Authority,TVA,Commercial Energy Solutions,Energy & the Environment,None of the above.,"This app is (1) an ETL tool for ancillary data (2) a light-weight MLOps tool for the modeling of energy efficiency and electrification measure-level impact shapes, primarily as it relates to system planning assumptions and (3) a user interface to ES&P's forecast aggregation engine as a supply-side resource.",Generation of EEDR shapes and forecasts,Operation and Maintenance,Neither,8/1/2019,8/1/2019,12/1/2021,Developed in-house.,Unknown,No,No,No,Unknown,Yes,"EnergyRight and Advanced Analytics databases (components originally from Enterprise Planning system marginal value analysis, system load forecast, TVA EE/BE technical resource manual).","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,Yes,Yes,R Shiny Web application,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6825396825396826,"This app is (1) an ETL tool for ancillary data (2) a light-weight MLOps tool for the modeling of energy efficiency and electrification measure-level impact shapes, primarily as it relates to system planning assumptions and (3) a user interface to ES&P's forecast aggregation engine as a supply-side resource. . Generation of EEDR shapes and forecasts","this app is (1) an etl tool for ancillary data (2) a light-weight mlops tool for the modeling of energy efficiency and electrification measure-level impact shapes, primarily as it relates to system planning assumptions and (3) a user interface to es&p's forecast aggregation engine as a supply-side resource. . generation of eedr shapes and forecasts"
CESASolarModel,Tennessee Valley Authority,TVA,Commercial Energy Solutions,Energy & the Environment,None of the above.,"This app allows internal TVA users to generate sample bills for directly served and BCD-rate customers based on active wholesale rates, historical load, and expected shape/magnitude of new behind-the-meter solar arrays.",Sample bills based on input data,Operation and Maintenance,Neither,3/3/2021,5/19/2021,1/7/2022,Developed in-house.,Unknown,No,Yes,No,Unknown,Yes,"Power Billing (PBS), Electricity Sales Statistics (ESS), EIA Henry Hub gas prices and weather, Enterprise Planning solar shape library, PV Watts solar shape generator.","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,Yes,Yes,R Shiny Web application,More than 12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6825396825396826,"This app allows internal TVA users to generate sample bills for directly served and BCD-rate customers based on active wholesale rates, historical load, and expected shape/magnitude of new behind-the-meter solar arrays. . Sample bills based on input data","this app allows internal tva users to generate sample bills for directly served and bcd-rate customers based on active wholesale rates, historical load, and expected shape/magnitude of new behind-the-meter solar arrays. . sample bills based on input data"
CESAFCAAllocator,Tennessee Valley Authority,TVA,Commercial Energy Solutions,Energy & the Environment,None of the above.,"This app is used by one of two co-calculators for the 3-class allocation of TVA's total monthly fuel cost (TMFC) to large general service customers, large manufacturing customers, and customers smaller than 5 MW.",Monthly fuel cost,Operation and Maintenance,Neither,3/31/2021,3/31/2021,1/25/2022,Developed in-house.,Unknown,No,Yes,No,Unknown,Yes,Electricity Sales Statistics (ESS) via CESA data warehouse (CESADB),"Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,Yes,Yes,R Shiny Web application,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6825396825396826,"This app is used by one of two co-calculators for the 3-class allocation of TVA's total monthly fuel cost (TMFC) to large general service customers, large manufacturing customers, and customers smaller than 5 MW. . Monthly fuel cost","this app is used by one of two co-calculators for the 3-class allocation of tva's total monthly fuel cost (tmfc) to large general service customers, large manufacturing customers, and customers smaller than 5 mw. . monthly fuel cost"
EnergyRightDERPortfolioEvaluation,Tennessee Valley Authority,TVA,Commercial Energy Solutions,Energy & the Environment,None of the above.,This app is primarily a visualization and export tool for the outputs of ES&P's system planning processes through which Enterprise Planning can obtain aggregations of ES&P forecasts and ES&P can gain insight into evaluation methodologies. It also contains a connection to the ES&P forecast aggregation engine for reporting purposes and stochastic simulation tools.,Report of ES&Ps aggregated forecasts,Operation and Maintenance,Neither,1/1/2019,1/1/2019,11/1/2023,Developed in-house.,Unknown,No,Yes,No,Unknown,Yes,"EnergyRight and Advanced Analytics databases (components originally from Enterprise Planning system marginal value analysis, system load forecast, TVA EE/BE technical resource manual).","Documentation is missing or not available: No documentation exists regarding maintenance, composition, quality, or intended use of the training and evaluation data.",Yes,Yes,Yes,R Shiny Web application,6-12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6825396825396826,This app is primarily a visualization and export tool for the outputs of ES&P's system planning processes through which Enterprise Planning can obtain aggregations of ES&P forecasts and ES&P can gain insight into evaluation methodologies. It also contains a connection to the ES&P forecast aggregation engine for reporting purposes and stochastic simulation tools. . Report of ES&Ps aggregated forecasts,this app is primarily a visualization and export tool for the outputs of es&p's system planning processes through which enterprise planning can obtain aggregations of es&p forecasts and es&p can gain insight into evaluation methodologies. it also contains a connection to the es&p forecast aggregation engine for reporting purposes and stochastic simulation tools. . report of es&ps aggregated forecasts
Prism Models for MDM,Tennessee Valley Authority,TVA,Monitoring Diagnostics Management,Energy & the Environment,None of the above.,Use of clustering and nearest neighbor models to analyze analog signal data looking for variations in signals. Goal is to detect signal anomalies and provide alarms on deviations.,OMR - measure of how far current vector is from normal . Output includes an automated generated emails for human in the loop validation,Operation and Maintenance,Neither,5/1/2013,5/1/2013,10/1/2013,Developed with both contracting and in-house resources.,Unknown,No,Unknown,Unknown,Unknown,Yes,Unknown,Unknown,Unknown,Unknown,Yes,Prism,More than 12 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.,No – Agency did not request an extension for this use case.,Yes,,Unknown,"Established Process of Machine Learning Operations: Alongside automated testing and drift detection, model re-training and re-deployments are supported by continuous integration pipelines that are managed by machine learning and data engineers on the platform, adapting work done by data science team into repeatable scripts for re-training and re-testing a model once deployed.",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.5873015873015873,Use of clustering and nearest neighbor models to analyze analog signal data looking for variations in signals. Goal is to detect signal anomalies and provide alarms on deviations. . OMR - measure of how far current vector is from normal . Output includes an automated generated emails for human in the loop validation,use of clustering and nearest neighbor models to analyze analog signal data looking for variations in signals. goal is to detect signal anomalies and provide alarms on deviations. . omr - measure of how far current vector is from normal . output includes an automated generated emails for human in the loop validation
Load Forecasting,Tennessee Valley Authority,TVA,Enterprise Planning,Energy & the Environment,None of the above.,Predict energy and demand out to 30 years in order to support power supply planning,30 year capacity and energy forecasts,Operation and Maintenance,Neither,9/1/2010,10/13/2010,5/12/2011,Developed with both contracting and in-house resources.,Unknown,No,Unknown,No,Unknown,Yes,"Economic forecasts, EEDR forecasts, EV forecasts, commodity forecasts","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,Yes,Itron system,Unknown,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Yes,Data quality,Unknown,"Established Process of Machine Learning Operations: Alongside automated testing and drift detection, model re-training and re-deployments are supported by continuous integration pipelines that are managed by machine learning and data engineers on the platform, adapting work done by data science team into repeatable scripts for re-training and re-testing a model once deployed.",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6666666666666666,Predict energy and demand out to 30 years in order to support power supply planning . 30 year capacity and energy forecasts . Data quality,predict energy and demand out to 30 years in order to support power supply planning . 30 year capacity and energy forecasts . data quality
ArcGIS,Tennessee Valley Authority,TVA,GIS,Energy & the Environment,Creating visual representations of data sets for reports and presentations using AI.,"Platform for mapping, analyzing, managing, and viewing geospatial data. It is designed to support and integrate with a variety of GIS services, providing a robust solution for organizations to leverage spatial information.","Maps, spatial analysis, image processing and image analysis",Operation and Maintenance,Neither,4/17/2023,6/10/2023,7/27/2023,Developed with both contracting and in-house resources.,Unknown,No,Unknown,No,Unknown,Yes,"GIS data, ESRI, image data, Maximo data","Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups. ",No,No – agency does not have access to source code.,Yes,ArcGIS ESRI,Unknown,Yes,Yes,Yes,None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.,Unknown,No – Agency did not request an extension for this use case.,Yes,Data quality,Unknown,"Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model. ",No - Some individual decisions or actions require direct human oversight.,N/A - no demographic information used,N/A - no demographic information used,Yes,Neither,0.6507936507936508,"Platform for mapping, analyzing, managing, and viewing geospatial data. It is designed to support and integrate with a variety of GIS services, providing a robust solution for organizations to leverage spatial information. . Maps, spatial analysis, image processing and image analysis . Data quality","platform for mapping, analyzing, managing, and viewing geospatial data. it is designed to support and integrate with a variety of gis services, providing a robust solution for organizations to leverage spatial information. . maps, spatial analysis, image processing and image analysis . data quality"
Advanced Network Anomaly Detection,Tennessee Valley Authority,TVA,Network,Mission-Enabling,None of the above.,Threat hunting and Security Operations Center (SOC) analysts leverage automated tooling to refine alerts that enable discovery and detection of anomalous behavior across the network. These tools use a massive amount of data and provide intelligence beyond what could be seen from a single vector. Correlated network traffic data with proxy communications help identify compromised systems and networks.,"Metrics, Alerting, and optional preventative/proactive protections",Operation and Maintenance,Neither,9/23/2021,9/23/2021,9/23/2021,Unknown,PO 7030410,No,No,No,Yes,Unknown,Unknown,Unknown,No,No – agency does not have access to source code.,Yes,Corporate,Unknown,Yes,No,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"Threat hunting and Security Operations Center (SOC) analysts leverage automated tooling to refine alerts that enable discovery and detection of anomalous behavior across the network. These tools use a massive amount of data and provide intelligence beyond what could be seen from a single vector. Correlated network traffic data with proxy communications help identify compromised systems and networks. . Metrics, Alerting, and optional preventative/proactive protections","threat hunting and security operations center (soc) analysts leverage automated tooling to refine alerts that enable discovery and detection of anomalous behavior across the network. these tools use a massive amount of data and provide intelligence beyond what could be seen from a single vector. correlated network traffic data with proxy communications help identify compromised systems and networks. . metrics, alerting, and optional preventative/proactive protections"
Security Information and Event Management (SIEM) Alerting Models,Tennessee Valley Authority,TVA,Cybersecurity,Mission-Enabling,None of the above.,"Leveraged for comprehensive context interpretation, rapid event detection, and greater productivity with human-assisted automation to SecOps, ITOps. Enables faster alerting, review, and response to terabytes of log data. These automated tools support further refining of alerts based on aggregated data and analyst expertise.","Metrics, Alerting, and optional preventative/proactive protections",Operation and Maintenance,Neither,9/9/2011,9/9/2011,1/3/2019,Unknown,PO 309131,No,No,No,Yes,Unknown,Unknown,Unknown,No,No – agency does not have access to source code.,Yes,Corporate,Unknown,Yes,No,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"Leveraged for comprehensive context interpretation, rapid event detection, and greater productivity with human-assisted automation to SecOps, ITOps. Enables faster alerting, review, and response to terabytes of log data. These automated tools support further refining of alerts based on aggregated data and analyst expertise. . Metrics, Alerting, and optional preventative/proactive protections","leveraged for comprehensive context interpretation, rapid event detection, and greater productivity with human-assisted automation to secops, itops. enables faster alerting, review, and response to terabytes of log data. these automated tools support further refining of alerts based on aggregated data and analyst expertise. . metrics, alerting, and optional preventative/proactive protections"
Vulnerability Tracking,Tennessee Valley Authority,TVA,Cybersecurity,Mission-Enabling,None of the above.,"Vulnerability analysts track 1000s of vulnerabilities in real time across and enterprise of technologies. Prioritizing severity is a challenging task when there can be 100s of the same level of severity CVE. The tools in use help prioritize data based on contextual information within the environment and industry while also feeding other factors such as exploitability,","Metrics, Alerting, and optional preventative/proactive protections",Initiated,Neither,2/22/2024,2/22/2024,2/22/2024,Unknown,PO 7596319,No,No,No,Yes,Unknown,Unknown,Unknown,No,No – agency does not have access to source code.,Yes,Corporate,Unknown,Yes,No,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"Vulnerability analysts track 1000s of vulnerabilities in real time across and enterprise of technologies. Prioritizing severity is a challenging task when there can be 100s of the same level of severity CVE. The tools in use help prioritize data based on contextual information within the environment and industry while also feeding other factors such as exploitability, . Metrics, Alerting, and optional preventative/proactive protections","vulnerability analysts track 1000s of vulnerabilities in real time across and enterprise of technologies. prioritizing severity is a challenging task when there can be 100s of the same level of severity cve. the tools in use help prioritize data based on contextual information within the environment and industry while also feeding other factors such as exploitability, . metrics, alerting, and optional preventative/proactive protections"
Advanced Threat Protection,Tennessee Valley Authority,TVA,Cybersecurity,Mission-Enabling,None of the above.,"Protecting against attackers and exploitations means testing using the same strategies that adversaries use. Leveraging billions of data points across operating systems, mail flow, and identity patterns, mathematical and probabilistic based decisions can be made to mitigate and even completed remove threats to the organization. ML and AI technology help reduce phishing emails, malware spread, leveraging compromised accounts, and further promote visibility to threats.","Metrics, Alerting, and optional preventative/proactive protections",Operation and Maintenance,Neither,9/30/2016,9/30/2016,9/30/2016,Unknown,Contract 11861,No,No,No,Yes,Unknown,Unknown,Unknown,No,No – agency does not have access to source code.,Yes,Corporate,Unknown,Yes,No,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"Protecting against attackers and exploitations means testing using the same strategies that adversaries use. Leveraging billions of data points across operating systems, mail flow, and identity patterns, mathematical and probabilistic based decisions can be made to mitigate and even completed remove threats to the organization. ML and AI technology help reduce phishing emails, malware spread, leveraging compromised accounts, and further promote visibility to threats. . Metrics, Alerting, and optional preventative/proactive protections","protecting against attackers and exploitations means testing using the same strategies that adversaries use. leveraging billions of data points across operating systems, mail flow, and identity patterns, mathematical and probabilistic based decisions can be made to mitigate and even completed remove threats to the organization. ml and ai technology help reduce phishing emails, malware spread, leveraging compromised accounts, and further promote visibility to threats. . metrics, alerting, and optional preventative/proactive protections"
AI -Driven Adaptive Protection,Tennessee Valley Authority,TVA,Cybersecurity,Mission-Enabling,None of the above.,"Enables the real-time exchange of machine-readable cyber threat indicators and defensive measures to help protect against and ultimately reduce the prevalence of cyber incidents. This is useful in helping protect networks against human-operated ransomware, where a threat actor can quickly adjust and maneuver inside the network. Leveraging predictive analytics, the technology can actively engage blocking measures to stop further exploitation.","Metrics, Alerting, and optional preventative/proactive protections",Operation and Maintenance,Neither,9/30/2016,9/30/2016,9/30/2016,Unknown,Contract 11861,No,No,No,Yes,Unknown,Unknown,Unknown,No,No – agency does not have access to source code.,Yes,Corporate,Unknown,Yes,No,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"Enables the real-time exchange of machine-readable cyber threat indicators and defensive measures to help protect against and ultimately reduce the prevalence of cyber incidents. This is useful in helping protect networks against human-operated ransomware, where a threat actor can quickly adjust and maneuver inside the network. Leveraging predictive analytics, the technology can actively engage blocking measures to stop further exploitation. . Metrics, Alerting, and optional preventative/proactive protections","enables the real-time exchange of machine-readable cyber threat indicators and defensive measures to help protect against and ultimately reduce the prevalence of cyber incidents. this is useful in helping protect networks against human-operated ransomware, where a threat actor can quickly adjust and maneuver inside the network. leveraging predictive analytics, the technology can actively engage blocking measures to stop further exploitation. . metrics, alerting, and optional preventative/proactive protections"
Attack Surface Management,Tennessee Valley Authority,TVA,Cybersecurity,Mission-Enabling,None of the above.,"Proprietary machine learning models help discover the systems and applications belonging to TVA, across the entire internet. This enables TVA to inventory and take action to ensure protections are in place for external services and applications. These ML tools continuously monitor the attack surface and help prioritize remediation actions for external TVA assets. ","Metrics, Alerting, and optional preventative/proactive protections",Operation and Maintenance,Neither,Unknown,Unknown,Unknown,Unknown,TBD,No,No,No,Yes,Unknown,Unknown,Unknown,No,No – agency does not have access to source code.,Yes,Corporate,Unknown,Yes,No,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.3968253968253968,"Proprietary machine learning models help discover the systems and applications belonging to TVA, across the entire internet. This enables TVA to inventory and take action to ensure protections are in place for external services and applications. These ML tools continuously monitor the attack surface and help prioritize remediation actions for external TVA assets. . Metrics, Alerting, and optional preventative/proactive protections","proprietary machine learning models help discover the systems and applications belonging to tva, across the entire internet. this enables tva to inventory and take action to ensure protections are in place for external services and applications. these ml tools continuously monitor the attack surface and help prioritize remediation actions for external tva assets. . metrics, alerting, and optional preventative/proactive protections"
Extended Threat Detection,Tennessee Valley Authority,TVA,Cybersecurity,Mission-Enabling,None of the above.,"Detecting threats within any environment requires large amounts of data, baselines of normalcy, and ability to detect anomalous behaviors within that baseline. Automated detection capabilities leveraging deep packet inspection and real-time analysis to identify the patterns that deviate from what's considered normal enables threat detection and response. Use of unsupervised machine learning models, that learn from previously seen behaviors and are able to determine when something becomes malicious or when an event is considered benign is paramount. New activities are closely monitored in order to most accurately attribute to benign or malicious. Overall, this AI based technology vastly improves visibility and accuracy in threat detection. ","Metrics, Alerting, and optional preventative/proactive protections",Operation and Maintenance,Neither,1/10/2018,1/10/2018,1/30/2019,Unknown,PO 3496587,No,No,No,Yes,Unknown,Unknown,Unknown,No,No – agency does not have access to source code.,Yes,Corporate,Unknown,Yes,No,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"Detecting threats within any environment requires large amounts of data, baselines of normalcy, and ability to detect anomalous behaviors within that baseline. Automated detection capabilities leveraging deep packet inspection and real-time analysis to identify the patterns that deviate from what's considered normal enables threat detection and response. Use of unsupervised machine learning models, that learn from previously seen behaviors and are able to determine when something becomes malicious or when an event is considered benign is paramount. New activities are closely monitored in order to most accurately attribute to benign or malicious. Overall, this AI based technology vastly improves visibility and accuracy in threat detection. . Metrics, Alerting, and optional preventative/proactive protections","detecting threats within any environment requires large amounts of data, baselines of normalcy, and ability to detect anomalous behaviors within that baseline. automated detection capabilities leveraging deep packet inspection and real-time analysis to identify the patterns that deviate from what's considered normal enables threat detection and response. use of unsupervised machine learning models, that learn from previously seen behaviors and are able to determine when something becomes malicious or when an event is considered benign is paramount. new activities are closely monitored in order to most accurately attribute to benign or malicious. overall, this ai based technology vastly improves visibility and accuracy in threat detection. . metrics, alerting, and optional preventative/proactive protections"
Augmented Email Protection,Tennessee Valley Authority,TVA,Cybersecurity,Mission-Enabling,None of the above.,Email protection service that analyzes activity of messages within email services to determine if messages and associated content embedded within is malicious based on signals that include Al-generated signals regarding normative human behavior and known malicious content. ,"Metrics, Alerting, and optional preventative/proactive protections",Operation and Maintenance,Neither,4/23/2024,4/23/2024,4/23/2024,Unknown,PO 7621572,No,No,No,Yes,Unknown,Unknown,Unknown,No,No – agency does not have access to source code.,Yes,Corporate,Unknown,Yes,No,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"Email protection service that analyzes activity of messages within email services to determine if messages and associated content embedded within is malicious based on signals that include Al-generated signals regarding normative human behavior and known malicious content. . Metrics, Alerting, and optional preventative/proactive protections","email protection service that analyzes activity of messages within email services to determine if messages and associated content embedded within is malicious based on signals that include al-generated signals regarding normative human behavior and known malicious content. . metrics, alerting, and optional preventative/proactive protections"
Natural Reader,Tennessee Valley Authority,TVA,Workforce Development,Mission-Enabling,None of the above.,"Text-to-speech desktop software for individual use. This easy-to-use software with natural-sounding voices can read any text such as Microsoft Word files, webpages, PDF files, and E-mails.
",Speech-to-Text,Operation and Maintenance,Neither,7/28/2023,7/28/2023,7/28/2023,Unknown,PO 7502095,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Corporate,Unknown,Yes,No,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.31746031746031744,"Text-to-speech desktop software for individual use. This easy-to-use software with natural-sounding voices can read any text such as Microsoft Word files, webpages, PDF files, and E-mails. . Speech-to-Text","text-to-speech desktop software for individual use. this easy-to-use software with natural-sounding voices can read any text such as microsoft word files, webpages, pdf files, and e-mails. . speech-to-text"
River Forecast,Tennessee Valley Authority,TVA,Riverfleet Management Services,Mission-Enabling,None of the above.,A multi-scale graph neural network-based autoregressive model. It is trained on historical weather data from ECMWF's ERA5 reanalysis archive. The tool generates predictions at 6-hour time steps for a set of surface and atmospheric variables.,Atomospheric variable predictions,Initiated,Neither,6/24/2024,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Corporate,Unknown,Yes,No,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.2698412698412698,A multi-scale graph neural network-based autoregressive model. It is trained on historical weather data from ECMWF's ERA5 reanalysis archive. The tool generates predictions at 6-hour time steps for a set of surface and atmospheric variables. . Atomospheric variable predictions,a multi-scale graph neural network-based autoregressive model. it is trained on historical weather data from ecmwf's era5 reanalysis archive. the tool generates predictions at 6-hour time steps for a set of surface and atmospheric variables. . atomospheric variable predictions
Security Operations Enhancement,Tennessee Valley Authority,TVA,Cybersecurity,Mission-Enabling,None of the above.,"Empowering SOC analysts with AI-enriched threat intelligence to understand landscapes, security posturing, and live attacks. Promoting the prioritization of event handling and investigative actions to identify and stop attacks by optimizing decision-making.","Metrics, Alerting, and optional preventative/proactive protections",Operation and Maintenance,Neither,9/29/2021,9/29/2021,9/29/2021,Unknown,PO 7040159,No,No,No,Yes,Unknown,Unknown,Unknown,No,No – agency does not have access to source code.,Yes,Corporate,Unknown,Yes,No,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"Empowering SOC analysts with AI-enriched threat intelligence to understand landscapes, security posturing, and live attacks. Promoting the prioritization of event handling and investigative actions to identify and stop attacks by optimizing decision-making. . Metrics, Alerting, and optional preventative/proactive protections","empowering soc analysts with ai-enriched threat intelligence to understand landscapes, security posturing, and live attacks. promoting the prioritization of event handling and investigative actions to identify and stop attacks by optimizing decision-making. . metrics, alerting, and optional preventative/proactive protections"
Advanced Threat Protection,Tennessee Valley Authority,TVA,Cybersecurity,Mission-Enabling,None of the above.,"Protecting against attackers and exploitations means testing using the same strategies that adversaries use. Leveraging billions of data points across operating systems, mail flow, and identity patterns, mathematical and probabilistic based decisions can be made to mitigate and even completed remove threats to the organization. ML and AI technology help reduce phishing emails, malware spread, leveraging compromised accounts, and further promote visibility to threats.","Metrics, Alerting, and optional preventative/proactive protections",Operation and Maintenance,Neither,3/31/2015,3/31/2015,3/31/2015,Unknown,PO 816550,No,No,No,Yes,Unknown,Unknown,Unknown,No,No – agency does not have access to source code.,Yes,Corporate,Unknown,Yes,No,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"Protecting against attackers and exploitations means testing using the same strategies that adversaries use. Leveraging billions of data points across operating systems, mail flow, and identity patterns, mathematical and probabilistic based decisions can be made to mitigate and even completed remove threats to the organization. ML and AI technology help reduce phishing emails, malware spread, leveraging compromised accounts, and further promote visibility to threats. . Metrics, Alerting, and optional preventative/proactive protections","protecting against attackers and exploitations means testing using the same strategies that adversaries use. leveraging billions of data points across operating systems, mail flow, and identity patterns, mathematical and probabilistic based decisions can be made to mitigate and even completed remove threats to the organization. ml and ai technology help reduce phishing emails, malware spread, leveraging compromised accounts, and further promote visibility to threats. . metrics, alerting, and optional preventative/proactive protections"
Perimeter Defense,Tennessee Valley Authority,TVA,Network,Mission-Enabling,None of the above.,"Using a combination of ruleset definitions specifically designed to detect malicious activity and augmented by a proprietary machine learning algorithm, this tool protects web applications regardless of location. Models trained based on millions of sampled network traffic enables this tool to proactively detect and prevent malicious network activity from reaching and affecting web applications.","Metrics, Alerting, and optional preventative/proactive protections",Acquisition and/or Development,Neither,9/3/2021,9/3/2021,9/3/2021,Unknown,PO 6997377,No,No,No,Yes,Other,Unknown,Unknown,No,No – agency does not have access to source code.,Yes,Corporate,Unknown,Yes,No,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"Using a combination of ruleset definitions specifically designed to detect malicious activity and augmented by a proprietary machine learning algorithm, this tool protects web applications regardless of location. Models trained based on millions of sampled network traffic enables this tool to proactively detect and prevent malicious network activity from reaching and affecting web applications. . Metrics, Alerting, and optional preventative/proactive protections","using a combination of ruleset definitions specifically designed to detect malicious activity and augmented by a proprietary machine learning algorithm, this tool protects web applications regardless of location. models trained based on millions of sampled network traffic enables this tool to proactively detect and prevent malicious network activity from reaching and affecting web applications. . metrics, alerting, and optional preventative/proactive protections"
Intrusion Detection Analysis,Tennessee Valley Authority,TVA,Cybersecurity,Mission-Enabling,None of the above.,Threat hunting and Security Operations Center (SOC) analysts leverage automated tooling to refine alerts that enable discovery and detection of anomalous behavior across the network. These tools use a massive amount of data and provide intelligence beyond what could be seen from a single vector. Correlated network traffic data with proxy communications help identify compromised systems and networks.,"Metrics, Alerting, and optional preventative/proactive protections",Operation and Maintenance,Neither,9/11/2018,9/11/2018,9/11/2018,Unknown,PO 4314465,No,No,No,Yes,Unknown,Unknown,Unknown,No,No – agency does not have access to source code.,Yes,Corporate,Unknown,Yes,No,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"Threat hunting and Security Operations Center (SOC) analysts leverage automated tooling to refine alerts that enable discovery and detection of anomalous behavior across the network. These tools use a massive amount of data and provide intelligence beyond what could be seen from a single vector. Correlated network traffic data with proxy communications help identify compromised systems and networks. . Metrics, Alerting, and optional preventative/proactive protections","threat hunting and security operations center (soc) analysts leverage automated tooling to refine alerts that enable discovery and detection of anomalous behavior across the network. these tools use a massive amount of data and provide intelligence beyond what could be seen from a single vector. correlated network traffic data with proxy communications help identify compromised systems and networks. . metrics, alerting, and optional preventative/proactive protections"
Malware Reverse Engineering,Tennessee Valley Authority,TVA,Cybersecurity,Mission-Enabling,None of the above.,"Reverse engineering of malware, and software analysis more broadly, will continue to be a critical activity in support of TVA's cyber protections efforts. Analysis of files, URLs, Hashes, etc., to determine if contents are malicious using a combination of different anti-virus scanners and blocking services to advance efficiency and reduce the time spent by analysts analyzing various content within TVA's technology eco-system. ",Threat Intelligence,Operation and Maintenance,Neither,10/26/2018,10/26/2018,10/31/2023,Unknown,PO 4480922,No,No,No,Yes,Unknown,Unknown,Unknown,No,No – agency does not have access to source code.,Yes,Corporate,Unknown,Yes,No,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4444444444444444,"Reverse engineering of malware, and software analysis more broadly, will continue to be a critical activity in support of TVA's cyber protections efforts. Analysis of files, URLs, Hashes, etc., to determine if contents are malicious using a combination of different anti-virus scanners and blocking services to advance efficiency and reduce the time spent by analysts analyzing various content within TVA's technology eco-system. . Threat Intelligence","reverse engineering of malware, and software analysis more broadly, will continue to be a critical activity in support of tva's cyber protections efforts. analysis of files, urls, hashes, etc., to determine if contents are malicious using a combination of different anti-virus scanners and blocking services to advance efficiency and reduce the time spent by analysts analyzing various content within tva's technology eco-system. . threat intelligence"
Threat Intelligence Feed,Tennessee Valley Authority,TVA,Cybersecurity,Mission-Enabling,None of the above.,Information Sharing; advanced technology and industry expertise to provide participants with near real-time delivery of relevant and actionable threat information of external network traffic.,Threat Intelligence,Operation and Maintenance,Neither,10/7/2020,10/7/2020,10/7/2020,Unknown,Unknown,No,No,No,Yes,Unknown,Unknown,Unknown,No,No – agency does not have access to source code.,Yes,Corporate,Unknown,Yes,No,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,Information Sharing; advanced technology and industry expertise to provide participants with near real-time delivery of relevant and actionable threat information of external network traffic. . Threat Intelligence,information sharing; advanced technology and industry expertise to provide participants with near real-time delivery of relevant and actionable threat information of external network traffic. . threat intelligence
AI-Enhanced Search,Tennessee Valley Authority,TVA,T&I Operations,Mission-Enabling,Searching for information using AI.,"Portal users can find answers more quickly and easily with features like auto-complete search queries, natural language support, and typo handling.",Knowledge Base Catalog Data,Operation and Maintenance,Neither,3/13/2024,6/1/2024,6/1/2024,Unknown,Unknown,No,No,Yes,Yes,Unknown,Knowledge Base Catalog Items,"Documentation is widely available: Documentation is not only complete, but is widely accessible within the agency, and has an owner and a regular update cadence.",No,No – agency does not have access to source code.,Yes,Corporate,Unknown,Yes,No,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.4603174603174603,"Portal users can find answers more quickly and easily with features like auto-complete search queries, natural language support, and typo handling. . Knowledge Base Catalog Data","portal users can find answers more quickly and easily with features like auto-complete search queries, natural language support, and typo handling. . knowledge base catalog data"
Network Anomaly Detection,Tennessee Valley Authority,TVA,Transmission,Mission-Enabling,None of the above.,"Real-time visibility into network communications, assets, connections, and protocols. ",Detection of anomalous behavior that might impact security or reliability of OT systems,Operation and Maintenance,Neither,6/1/2024,9/1/2024,9/1/2024,Unknown,Unknown,No,No,No,No,Unknown,Unknown,Unknown,No,No – agency does not have access to source code.,Yes,Corporate,Unknown,Yes,No,Yes,Unknown,Unknown,No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.42857142857142855,"Real-time visibility into network communications, assets, connections, and protocols. . Detection of anomalous behavior that might impact security or reliability of OT systems","real-time visibility into network communications, assets, connections, and protocols. . detection of anomalous behavior that might impact security or reliability of ot systems"
USDOT Compliance Plan for OMB Memorandum M-24-10 (September 2024),Department of Transportation,DOT,"CAIO, NETT","Administration of AI Governance, Processes, and Procedures",None of the Above,"This Compliance Plan conveys DOT’s approach to achieving consistency with OMB Memorandum M-24-10 Advancing Governance, Innovation, and Risk Management for Agency Use of Artificial Intelligence. The plan aligns with M-24-10’s three main pillars of Strength",Provides the Department with a roadmap for accelerating the responsible use of AI and complying with OMB directives.,Research or  Administrative Action Complete,Neither,7/25/2024,7/25/2024,Unknown,No contract/external resources used in development.,Unknown,No,No,No,Unknown,Not Applicable,Unknown,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6666666666666666,"This Compliance Plan conveys DOT’s approach to achieving consistency with OMB Memorandum M-24-10 Advancing Governance, Innovation, and Risk Management for Agency Use of Artificial Intelligence. The plan aligns with M-24-10’s three main pillars of Strength . Provides the Department with a roadmap for accelerating the responsible use of AI and complying with OMB directives. . Not Applicable","this compliance plan conveys dot’s approach to achieving consistency with omb memorandum m-24-10 advancing governance, innovation, and risk management for agency use of artificial intelligence. the plan aligns with m-24-10’s three main pillars of strength . provides the department with a roadmap for accelerating the responsible use of ai and complying with omb directives. . not applicable"
Advanced Research and Testing (ART) Network,Department of Transportation,DOT,"CAIO, Volpe, OST-R",Department-Level AI Capabilities and Capacity,None of the Above,"The ART Network is the shared service environment for AI research and development activities that provides researchers with access to a rapid AI innovation, exploration, development, and sharing platform using secure and approved IT infrastructure.",The ART Network shared service accelerates AI-enabled research at the Department by providing researchers with direct access to an established platform that contains all available AI tools for that environment rather than making changes to an existing env,Acquisition and/or Development,Neither,10/24/2024,12/9/2024,Unknown,No contract/external resources used in development.,Unknown,No,No,No,Unknown,Not Applicable,Unknown,Not Applicable,Not Applicable,Not Applicable,Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6507936507936508,"The ART Network is the shared service environment for AI research and development activities that provides researchers with access to a rapid AI innovation, exploration, development, and sharing platform using secure and approved IT infrastructure. . The ART Network shared service accelerates AI-enabled research at the Department by providing researchers with direct access to an established platform that contains all available AI tools for that environment rather than making changes to an existing env . Not Applicable","the art network is the shared service environment for ai research and development activities that provides researchers with access to a rapid ai innovation, exploration, development, and sharing platform using secure and approved it infrastructure. . the art network shared service accelerates ai-enabled research at the department by providing researchers with direct access to an established platform that contains all available ai tools for that environment rather than making changes to an existing env . not applicable"
AI Operations Laboratory (OPSLAB),Department of Transportation,DOT,CAIO,Department-Level AI Capabilities and Capacity,None of the Above,"Provides operational AI use case developers with access to shared environments with all OCIO-cleared AI functionality for use case experimentation, development, and initial data and model risk management identification and mitigation.",The OPSLAB shared service accelerates AI-enabled operational use case development at the Department by providing AI developers  with direct access to an established platform that contains all available AI tools for that environment rather than making chan,Ideation,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,Not Applicable,Unknown,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6190476190476191,"Provides operational AI use case developers with access to shared environments with all OCIO-cleared AI functionality for use case experimentation, development, and initial data and model risk management identification and mitigation. . The OPSLAB shared service accelerates AI-enabled operational use case development at the Department by providing AI developers  with direct access to an established platform that contains all available AI tools for that environment rather than making chan . Not Applicable","provides operational ai use case developers with access to shared environments with all ocio-cleared ai functionality for use case experimentation, development, and initial data and model risk management identification and mitigation. . the opslab shared service accelerates ai-enabled operational use case development at the department by providing ai developers with direct access to an established platform that contains all available ai tools for that environment rather than making chan . not applicable"
Transportation AI-enabled Network (TrAIN),Department of Transportation,DOT,CAIO,Department-Level AI Capabilities and Capacity,None of the Above,"The TrAIN is the set of shared service environments for operational AI use cases that provides AI developers with a pre-established, secure AI-enabled environments for development, testing and deployment under the Chief AI Officer's compliance and risk ma",The TrAIN shared service accelerates AI-enabled operational use case deployment at the Department by providing AI developers  with direct access to an established platform that contains all available AI tools for that environment rather than making change,Ideation,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,Not Applicable,Unknown,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6190476190476191,"The TrAIN is the set of shared service environments for operational AI use cases that provides AI developers with a pre-established, secure AI-enabled environments for development, testing and deployment under the Chief AI Officer's compliance and risk ma . The TrAIN shared service accelerates AI-enabled operational use case deployment at the Department by providing AI developers  with direct access to an established platform that contains all available AI tools for that environment rather than making change . Not Applicable","the train is the set of shared service environments for operational ai use cases that provides ai developers with a pre-established, secure ai-enabled environments for development, testing and deployment under the chief ai officer's compliance and risk ma . the train shared service accelerates ai-enabled operational use case deployment at the department by providing ai developers with direct access to an established platform that contains all available ai tools for that environment rather than making change . not applicable"
AI Support and Collaboration Center (AISCC),Department of Transportation,DOT,"OIE, CAIO, OST-R, OCIO","Administration of AI Governance, Processes, and Procedures",None of the Above,"The AISCC provides all DOT employees and contractors with a website for getting educated on AI topics, get inspired by DOT and external use cases and capabilities, collaborate with internal and external AI communities of practice, partner with AI subject ","Provides the Department's north star for the delivery of AI education, inspiration, and direction to the workforce.",Acquisition and/or Development,Neither,10/9/2024,12/2/2024,Unknown,Unknown,Unknown,No,No,No,Unknown,Not Applicable,Unknown,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6507936507936508,"The AISCC provides all DOT employees and contractors with a website for getting educated on AI topics, get inspired by DOT and external use cases and capabilities, collaborate with internal and external AI communities of practice, partner with AI subject . Provides the Department's north star for the delivery of AI education, inspiration, and direction to the workforce. . Not Applicable","the aiscc provides all dot employees and contractors with a website for getting educated on ai topics, get inspired by dot and external use cases and capabilities, collaborate with internal and external ai communities of practice, partner with ai subject . provides the department's north star for the delivery of ai education, inspiration, and direction to the workforce. . not applicable"
Enterprise Personal Productivity Assistant Capability,Department of Transportation,DOT,"CAIO, HASS, Volpe",Department-Level AI Capabilities and Capacity,None of the Above,Secure enterprise AI-enabled document generative AI capabilities.,Research and operating administrations are able to use application program interfaces (APIs) to develop employee productivity enhancing solutions for querying user-provided documents.,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,No contract/external resources used in development.,Unknown,No,No,No,Unknown,Not Applicable,Unknown,Not Applicable,Not Applicable,Not Applicable,Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6190476190476191,Secure enterprise AI-enabled document generative AI capabilities. . Research and operating administrations are able to use application program interfaces (APIs) to develop employee productivity enhancing solutions for querying user-provided documents. . Not Applicable,secure enterprise ai-enabled document generative ai capabilities. . research and operating administrations are able to use application program interfaces (apis) to develop employee productivity enhancing solutions for querying user-provided documents. . not applicable
"Enterprise ""Ask Dottie"" ChatBot Capability",Department of Transportation,DOT,"CAIO, HASS, Volpe",Department-Level AI Capabilities and Capacity,None of the Above,Secure enterprise AI-enabled question and answer ChatBot capabilities for natural language querying of documents and data.,Provides the Department with secure enterprise-wide LLM ChatBot capabilities that can be tailored to specific use cases.,Ideation,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,Not Applicable,Unknown,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6190476190476191,Secure enterprise AI-enabled question and answer ChatBot capabilities for natural language querying of documents and data. . Provides the Department with secure enterprise-wide LLM ChatBot capabilities that can be tailored to specific use cases. . Not Applicable,secure enterprise ai-enabled question and answer chatbot capabilities for natural language querying of documents and data. . provides the department with secure enterprise-wide llm chatbot capabilities that can be tailored to specific use cases. . not applicable
 Certified Professional Controller (CPC) On-Board Success Evaluator,Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,"CPC Outlook Generator (COG) is a front end Graphical User Interface built to interact with a data model called CPC Onboarding Success Evaluator (COSE), a machine learning model capable of predicting how many Certified Professional Controller (CPCs) the FA","Based on what-if scenario, the system provides a graphical representation of a 5 or 10 year CPC staffing outlook (which is the predictive component provided by the ML).  Alongside with the cost (training and salary) the ""when"" the target shifts (is it ear",Operation and Maintenance,Neither,2/10/2023,3/23/2023,4/24/2024,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,Yes,Yes,No,"Python-code algorithms are applied to multiple data sources owned by the agency such as Staffing Work Book, Federal Personnel Payroll Sytem, National Training Database, Controller workforce plan, etc. to train, fine-tune,  and evaluate performance of the ",Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6984126984126984,"CPC Outlook Generator (COG) is a front end Graphical User Interface built to interact with a data model called CPC Onboarding Success Evaluator (COSE), a machine learning model capable of predicting how many Certified Professional Controller (CPCs) the FA . Based on what-if scenario, the system provides a graphical representation of a 5 or 10 year CPC staffing outlook (which is the predictive component provided by the ML).  Alongside with the cost (training and salary) the ""when"" the target shifts (is it ear . Not Applicable","cpc outlook generator (cog) is a front end graphical user interface built to interact with a data model called cpc onboarding success evaluator (cose), a machine learning model capable of predicting how many certified professional controller (cpcs) the fa . based on what-if scenario, the system provides a graphical representation of a 5 or 10 year cpc staffing outlook (which is the predictive component provided by the ml). alongside with the cost (training and salary) the ""when"" the target shifts (is it ear . not applicable"
Automated Delay detection using voice processing,Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,"AI-based voice recognition of Air Traffic Control (ATC) and pilot communications associated with a flight is required for enhanced and accurate delay reporting and attribution.  Many delay events, such as vectoring, are not currently reported/detected/acc",The output of the AI system is data that captures a delay and the associated cause.  This will be combined with other data sources to provide a more accurate representation of a flight.,Acquisition and/or Development,Neither,2/10/2023,Unknown,Unknown,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,No,Unknown,No,The AI-based voice recognition process is trained using audio recording of ATC to pilot communications.,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6507936507936508,"AI-based voice recognition of Air Traffic Control (ATC) and pilot communications associated with a flight is required for enhanced and accurate delay reporting and attribution.  Many delay events, such as vectoring, are not currently reported/detected/acc . The output of the AI system is data that captures a delay and the associated cause.  This will be combined with other data sources to provide a more accurate representation of a flight. . Not Applicable","ai-based voice recognition of air traffic control (atc) and pilot communications associated with a flight is required for enhanced and accurate delay reporting and attribution. many delay events, such as vectoring, are not currently reported/detected/acc . the output of the ai system is data that captures a delay and the associated cause. this will be combined with other data sources to provide a more accurate representation of a flight. . not applicable"
Technical Operations Predictive Maintenance,Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,"""Utilize equipment telemetry data, statistical modeling and Machine Learning to predict equipment failures before they occur.  This will improve operational efficiency and safety by reducing unscheduled outages and/or shortening outage times as replacemen","Utilize equipment telemetry data, statistical modeling and Machine Learning to predict equipment failures before they occur.  This will improve operational efficiency and safety by reducing unscheduled outages and/or shortening outage times as replacement",Initiated,Neither,11/21/2024,Unknown,Unknown,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,No,Unknown,No,"Utilize equipment telemetry data, statistical modeling and Machine Learning to predict equipment failures before they occur.  This will improve operational efficiency and safety by reducing unscheduled outages and/or shortening outage times as replacement",Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6666666666666666,"""Utilize equipment telemetry data, statistical modeling and Machine Learning to predict equipment failures before they occur.  This will improve operational efficiency and safety by reducing unscheduled outages and/or shortening outage times as replacemen . Utilize equipment telemetry data, statistical modeling and Machine Learning to predict equipment failures before they occur.  This will improve operational efficiency and safety by reducing unscheduled outages and/or shortening outage times as replacement . Not Applicable","""utilize equipment telemetry data, statistical modeling and machine learning to predict equipment failures before they occur. this will improve operational efficiency and safety by reducing unscheduled outages and/or shortening outage times as replacemen . utilize equipment telemetry data, statistical modeling and machine learning to predict equipment failures before they occur. this will improve operational efficiency and safety by reducing unscheduled outages and/or shortening outage times as replacement . not applicable"
Airborne Safety Metric (ASM),Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,The use of AI modeling can categorize the types of events based on the information provided in the accident and incident report and its result will support the safety metric calculation.,"The AI system can classify the event types such as mid-air collisions, flight into terrain accidents, and turbulence events for a given accident or incident reports.",Operation and Maintenance,Neither,10/1/2019,10/1/2020,9/30/2021,No contract/external resources used in development.,Unknown,No,No,No,Unknown,No,ASM ingests accident and incident report such as Mandatory Occurrence Reports (MOR) and Aviation Safety Information Analysis & Sharing (ASIAS) data as the data source for the safety metrics calculation,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6825396825396826,"The use of AI modeling can categorize the types of events based on the information provided in the accident and incident report and its result will support the safety metric calculation. . The AI system can classify the event types such as mid-air collisions, flight into terrain accidents, and turbulence events for a given accident or incident reports. . Not Applicable","the use of ai modeling can categorize the types of events based on the information provided in the accident and incident report and its result will support the safety metric calculation. . the ai system can classify the event types such as mid-air collisions, flight into terrain accidents, and turbulence events for a given accident or incident reports. . not applicable"
Surface Safety Metric (SSM),Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,Automated review of accident narratives..,"Classification of aircraft accidents as a
 - runway collision
 - taxiway collision
 - runway excursion
 - not a surface event",Operation and Maintenance,Neither,10/1/2018,Unknown,Unknown,No contract/external resources used in development.,Unknown,No,No,No,Unknown,No,Internal and external accident report narratives.,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6507936507936508,"Automated review of accident narratives.. . Classification of aircraft accidents as a
 - runway collision
 - taxiway collision
 - runway excursion
 - not a surface event . Not Applicable",automated review of accident narratives.. . classification of aircraft accidents as a - runway collision - taxiway collision - runway excursion - not a surface event . not applicable
Surface Report Classifiier (SCM/Auto-Class),Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,To classify surface Mandatory Occurrence Reports (MORs) to reduce the amount of manual event review.,"Classes  - runway excursion, runway incursion, surface incident.",Operation and Maintenance,Neither,1/1/2019,1/1/2019,6/23/2020,No contract/external resources used in development.,Unknown,No,No,No,Unknown,No,Manually curated dataset for surface events,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6825396825396826,"To classify surface Mandatory Occurrence Reports (MORs) to reduce the amount of manual event review. . Classes  - runway excursion, runway incursion, surface incident. . Not Applicable","to classify surface mandatory occurrence reports (mors) to reduce the amount of manual event review. . classes - runway excursion, runway incursion, surface incident. . not applicable"
Pilot-Controller Voice to Text Transcription,Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,To transcribe recorded radio communications between pilots and air traffic controllers operating within the national airspace.,Text transcriptions and speaker role (pilot/controller),Acquisition and/or Development,Neither,10/21/2020,5/10/2022,Unknown,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,No,Unknown,No,1000 hour voice corpus,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6666666666666666,To transcribe recorded radio communications between pilots and air traffic controllers operating within the national airspace. . Text transcriptions and speaker role (pilot/controller) . Not Applicable,to transcribe recorded radio communications between pilots and air traffic controllers operating within the national airspace. . text transcriptions and speaker role (pilot/controller) . not applicable
Detection of Unmanned Aircraft Systems (UAS) Encounter,Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,Unmanned Aircraft Systems (UAS) are a source of potential safety risk.  Manual data exploration by a human is labor and time consuming.  Automatic data mining can help to identify safety issues.,Binary classification (yes/no) label for UAS event detection,Acquisition and/or Development,Neither,5/10/2022,5/10/2022,Unknown,No contract/external resources used in development.,Unknown,No,No,No,Unknown,No,Manually labeled training set,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6666666666666666,Unmanned Aircraft Systems (UAS) are a source of potential safety risk.  Manual data exploration by a human is labor and time consuming.  Automatic data mining can help to identify safety issues. . Binary classification (yes/no) label for UAS event detection . Not Applicable,unmanned aircraft systems (uas) are a source of potential safety risk. manual data exploration by a human is labor and time consuming. automatic data mining can help to identify safety issues. . binary classification (yes/no) label for uas event detection . not applicable
Human Performance Taxonomy labeling,Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,to label radio communication transcripts according to the HP Taxonomy labeling system.,One of 400 HP taxonomy labels.,Acquisition and/or Development,Neither,12/15/2022,3/27/2023,Unknown,No contract/external resources used in development.,Unknown,No,No,No,Unknown,No,SME-labeled training set,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6666666666666666,to label radio communication transcripts according to the HP Taxonomy labeling system. . One of 400 HP taxonomy labels. . Not Applicable,to label radio communication transcripts according to the hp taxonomy labeling system. . one of 400 hp taxonomy labels. . not applicable
Regulatory Compliance Mapping Tool,Department of Transportation,DOT,FAA AVS,AI used in Transportation Operations,None of the Above,The AVS International office is required to identify means of compliance to ICAO Standards and Recommended Practices (SARPs).  Both SARPs and means of compliance evidence are text paragraphs scattered across thousands of pages of documents.  AOV identifie,,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,No,Unknown,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.5873015873015873,The AVS International office is required to identify means of compliance to ICAO Standards and Recommended Practices (SARPs).  Both SARPs and means of compliance evidence are text paragraphs scattered across thousands of pages of documents.  AOV identifie . Not Applicable,the avs international office is required to identify means of compliance to icao standards and recommended practices (sarps). both sarps and means of compliance evidence are text paragraphs scattered across thousands of pages of documents. aov identifie . not applicable
JASC Code classification in Safety Difficulty Reports (SDR),Department of Transportation,DOT,FAA AVS,AI used in Transportation Operations,None of the Above,"AVS identified a need to derive the joint aircraft system codes (JASC) chapter codes from the narrative description within service difficulty reports (SDR), a form of safety event reporting from aircraft operators. A team of graduate students at George Ma",,Ideation,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,No,Unknown,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6031746031746031,"AVS identified a need to derive the joint aircraft system codes (JASC) chapter codes from the narrative description within service difficulty reports (SDR), a form of safety event reporting from aircraft operators. A team of graduate students at George Ma . Not Applicable","avs identified a need to derive the joint aircraft system codes (jasc) chapter codes from the narrative description within service difficulty reports (sdr), a form of safety event reporting from aircraft operators. a team of graduate students at george ma . not applicable"
Offshore Precipitation Capability (OPC),Department of Transportation,DOT,FAA ANG,AI used in Transportation Operations,None of the Above,"The AI/ML tool helps detect and predict where precipitation currently is and where it will be at the future at various altitudes in parts of the NAS that do not have traditional ground based weather radar. 
Benefits are better weather dectection and avoid","The output of the system is meant to simulate weather radar where cooler colors (blues and greens) correspond to light precip where hotter colors (yellow, oranges and red) indicate heavy precip and/or more intense precipitation. A similar process is down ",Acquisition and/or Development,Neither,12/1/2013,10/1/2013,12/17/2024,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,No,Unknown,No,None. It's public data from NOAA/NWS. ,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6825396825396826,"The AI/ML tool helps detect and predict where precipitation currently is and where it will be at the future at various altitudes in parts of the NAS that do not have traditional ground based weather radar. 
Benefits are better weather dectection and avoid . The output of the system is meant to simulate weather radar where cooler colors (blues and greens) correspond to light precip where hotter colors (yellow, oranges and red) indicate heavy precip and/or more intense precipitation. A similar process is down . Not Applicable","the ai/ml tool helps detect and predict where precipitation currently is and where it will be at the future at various altitudes in parts of the nas that do not have traditional ground based weather radar. benefits are better weather dectection and avoid . the output of the system is meant to simulate weather radar where cooler colors (blues and greens) correspond to light precip where hotter colors (yellow, oranges and red) indicate heavy precip and/or more intense precipitation. a similar process is down . not applicable"
Course Deviation Identification for Multiple Airport Route Separation (MARS),Department of Transportation,DOT,FAA AVS,AI used in Transportation Operations,None of the Above,The Multiple Airport Route Separation (MARS) program is developing a safety case for reduced separation standards between Performance Based Navigation (PBN) routes in terminal airspace. These new standards may enable deconfliction of airports in high-dema,,Ideation,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,No,Unknown,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6031746031746031,The Multiple Airport Route Separation (MARS) program is developing a safety case for reduced separation standards between Performance Based Navigation (PBN) routes in terminal airspace. These new standards may enable deconfliction of airports in high-dema . Not Applicable,the multiple airport route separation (mars) program is developing a safety case for reduced separation standards between performance based navigation (pbn) routes in terminal airspace. these new standards may enable deconfliction of airports in high-dema . not applicable
Improve and Speed up the Certification Service Oversight Process Using Intelligent Document Review ,Department of Transportation,DOT,FAA AVS,AI used in Transportation Operations,None of the Above,"Before digitization: 
* The Certification Services Oversight Process (CSOP) is a manually intensive paper process 

* Requires the public and small businesses to provide input for Certification requests

* No external applicant tracking which results in p","""Before digitization: 

* The Certification Services Oversight Process (CSOP) is a manually intensive paper process 

* Requires the public and small businesses to provide input for Certification requests

* No external applicant tracking which results in",Initiated,Neither,7/1/2024,Unknown,Unknown,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,Yes,Yes,No,Dynamic Regulatory System,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6825396825396826,"Before digitization: 
* The Certification Services Oversight Process (CSOP) is a manually intensive paper process 

* Requires the public and small businesses to provide input for Certification requests

* No external applicant tracking which results in p . ""Before digitization: 

* The Certification Services Oversight Process (CSOP) is a manually intensive paper process 

* Requires the public and small businesses to provide input for Certification requests

* No external applicant tracking which results in . Not Applicable","before digitization: * the certification services oversight process (csop) is a manually intensive paper process * requires the public and small businesses to provide input for certification requests * no external applicant tracking which results in p . ""before digitization: * the certification services oversight process (csop) is a manually intensive paper process * requires the public and small businesses to provide input for certification requests * no external applicant tracking which results in . not applicable"
Determining Surface Winds with Machine Learning Software,Department of Transportation,DOT,FAA ANG,AI used in Transportation Operations,None of the Above,Successfully demonstrated use of an AI capability to analyze camera images of a wind sock to produce highly accurate surface wind speed and direction information in remote areas that don’t have a weather observing sensor.,,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,No,Unknown,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.5873015873015873,Successfully demonstrated use of an AI capability to analyze camera images of a wind sock to produce highly accurate surface wind speed and direction information in remote areas that don’t have a weather observing sensor. . Not Applicable,successfully demonstrated use of an ai capability to analyze camera images of a wind sock to produce highly accurate surface wind speed and direction information in remote areas that don’t have a weather observing sensor. . not applicable
Remote Oceanic Meteorological Information Operations (ROMIO),Department of Transportation,DOT,FAA ANG,AI used in Transportation Operations,None of the Above,"ROMIO is an operational demonstration to evaluate the feasibility to uplink convective weather information to aircraft operating over the ocean and remote regions. Capability converted weather satellite data, lightning and weather prediction model data in",,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,No,Unknown,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.5873015873015873,"ROMIO is an operational demonstration to evaluate the feasibility to uplink convective weather information to aircraft operating over the ocean and remote regions. Capability converted weather satellite data, lightning and weather prediction model data in . Not Applicable","romio is an operational demonstration to evaluate the feasibility to uplink convective weather information to aircraft operating over the ocean and remote regions. capability converted weather satellite data, lightning and weather prediction model data in . not applicable"
Head Kinematics Prediction,Department of Transportation,DOT,NHTSA,Internal DOT Research Project,None of the Above,"Description: Utilize deep learning models for predicting head kinematics directly from crash videos. The utilization of deep learning techniques enables the extraction of 3D kinematics from 2D views, offering a viable alternative for calculating head kine",Angular velocity - injury prediction,Research or  Administrative Action Complete,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,No,Unknown,Not Applicable,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6031746031746031,"Description: Utilize deep learning models for predicting head kinematics directly from crash videos. The utilization of deep learning techniques enables the extraction of 3D kinematics from 2D views, offering a viable alternative for calculating head kine . Angular velocity - injury prediction . Not Applicable","description: utilize deep learning models for predicting head kinematics directly from crash videos. the utilization of deep learning techniques enables the extraction of 3d kinematics from 2d views, offering a viable alternative for calculating head kine . angular velocity - injury prediction . not applicable"
Crash Parameter Prediction,Department of Transportation,DOT,NHTSA,Internal DOT Research Project,None of the Above,"Description: Utilize deep learning for predicting crash parameters, Delta-V (change in velocity) and PDOF (principal direction of force), directly from real-world crash images. Delta-V and PDOF are two most important parameters affecting injury outcome. D",Delta-V & PDOF,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,No,Unknown,Not Applicable,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6031746031746031,"Description: Utilize deep learning for predicting crash parameters, Delta-V (change in velocity) and PDOF (principal direction of force), directly from real-world crash images. Delta-V and PDOF are two most important parameters affecting injury outcome. D . Delta-V & PDOF . Not Applicable","description: utilize deep learning for predicting crash parameters, delta-v (change in velocity) and pdof (principal direction of force), directly from real-world crash images. delta-v and pdof are two most important parameters affecting injury outcome. d . delta-v & pdof . not applicable"
Crushed Aggregate Gradation Evaluation System,Department of Transportation,DOT,FRA,Internal DOT Research Project,None of the Above,"Description: Deep learning computer vision algorithms aimed at analyzing aggregate particle size grading.
Input: Images of ballast cross sections",Ballast fouling index,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,No,Unknown,Not Applicable,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6031746031746031,"Description: Deep learning computer vision algorithms aimed at analyzing aggregate particle size grading.
Input: Images of ballast cross sections . Ballast fouling index . Not Applicable",description: deep learning computer vision algorithms aimed at analyzing aggregate particle size grading. input: images of ballast cross sections . ballast fouling index . not applicable
Automatic Track Change Detection Demonstration and Analysis,Department of Transportation,DOT,FRA,Internal DOT Research Project,None of the Above,"Description: DeepCNet-based neural network to identify and classify track-related  features (e.g., track components, such as fasteners and ties) for """"change detection"""" applications. Input: Line-scan images from rail-bound inspection systems.
",Notification of changes from status quo or between different inspections based on geolocation.,Initiated,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,No,Unknown,Not Applicable,No,"Yes – agency has access to source code, but it is not public.",Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6190476190476191,"Description: DeepCNet-based neural network to identify and classify track-related  features (e.g., track components, such as fasteners and ties) for """"change detection"""" applications. Input: Line-scan images from rail-bound inspection systems. . Notification of changes from status quo or between different inspections based on geolocation. . Not Applicable","description: deepcnet-based neural network to identify and classify track-related features (e.g., track components, such as fasteners and ties) for """"change detection"""" applications. input: line-scan images from rail-bound inspection systems. . notification of changes from status quo or between different inspections based on geolocation. . not applicable"
Predictive Analytics Using Autonomous Track Geometry Measurement System (ATGMS) Data,Department of Transportation,DOT,FRA,Internal DOT Research Project,None of the Above,"Description: Leveraging large volumes of these recursive track geometry measurements to develop and implement automated machine-learning-based processes for analyzing, predicting, and reporting track locations of concern, including those with significant ","Inspection report that includes the trending of track geometry measures and time to failure (i.e., maintenance and safety limits).",Initiated,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,No,Unknown,Not Applicable,No,"Yes – agency has access to source code, but it is not public.",Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6190476190476191,"Description: Leveraging large volumes of these recursive track geometry measurements to develop and implement automated machine-learning-based processes for analyzing, predicting, and reporting track locations of concern, including those with significant . Inspection report that includes the trending of track geometry measures and time to failure (i.e., maintenance and safety limits). . Not Applicable","description: leveraging large volumes of these recursive track geometry measurements to develop and implement automated machine-learning-based processes for analyzing, predicting, and reporting track locations of concern, including those with significant . inspection report that includes the trending of track geometry measures and time to failure (i.e., maintenance and safety limits). . not applicable"
Integration of small Unmanned Aircraft System (sUAS) Geospacial Information System (GIS) Technologie,Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,This use case will be used to enable new methods and tools that support the maintenance of life cycle of National Airpsace System (NAS) infrastructure.,Used to enable non-Georefereced data to be integrated into digital form. ,Operation and Maintenance,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,No,Esri ArcGIS and associated tools and applications ,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6190476190476191,This use case will be used to enable new methods and tools that support the maintenance of life cycle of National Airpsace System (NAS) infrastructure. . Used to enable non-Georefereced data to be integrated into digital form. . Not Applicable,this use case will be used to enable new methods and tools that support the maintenance of life cycle of national airpsace system (nas) infrastructure. . used to enable non-georefereced data to be integrated into digital form. . not applicable
"Geolocating and Identifying Vehicle Hard Brake, Acceleration and Seat Belt Usage with CV Data",Department of Transportation,DOT,FHWA,Internal DOT Research Project,None of the Above,"Description:  The ongoing work leverages AI/ML in big traffic data analytics to identify roadway geolocations where potential safety issue may exist through the integration of connected vehicle data, speed and Individual Vehicle Record data, historical cr",Maps and Reports,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,No,Unknown,Not Applicable,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6031746031746031,"Description:  The ongoing work leverages AI/ML in big traffic data analytics to identify roadway geolocations where potential safety issue may exist through the integration of connected vehicle data, speed and Individual Vehicle Record data, historical cr . Maps and Reports . Not Applicable","description: the ongoing work leverages ai/ml in big traffic data analytics to identify roadway geolocations where potential safety issue may exist through the integration of connected vehicle data, speed and individual vehicle record data, historical cr . maps and reports . not applicable"
"Path to Advanced Novel Data Analytics (PANDA), a data science lab.",Department of Transportation,DOT,FHWA Turner-Fairbank,Department-Level AI Capabilities and Capacity,None of the Above,Description: A data science lab established at Turner-Fairbank Highway Research Center.,Promote usage of AI/ML tools on Databricks platform and implement research use cases across multiple highway desciplines. ,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,Not Applicable,Unknown,Not Applicable,Not Applicable,Not Applicable,Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6031746031746031,Description: A data science lab established at Turner-Fairbank Highway Research Center. . Promote usage of AI/ML tools on Databricks platform and implement research use cases across multiple highway desciplines. . Not Applicable,description: a data science lab established at turner-fairbank highway research center. . promote usage of ai/ml tools on databricks platform and implement research use cases across multiple highway desciplines. . not applicable
Exploratory Advanced Research group of projects ,Department of Transportation,DOT,FHWA,DOT Sponsored External Research,None of the Above,"Description: The Exploratory Advanced Research (EAR) Program addresses the need for longer term, higher risk research with the potential for transformative improvements to transportation systems. The EAR Program currently funds several projects that focus",Inputs and outputs vary across the projects.,Acquisition and/or Development,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,Not Applicable,Unknown,Not Applicable,Not Applicable,"Yes – agency has access to source code, but it is not public.",Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6190476190476191,"Description: The Exploratory Advanced Research (EAR) Program addresses the need for longer term, higher risk research with the potential for transformative improvements to transportation systems. The EAR Program currently funds several projects that focus . Inputs and outputs vary across the projects. . Not Applicable","description: the exploratory advanced research (ear) program addresses the need for longer term, higher risk research with the potential for transformative improvements to transportation systems. the ear program currently funds several projects that focus . inputs and outputs vary across the projects. . not applicable"
Enterprise Knowledge Graph and Advanced AI Data Structures Collaboration Group,Department of Transportation,DOT,"CAIO, FAA, OST-R, Volpe, OCIO",Department-Level AI Capabilities and Capacity,None of the Above,"DOT-Private Sector collaboration to develop advanced AI data structure expertise, lessons learned, and best practices to accelerate AI development across the agency.",High-quality and efficient data structures increase AI model efficiency and accuracy.,Initiated,Neither,10/24/2024,Unknown,Unknown,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,No,Unknown,Not Applicable,Unknown,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6507936507936508,"DOT-Private Sector collaboration to develop advanced AI data structure expertise, lessons learned, and best practices to accelerate AI development across the agency. . High-quality and efficient data structures increase AI model efficiency and accuracy. . Not Applicable","dot-private sector collaboration to develop advanced ai data structure expertise, lessons learned, and best practices to accelerate ai development across the agency. . high-quality and efficient data structures increase ai model efficiency and accuracy. . not applicable"
Transportation Use Case Knowledge Repository (TrUCKR),Department of Transportation,DOT,CAIO,"Administration of AI Governance, Processes, and Procedures",None of the Above,"TrUCKR is the CAIO managed DOT platform for tracking the Department's unclassified AI use case development, maturity, assessments, clearances, risk evaluations and mitigations, and authorities to operate across the use case lifecycle for operations, resea","TrUCKR is the Department's data source for prioritizing and driving the acceleration of AI, monitoring AI governance and compliance, and internal and external collaboration.",Research or  Administrative Action Complete,Neither,9/24/2024,10/1/2024,Unknown,Unknown,Unknown,No,No,No,Unknown,Not Applicable,Unknown,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6507936507936508,"TrUCKR is the CAIO managed DOT platform for tracking the Department's unclassified AI use case development, maturity, assessments, clearances, risk evaluations and mitigations, and authorities to operate across the use case lifecycle for operations, resea . TrUCKR is the Department's data source for prioritizing and driving the acceleration of AI, monitoring AI governance and compliance, and internal and external collaboration. . Not Applicable","truckr is the caio managed dot platform for tracking the department's unclassified ai use case development, maturity, assessments, clearances, risk evaluations and mitigations, and authorities to operate across the use case lifecycle for operations, resea . truckr is the department's data source for prioritizing and driving the acceleration of ai, monitoring ai governance and compliance, and internal and external collaboration. . not applicable"
NETT Council AI Coordination and Activities (AICA) Working Group,Department of Transportation,DOT,"CAIO, NETT","Administration of AI Governance, Processes, and Procedures",None of the Above,The AICA Working Group consists of Operating Administration (OA) and Secretarial Offices AI leaders and subject matter experts across the Department.  It is chaired by the CAIO and vice-chaired by representatives from the DOT Office of Research and Techno,"Provides DOT with the necessary administrative infrastructure for coordinating Executive Order and other federal guidance and mandates, tracking AI activities, and collaborating on AI initiatives and compliance, governance, and guidance documents.",Research or  Administrative Action Complete,Neither,9/24/2024,10/1/2024,Unknown,Unknown,Unknown,No,No,No,Unknown,Not Applicable,Unknown,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6507936507936508,"The AICA Working Group consists of Operating Administration (OA) and Secretarial Offices AI leaders and subject matter experts across the Department.  It is chaired by the CAIO and vice-chaired by representatives from the DOT Office of Research and Techno . Provides DOT with the necessary administrative infrastructure for coordinating Executive Order and other federal guidance and mandates, tracking AI activities, and collaborating on AI initiatives and compliance, governance, and guidance documents. . Not Applicable","the aica working group consists of operating administration (oa) and secretarial offices ai leaders and subject matter experts across the department. it is chaired by the caio and vice-chaired by representatives from the dot office of research and techno . provides dot with the necessary administrative infrastructure for coordinating executive order and other federal guidance and mandates, tracking ai activities, and collaborating on ai initiatives and compliance, governance, and guidance documents. . not applicable"
DOT Workforce Acquisitions and Training Team,Department of Transportation,DOT,"CAIO, OST-M, OCIO, OIE","Administration of AI Governance, Processes, and Procedures",None of the Above,The Workforce Acquisitions and Training Team members are DOT OA and Secretarial Office leaders responsible for AI talent acquisition and training and inspiring the workforce to accelerate AI adoption.,Provides DOT with coordination and collaboration on AI hiring and training initiatives.,Initiated,Neither,12/11/2024,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,Not Applicable,Unknown,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6349206349206349,The Workforce Acquisitions and Training Team members are DOT OA and Secretarial Office leaders responsible for AI talent acquisition and training and inspiring the workforce to accelerate AI adoption. . Provides DOT with coordination and collaboration on AI hiring and training initiatives. . Not Applicable,the workforce acquisitions and training team members are dot oa and secretarial office leaders responsible for ai talent acquisition and training and inspiring the workforce to accelerate ai adoption. . provides dot with coordination and collaboration on ai hiring and training initiatives. . not applicable
"NETT Council Safety, Rights, and Security Review (SR2) Committee",Department of Transportation,DOT,"CAIO, OCIO, OST-P, OST-M, NETT","Administration of AI Governance, Processes, and Procedures",None of the Above,"The SR2 Committee reviews and approves the operational deployment of all AI use cases and the public sharing of use case data, models, and code. The SR2 Committee is also responsible for performing the Security Review required by Executive Order 14110 Sec","Provides the Department with expert oversight of AI use case safety, security, rights, privacy, and data.",Acquisition and/or Development,Neither,9/24/2024,10/9/2024,Unknown,Unknown,Unknown,No,No,No,Unknown,Not Applicable,Unknown,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6507936507936508,"The SR2 Committee reviews and approves the operational deployment of all AI use cases and the public sharing of use case data, models, and code. The SR2 Committee is also responsible for performing the Security Review required by Executive Order 14110 Sec . Provides the Department with expert oversight of AI use case safety, security, rights, privacy, and data. . Not Applicable","the sr2 committee reviews and approves the operational deployment of all ai use cases and the public sharing of use case data, models, and code. the sr2 committee is also responsible for performing the security review required by executive order 14110 sec . provides the department with expert oversight of ai use case safety, security, rights, privacy, and data. . not applicable"
DOT AI Procurement Team,Department of Transportation,DOT,"CAIO, OST-M, FHWA, OCIO","Administration of AI Governance, Processes, and Procedures",None of the Above,The Procurement Team members are DOT OA and Secretarial Office leaders responsible for AI procurement and acquisitions.,Provides DOT with internal and external coordination and collaboration on AI procurement and acquisition policy and procedures.,Acquisition and/or Development,Neither,9/24/2024,12/11/2024,Unknown,Unknown,Unknown,No,No,No,Unknown,Not Applicable,Unknown,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6507936507936508,The Procurement Team members are DOT OA and Secretarial Office leaders responsible for AI procurement and acquisitions. . Provides DOT with internal and external coordination and collaboration on AI procurement and acquisition policy and procedures. . Not Applicable,the procurement team members are dot oa and secretarial office leaders responsible for ai procurement and acquisitions. . provides dot with internal and external coordination and collaboration on ai procurement and acquisition policy and procedures. . not applicable
DOT AI Emerging Technology Team,Department of Transportation,DOT,"CAIO, OST-R, OCIO","Administration of AI Governance, Processes, and Procedures",None of the Above,"The AI Emerging Technology Team members are DOT OA and Secretarial Office leaders most knowledgeable in leading-edge AI technology, capacity, and capabilities.","Provides DOT with internal and external coordination and collaboration on establishing and maintaining leading-edge AI technology, capacity, and capabilities.",Acquisition and/or Development,Neither,9/24/2024,12/11/2024,Unknown,Unknown,Unknown,No,No,No,Unknown,Not Applicable,Unknown,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6507936507936508,"The AI Emerging Technology Team members are DOT OA and Secretarial Office leaders most knowledgeable in leading-edge AI technology, capacity, and capabilities. . Provides DOT with internal and external coordination and collaboration on establishing and maintaining leading-edge AI technology, capacity, and capabilities. . Not Applicable","the ai emerging technology team members are dot oa and secretarial office leaders most knowledgeable in leading-edge ai technology, capacity, and capabilities. . provides dot with internal and external coordination and collaboration on establishing and maintaining leading-edge ai technology, capacity, and capabilities. . not applicable"
FHWA Interim Generative AI (GenAI) Usage Guidance,Department of Transportation,DOT,NHTSA,"Administration of AI Governance, Processes, and Procedures",None of the Above,Guidelines on usage of commercially available Generative Artificial Intelligence (GenAI) Tools and Services.,NHTSA guidelines (do's and don'ts) for using publicly available GenAI services while protecting security and privacy of DOT data.,Research or  Administrative Action Complete,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,Not Applicable,Unknown,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6190476190476191,Guidelines on usage of commercially available Generative Artificial Intelligence (GenAI) Tools and Services. . NHTSA guidelines (do's and don'ts) for using publicly available GenAI services while protecting security and privacy of DOT data. . Not Applicable,guidelines on usage of commercially available generative artificial intelligence (genai) tools and services. . nhtsa guidelines (do's and don'ts) for using publicly available genai services while protecting security and privacy of dot data. . not applicable
DOT Generative AI (GenAI) Policy,Department of Transportation,DOT,"CAIO, OST-P, OCIO, OST-R","Administration of AI Governance, Processes, and Procedures",None of the Above,Provide DOT-wide generative AI (GenAI) policy.,Establish guidelines and safeguards for GenAI use within the Department.,Ideation,Neither,Unknown,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,Not Applicable,Unknown,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6190476190476191,Provide DOT-wide generative AI (GenAI) policy. . Establish guidelines and safeguards for GenAI use within the Department. . Not Applicable,provide dot-wide generative ai (genai) policy. . establish guidelines and safeguards for genai use within the department. . not applicable
Surface Safety Metric (DOT Key Performance Indicator),Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,"Creating weighting scheme values based on safety data for measuring and monitoring safety risk in the National Air Space. The weights are used to generate safety indices for surface and airborne settings, for both commercial and non-commercial flights.
",Safety indices for Commercial and Non-Commercial accident and incident categories with the SSM Safety Performance Target.,Operation and Maintenance,Neither,10/1/2024,Unknown,Unknown,No contract/external resources used in development.,Unknown,No,No,No,Unknown,No,"Aviation Risk Identification and Assessment (ARIA), Aviation Safety Information Analysis & Sharing (ASIAS) data.",Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6507936507936508,"Creating weighting scheme values based on safety data for measuring and monitoring safety risk in the National Air Space. The weights are used to generate safety indices for surface and airborne settings, for both commercial and non-commercial flights. . Safety indices for Commercial and Non-Commercial accident and incident categories with the SSM Safety Performance Target. . Not Applicable","creating weighting scheme values based on safety data for measuring and monitoring safety risk in the national air space. the weights are used to generate safety indices for surface and airborne settings, for both commercial and non-commercial flights. . safety indices for commercial and non-commercial accident and incident categories with the ssm safety performance target. . not applicable"
Power Platform Use,Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,Speeds up the development of code structure. ,Provides programming language recommendations (C#) to support coding/software development. ,Operation and Maintenance,Neither,4/1/2025,Unknown,4/1/2025,Unknown,Unknown,No,No,No,Unknown,No,Unknown,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6349206349206349,Speeds up the development of code structure. . Provides programming language recommendations (C#) to support coding/software development. . Not Applicable,speeds up the development of code structure. . provides programming language recommendations (c#) to support coding/software development. . not applicable
Tech Ops LLM Document Search,Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,"The AI system designed for Tech-Ops personnel is a comprehensive support tool that enhances efficiency by providing rapid, contextually accurate answers drawn from a central Knowledge Library. This Knowledge Library contains Technical Instruction Manuals,","Direct Answers to Queries: When a technician inputs a question in plain language, the AI generates a concise, relevant answer drawn directly from the Technical Instruction Manuals, Maintenance Handbooks, or other materials in the Knowledge Library. This r",Acquisition and/or Development,Neither,5/1/2024,8/7/2023,Unknown,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,No,Unknown,No,"Technical Instruction Manuals (TI)
Maintenance Hadbooks (MHB)
Training Course Material
Historical Maintenance Logs",Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6666666666666666,"The AI system designed for Tech-Ops personnel is a comprehensive support tool that enhances efficiency by providing rapid, contextually accurate answers drawn from a central Knowledge Library. This Knowledge Library contains Technical Instruction Manuals, . Direct Answers to Queries: When a technician inputs a question in plain language, the AI generates a concise, relevant answer drawn directly from the Technical Instruction Manuals, Maintenance Handbooks, or other materials in the Knowledge Library. This r . Not Applicable","the ai system designed for tech-ops personnel is a comprehensive support tool that enhances efficiency by providing rapid, contextually accurate answers drawn from a central knowledge library. this knowledge library contains technical instruction manuals, . direct answers to queries: when a technician inputs a question in plain language, the ai generates a concise, relevant answer drawn directly from the technical instruction manuals, maintenance handbooks, or other materials in the knowledge library. this r . not applicable"
Human Resources Policy Manual (HRPM) LLM Document Search,Department of Transportation,DOT,FAA AHR,AI used in Transportation Operations,None of the Above,To allow employees to quickly and easily (via natural language processing) ask Human Resource policy questions and quickly receive back an easy to understand response.,The system outputs HR policy in response to questions which are input by users (employees).,Acquisition and/or Development,Neither,4/1/2024,4/1/2024,Unknown,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,No,Unknown,No,No training and/or fine-tuning done by FAA.  AHR has peformed internal testing to evaluate both the consistency and accuracy of reponses.,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6666666666666666,To allow employees to quickly and easily (via natural language processing) ask Human Resource policy questions and quickly receive back an easy to understand response. . The system outputs HR policy in response to questions which are input by users (employees). . Not Applicable,to allow employees to quickly and easily (via natural language processing) ask human resource policy questions and quickly receive back an easy to understand response. . the system outputs hr policy in response to questions which are input by users (employees). . not applicable
Financials LLM Document Search,Department of Transportation,DOT,FAA AFN,AI used in Transportation Operations,None of the Above,Improving efficiency and productivity in budget and finance.,financial data retrieved from the text resources,Initiated,Neither,2/1/2024,Unknown,Unknown,No contract/external resources used in development.,Unknown,No,No,No,Unknown,No,finacial reports and budget estimates,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6666666666666666,Improving efficiency and productivity in budget and finance. . financial data retrieved from the text resources . Not Applicable,improving efficiency and productivity in budget and finance. . financial data retrieved from the text resources . not applicable
Civil Aviation Registry Electronic Services (CARES) LLM Document Search,Department of Transportation,DOT,FAA AVS,AI used in Transportation Operations,None of the Above,CARES will use AI (Azure chatbot) to scan thousands of pages and generate an accurate answer. The benefits will be a faster and deeper understanding of the business requirements so the FAA can speed up our development. ,"The Azure chatbot will find the information requested and output the information in a clear, easy to understand answer. ",Acquisition and/or Development,Neither,7/26/2024,Unknown,Unknown,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,No,Unknown,No,Chabot utilizes OpenAI GPT models within the FAA security boundary via the FAA's subscription to Azure. OpenAI GPT models within the FAA's subscription on Azure do not retain user data and do not use user data to train their models.  ,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6507936507936508,"CARES will use AI (Azure chatbot) to scan thousands of pages and generate an accurate answer. The benefits will be a faster and deeper understanding of the business requirements so the FAA can speed up our development. . The Azure chatbot will find the information requested and output the information in a clear, easy to understand answer. . Not Applicable","cares will use ai (azure chatbot) to scan thousands of pages and generate an accurate answer. the benefits will be a faster and deeper understanding of the business requirements so the faa can speed up our development. . the azure chatbot will find the information requested and output the information in a clear, easy to understand answer. . not applicable"
Financial Policy LLM Document Search,Department of Transportation,DOT,FAA AFN,AI used in Transportation Operations,None of the Above,"The Financial Policy Chatbot (AI system) will search a variety of financial policy documents to answer users' financial policy questions, and more importantly, will link to the document(s) where it found the answer.  The the chatbot will initially be used","The Financial Policy Chatbot (AI system) will search a variety of financial policy documents to answer users' financial policy questions, and more importantly, will link to the document(s) where it found the answer. ",Operation and Maintenance,Neither,6/13/2024,6/13/2024,7/18/2024,No contract/external resources used in development.,Unknown,No,No,No,Unknown,No,"Financial Policy Chatbot input sources:
-	FAA Financial Manual
-	FAA Acquisition Management System
-	FAA Financial Policy SOPs
-	FAA Financial Policy Q&As
-	Dollars & Sense transcripts
-	FAA Financial Policy E-Learning course transcripts
-	FAA Financial P",Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6825396825396826,"The Financial Policy Chatbot (AI system) will search a variety of financial policy documents to answer users' financial policy questions, and more importantly, will link to the document(s) where it found the answer.  The the chatbot will initially be used . The Financial Policy Chatbot (AI system) will search a variety of financial policy documents to answer users' financial policy questions, and more importantly, will link to the document(s) where it found the answer. . Not Applicable","the financial policy chatbot (ai system) will search a variety of financial policy documents to answer users' financial policy questions, and more importantly, will link to the document(s) where it found the answer. the the chatbot will initially be used . the financial policy chatbot (ai system) will search a variety of financial policy documents to answer users' financial policy questions, and more importantly, will link to the document(s) where it found the answer. . not applicable"
Spectrum Assignment & Engineering Team LLM Document Search,Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,"Intended purpose: To make the user experience of end users more easy and  interactive and engaging.

Benefits: Increased customer satisfaction of FAA's coordination systems' users",Responses to help questions,Operation and Maintenance,Neither,4/10/2024,4/10/2024,6/25/2024,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,No,Unknown,No,FAA owns the WebFCR system that requires coordination request for frequency spectrum be provided in particular formats as prescribed by the NTIA Spectrum Rebook manual. All data used is from the help file for WebFCR and the National Telecomunications and ,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6825396825396826,"Intended purpose: To make the user experience of end users more easy and  interactive and engaging.

Benefits: Increased customer satisfaction of FAA's coordination systems' users . Responses to help questions . Not Applicable",intended purpose: to make the user experience of end users more easy and interactive and engaging. benefits: increased customer satisfaction of faa's coordination systems' users . responses to help questions . not applicable
National Program Office LLM Document Search,Department of Transportation,DOT,FAA AVS,AI used in Transportation Operations,None of the Above,"The intended purpose is to provide answers to Element Design Data collection Tool (ED DCT) questions used during COS (continued operational safety), IC (initial certification), and configuration changes of a certificate holder's operation. The benefits in",The outputs are aswers to Element Design Data collection Tool (EDDCT) questions.,Acquisition and/or Development,Neither,7/25/2024,8/1/2024,Unknown,No contract/external resources used in development.,Unknown,No,No,No,Unknown,No,FAA Safety Assurance System (SAS) data is used to evluate the performance.,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6666666666666666,"The intended purpose is to provide answers to Element Design Data collection Tool (ED DCT) questions used during COS (continued operational safety), IC (initial certification), and configuration changes of a certificate holder's operation. The benefits in . The outputs are aswers to Element Design Data collection Tool (EDDCT) questions. . Not Applicable","the intended purpose is to provide answers to element design data collection tool (ed dct) questions used during cos (continued operational safety), ic (initial certification), and configuration changes of a certificate holder's operation. the benefits in . the outputs are aswers to element design data collection tool (eddct) questions. . not applicable"
FAA Orders and associated supplemental change notices LLM Document Search,Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,"The intended purpose of the AI chatbot is to streamline the retrieval of information from multiple FAA Orders and change notices. The expected benefits include saving time, improving accuracy in finding specific citations, and reducing manual effort for u","The chatbot outputs specific citations and relevant sections from multiple FAA Orders and change notices, which provides quick access to  information based on user queries. It delivers clear and concise responses by searching through documents like JO 711",Acquisition and/or Development,Neither,5/20/2024,6/1/2024,Unknown,No contract/external resources used in development.,Unknown,No,No,No,Unknown,No,"Currently, we do not have access to an enterprise data catalog or agency-wide data repository. Although we attempted to contact those involved with the Dynamic Regulatory System (DRS) project, which is intended to be the central repository for FAA Orders,",Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6666666666666666,"The intended purpose of the AI chatbot is to streamline the retrieval of information from multiple FAA Orders and change notices. The expected benefits include saving time, improving accuracy in finding specific citations, and reducing manual effort for u . The chatbot outputs specific citations and relevant sections from multiple FAA Orders and change notices, which provides quick access to  information based on user queries. It delivers clear and concise responses by searching through documents like JO 711 . Not Applicable","the intended purpose of the ai chatbot is to streamline the retrieval of information from multiple faa orders and change notices. the expected benefits include saving time, improving accuracy in finding specific citations, and reducing manual effort for u . the chatbot outputs specific citations and relevant sections from multiple faa orders and change notices, which provides quick access to information based on user queries. it delivers clear and concise responses by searching through documents like jo 711 . not applicable"
Remote Maintenance Monitoring (RMM) Analyzer Copilot,Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,"The intended purpose of the AI is to serve as a ""Copilot"" for technical operations, allowing users to interact with complex data in plain language. This Copilot will retrieve, interpret, and present relevant information from the Instrument Landing Systems","""Enhanced Accessibility: Users can pose questions in plain language, bypassing the need for technical know-how on data filtering or analysis, making data insights available to a broader range of personnel.
Time Efficiency: By quickly retrieving relevant d",Initiated,Neither,10/30/2024,Unknown,Unknown,Unknown,Unknown,No,No,No,Unknown,No,Unknown,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6349206349206349,"The intended purpose of the AI is to serve as a ""Copilot"" for technical operations, allowing users to interact with complex data in plain language. This Copilot will retrieve, interpret, and present relevant information from the Instrument Landing Systems . ""Enhanced Accessibility: Users can pose questions in plain language, bypassing the need for technical know-how on data filtering or analysis, making data insights available to a broader range of personnel.
Time Efficiency: By quickly retrieving relevant d . Not Applicable","the intended purpose of the ai is to serve as a ""copilot"" for technical operations, allowing users to interact with complex data in plain language. this copilot will retrieve, interpret, and present relevant information from the instrument landing systems . ""enhanced accessibility: users can pose questions in plain language, bypassing the need for technical know-how on data filtering or analysis, making data insights available to a broader range of personnel. time efficiency: by quickly retrieving relevant d . not applicable"
National Airspace System (NAS) Safety Anomaly Metric with STAD integration (Safety Trend Analytics D,Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,"To measure and monitor the degree of anomaly or deviation from a baseline based on safety data in the National Air Space to the Service Area, district, and facility level.",Day look ahead forecast of anomalous/non-anomalous conditions at levels from NAS-wide down to facilities. ,Initiated,Neither,10/1/2024,Unknown,Unknown,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,No,Unknown,No,"SysOps performance data, Meteorological Terminal Aviation Routine Weather Report (METAR), Remote Monitoring and Logging System (RMLS), limited staffing data",Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6666666666666666,"To measure and monitor the degree of anomaly or deviation from a baseline based on safety data in the National Air Space to the Service Area, district, and facility level. . Day look ahead forecast of anomalous/non-anomalous conditions at levels from NAS-wide down to facilities. . Not Applicable","to measure and monitor the degree of anomaly or deviation from a baseline based on safety data in the national air space to the service area, district, and facility level. . day look ahead forecast of anomalous/non-anomalous conditions at levels from nas-wide down to facilities. . not applicable"
Using Generative AI to Automatically Tag Narratives with Human Performace Common Taxonomy Factors ,Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,"Using genAI, automatically tag text narratives from Comprehensive Electronic Data Analysis and Reporting (CEDAR) Mandatory Occurrence Reports (MOR's) with factors from Human Performance Common Taxonomy (HP CT) at scale and with significant speed up.",HP CT factors related to an MOR's narrative.,Acquisition and/or Development,Neither,8/1/2024,Unknown,Unknown,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,No,Unknown,No,"Human Performance Common Taxonomy
Shared Application for Factor Evalution (SAFE)",Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6507936507936508,"Using genAI, automatically tag text narratives from Comprehensive Electronic Data Analysis and Reporting (CEDAR) Mandatory Occurrence Reports (MOR's) with factors from Human Performance Common Taxonomy (HP CT) at scale and with significant speed up. . HP CT factors related to an MOR's narrative. . Not Applicable","using genai, automatically tag text narratives from comprehensive electronic data analysis and reporting (cedar) mandatory occurrence reports (mor's) with factors from human performance common taxonomy (hp ct) at scale and with significant speed up. . hp ct factors related to an mor's narrative. . not applicable"
Generative AI Use Case from Inventory,Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,This use case will be used increase trust in new technology approach through this socialzation exercise. And increase buy-in and education of stakeholders with a useful product from their input.,Execute a use case chosen from a collection of candidates from Safety Trend Analytics Dashboard (STAD) stakeholders. ,Initiated,Neither,10/1/2024,Unknown,Unknown,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,No,Unknown,No,Unknown,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6507936507936508,This use case will be used increase trust in new technology approach through this socialzation exercise. And increase buy-in and education of stakeholders with a useful product from their input. . Execute a use case chosen from a collection of candidates from Safety Trend Analytics Dashboard (STAD) stakeholders. . Not Applicable,this use case will be used increase trust in new technology approach through this socialzation exercise. and increase buy-in and education of stakeholders with a useful product from their input. . execute a use case chosen from a collection of candidates from safety trend analytics dashboard (stad) stakeholders. . not applicable
Human Performance (HP) Fatigue Recommendations,Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,This use case will be used to develop and understanding of HP Fatigue recommendations and ties to safety risk.,To be determined,Initiated,Neither,11/18/2024,Unknown,Unknown,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,Yes,Yes,No,Administrator's fatigue recommendations (FY24). Safety data,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6825396825396826,This use case will be used to develop and understanding of HP Fatigue recommendations and ties to safety risk. . To be determined . Not Applicable,this use case will be used to develop and understanding of hp fatigue recommendations and ties to safety risk. . to be determined . not applicable
Prognosis and Diagnosis Based on Contributing Factors to Safety Risk,Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,To increase expainability and interpretability of changes in safety risk based on contributing factors from and LLM tuned to Human Factors Common Taxonomy.,What are the Human Factors Common Taxonomy factors that are associated with a change in safety risk in the NAS.,Initiated,Neither,Unknown,Unknown,Unknown,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,No,Unknown,No,Human Performance Common Taxonomy as collected with the Shared Application for Factor Evaluation (SAFE) tool ,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6507936507936508,To increase expainability and interpretability of changes in safety risk based on contributing factors from and LLM tuned to Human Factors Common Taxonomy. . What are the Human Factors Common Taxonomy factors that are associated with a change in safety risk in the NAS. . Not Applicable,to increase expainability and interpretability of changes in safety risk based on contributing factors from and llm tuned to human factors common taxonomy. . what are the human factors common taxonomy factors that are associated with a change in safety risk in the nas. . not applicable
Knowledge Representation for Deep Learning,Department of Transportation,DOT,FAA AJO,AI used in Transportation Operations,None of the Above,"The overall purpose of the effort is to establish Knowledge Graph usage to develop Deep Learning, starting with a specific problem statement. The problem statement is to ""Enhance the management and resolution of deviation events by providing actionable in","1. Diagnostic Insights
-	Event Analysis: Contributing factors to a deviation, with root cause identification based on historical and real-time data.
-	Priority Assessment: Highlight of critical issues requiring immediate attention, ranked by severity and ",Initiated,Neither,6/3/2024,Unknown,Unknown,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,No,Unknown,No,"- Historical Deviation Logs: Comprehensive records of past Code 80 and related sub-codes events, detailing the conditions, actions taken, and resolutions. These logs help train and evaluate the model's ability to identify patterns and predict future devia",Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6666666666666666,"The overall purpose of the effort is to establish Knowledge Graph usage to develop Deep Learning, starting with a specific problem statement. The problem statement is to ""Enhance the management and resolution of deviation events by providing actionable in . 1. Diagnostic Insights
-	Event Analysis: Contributing factors to a deviation, with root cause identification based on historical and real-time data.
-	Priority Assessment: Highlight of critical issues requiring immediate attention, ranked by severity and . Not Applicable","the overall purpose of the effort is to establish knowledge graph usage to develop deep learning, starting with a specific problem statement. the problem statement is to ""enhance the management and resolution of deviation events by providing actionable in . 1. diagnostic insights - event analysis: contributing factors to a deviation, with root cause identification based on historical and real-time data. - priority assessment: highlight of critical issues requiring immediate attention, ranked by severity and . not applicable"
Identification of Similar Continued Operational Safety (COS) Events,Department of Transportation,DOT,FAA AVS,AI used in Transportation Operations,None of the Above,"AIR is utilizing AI within Palantir Foundry (i.e., Foundry's ""Artificial Intelligence Platform"" (AIP)) to search, find and link similar types of events (e.g., Continued Operational Safety (COS) event reports, Notices of Noncompliance (NCN), semantic searc",The output aggregates information from disparate sources to provide an integrated view of events that are similar.,Operation and Maintenance,Neither,1/2/2024,1/2/2024,6/1/2024,Exclusively developed with contract/external resources.,Unknown,No,No,No,Unknown,No,Unknown,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6666666666666666,"AIR is utilizing AI within Palantir Foundry (i.e., Foundry's ""Artificial Intelligence Platform"" (AIP)) to search, find and link similar types of events (e.g., Continued Operational Safety (COS) event reports, Notices of Noncompliance (NCN), semantic searc . The output aggregates information from disparate sources to provide an integrated view of events that are similar. . Not Applicable","air is utilizing ai within palantir foundry (i.e., foundry's ""artificial intelligence platform"" (aip)) to search, find and link similar types of events (e.g., continued operational safety (cos) event reports, notices of noncompliance (ncn), semantic searc . the output aggregates information from disparate sources to provide an integrated view of events that are similar. . not applicable"
Flight Standards Inspector Readiness Program (FSIRP),Department of Transportation,DOT,FAA AVS,AI used in Transportation Operations,None of the Above,This use case will help automate current manual processes and reduce tasks.,,Initiated,Neither,1/2/2024,Unknown,Unknown,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,Yes,Yes,No,Existing FSIRP database,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6666666666666666,This use case will help automate current manual processes and reduce tasks. . Not Applicable,this use case will help automate current manual processes and reduce tasks. . not applicable
Court Reporting and Testimony Management Platform ,Department of Transportation,DOT,FAA AGC,AI used in Transportation Operations,None of the Above,"This technology will provide solutions for remote, hybrid, and in-person proceedings that will include both transcription and translation services.  ","The FAA Office of General Counsel (AGC) personnel would receive detailed summaries for both rough and certified transcripts from a variety of proceeding depositions, arbitrations, expert interviews, meetings, and more.  In addition, such a product would p",Ideation,Neither,Unknown,Unknown,Unknown,Exclusively developed with contract/external resources.,Unknown,No,No,No,Unknown,No,Unknown,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6349206349206349,"This technology will provide solutions for remote, hybrid, and in-person proceedings that will include both transcription and translation services. . The FAA Office of General Counsel (AGC) personnel would receive detailed summaries for both rough and certified transcripts from a variety of proceeding depositions, arbitrations, expert interviews, meetings, and more.  In addition, such a product would p . Not Applicable","this technology will provide solutions for remote, hybrid, and in-person proceedings that will include both transcription and translation services. . the faa office of general counsel (agc) personnel would receive detailed summaries for both rough and certified transcripts from a variety of proceeding depositions, arbitrations, expert interviews, meetings, and more. in addition, such a product would p . not applicable"
AI-Assisted Legal Research,Department of Transportation,DOT,FAA AGC,AI used in Transportation Operations,None of the Above,"The AI-Assisted Research will return a summarized overview or draft with detailed insights from top results, along with a list of key cases statutes and regulations.","The ability to type a question, get an answer, and have all the supporting resources right underneath that answer.  Additionally, the AI-Assisted Research answer or draft generated is supported by case law that is already within the database.",Ideation,Neither,Unknown,Unknown,Unknown,Exclusively developed with contract/external resources.,Unknown,No,No,No,Unknown,No,Unknown,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6349206349206349,"The AI-Assisted Research will return a summarized overview or draft with detailed insights from top results, along with a list of key cases statutes and regulations. . The ability to type a question, get an answer, and have all the supporting resources right underneath that answer.  Additionally, the AI-Assisted Research answer or draft generated is supported by case law that is already within the database. . Not Applicable","the ai-assisted research will return a summarized overview or draft with detailed insights from top results, along with a list of key cases statutes and regulations. . the ability to type a question, get an answer, and have all the supporting resources right underneath that answer. additionally, the ai-assisted research answer or draft generated is supported by case law that is already within the database. . not applicable"
Service Difficulty Reporting System (SDRS) Joint Aircraft System/Component Code (JASC) Code Picker,Department of Transportation,DOT,FAA AVS,AI used in Transportation Operations,None of the Above,This use case will leverage AI to assist in sorting and assigning proper JASC codes within the SDRS environement as an addition into the next development cycle for SDRS. ,Modified original input of JASC codes to ensure correct and consistent JASC Codes within the system,Initiated,Neither,11/25/2024,Unknown,Unknown,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,No,Unknown,No,Current SDRS database available in the Enterprise Information Management Data Platform (EDP) ,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6666666666666666,This use case will leverage AI to assist in sorting and assigning proper JASC codes within the SDRS environement as an addition into the next development cycle for SDRS. . Modified original input of JASC codes to ensure correct and consistent JASC Codes within the system . Not Applicable,this use case will leverage ai to assist in sorting and assigning proper jasc codes within the sdrs environement as an addition into the next development cycle for sdrs. . modified original input of jasc codes to ensure correct and consistent jasc codes within the system . not applicable
Case and Document Management Copilot,Department of Transportation,DOT,FAA AGC,AI used in Transportation Operations,None of the Above,"The Copilot would be natively embedded across an application to deliver a consistent user experience that can answer questions, generate content, and dynamically automate any action.","The copilot includes a library of actions, which are pre-programmed capabilities that enable the copilot to not only answer questions using business data, but also string together workflows to get things done in support of users.",Ideation,Neither,Unknown,Unknown,Unknown,Exclusively developed with contract/external resources.,Unknown,No,No,No,Unknown,No,Legal and operational data in the Case and Document Management System or FAA intranet. ,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6507936507936508,"The Copilot would be natively embedded across an application to deliver a consistent user experience that can answer questions, generate content, and dynamically automate any action. . The copilot includes a library of actions, which are pre-programmed capabilities that enable the copilot to not only answer questions using business data, but also string together workflows to get things done in support of users. . Not Applicable","the copilot would be natively embedded across an application to deliver a consistent user experience that can answer questions, generate content, and dynamically automate any action. . the copilot includes a library of actions, which are pre-programmed capabilities that enable the copilot to not only answer questions using business data, but also string together workflows to get things done in support of users. . not applicable"
AI Freedom of Information Act (FOIA) Request Classification,Department of Transportation,DOT,FAA AFN,AI used in Transportation Operations,None of the Above,"Will strengthen the Intake and Assignment Branch's precision of FOIA request assignments at the first take, freeing up time to actively engage with requesters and customers alike.  Overall, these FOIA tools will allow for a more informed decision-making p",The output will provide guidance for the FOIA Xpress end user in determining the routing assigment to the approprioate LOB/Organization.  ,Acquisition and/or Development,Neither,9/16/2024,10/1/2024,Unknown,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,No,Unknown,No,Order 1100.1C,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6666666666666666,"Will strengthen the Intake and Assignment Branch's precision of FOIA request assignments at the first take, freeing up time to actively engage with requesters and customers alike.  Overall, these FOIA tools will allow for a more informed decision-making p . The output will provide guidance for the FOIA Xpress end user in determining the routing assigment to the approprioate LOB/Organization. . Not Applicable","will strengthen the intake and assignment branch's precision of foia request assignments at the first take, freeing up time to actively engage with requesters and customers alike. overall, these foia tools will allow for a more informed decision-making p . the output will provide guidance for the foia xpress end user in determining the routing assigment to the approprioate lob/organization. . not applicable"
Community Engagement Chat Bot,Department of Transportation,DOT,FAA APL,AI used in Transportation Operations,None of the Above,Assisting the public in finding information on the FAA website. ,We are using our AI Chat Bot to better inform the public. In our efforts to engage with the public we wanted to use this tool to direct the public to the tremendous amount of information that is on the FAA Website.  ,Operation and Maintenance,Neither,6/8/2021,Unknown,11/14/2022,Developed with a combination of in-house and contract/external resources.,Unknown,No,No,No,Unknown,No,We are constantly working to refine the Chat Bot responses and help it pull the best information that is available on the FAA Website to provide value to the public,Documentation has been partially complete.,No,"Yes – agency has access to source code, but it is not public.",Yes,Withheld - Cybersecurity,6-12 months,Yes,Yes,No,Unknown,Limited documentation for review.,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Not Applicable,Neither,0.6666666666666666,Assisting the public in finding information on the FAA website. . We are using our AI Chat Bot to better inform the public. In our efforts to engage with the public we wanted to use this tool to direct the public to the tremendous amount of information that is on the FAA Website. . Not Applicable,assisting the public in finding information on the faa website. . we are using our ai chat bot to better inform the public. in our efforts to engage with the public we wanted to use this tool to direct the public to the tremendous amount of information that is on the faa website. . not applicable
Machine Learning Data Validation,National Credit Union Administration,NCUA,NCUA,Mission-Enabling,None of the Above,Improve Call Report data quality ,Provide lists of potential data outliers for each credit union,Operation and Maintenance,Neither,2/1/2023,10/21/2020,2/1/2023,Developed in-house.,Unknown,No,No,No,Yes,Yes,NCUA Quarterly Call Report Data.,Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,More than 12 months,Yes,No,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,Improve Call Report data quality . Provide lists of potential data outliers for each credit union,improve call report data quality . provide lists of potential data outliers for each credit union
Supervisory Stress Testing,National Credit Union Administration,NCUA,NCUA,Mission-Enabling,None of the Above,Estimate loan default probability for Supervisory Stress Testing.,Analyzes credit union data to predict cash flows under economic stress scenarios,Operation and Maintenance,Neither,3/27/2023,3/27/2023,5/31/2023,Developed in-house.,Unknown,No,No,No,No,Yes,Vendor sourced. Use some NCUA credit union and Call Report Data.,Documentation is complete,Yes,"Yes – agency has access to source code, but it is not public.",No,Unknown,Less than 6 months,Yes,Yes,Yes,Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.,"Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.",No – Agency did not request an extension for this use case.,Unknown,,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Neither,0.5238095238095238,Estimate loan default probability for Supervisory Stress Testing. . Analyzes credit union data to predict cash flows under economic stress scenarios,estimate loan default probability for supervisory stress testing. . analyzes credit union data to predict cash flows under economic stress scenarios
